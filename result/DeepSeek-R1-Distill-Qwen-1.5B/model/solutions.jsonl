{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    n = X_train.shape[0]\n    m = X_test.shape[0]\n    d = X_train.shape[1]\n    X_train_minus_z = X_train - X_test[:, None, None]\n    squared = np.sum(X_train_minus_z ** 2, axis=1)\n    exponent = -gamma * squared\n    term = alpha * y_train * np.exp(exponent)\n    sum_term = np.sum(term, axis=1)\n    predicted = np.sign(sum_term)\n    return predicted.astype(int)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape, sparsity=0.5, seed=None):\n    if sparsity < 0 or sparsity > 1:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_numbers = np.random.random(shape)\n    threshold = 1 - sparsity\n    binary = (random_numbers >= threshold).astype(float)\n    return binary"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    if not isinstance(x, np.ndarray):\n        raise AssertionError('x must be a numpy array.')\n    if frame_width <= 0:\n        raise AssertionError('frame_width must be positive.')\n    if stride <= 0:\n        raise AssertionError('stride must be positive.')\n    if len(x) < frame_width:\n        raise AssertionError('len(x) must be >= frame_width.')\n    if writeable is not False and (not isinstance(x, np.ndarray)):\n        raise AssertionError('writeable must be False if x is not a numpy array.')\n    n_frames = (len(x) - frame_width) // stride + 1\n    new_array = as_strided(x, (n_frames, frame_width), (0, frame_width), writeable=writeable)\n    return new_array"}
{"task_id": 48, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": "def backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    N = len(A[0])\n    T = len(obs)\n    if t == T - 1:\n        return [round(x, 4) for x in [1.0] * N]\n    beta = [1.0] * T\n    for i in range(T - 2, t - 1, -1):\n        for j in range(N):\n            beta[i] += A[i][j] * B[j][obs[t + 1]] * beta[j + 1]\n    return [round(x, 4) for x in beta[t]]"}
{"task_id": 65, "completion_id": 0, "solution": "def backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    if len(A) == 0 or len(B) == 0 or len(pi) == 0 or (len(obs) == 0):\n        return 0.0\n    N = len(pi)\n    T = len(obs)\n    if T == 0:\n        return 0.0\n    beta = [[0.0 for _ in range(N)] for _ in range(T)]\n    for t in range(T - 1, -1, -1):\n        if t == T - 1:\n            for i in range(N):\n                beta[t][i] = 1.0\n        else:\n            for i in range(N):\n                sum_val = 0.0\n                for j in range(N):\n                    sum_val += A[i][j] * B[j][obs[t + 1]] * beta[t + 1][j]\n                beta[t][i] = sum_val\n    total = 0.0\n    for i in range(N):\n        total += pi[i] * B[i][obs[0]] * beta[0][i]\n    return round(total, 6)"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    epsilon = np.finfo(np.float64).eps\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)"}
{"task_id": 77, "completion_id": 0, "solution": "import numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n    \n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimal places and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    A = X\n    caches = []\n    L = len(parameters) - 1\n    for i in range(L):\n        W = parameters[f'W{i + 1}']\n        b = parameters[f'b{i + 1}']\n        Z = np.dot(W, A) + b\n        if i < L - 1:\n            A = np.maximum(0, Z)\n        else:\n            A = 1 / (1 + np.exp(-Z))\n        caches.append((Z, A))\n        A = A\n    AL = A\n    AL = [round(x, 4) for x in AL]\n    return AL"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    epsilon = 1e-15\n    m = Y.shape[0]\n    A2 = np.clip(A2, epsilon, 1 - epsilon)\n    cost = 1.0 / m * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n    return round(cost, 6)"}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    result = []\n    for col in zip(*predictions):\n        column = list(col)\n        counts = Counter(column)\n        max_count = max(counts.values())\n        candidates = [k for (k, v) in counts.items() if v == max_count]\n        if len(candidates) == 1:\n            result.append(candidates[0])\n        else:\n            candidates_sorted = sorted(candidates)\n            result.append(candidates_sorted[0])\n    return result"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nTIME_STEPS = 20\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    text_list = list(text)\n    encoded = []\n    for c in text_list:\n        if c in vocabulary:\n            encoded.append(vocabulary[c])\n        else:\n            encoded.append(0)\n    if len(encoded) > time_steps:\n        encoded = encoded[:time_steps]\n    else:\n        encoded += [0] * (time_steps - len(encoded))\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    result = []\n    for id in indices:\n        if id == 0:\n            continue\n        result.append(inverse_vocab[id])\n    return ''.join(result)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    batch = np.array([encoded])\n    predictions = model.predict(batch)\n    predicted_ids = np.argmax(predictions, axis=-1)\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    global EXAMPLES\n    if examples is None:\n        examples = EXAMPLES\n    outputs = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        outputs.append(''.join(output_chars))\n    return outputs"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    m = X.shape[0]\n    z = X @ w\n    p = 1 / (1 + np.exp(-z))\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    loss = -y * np.log(p) - (1 - y) * np.log(1 - p)\n    loss_rounded = round(loss, 4)\n    gradient = X.T @ (p - y) / m\n    gradient_rounded = round(gradient, 4)\n    gradient_list = list(gradient_rounded)\n    return (loss_rounded, gradient_list)"}
{"task_id": 128, "completion_id": 0, "solution": ""}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    W1 = weights['W1']\n    b1 = weights['b1']\n    W2 = weights['W2']\n    b2 = weights['b2']\n    W3 = weights['W3']\n    b3 = weights['b3']\n    x = np.array(state)\n    x = np.dot(W1, x) + b1\n    x = np.maximum(0, x)\n    x = np.dot(W2, x) + b2\n    x = np.maximum(0, x)\n    x = np.dot(W3, x) + b3\n    x = np.tanh(x)\n    action = [round(num, 4) for num in x]\n    return action"}
{"task_id": 198, "completion_id": 0, "solution": "import numpy as np\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    T = len(phi[0][0])\n    beta = np.zeros((V, T))\n    for d in range(len(phi)):\n        for n in range(len(phi[d])):\n            v = corpus[d][n]\n            for t in range(T):\n                beta[v, t] += phi[d][n][t]\n    for t in range(T):\n        column = beta[:, t]\n        column_sum = np.sum(column)\n        if column_sum == 0:\n            continue\n        beta[:, t] /= column_sum\n    for v in range(V):\n        for t in range(T):\n            beta[v, t] = round(beta[v, t], 4)\n    return beta"}
{"task_id": 217, "completion_id": 0, "solution": "import math"}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n\n    def process_array(x, a):\n        return np.array([process_element(e, a) for e in x])\n\n    def process_element(x, a):\n        if isinstance(x, (list, np.ndarray)):\n            return [process_element(e, a) for e in x]\n        elif x >= 0:\n            return x\n        else:\n            return a * x\n\n    def process_list(x, a):\n        return [process_element(e, a) for e in x]\n    if isinstance(z, np.ndarray):\n        return process_array(z, a)\n    else:\n        return process_list(z, a)"}
{"task_id": 261, "completion_id": 0, "solution": "import numpy as np\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    in_channels = shape[0]\n    out_channels = shape[1]\n    kernel_size = 1\n    for dim in shape[2:]:\n        kernel_size *= dim\n    fan_in = in_channels * kernel_size\n    fan_out = out_channels * kernel_size\n    s = np.sqrt(2.0 / (fan_in + fan_out))\n    return np.random.normal(0.0, s, shape)"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int): Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf):  Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object\n    right: object\ndef compare_trees(tree_a, tree_b):\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, rtol=1e-08, atol=1e-08)\n    elif (isinstance(tree_a, Leaf) or isinstance(tree_b, Leaf)) and (not (isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf))):\n        return False\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, rtol=1e-08, atol=1e-08):\n            return False\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    else:\n        return False"}
{"task_id": 308, "completion_id": 0, "solution": "import numpy as np\ndef mel_filterbank(N: int, n_filters: int=20, fs: int=44000, min_freq: int=0, max_freq: int | None=None, normalize: bool=True) -> list[list[float]]:\n    n = n_filters\n    lower_mel = min_freq\n    if max_freq is None:\n        upper_mel = fs / 2\n    else:\n        upper_mel = max_freq\n    n_points = n + 2\n    step = (upper_mel - lower_mel) / (n_points - 1)\n    mel_points = [lower_mel + i * step for i in range(n_points)]\n    filter_corners = [700 * (10 ** (m / 2595) - 1) for m in mel_points]\n    filter_edges = []\n    for i in range(n):\n        m_i = filter_corners[i]\n        m_i_plus_1 = filter_corners[i + 1]\n        filter_edges.append((m_i, m_i_plus_1))\n    filterbank = []\n    for k in range(N // 2 + 1):\n        f_center = 700 * (10 ** ((k + 0.5) * fs / N / 2595) - 1)\n        weight = 0.0\n        for i in range(n):\n            (m_i, m_i_plus_1) = filter_edges[i]\n            m_center = np.log10(1 + f_center / 700) * 2595\n            weight = abs((m_center - m_i) * (m_i_plus_1 - m_center) / (m_i_plus_1 - m_i))\n        filterbank.append([weight])\n    rounded = [[round(x, 4) for x in row] for row in filterbank]\n    return rounded"}
{"task_id": 313, "completion_id": 0, "solution": "import math"}
{"task_id": 317, "completion_id": 0, "solution": ""}
{"task_id": 343, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_update(theta_samples: np.ndarray, rewards: list[float] | np.ndarray, retain_prcnt: float) -> tuple[list[float], list[float]]:\n    max_reward = np.max(rewards)\n    samples = list(zip(rewards, theta_samples))\n    sorted_samples = sorted(samples, key=lambda x: -x[0])\n    k = int(retain_prcnt * len(theta_samples))\n    selected_theta = [s[1] for s in sorted_samples[:k]]\n    D = len(selected_theta[0])\n    means = []\n    for d in range(D):\n        d_values = [s[d] for s in selected_theta]\n        mean = np.mean(d_values)\n        means.append(mean)\n    variances = []\n    for d in range(D):\n        mean_d = means[d]\n        squared_diff = [(x - mean_d) ** 2 for x in selected_theta[:k][d]]\n        var = np.mean(squared_diff)\n        variances.append(var)\n    means_rounded = [round(m, 4) for m in means]\n    variances_rounded = [round(v, 4) for v in variances]\n    return (means_rounded, variances_rounded)"}
{"task_id": 353, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nimport numpy as np\ndef entropy(labels) -> float:\n    labels = list(labels)\n    if not labels:\n        return 0.0\n    counts = Counter(labels)\n    unique_labels = list(counts.keys())\n    total = len(labels)\n    probabilities = {label: count / total for (label, count) in counts.items()}\n    entropy_value = 0.0\n    for (label, p) in probabilities.items():\n        if p == 0:\n            continue\n        entropy_value += -p * math.log2(p)\n    return round(entropy_value, 5)"}
{"task_id": 356, "completion_id": 0, "solution": "import numpy as np\ndef leaf_predict(leaf: 'Leaf', classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A Leaf object whose `value` attribute is either a sequence of\n               class probabilities (classification) or a single number\n               (regression).\n        classifier: When `True` treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        if isinstance(leaf.value, list):\n            return np.argmax(leaf.value)\n        else:\n            return leaf.value\n    else:\n        return leaf.value"}
{"task_id": 369, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    running_pred = np.zeros_like(updates[0])\n    for u in updates:\n        running_pred -= learning_rate * u\n    if regression:\n        return running_pred.round(4).flatten()\n    else:\n        exps = np.exp(running_pred)\n        sum_exps = np.sum(exps, axis=1, keepdims=True)\n        probs = exps / sum_exps\n        return probs.argmax(axis=1).flatten()"}
{"task_id": 373, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef gini(y: list[int] | 'np.ndarray') -> float:\n    if not y:\n        return 0.0\n    counts = Counter(y)\n    N = len(y)\n    total = 0.0\n    for c in counts:\n        p = counts[c] / N\n        total += p ** 2\n    gini_impurity = 1.0 - total\n    return round(gini_impurity, 4)"}
{"task_id": 376, "completion_id": 0, "solution": "import numpy as np\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    if len(X) == 0 or len(X[0]) == 0:\n        return False\n    n = len(X)\n    if n != len(X[0]):\n        return False\n    X_transpose = np.transpose(X)\n    return np.allclose(X, X_transpose, rtol=1e-08, atol=1e-08)"}
{"task_id": 377, "completion_id": 0, "solution": "import numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    for yi in y:\n        if yi not in (0, 1):\n            return -1\n    g = []\n    for (yi, fi) in zip(y, f):\n        y_prime = 2 * yi - 1\n        denominator = 1 + np.exp(y_prime * fi)\n        g_i = y_prime / denominator\n        g.append(g_i)\n    return [round(g_i, 4) for g_i in g]"}
{"task_id": 394, "completion_id": 0, "solution": "import numpy as np\ndef rmsprop_update(w: np.ndarray, grad: np.ndarray, Eg: np.ndarray | None=None, learning_rate: float=0.01, rho: float=0.9) -> tuple[list, list]:\n    if Eg is None:\n        Eg = np.zeros_like(w)\n    else:\n        new_Eg = rho * Eg + (1 - rho) * grad ** 2\n    denominator = np.sqrt(new_Eg + 1e-08)\n    w_new = w - learning_rate * grad / denominator\n    w_rounded = list(np.round(w_new, 4))\n    eg_rounded = list(np.round(new_Eg, 4))\n    return (w_rounded, eg_rounded)"}
{"task_id": 411, "completion_id": 0, "solution": ""}
{"task_id": 413, "completion_id": 0, "solution": ""}
{"task_id": 416, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"Compute the multivariate Gaussian PDF for each data point in X.\"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    d = X.shape[1]\n    x_diff = X - mean\n    inv_cov = np.linalg.inv(cov)\n    exponent = -0.5 * (x_diff.T @ inv_cov @ x_diff)\n    det_cov = np.linalg.det(cov)\n    denominator = np.sqrt((2 * math.pi) ** d * det_cov)\n    pdf = 1.0 / denominator * math.exp(exponent)\n    return [round(p, 4) for p in pdf]"}
{"task_id": 423, "completion_id": 0, "solution": "import numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    if len(actual) != len(predicted):\n        return -1\n    if not (np.all(actual >= 0) and np.all(predicted >= 0)):\n        return -1\n    diff = np.log(1 + predicted) - np.log(1 + actual)\n    squared_diff = diff ** 2\n    mean = np.mean(squared_diff)\n    rmsle = np.sqrt(mean)\n    return round(rmsle, 4)"}
{"task_id": 428, "completion_id": 0, "solution": ""}
{"task_id": 452, "completion_id": 0, "solution": ""}
{"task_id": 471, "completion_id": 0, "solution": "import numpy as np\ndef expected_sarsa_update(q_table: list[list[float]], state: int, action: int, reward: float, next_state: int | None, epsilon: float, lr: float, gamma: float) -> list[list[float]]:\n    new_q = [row.copy() for row in q_table]\n    if next_state is None:\n        target = reward\n    else:\n        next_row = new_q[next_state]\n        max_q = max(next_row)\n        greedy_action = next_row.index(max_q)\n        num_actions = len(next_row)\n        expected_q = 0.0\n        for a in range(num_actions):\n            if a == greedy_action:\n                prob = (1 - epsilon) / num_actions\n            else:\n                prob = epsilon / num_actions\n            expected_q += prob * next_row[a]\n        target = reward + gamma * expected_q\n    new_q[state][action] += lr * (target - new_q[state][action])\n    for row in new_q:\n        for val in row:\n            row[0] = round(val, 4)\n    return new_q"}
{"task_id": 475, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-08):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to `w`.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size \u03b1.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        (w_new, m_new, v_new) where:\n        * w_new \u2013 updated parameters (same shape as `w`)\n        * m_new \u2013 updated first moment\n        * v_new \u2013 updated second moment\n    \"\"\"\n    if m_prev is None:\n        m_prev = np.zeros_like(w)\n    if v_prev is None:\n        v_prev = np.zeros_like(w)\n    m_t = b1 * m_prev + (1 - b1) * grad\n    v_t = b2 * v_prev + (1 - b2) * grad ** 2\n    m_new = m_t / (1 - b1 ** t)\n    v_new = v_t / (1 - b2 ** t)\n    w_new = w - learning_rate * m_new / (np.sqrt(v_new) + eps)\n    return (w_new, m_new, v_new)"}
{"task_id": 479, "completion_id": 0, "solution": "import numpy as np\ndef epsilon_greedy_bandit(rewards: List[List[float]], epsilon: float=0.05, ev_prior: float=0.5, seed: Optional[int]=None) -> List[float]:\n    if seed is not None:\n        np.random.seed(seed)\n    N = len(rewards[0])\n    estimates = [ev_prior] * N\n    pulls = [0] * N\n    for t in range(len(rewards)):\n        if np.random.random() < epsilon:\n            arm = np.random.choice(N)\n        else:\n            max_index = np.argmax(estimates)\n            arm = max_index\n        reward = rewards[t][arm]\n        pulls[arm] += 1\n        estimates[arm] = estimates[arm] + (reward - estimates[arm]) / pulls[arm]\n    return [round(x, 4) for x in estimates]"}
{"task_id": 482, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype=int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    if not y:\n        return np.array([], dtype=int)\n    if num_classes is None:\n        num_classes = max(y) + 1\n    else:\n        if num_classes < 1:\n            raise ValueError('num_classes must be at least 1')\n        for label in y:\n            if label < 0 or label >= num_classes:\n                raise ValueError('Invalid label')\n    mask = np.zeros((len(y), num_classes), dtype=int)\n    mask[np.arange(len(y)), y] = 1\n    return mask"}
{"task_id": 490, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    arr = list(x)\n    n = len(arr)\n    if n == 0:\n        return []\n    matrix = np.zeros((n, n), dtype=object)\n    for i in range(n):\n        matrix[i][i] = arr[i]\n    return matrix.tolist()"}
{"task_id": 491, "completion_id": 0, "solution": ""}
{"task_id": 493, "completion_id": 0, "solution": "import numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n    \n    The MSE is the average of squared differences between each element\n    and the mean of those elements. If the input sequence is empty,\n    the function should return -1.\n    \n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n    \n    Returns:\n        float: Mean-squared error rounded to four decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    if len(y) == 0:\n        return -1\n    n = len(y)\n    mean = np.mean(y)\n    squared_diffs = (y - mean) ** 2\n    mse_value = np.mean(squared_diffs)\n    return round(mse_value, 4)"}
{"task_id": 509, "completion_id": 0, "solution": "import numpy as np\ndef chebyshev(x: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray') -> float:\n    if len(x) != len(y):\n        return -1\n    x_np = np.array(x)\n    y_np = np.array(y)\n    diffs = np.abs(x_np - y_np)\n    max_diff = np.max(diffs)\n    return round(max_diff, 4)"}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n            list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent factors that\n            model pair-wise feature interactions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    n_samples = len(X)\n    n_features = len(w)\n    k = len(V[0])\n    linear = np.dot(w, X.T)\n    linear_sum = np.sum(linear, axis=1)\n    a = np.sum(V * X, axis=1)\n    b = np.sum(V ** 2 * X ** 2, axis=1)\n    interaction = 0.5 * np.sum(a ** 2 - b, axis=1)\n    predictions = linear_sum + interaction\n    predictions = [round(p, 4) for p in predictions]\n    return predictions"}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\"\"\"\n\n    def decorator(func):\n\n        def wrapper(actual, predicted):\n            if isinstance(actual, np.ndarray) and len(actual.shape) == 2 and (actual.shape[1] > 1):\n                processed_actual = actual.argmax(axis=1)\n            else:\n                processed_actual = actual\n            if isinstance(predicted, np.ndarray) and len(predicted.shape) == 2 and (predicted.shape[1] > 1):\n                processed_predicted = predicted.argmax(axis=1)\n            else:\n                processed_predicted = predicted\n            return func(processed_actual, processed_predicted)\n        return wrapper\n    return decorator"}
{"task_id": 528, "completion_id": 0, "solution": "import numpy as np\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int=100) -> list[list[int]]:\n    x1_min = min(X[:, 0])\n    x1_max = max(X[:, 0])\n    x2_min = min(X[:, 1])\n    x2_max = max(X[:, 1])\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    x1_values = x1_plot\n    x2_values = x2_plot\n    (x1_grid, x2_grid) = np.meshgrid(x1_values, x2_values)\n    linear = W[0] * x1_grid + W[1] * x2_grid + b\n    predictions = (linear >= 0).astype(int)\n    return [row for row in predictions]"}
{"task_id": 539, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    epsilon = 1e-15\n    m = len(Y)\n    clipped_AL = np.clip(AL, epsilon, 1 - epsilon)\n    y = Y\n    a = clipped_AL\n    log_a = np.log(a)\n    log_1_minus_a = np.log(1 - a)\n    term = y * log_a + (1 - y) * log_1_minus_a\n    sum_term = np.sum(term)\n    cost = -sum_term / m\n    return cost"}
{"task_id": 552, "completion_id": 0, "solution": "def gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm and return the resulting\n    vector rounded to four decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A', and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time index for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    import numpy as np\n    pi = hmm['pi']\n    A = hmm['A']\n    B = hmm['B']\n    N = len(pi)\n    T = len(observations)\n    if t < 0 or t >= T:\n        return []\n    alpha = []\n    for t in range(T):\n        alpha.append([0.0] * N)\n    o0 = observations[0]\n    alpha[0] = [pi[i] * B[i][o0] for i in range(N)]\n    for t in range(1, T):\n        o_t = observations[t]\n        current = [0.0] * N\n        for i in range(N):\n            for j in range(N):\n                current[i] += alpha[t - 1][j] * A[j][i] * B[i][o_t]\n        alpha[t] = current\n    beta = []\n    for t in range(T):\n        beta.append([0.0] * N)\n    o_T_minus_1 = observations[-1]\n    beta[T - 1] = [1.0] * N\n    for t in range(T - 2, -1, -1):\n        o_t_plus_1 = observations[t + 1]\n        current = [0.0] * N\n        for i in range(N):\n            for j in range(N):\n                current[i] += A[i][j] * B[j][o_t_plus_1] * beta[t + 1][j]\n        beta[t] = current\n    gamma = []\n    for i in range(N):\n        numerator = alpha[t][i] * beta[t][i]\n        denominator = sum((alpha[t][j] * beta[t][j] for j in range(N)))\n        gamma_i = numerator / denominator\n        gamma.append(round(gamma_i, 4))\n    return gamma"}
{"task_id": 565, "completion_id": 0, "solution": "import numpy as np\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]:\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    y_counts = np.bincount(y)\n    total = len(y)\n    if total == 0:\n        H_y = 0.0\n    else:\n        p = y_counts / total\n        H_y = -np.sum(p * np.log2(p))\n    gains = []\n    for feature in range(X.shape[1]):\n        unique_values = np.unique(X[:, feature])\n        H_y_given_A = 0.0\n        for a in unique_values:\n            mask = X[:, feature] == a\n            group_x = X[mask]\n            group_y = y[mask]\n            counts = np.bincount(group_y)\n            total_group = len(group_y)\n            if total_group == 0:\n                continue\n            p = counts / total_group\n            H_i = -np.sum(p * np.log2(p))\n            weight = len(group_x) / total\n            H_y_given_A += weight * H_i\n        gain = H_y - H_y_given_A\n        gains.append((gain, feature))\n    max_gain = -1.0\n    best_feature_index = 0\n    for (i, (gain, feature)) in enumerate(gains):\n        if gain > max_gain:\n            max_gain = gain\n            best_feature_index = feature\n        elif gain == max_gain:\n            if feature < best_feature_index:\n                best_feature_index = feature\n    max_gain_rounded = round(max_gain, 6)\n    return (best_feature_index, max_gain_rounded)"}
{"task_id": 571, "completion_id": 0, "solution": "import numpy as np\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    (n_samples, n_features) = X.shape\n    if n_features == 1:\n        w = [X.T @ (alpha * y)[0]]\n    else:\n        w = X.T @ (alpha * y)\n    w_rounded = np.round(w, 4)\n    b = 1.0 / n_samples * np.sum(y - w @ X.T)\n    b_rounded = np.round(b, 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 591, "completion_id": 0, "solution": "import numpy as np\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    current_input = np.array(x)\n    num_layers = len(weights)\n    for L in range(num_layers):\n        layer_weights = weights[L]\n        if L == num_layers - 1:\n            pre_activation = current_input.dot(layer_weights)\n            current_input = pre_activation\n        else:\n            pre_activation = current_input.dot(layer_weights) + layer_weights[0]\n            current_input = np.maximum(pre_activation, 0)\n    if len(weights[-1]) == 1:\n        return [current_input[0]]\n    else:\n        return current_input"}
