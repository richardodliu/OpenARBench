{"task_id": 0, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(data: np.ndarray, alpha: float=0.0) -> list[float]:\n    \"\"\"\n    Solve for the weights in linear regression using the closed-form solution.\n\n    Given an array `data` where the last element is the target `y`, and all other elements are features,\n    perform L2 regularization on the model such that coefficients are updated with \n    \\\\[w = (\\\\mathbf{X}^T\\\\mathbf{X} + \\x07lpha\\\\mathbf{I})^{-1}\\\\mathbf{X}^T\\\\mathbf{y}\\\\]\n\n    Parameters:\n        data (np.ndarray): A 2D array where the first column is the target `y` and the rest are features.\n        alpha (float): Regularization strength parameter between 0 and positive infinity.\n\n    Returns:\n        A list containing the updated weights [w], which will be returned as a Python list.\n        \n    Raises:\n        ValueError: If the matrix (X^TX + alpha * I) is not invertible.\n    \"\"\"\n    determinant = np.linalg.det(np.dot(data.T, data))\n    if determinant == 0:\n        raise ValueError('Matrix (X^TX + alpha * I) is not invertible')\n    inv_matrix = np.linalg.inv(np.dot(data.T, data) + alpha * np.eye(data.shape[1]))\n    w = np.dot(inv_matrix, np.dot(data.T, data))\n    return list(w)\ndata = np.array([[1, 2, 3], [2, 4, 6], [3, 6, 9]])"}
{"task_id": 1, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.utils import check_random_state"}
{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n\n    def identity(link_func):\n        return lambda mu: link_func(mu)\n\n    def log(link_func):\n        return lambda mu: np.log(mu) - 1\n\n    def logit(link_func):\n        return lambda mu: (np.exp(mu) - 1) / (mu + 1)\n    return {'identity': identity, 'log': log, 'logit': logit}\nmu = np.array([0.2, 0.8])"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    node_depths = []\n    data_copy = data.copy()\n\n    def grow_tree(node_index):\n        node_depth = 0\n        while len(tree[node_index]) < 3:\n            parent_node_index = data_copy[node_index][node_depth].argmax()\n            left_child_index = node_index[tree[parent_node_index]]\n            if np.random.random() < 0.5:\n                data_copy[left_child_index][node_depth] = np.min(data_copy[node_index])\n                data_copy[node_index][node_depth] = np.max(data_copy[node_index])\n            else:\n                data_copy[right_child_index][node_depth] = np.min(data_copy[node_index])\n                data_copy[node_index][node_depth] = np.max(data_copy[node_index])\n            (tree[node_index][node_depth], tree[parent_node_index][node_depth + 1]) = (tree[parent_node_index][node_depth], tree[node_index][node_depth + 1])\n            tree[parent_node_index][node_depth + 1] += 1\n            node_depth += 1\n        node_depths.append(node_depth)\n        if not data_copy[node_index]:\n            return\n    tree = {}\n    tree[None] = {0: 1}\n    grow_tree(None)\n    return node_depths"}
{"task_id": 7, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    freq_dict = {}\n    for t in transactions:\n        freq_dict[t] = sum(Counter(t).values()) / len(t)\n    frequent_items = {k: v for (k, v) in freq_dict.items() if v >= minsup * len(freq_dict)}\n    return dict(sorted(frequent_items.items()))"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float=0.001, max_iter: int=2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Clustering data points into k clusters.\"\"\"\n    centroids = np.array([[mean(point) for point in data[:k]] for avg in data[k:]]) / len(data[k:])\n    for _ in range(max_iter):\n        distances = [(np.linalg.norm(centroid - point), centroid, i) for (i, centroid) in enumerate(centroids)]\n        sorted_indices = sorted(distances)[::-1]\n        new_centroids = []\n        for (dist, old_centroid, i) in sorted_indices:\n            closest_index = np.argmin(np.linalg.norm(data[i] - old_centroid))\n            new_centroids.append((dist, data[closest_index]))\n        if len(new_centroids) == k:\n            break\n        centroids = [centroid[1] for (_, _, i) in new_centroids]\n    return (centroids.tolist(), [i + 1 for (i, v) in enumerate(centroids) if v >= epsilon])"}
{"task_id": 13, "completion_id": 0, "solution": "import numpy as np\ndef knn_recommend(data: np.ndarray, user_ind: int, k: int, criterion: str='cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    sim_matrix = data.dot(data.T)\n    current_ratings = data[user_ind]\n    distances = np.linalg.norm(sim_matrix - current_ratings, axis=1)\n    filtered_indices = distances.argsort()[::-1][:min(k, len(distances))]\n    unrated_items = [filtered_indices[i] for i in range(len(filtered_indices)) if filtered_indices[i] != i]\n    return unrated_items"}
{"task_id": 19, "completion_id": 0, "solution": "from typing import Tuple\nimport numpy as np\ndef best_split(X: np.ndarray, g: np.ndarray, h: np.ndarray, gamma: float, lam: float) -> Tuple[int, float]:\n    max_gain = -float('inf')\n    best_feature_index = -1\n    best_threshold = -1\n    for idx in range(X.shape[1]):\n        if not (np.sum(g >= 0) > 1 and np.sum(h >= 0) > 1):\n            continue\n        gain = sum(np.square(g[idx] / np.linalg.norm(X[:, idx]) + lam)) - sum(np.cross(h[idx] / np.linalg.norm(X[:, idx]) ** 2 + lam, g[idx] / np.linalg.norm(X[:, idx]) + gamma)[:len(X[idx]), :])\n        if gain > max_gain:\n            max_gain = gain\n            best_feature_index = idx\n            best_threshold = X[best_feature_index, idx]\n    return (best_feature_index, best_threshold)"}
{"task_id": 20, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_activation(x):\n    x = [x] if isinstance(x, (list, np.ndarray)) else [x]\n    sigmas = [np.exp(-v) / (1 + np.exp(-v)) for v in x]\n    sig_mats = [np.multiply(v, 1 - v) for v in x]\n    return (round(np.array(sigmas), 4), round(np.array(sig_mats), 4))"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_1d_predict(x_train: list[float], y_train: list[int], x_test: list[float], epsilon: float=0.0) -> list[int]:\n    alpha = []\n    weights = []\n    while True:\n        d = int(np.median([np.max(x_train), np.min(x_test)]))\n        if d == 0:\n            alpha.append((-1) ** math.log(1 - sum(y_train)))\n        elif d == 1:\n            alpha.append((-1) ** (-1 / 2 * np.log((1 - sum(y_train)) / sum(y_train))))\n        for i in range(len(x_train) - 1):\n            x_i = x_train[i]\n            x_j = x_train[i + 1]\n            if x_i <= d < x_j > d:\n                weight_i = (x_j - x_i) * (y_train[i] + y_train[i + 1])\n                weight_j = (x_j - d) * (y_train[i] - y_train[j])\n                weight = weight_i + weight_j\n                alpha.append(alpha[i] * np.exp(weight_j - weight_i) / (weight_i + weight_j))\n                weights.append(weights[i])\n        if all((abs(a) <= epsilon for a in alpha)):\n            break\n    predictions = [int(sign(sum((alpha[i] * h(x_test) for (i, h) in enumerate(stump)))), 1) for (h, alpha) in zip(y_train, alpha)]\n    return predictions"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    if k < 1 or k > min(len(X), len(X[0])):\n        return -1\n    (U, s, VT) = np.linalg.svd(X, full_matrices=False)\n    k_sums = sorted(np.abs(s[:k]), reverse=True)[:k]\n    k_sums.sort()\n    selected_singular_values = [s[i] for i in range(k)]\n    selected_singular_vectors = np.array([VT[:, i] for i in range(k)])\n    X_hat = np.dot(U, np.dot(selected_singular_vectors.T, K))\n    return (X_hat.round(4).tolist(), np.mean(np.abs(X - X_hat) ** 2))"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef predict_boosting_tree(x_train: list[float], y_train: list[float], x_query: float, epsilon: float=0.01) -> float:\n    n_features = len(x_train)\n\n    def create_split_points(features):\n        sorted_indices = np.argsort(features)\n        return [sorted_indices[0], sorted_indices[-1]]\n\n    def calculate_residual_sum_of_squares(residuals):\n        return np.sum((residuals - np.mean(residuals)) ** 2)"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"\n    Generates a 2D NumPy array filled with binary values (0 or 1), given the shape and sparsity\n    probability and optionally a seed for reproducibility.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    binary_tensor = np.array([random.uniform(0, 1) for _ in range(len(shape))])\n    binary_tensor = binary_tensor * (1 - sparsity) + sparsity\n    return binary_tensor.astype(np.float64)\nshape = (2, 3)\nsparsity = 0.3"}
{"task_id": 39, "completion_id": 0, "solution": "from typing import List\nimport numpy as np\ndef get_split(data: np.ndarray, d: int) -> Tuple[int, List[int], List[int]]:\n    median_index = np.median(data, axis=0, kind='uniform').astype(int)\n    left_indices = []\n    pivot_row = []\n    for i in range(len(data[0])):\n        if i == median_index[d]:\n            continue\n        else:\n            left_indices.append(i)\n            pivot_row.append(median_index[d][i])\n    right_indices = [index for (_, index) in sorted(zip(left_indices, pivot_row))]\n    return (median_index[d], pivot_row, right_indices)"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    assert isinstance(frame_width, int), 'Frame width must be an integer.'\n    assert frame_width > 0, 'Frame width must be greater than zero.'\n    assert isinstance(stride, int), 'Stride must be an integer.'\n    assert stride > 0, 'Stride must be greater than zero.'\n    n_frames = (len(x) - frame_width) // stride + 1\n    frames = []\n    for i in range(n_frames):\n        start_index = i * stride\n        end_index = min(start_index + frame_width, len(x))\n        frame = x[start_index:end_index]\n        frames.append(frame)\n    result_array = np.array(frames, dtype=np.float64)\n    if writeable:\n        result_array.flags.writeable = True\n    return result_array\nx = np.random.rand(8000, 800)"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef is_stochastic(X: list[list[float]] | 'np.ndarray') -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    if isinstance(X, str):\n        X = np.array([X], dtype=float)\n    if not np.allclose(np.sum(X, axis=1), 1):\n        return False\n    for row in X.T:\n        if not np.all((row > 0) & (row <= 1)):\n            return False\n    return True"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef softplus(x, deriv: bool=False):\n    if deriv == True:\n        sig_x = np.log(1 + np.exp(-np.abs(x)))\n        return sig_x.round(4)\n    else:\n        return np.log(1 + np.exp(x))"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nfrom math import log\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    word_counts = Counter(sequence)\n    ngram_matrix = np.zeros((len(word_counts), N + 1))\n    for (i, word) in enumerate(word_counts):\n        for j in range(1, N + 1):\n            ngram_matrix[i, j] = word_counts[word] / len(word_counts)\n    log_probabilities = [np.log(n) for n in ngram_matrix.T]\n    final_log_prob = round(sum(log_probabilities[0]), 4)\n    return final_log_prob"}
{"task_id": 62, "completion_id": 0, "solution": "from sklearn.linear_model import Lasso\ndef lasso_regression(X: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray', degree: int, reg_factor: float, n_iterations: int=1000, tol: float=1e-06) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\"\"\"\n    if not isinstance(X, list):\n        raise TypeError(\"Input 'X' must be a list.\")\n    if not all((isinstance(x, list) for x in X)):\n        raise ValueError(\"All elements in 'X' must be lists.\")\n    if not isinstance(y, list):\n        raise TypeError(\"Input 'y' must be a list.\")\n    if not all((isinstance(y, list) for y in y)):\n        raise ValueError(\"All elements in 'y' must be lists.\")\n    if len(X[0]) == 1 and len(y) != 1:\n        raise ValueError(\"'X' must have at least two dimensions.\")\n    if isinstance(X, str):\n        X = np.array([list(map(float, line.split(','))) for line in open(X)])\n    if isinstance(y, str):\n        y = np.array([list(map(float, line.split(','))) for line in open(y)])\n    design_matrix = np.ones((len(X), degree + 1))\n    for i in range(len(design_matrix)):\n        design_matrix[i] = X[:i + 1].reshape(-1, 1)\n    y = np.array(y).reshape(-1, 1)\n    alpha = np.linalg.inv(np.dot(design_matrix.T, design_matrix)) * reg_factor\n    w = np.linalg.solve(alpha, y - np.dot(design_matrix, alpha))\n    w = map(round, w)\n    max_iter = min(n_iterations, len(w) - 1)\n    last_change = None\n    for _ in range(max_iter):\n        last_change = np.abs(np.max(np.abs(w)))\n        w += alpha @ (np.sign(last_change) * np.zeros_like(w))\n        if np.linalg.norm(last_change) < tol:\n            break\n    return w.tolist()"}
{"task_id": 63, "completion_id": 0, "solution": "from typing import List\ndef backward_beta(A: List[List[float]], B: List[List[float]], obs: List[int], t: int) -> List[float]:\n    \"\"\"\n    Calculates the backward probability vector for a given time index t for a Hidden Markov Model (HMM).\n    \n    Parameters:\n        - A: Transition-probability matrix of shape (N, N)\n        - B: Emission-probability matrix of shape (N, M)\n        - obs: List of observation indices (length T)\n        - t: Integer time index (0 \u2264 t < T)\n        \n    Returns:\n        - A list of N backward probabilities rounded to 4 decimal places\n    \"\"\"\n    beta = [1] * len(obs)\n    for (j, obs_symbol) in enumerate(obs[::-1]):\n        if not obs[j]:\n            break\n        for i in range(len(obs)):\n            alpha_i = sum(B[i][j].p_obs[obs_symbol] * A[i][j])\n            beta[i] += alpha_i / B[i][j]\n    return beta"}
{"task_id": 65, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    if len(S) != len(A) or any((len(observation) != len(B[observation]) for observation in observations)):\n        return -1\n    likelihood = np.zeros((len(observations), A.shape[1]))\n    for observation_index in range(len(observations)):\n        current_state_probability = S.copy()\n        next_probability = np.dot(current_state_probability, A)\n        for observation_index in reversed(range(len(observations))):\n            next_observation = observations[observation_index]\n            if current_state_probability[next_observation]:\n                next_probabilities = np.dot(next_probability, B[next_observation])\n                likelihood[observation_index][next_observation] += next_probabilities\n            else:\n                break\n        if not all((next_probabilities > 0 for observation_index in range(len(observations)) for next_probabilities in next_peoblems.values())):\n            return -1\n    normalized_likelihood = likelihood / np.sum(likelihood)\n    return np.around(normalized_likelihood, 4)\nS = [0.5, 0.3, 0.2]\nA = [[0.8, 0.1, 0.1], [0.1, 0.7, 0.2], [0.1, 0.2, 0.7]]\nB = [[0.7, 0.09, 0.2], [0.07, 0.91, 0.03], [0.04, 0.03, 0.96]]"}
{"task_id": 70, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nimport heapq"}
{"task_id": 75, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    dist_matrix = ((np.array(X_train) - np.array(X_test)) ** 2).sum(axis=1)\n    k_nearest_idx = np.argsort(dist_matrix)[:k]\n    class_labels = [y_train[i] if i in k_nearest_idx else None for i in range(len(y_train))]\n    return class_labels\ny_train = [0, 1, 1, 0]\nk = 2"}
{"task_id": 76, "completion_id": 0, "solution": "import torch\nfrom torch import nn\ndef cross_entropy_loss(y: torch.Tensor | 'torch.LongTensor', y_pred: torch.Tensor | 'torch.LongTensor') -> float:\n    if not isinstance(y_pred, type(torch.tensor())) and (not isinstance(y_pred, torch.Tensor)):\n        raise ValueError('y_pred must be a torch tensor')\n    y_pred = y_pred.type(torch.float)\n    if len(y_pred.size()) != 2 or y_pred.size(1) != y.size(1):\n        raise ValueError('Input tensors must be 2-dimensional with the same number of columns.')\n    if y_pred.min() < 0 or y_pred.max() > 1:\n        raise ValueError('Probabilities in y_pred must sum up to 1.')\n    y_pred = torch.nn.functional.softmax(y_pred, dim=-1)\n    loss = (-torch.log(y_pred)).sum(dim=1).mean()\n    return round(loss.item(), 4)"}
{"task_id": 77, "completion_id": 0, "solution": "import numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"\n    Forward propagation for an L-layer neural network (ReLU...ReLU \u2192 Sigmoid).\n\n    Parameters:\n    - X: Input matrix of shape (n_x, m)\n    - parameters: Dictionary containing the network parameters (W1, b1, ..., WL, bL)\n\n    Returns:\n    - AL: Final activation at layer L, rounded to 4 decimal places and converted to a regular Python list. Shape is (1, m). \n    \"\"\"\n    AL = X.copy()\n    AL[:, :] = 0\n    cache_L = []\n    for l in range(1, len(parameters) + 1):\n        if l <= len(parameters) // 2:\n            AL[:, :l + 1] += AL[:, :(l + 1) * (parameters['WL'][l] > 0)]\n        else:\n            AL[:, :len(parameters) + 1] += AL[:, :len(parameters) + 1 - (l - len(parameters)) * parameters['WL'][l]]\n        AL[:, l] = np.maximum(AL[:, l], 0)\n        cache_L.append(AL[:, l])\n    AL[:, -1] /= 2.0\n    AL = AL.tolist()\n    return (AL, cache_L)\nparameters = {'WL': [[1, 0]], 'bL': [1.0]}"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    A2_clipped = np.clip(A2, a_min=1e-15, a_max=1 - 1e-15)\n    log_probs = np.log(A2_clipped)\n    total_log_prob = np.sum(log_probs, axis=1)\n    cost = -total_log_prob[np.arange(len(Y)), Y]\n    return round(np.mean(cost), 6)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef epsilon_soft(Q: list[float] | 'np.ndarray', epsilon: float) -> list[float]:\n    \"\"\"\n    Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Parameters:\n    Q (list[float] or np.ndarray): A sequence containing the Q-values for each possible action in a single state.\n    epsilon (float): The exploration parameter.\n\n    Returns:\n    list[float]: A list of probabilities representing the \u03b5-soft policy.\n    \"\"\"\n    if not isinstance(Q, list):\n        Q = [Q]\n    n_actions = len(Q[0])\n    greedy_index = np.argmax(Q)\n    base = (-epsilon + epsilon * (n_actions - 1)) / n_actions\n    return [base * i + (1 - base) for i in range(n_actions)]"}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\nimport numpy as np"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\"\"\"\n    if not isinstance(z, int) and (not isinstance(z, float)):\n        z = int(z)\n    softplus_value = np.logaddexp(0.0, z)\n    return round(softplus_value, 4)"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\nfrom typing import Any\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    if not isinstance(policy, dict):\n        raise ValueError('Policy must be provided as a dictionary')\n    arm_payouts = []\n    for arm in bandit['arm_evs']:\n        arm_payouts.append(policy[arm])\n    if len(arm_payouts) == 0:\n        return np.nan\n    mse_value = 0\n    for i in range(len(arm_payouts)):\n        mse_value += (arm_payouts[i] - arm_payouts[i]) ** 2\n    mse_value /= len(arm_payouts)\n    return round(mse_value, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "import math\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float=1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    if sequence == '':\n        return 0.0\n    context = []\n    for i in range(len(sequence)):\n        if sequence[i] != '<UNK>':\n            context.append(sequence[i])\n    context_count = Counter(context)\n    vocab_size = len(set(context_count.keys()))\n    ngrams = [(word, context_count[word]) for word in context]\n    cumulative_counts = [float(ngrams[0][-1][1]) + K] * vocab_size\n    for ngram in ngrams[1:]:\n        new_count = (ngram[-1][1] + K) / (vocab_size + K * ngram[0][-1])\n        cumulative_counts += [new_count] * vocab_size\n    log_probs = [math.log(1 - p + K) if p > 0 else 0.5 * math.log(K + 1) for p in cumulative_counts]\n    log_prob = sum(log_probs)\n    return round(log_prob, 4)\nsequence = 'you'"}
{"task_id": 108, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.metrics import mean_squared_error\ndef als_factorization(X: np.ndarray, K: int, alpha: float=1.0, max_iter: int=200, tol: float=0.0001, n_jobs: int=-1) -> list[list[float]]:\n    best_loss = np.inf\n    best_W = None\n    best_H = None\n    best_X_hat = None\n    for _ in range(max_iter):\n        W = np.random.randn(K, X.shape[1])\n        H = np.random.randn(K, X.shape[0]) * np.sqrt(alpha)\n        loss = np.linalg.norm(W @ H - X)\n        for i in range(n_jobs):\n            if np.log(np.random.rand()) > -alpha / (i + 1):\n                (W_opt, H_opt, loss_als) = ALS().fit(X, W).predict(X)\n                if loss < loss_als:\n                    (W, H, loss) = (W_opt, H_opt, loss_)\n        if loss < best_loss or all((h == h for h in H)):\n            best_loss = loss\n            best_W = W\n            best_H = H\n            best_X_hat = W @ H\n    if best_loss >= tol:\n        raise ValueError('No convergence was achieved within max_iter')\n    X_hat = [[round(float(e), 4) for e in row] for row in best_X_hat]\n    return X_hat.tolist()"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\nimport random\nimport numpy as np\nimport random\ndef kmeans(X: np.ndarray, K: int, max_iter=50, random_state=None) -> list[list[float]]:\n    if not X.shape[0] == len(X[0]):\n        raise ValueError(\"Input matrix 'X' must have the same dimensions.\")\n    if random_state is None:\n        random.seed(42)\n        centroids = [np.random.rand(K, X.shape[1])]\n    else:\n        np.random.seed(random_state)\n        centroids = [np.random.rand(K, X.shape[1])]\n    while True:\n        distances = []\n        for i in range(len(X)):\n            distances.append(np.linalg.norm(X[i] - centroids[0]))\n        centroids[0] = np.mean(distances[:K], axis=0)\n        for j in range(1, K):\n            distances[j] = np.linalg.norm(x - centroids[j]) ** 2\n            new_centroid = np.mean([d[j] for d in distances[:K]], axis=0)\n            if np.allclose(new_centroid, centroids[j]):\n                break\n        if np.any(np.isnan(centroids[0])):\n            print('K-means convergence failed after', max_iter, 'iterations.')\n            return None"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nclass SentenceEncoder:\n\n    def __init__(self, time_steps):\n        self.time_steps = time_steps\n\n    def encode(self, text: str, vocab: dict[str, int]) -> list[np.ndarray]:\n        \"\"\"Encode each character of the input text into a fixed-length list of integers.\"\"\"\n        padded_text = []\n        for char in text + [0]:\n            index = self._find_index(char, vocab)\n            if index == -1:\n                continue\n            padded_text.append(index)\n        return padded_text[:self.time_steps]\n\n    def _find_index(self, char, vocab):\n        \"\"\"Find the position of the given character in the vocabulary.\"\"\"\n        for (i, (prev_char, prev_idx)) in enumerate(vocab.items()):\n            if prev_char == char:\n                return i\n        return -1\n\n    def decode(self, indices: list[np.ndarray]) -> str:\n        \"\"\"Decode the list of indices back to a string of characters.\"\"\"\n        ids = []\n        for idx in indices:\n            index = idx // self.time_steps % len(self.vocabulary)\n            ids.append(self.vocabulary[index])\n        return ''.join(ids)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encode the input text using the SentenceEncoder class and then predict using a model.\"\"\"\n    encoder = SentenceEncoder(TIME_STEPS)\n    encoded_text = encoder.encode(text, input_vocabulary)\n    predictions = model.predict(encoded_text)\n    decoded_predictions = [inv_output_vocabulary[prediction] for prediction in predictions]\n    return ' '.join(decoded_predictions)"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    if not examples:\n        EXAMPLES = 'example_0001-09999'\n    results = []\n    for example in examples:\n        result = model(example)\n        char_list = [inv_output_vocabulary[c] for c in result]\n        predicted_string = ''.join(char_list)\n        results.append(predicted_string)\n    return results"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\"\"\"\n    p = 1 / (1 + np.exp(-X.dot(w)))\n    mean_cross_entropy = -1.0 / len(y) * np.sum(y * np.log(p), axis=0) - -1.0 / len(y) * np.sum((1 - y) * np.log(1 - p), axis=0)\n    gradient_weights = -X.T.dot(p - y)\n    return (round(mean_cross_entropy, 4), round(gradient_weights, 4))"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    if n_clf < 1:\n        n_clf = 1\n\n    def calculate_weight(y_train):\n        return np.sum(np.sign(y_train == 1)) / len(y_train)\n\n    def decision_stump(X, y, alpha, threshold):\n        scores = np.dot(X.T, y)\n        sorted_scores = np.sort(scores)[::-1]\n        return np.argmax(sorted_scores * alpha)\n    clf_weights = [calculate_weight(y_train)]\n    for _ in range(n_clf):\n        y_pred = []\n        for X_test_i in X_test:\n            y_pred.append(decision_stump(X_train, y_train, clf_weights[-1], 1))\n        aggregated_pred = np.mean(y_pred)\n        for i in range(len(y_pred)):\n            if aggregated_pred != np.sign(y_train[i]):\n                clf_weights.append(calculate_weight(y_train))\n                break\n        else:\n            continue\n    return [int(aggregated_pred)] * X_test.shape[0]\nX_train = np.random.rand(100, 2)\ny_train = np.array([1] * 80 + [-1] * 20)\nX_test = np.random.rand(100, 2)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    if n_components <= 1:\n        return []\n    S_W = np.sum(np.outer(X[:, None] - X[np.newaxis, :], X[:, None] - X[np.newaxis, :]), axis=0)\n    S_B = np.zeros_like(S_W)\n    cov_X = np.cov(X)\n    cov_Y = np.cov(y.T)\n    A = np.linalg.inv(np.dot(cov_X, cov_X)) * np.linalg.inv(np.dot(S_W, S_B))\n    eigen_pairs = np.abs(eigvalsh(A)).argsort()\n    W = np.zeros((len(y), len(X[0])))\n    for (i, ci) in enumerate(y):\n        W[i, :] = np.dot(eigen_pairs[eigen_pairs < i].reshape(1, -1), np.dot(X, X.T)[ci, :, :] / n_components)\n    projected_data = np.apply_along_axis(lambda x: x[W[i]], 0, X)\n    return projected_data.tolist()"}
{"task_id": 140, "completion_id": 0, "solution": "from collections import deque\nimport math\nfrom collections import deque\nimport math\ndef escape_fire_maze(grid: list[str]) -> int:\n    (ROWS, COLS) = (len(grid), len(grid[0]))\n    directions = [(0, 1), (1, 0), (-1, 0), (0, -1)]\n\n    def bfs(start_row, start_col):\n        queue = deque([(start_row, start_col)])\n        seen = set()\n        while queue:\n            (current_row, current_col) = queue.popleft()\n            if (current_row, current_col) not in seen and grid[current_row][current_col] != '#':\n                seen.add((current_row, current_col))\n                if current_row == rows - 1 and current_col == cols - 1:\n                    return 0\n                for (dr, dc) in directions:\n                    (new_row, new_col) = (current_row + dr, current_col + dc)\n                    if 0 <= new_row < rows and 0 <= new_col < cols and (grid[new_row][new_col] != '#'):\n                        queue.append(((new_row, new_col), (current_row, current_col)))\n    (rows, cols) = (len(grid), len(grid[0]))\n    best_time = float('inf')\n    for i in range(ROWS):\n        for j in range(COLS):\n            if grid[i][j] != '#':\n                remaining_rows = rows - i\n                remaining_cols = cols - j\n                total_steps = math.ceil(math.log2(remaining_rows * 2)) + math.ceil(math.log2(remaining_cols * 2)) - 1\n                if remaining_rows > 0 and remaining_cols > 0:\n                    steps = min(total_steps, bfs(i, j))\n                else:\n                    steps = max(total_steps, bfs(i, j))\n                if steps <= best_time:\n                    best_time = min(best_time, steps)\n    return best_time if best_time != float('inf') else -1\ngrid = ['.', '.', '#', '.', '.', '.', '#', '.', '.', '.', '.', '#']"}
{"task_id": 141, "completion_id": 0, "solution": "import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\ndef knn_predict(X_train, y_train, X_test, k, metric='euclidean'):\n    \"\"\"\n    Predict classes for test data from a knn model trained on labeled data\n    \n    Parameters:\n    - X_train (np.array): Labeled input features\n    - y_train (np.array): Corresponding output labels\n    - X_test (np.array): Test input features\n    - k (int): Number of nearest neighbors to consider\n    - metric (str): Metric used for calculating distances ('euclidean', 'manhattan', or 'cosine')\n    \n    Returns:\n    - predictions (numpy.ndarray): Predicted classes for the test data points\n    \"\"\"\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    predictions = knn.predict(X_test)\n    return predictions\nX_train = np.random.rand(100, 5)\ny_train = np.random.randint(0, 2, size=100)"}
{"task_id": 146, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, k: int=5, metric: str='euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan' or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    if not isinstance(y, np.ndarray):\n        raise TypeError('y must be a numpy.ndarray')\n    if not isinstance(X, np.ndarray):\n        raise TypeError('X must be a numpy.ndarray')\n    if not isinstance(X_test, np.ndarray):\n        raise TypeError('X_test must be a numpy.ndarray')\n    if not isinstance(X_test, np.ndarray):\n        raise TypeError('X_test can only have 2 dimensions')\n    if len(y.shape) != 1 or y.ndim != 1:\n        raise ValueError('y must be a 1D numpy-array')\n    if len(X.shape) != 2 or X.ndim != 2:\n        raise ValueError('X must be a 2D numpy-array')\n    if len(X_test.shape) != 2 or X_test.ndim != 2:\n        raise ValueError('X_test must be a 2D numpy-array')\n    if not np.isscalar(k):\n        raise ValueError('k must be a scalar')\n    if not np.isinteger(k):\n        raise ValueError('k must be an integer')\n    if k > len(y):\n        raise ValueError(f'k cannot exceed {len(y)}')\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        raise ValueError('metric must be \"euclidean\", \"manhattan\" or \"cosine\"')\n    prediction_list = []\n    for test_sample_index in range(X_test.shape[0]):\n        distances = []\n        for (i, feature) in enumerate(X_test[test_sample_index].T):\n            distances.append(np.sqrt(np.sum((feature - X_test[i]) ** 2)))\n        sorted_distances = np.argsort(distances)\n        k_nearest_indices = sorted_distances[:k]\n        k_nearest_labels = [y[index] for index in k_nearest_indices]\n        majority_vote_result = dict()\n        votes = {}\n        for label in set(k_nearest_labels):\n            count = sum([1 for k in k_nearest_labels if k == label])\n            votes[label] = count\n        for label in k_nearest_labels:\n            if label in votes.keys():\n                majority_vote_result[label] = max(votes.values())\n        prediction_list.append(majority_vote_result[min(prediction_list)])\n    return prediction_list.tolist()\nX_test = np.random.rand(50, 3)"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\ndef actor_forward(state, weights):\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    num_hidden_units = len(weights['W1'])\n    hidden_layer_size = len(weights['W1']) + len(weights['W2']) + 1\n    state_flattened = np.array(state).reshape(1, -1)\n    output_dimensionality = len(weights['W3']) + len(weights['b3'])\n    output_vector = np.einsum('ij,kl->ik', state_flattened, weights['W3'][:, None])\n    output_vector += weights['b3'].reshape(-1, 1)\n    relu_output_vector = np.maximum(output_vector, 0)\n    tanh_output_vector = np.tanh(relu_output_vector)\n    return np.round(tanh_output_vector[0])"}
{"task_id": 160, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 165, "completion_id": 0, "solution": "import numpy as np\ndef hamming_distance(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    n = min(len(x), len(y))\n    distance = sum((1 if x[i] != y[i] else 0 for i in range(n)))\n    return round(distance, 4)"}
{"task_id": 169, "completion_id": 0, "solution": "import numpy as np\ndef generate_window(window: str, N: int, coefficients: list[float] | None=None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n\n    def hamming_coefficient(n):\n        return 0.54 - 0.46 * np.cos(2.0 * np.pi * n / (N - 1))\n\n    def hann_coefficient(n):\n        return 0.5 - 0.5 * np.cos(2.0 * np.pi * n / (N - 1))\n\n    def blackman_harris_coefficient(n):\n        a0 = 0.35875\n        a1 = 0.48829\n        a2 = 0.14128\n        a3 = 0.01168\n        return a0 + a1 * np.cos(2.0 * np.pi * n / (N - 1)) + a2 + a3 * np.cos(4.0 * np.pi * n / (N - 1)) + a1 * np.cos(6.0 * np.pi * n / (N - 1))\n    if window == 'hamming':\n        return [hamming_coefficient(k) for k in range(N)]\n    elif window == 'hann':\n        return [hann_coefficient(k) for k in range(N)]\n    elif window == 'blackman_harris':\n        return [blackman_harris_coefficient(k) for k in range(N)]\n    elif window == 'generalized_cosine':\n        if not coefficients:\n            raise ValueError(\"For 'generalized_cosine', coefficients must be supplied.\")\n        return [a.item() for a in coefficients]\n    else:\n        raise ValueError(f'{window} is not recognized as supported window.')"}
{"task_id": 171, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression_train_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], epochs: int=5000, learning_rate: float=0.1, batch_size: int=32) -> list[int]:\n    y_train_binary = [int(label == -1 or label == 1) for label in y_train]\n    W = np.random.randn(X_train[0].shape[1]) / np.sqrt(X_train[0].shape[1])\n    for epoch in range(epochs):\n        indices = np.arange(len(y_train_binary))\n        np.random.shuffle(indices)\n        for (i, example) in enumerate(zip(X_train, y_train_binary)):\n            (x, y) = example\n            A = np.dot(x, W)\n            if y[i] == -1:\n                Z = max(0, 1 - A) + min(0, 1 - A)\n            else:\n                Z = max(0, 1 - A)\n            dW = -(Z * A).reshape(-1, 1) / len(y_train_binary)\n            W -= learning_rate * dW\n    predictions = np.array([1 if pred > 0.5 else -1 for pred in y_train_binary])\n    return predictions.tolist()"}
{"task_id": 176, "completion_id": 0, "solution": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier"}
{"task_id": 178, "completion_id": 0, "solution": "import numpy as np\ndef particle_swarm_optimisation(n_dims: int, num_particles: int, num_iterations: int, seed: int=1) -> float:\n    if not (isinstance(n_dims, int) and n_dims > 0):\n        raise ValueError('n_dims must be a positive integer')\n    if not (isinstance(num_particles, int) and num_particles > 0):\n        raise ValueError('num_particles must be a positive integer')\n    if not (isinstance(num_iterations, int) and num_iterations > 0):\n        raise ValueError('num_iterations must be a positive integer')\n    if not isinstance(seed, int):\n        raise TypeError('seed must be an integer')\n    particles = []\n    for _ in range(num_particles):\n        x = np.clip(np.random.uniform(-1, 1, n_dims), 0, 1)\n        particles.append(x)\n    v = []\n    for i in range(len(particles)):\n        v.append(0.5 * np.sum((particles[i] - particles[np.argmax(particles[i])]) ** 2))\n    while num_iterations > 0:\n        for _ in range(num_particles):\n            if len(v) == 0:\n                break\n            next_v = np.zeros_like(v[-1])\n            for i in range(len(v) - 1):\n                next_v[i] = np.clip(w * v[-1][i] + c1 * r1 * (p_best[-1] - particles[i]) ** 2 + c2 * r2 * (g_best[-1] - particles[i]), min(-1, max(0, p_best[-1])), max(1, min(1, g_best[-1])))\n            v.append(next_v)\n        global_best = np.max(v, axis=0)\n        for i in range(len(particles)):\n            particles[i] += v[i]\n            particles[i] = np.clip(particles[i], -1, 1)\n        particles = [p / 2 for p in particles]\n        p_best = particles[0]\n        for j in range(1, len(particles)):\n            if np.any(particles[j] - particles[p_best] < 1e-06):\n                p_best = particles[j]\n        g_best = np.min(v, axis=0)\n        global_best = np.max(global_best, axis=0)\n        num_iterations -= 1\n    return global_best[0]"}
{"task_id": 180, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]], y_train: list, X_test: list[list[float]], k: int=5, task: str='classification') -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    if not isinstance(y_train, np.ndarray):\n        raise ValueError('y_train needs to be a numpy array')\n    train_data = np.array(X_train)\n    dists = ((train_data - X_test) ** 2).sum(axis=1)\n    sorted_indices = np.argsort(dists)[:k]\n    neighbors = [X_train[i] for i in sorted_indices]\n    counts = Counter(y_train[sorted_indices])\n    max_count = max(counts.values())\n    majority_class = next(((class_, len(class_)) for (class_, count) in counts.items() if count == max_count), None)\n    if task == 'classification':\n        if majority_class is None:\n            return [None] * len(neighbors)\n        else:\n            return majority_class[:-1] + [majority_class[-1] / float(max_count)]\n    elif task == 'regression':\n        return [np.mean(neighbors[i]) for i in range(len(neighbors))]\nX_train = [[1], [2], [9, 8], [6, 7, 10]]\ny_train = [1, 0, 1, 0]"}
{"task_id": 184, "completion_id": 0, "solution": "import numpy as np\ndef decision_tree_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_depth: int | None=None) -> list[int]:\n    (num_samples, num_features) = X_train.shape\n    root_feature = None\n\n    def dfs(node_index):\n        if len(X_train[node_index]) == num_samples or not isinstance(root_feature, tuple):\n            if max_depth is None:\n                return X_train[node_index]\n            elif len(X_train[node_index]) < max_depth:\n                return X_train[node_index][np.argmax(np.bincount(y_train[node_index]))]\n            elif (len(X_train[node_index]) > max_depth) & (not isinstance(root_feature, tuple)):\n                return np.max([X_train[node_index], Y_train[node_index]])\n            else:\n                return np.mean([X_train[node_index], Y_train[node_index]])\n        if root_feature is not None:\n            left_child = [i for i in range(node_index + 1, node_index + num_features + 1) if X_train[i] != X_train[node_index] and X_train[i] != X_train[node_index][:node_index]]\n            right_child = [i for i in range(node_index + 1, node_index + num_features + 1) if (X_train[i] != X_train[node_index] or X_train[i] != X_train[node_index][:node_index]) and i not in left_child]\n            if left_child == [] and right_child == []:\n                return root_feature\n            if len(left_child) >= len(right_child):\n                return dfs([i for i in left_child if not isinstance(root_feature, tuple)])\n            else:\n                return dfs([i for i in right_child if not isinstance(root_feature, tuple)])\n        return root_feature\n    root_feature = dfs(range(num_samples))\n    prediction_list = []\n    for x_test_sample in X_test:\n        result = dfs(x_test_sample)\n        prediction_list.append(result)\n    return prediction_list"}
{"task_id": 190, "completion_id": 0, "solution": "import numpy as np\ndef best_gini_split(X, y):\n\n    def calculate_gini(leaf_indices):\n        leaf_sums = [sum(y == i) for i in set(y[leaf_indices])]\n        total_samples = len(y)\n        (left_sum, right_sum) = (0, 0)\n        for (i, sum_) in enumerate(leaf_sums):\n            left_sum += sum_\n            if i != 0:\n                right_sum -= sum_\n        n_total = len(y)\n        n_leaf = len(leaf_indices)\n        numerator = n_leaf * (n_leaf - 1) * sum((leaf_sums[i] ** 2 for i in range(n_leaf)))\n        denominator = n_total * (n_total - 1) * (n_total - 2) * (n_total - 3) // n_leaf\n        gini = 1 - (numerator + (n_leaf - 1) * (n_total - 1) * ((n_total - 2) / denominator))\n        if gini <= 0:\n            return (0, None, gini)\n        return (gini, sum(leaf_sums))\n    gini_values = []\n    for idx in range(len(X)):\n        Y = np.array([y[i] for i in set(idx)])\n        indices = np.argsort(np.sort(Y)[-len(indices):])[:len(indices)]\n        leaf_indices = sorted(range(len(Y)), key=lambda k: Y[k], reverse=False)[:indices.size]\n        (gini, _) = calculate_gini(leaf_indices)\n        gini_values.append(gini)\n    if not gini_values:\n        return (-1, None, round(np.mean(gini_values), 4))\n    return (max(gini_values) + 0.01, max(gini_values).argmax() + 0.01, min(gini_values))\nX = [[1, 2], [3, 4], [5, 6]]\ny = [0, 1, 0]"}
{"task_id": 191, "completion_id": 0, "solution": "import numpy as np\ndef metric(name: str):\n\n    def d(x, y):\n        if len(x) != len(y) or any((not isinstance(item, (int, float)) for item in x)) or all((item == 0 for item in [np.linalg.norm(x), np.linalg.norm(y)])) or (name.lower() not in ['euclidean', 'manhattan', 'chebyshev', 'cosine']):\n            return -1\n        if name.lower() == 'cosine':\n            norm_x = np.linalg.norm(x)\n            norm_y = np.linalg.norm(y)\n            return 1 - x @ y / (norm_x * norm_y)\n        return np.sqrt(np.sum((x - y) ** 2))\n    return d"}
{"task_id": 197, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import product\nfrom itertools import product\nimport numpy as np\nfrom typing import Any, List, Tuple, Union\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    n_actions = env.action_space.n\n    shape = env.action_space.shape\n    valid_discrete_actions = [tuple(dim.values()) for dim in env.action_space.spaces]\n    if md_action:\n        n_actions_per_dim = [len(dim) for dim in shape]\n    else:\n        n_actions_per_dim = [1] + [0] * (len(shape) - 1)\n    if cont_action:\n        action_ids = None\n    elif len(valid_discrete_actions) == 0:\n        action_ids = []\n        for i in range(len(shape)):\n            for j in range(i + 1, len(shape)):\n                actions = [(i,) + s for s in valid_discrete_actions[:i] + valid_discrete_actions[i + 1:]]\n                action_ids.extend(actions)\n    else:\n        action_ids = [tuple(dim.values()) for dim in env.action_space.spaces]\n    result = (n_actions_per_dim, action_ids, n_actions)\n    return result"}
{"task_id": 198, "completion_id": 0, "solution": "import numpy as np\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    beta_matrix = []\n    for (doc_index, doc) in enumerate(corpus):\n        phi_doc = [np.sum(np.array([phi[i][doc[i]] for i in range(len(doc))]) >= 1) for _ in range(V)]\n        while True:\n            new_phi_doc = []\n            for v in range(V):\n                if not np.any(phi_doc[v]):\n                    continue\n                normalized_phi_doc = phi_doc.copy()\n                for d in range(len(corpus)):\n                    normalized_phi_doc[d] /= np.sum(normalized_phi_doc[d])\n                new_phi_v = np.dot(normalized_phi_doc.T, phi[v])\n                new_phi = normalized_phi_v / np.linalg.norm(new_phi_v)\n                new_phi_doc.append(new_phi)\n            if all((v < 50 for v in np.sum(new_phi_doc, axis=0))):\n                break\n            else:\n                phi_doc = new_phi_doc\n    return [np.array([new_phi_doc[v].tolist() for v in range(V)]) for new_phi_doc in phi_doc]"}
{"task_id": 202, "completion_id": 0, "solution": "import numpy as np\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length n where each element is an integer label in the range 0 \u2026 k\u22121.\n\n    \"\"\"\n    medoids = X[:k]\n    for i in range(1, len(X)):\n        pairwise_distances = np.sum((X[i] - medoids) ** 2, axis=1)\n        min_index = np.argmin(pairwise_distances)\n        temp_medoid = medoids[min_index]\n        medoids[min_index] = X[i][min_index]\n        X[i] = X[i][np.where(pairwise_distances == pairwise_distances.min())[0]]\n        if not np.any(np.abs(pairwise_distances - np.min(pairwise_distances)) <= np.abs(pairwise_distances.min() - np.min(pairwise_distances))):\n            break\n    return medoids.tolist()\nX = np.array([[0, 0], [1, 0], [0, 1], [5, 5], [6, 5], [5, 6]])\nk = 2"}
{"task_id": 216, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    if not isinstance(n_iterations, int):\n        return -1\n    if all([a >= 0 for a in [len(x_train[0]), len(y_train)]]):\n        return -1\n    X_train = []\n    for i in range(len(x_train)):\n        X_train.append(np.ones((len(x_train[i]), 1)) * x_train[i][i])\n    X_train = np.array(X_train)\n    w = np.zeros(degree + 1)\n    for _ in range(n_iterations):\n        for j in range(degree + 1):\n            partial_wj = 0\n            for i in range(len(x_train)):\n                if i != j:\n                    partial_wj += X_train[i][j] * y_train[i]\n            partial_wj /= len(x_train)\n            for k in range(degree + 1):\n                if k == j:\n                    continue\n                partial_wk = reg_factor * w[k]\n                if k < j:\n                    partial_wk -= abs(partial_wj)\n                if k > j:\n                    partial_wk += abs(partial_wj)\n                partial_wk *= l1_ratio\n                w[k] -= learning_rate * partial_wk\n    return np.round(w.tolist(), 4)"}
{"task_id": 217, "completion_id": 0, "solution": "import numpy as np\ndef logistic_components(actual, predicted):\n    actual_array = np.array(actual)\n    predicted_array = np.array(predicted)\n    gradient_list = []\n    hessian_list = []\n    for i in range(len(actual)):\n        z = -actual_array[i] * predicted_array[i]\n        gradient = actual_array[i] * np.exp(-z)\n        gradient_list.append(round(np.real(gradient), 6))\n        hessian = np.multiply(np.exp(-z), 1 - np.exp(-z))\n        hessian_list.append(round(np.real(hessian), 6))\n    proba_array = np.apply_along_axis(sigmoid_function, axis=1, arr=predicted_array)\n    return tuple(zip(*[gaugelist, hessimatelist, probalist]))"}
{"task_id": 218, "completion_id": 0, "solution": "def blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    total_score = sum(player) + sum(dealer)\n    if total_score > 21:\n        return -1\n    winner = 'dealer' if total_score < 17 else 'player'\n    dealer_score = sum(dealer)\n    if dealer_score == 0:\n        return -1 if winner == 'player' else 1\n    score_difference = abs(total_score - dealer_score)\n    if score_difference <= 10:\n        return 0\n    if total_score == dealer_score:\n        return 0\n    if total_score // 11 > 0:\n        return 1\n    if player[0] == 10:\n        return 1.5\n    return 1"}
{"task_id": 221, "completion_id": 0, "solution": "import numpy as np\ndef squared_log_error(actual, predicted):\n    errors = [(np.log(1 + a) - np.log(1 + b)) ** 2 for (a, b) in zip(actual, predicted)]\n    return np.mean(errors)\ndef mean_squared_log_error(actual, predicted):\n    if len(actual) != len(predicted):\n        return -1\n    for i in range(len(actual)):\n        if not all((a > 0 for a in [actual[i], predicted[i]])):\n            return -1\n    return squared_log_error(actual, predicted)"}
{"task_id": 222, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays\n    and must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        return np.exp(z - np.log1p(np.e)) / (1 + np.exp(z))\n    elif isinstance(z, np.ndarray):\n        result = []\n        for x in z:\n            if x >= 0:\n                result.append(np.exp(x - np.log1p(np.e))) / (1 + np.exp(x))\n            else:\n                result.append(1 - np.exp(x - np.log1p(np.e)))\n        return np.array(result).round(4)\n    else:\n        raise TypeError('Input must be a scalar, list or NumPy array')"}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        z = z.tolist()\n    result = []\n    for (i, val) in enumerate(z):\n        if val < 0:\n            result.append(a * val)\n        else:\n            result.append(val)\n    return result\nz = [np.array([1, -4]), np.array([-7, 6])]"}
{"task_id": 226, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"\n    Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    X_train_normalized = X_train / np.linalg.norm(X_train)\n    y_train_normalized = y_train / np.max(y_train)\n    X_train_split = X_train_normalized[:-1]\n    y_train_split = y_train_normalized[:-1]\n    stump_indices = np.random.choice(len(X_train_split), size=n_clf, replace=False)\n    stump_weights = np.ones(n_clf) / float(n_clf)\n    stump_weights[stump_indices] -= 1\n    stump_weights /= 1 + np.exp(-len(stump_indices) * stump_weights)\n    for _ in range(100):\n        for i in range(n_clf):\n            train_stump = stumps[i]\n            feature_index = train_stump[0]\n            threshold = train_stump[1]\n            polarity = train_stump[2]\n            y_pred = (train_stump[3] > threshold).astype(int)\n            new_weighs = (1 - np.sign(np.sum(y_pred != y_train_split))) * np.sign(polarity)\n            stump_weights[:, i] += new_weighs\n    adjusted_weights = np.multiply(stump_weights, n_clf)\n    total_weighted_error = (adjusted_weights + 1e-10).sum(axis=0)\n    selected_stump = stump_indices[np.argmin(total_weighted_error)]\n    return [selected_stump]"}
{"task_id": 241, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal(shape: tuple[int, ...], scale: float=0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    tensor = np.random.randn(*shape)\n    (u, s, v_T) = np.linalg.svd(tensor, full_matrices=False)\n    (u, s, _) = (np.array(u).T, np.max(s), v_T)\n    result = u.reshape(shape)\n    squared_norm = np.sum(result ** 2, axis=0)\n    if not np.allclose(squared_norm, scale ** 2):\n        raise ValueError(f'The computed squared norm does not match the given scale. Result: {squared_norm}')\n    return result.tolist()"}
{"task_id": 243, "completion_id": 0, "solution": "from typing import List\ndef actor_critic_forward(state: List[float]) -> Tuple[List[float], float]:\n    x = 4 * np.array(state[0]) + 4 * np.array(state[1]) + 4 * np.array(state[2])\n    z1 = np.maximum(x, 0)\n    z2 = np.maximum(z1, 0)\n    z3 = np.maximum(z2, 0)\n    logits = np.dot(z3, np.array([1, 4, 16])) / 64\n    action_probs = np.exp(logits) / sum(np.exp(logits))\n    state_value = np.dot(z3, np.array([1, 4, 16])) / 64\n    return (action_probs, state_value)"}
{"task_id": 249, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    state = np.array(state)\n    for (k, v) in params.items():\n        if k != 'Wa':\n            params[k] += np.zeros(v.shape[0])\n    W1 = params['Wa']\n    b1 = params['ba']\n    W2 = np.tanh(params['W2']).reshape(-1, params['Wa'].shape[-1])\n    b2 = np.zeros(W2.shape[0])\n    W3 = np.tanh(params['W3']).reshape(-1, params['Wa'].shape[-1])\n    b3 = np.zeros(W3.shape[0])\n    hidden_vector = np.append(np.dot(W1, state), np.dot(W2, W3))\n    hidden_vector = np.append(hidden_vector, b1 + W2 * b2 + W3 * b3)\n    hidden_vector = np.maximum(0, hidden_vector)\n    hidden_vector = np.minimum(params['W3'], np.tanh(params['W3'])).dot(hidden_vector)\n    hidden_vector = np.dot(W3, hidden_vector)\n    probabilities = np.apply_along_axis(lambda x: np.exp(x - np.max(x)) / np.sum(np.exp(x)), axis=1, arr=hidden_vector)\n    value = np.mean(probabilities)\n    return (probabilities, value)\nstate = [0.8, 0.5, 0.2]\nparams = {'W1': np.array([[0.2, 0.2, 0.2]]), 'b1': np.array([np.random.randn()]), 'W2': np.array([[0.1, 0.1, 0.1]]), 'b2': np.array([np.random.randn()]), 'W3': np.array([[0.05]])}"}
{"task_id": 253, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    penalty = -np.sqrt(alpha * 0.5 * w.T @ w) / 2\n    l1_norm = np.linalg.norm(w)\n    total_penalty = penalty + l1_norm * l1_ratio * 0.5\n    total_penalty_rounded = round(total_penalty, 4)\n    return total_penalty_rounded\n    if gradient:\n        return np.round(np.abs(w).T, 4)\n    else:\n        return total_penalty_rounded"}
{"task_id": 256, "completion_id": 0, "solution": "import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable"}
{"task_id": 257, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    alpha = np.ones(len(y_train)) / len(y_train)\n    predictions = []\n    while len(predictions) < len(y_test):\n        e = np.sum(alpha * (1 - y_train) * X_train, axis=0)\n        mask = np.argmin(e)\n        predictions.append((mask + 1) % 2)\n        alpha[mask] *= e[~mask] / e[~mask]\n        if not alpha[mask].any():\n            break\n    return predictions"}
{"task_id": 261, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n\n    Note:\n        - If the input shape is not a valid pair of (fan_in, fan_out),\n          then returns np.zeros(dtype=float).  \n        - The shape must be square or equal if we use PyTorch.\n    \"\"\"\n    assert len(shape) == 2 and shape[0] >= 2 and (shape[1] >= 2)\n    (fan_in, fan_out) = (shape[:-1], shape[-1])\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n    result = np.random.randn(*shape)\n    return result"}
{"task_id": 266, "completion_id": 0, "solution": "from typing import List, Tuple, Any\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    G = [[] for _ in range(len(V))]\n    for (u, v) in E:\n        G[u].append(v)\n        if len(G[v]) == 0:\n            break\n    return G"}
{"task_id": 267, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeClassifier\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"\n    Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n\n    def weighted_loss(y_pred, y_true):\n        \"\"\"\n        Calculate weighted loss based on the given weight distribution.\n\n        Parameters\n        ----------\n        y_pred : np.ndarray\n            Predictions made by the model.\n        y_true : np.ndarray\n            True labels.\n\n        Returns\n        -------\n        float\n            The weighted loss value.\n        \"\"\"\n        num_labels = len(set(y_true))\n        if num_labels == 2:\n            raise ValueError('Weighted loss calculation not implemented for binary classification.')\n        positive_weight_sum = sum(sample_weights[y_true >= y_pred])\n        negative_weight_sum = sum(sample_weights[y_true <= y_pred])\n        overall_positive_wts = sum([sample_weights[y_true == i] for i in range(num_labels)])\n        overall_negative_wts = sum([-sample_weights[i] for i in range(num_labels)])\n        loss = 0\n        for i in range(num_labels):\n            if y_pred[i]:\n                loss += overall_negative_wts * abs(positive_weight_sum + sample_weights[i]) / num_labels\n            else:\n                loss -= overall_positive_wts * abs(negative_weight_sum + sample_weights[i]) / num_labels\n        return loss\n    clf = DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, random_state=42)\n    cv_results = {}\n    for criterion in ['gini', 'entropy']:\n        for max_depth in [None]:\n            cv_results[criterion, max_depth] = []\n            for sample_size in [100, 500, 1000]:\n                clf.fit(X, y)\n                y_pred = clf.predict_proba(X)[:, 1]\n                weighted_loss_value = weighted_loss(y_pred=y_pred, y_true=y)\n                cv_results[criterion, max_depth].append(weighted_loss_value)\n    best_params = max(cv_results.items(), key=lambda x: x[1][1])\n    best_model = DecisionTreeClassifier(**best_params)\n    return {'feature_index': len(y.columns) + 1, 'threshold': best_models.feature_importances_[1], 'polarity': best_models.classes_[1], 'weighted_error': mean_squared_error(y, best_models.predict(X))}\ndef predict(model: DecisionTreeClassifier, X_test: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Predict the probability of the classes given the features.\n\n    Parameters\n    ----------\n    model : DecisionTreeClassifier\n        The trained decision tree classifier.\n    X_test : np.ndarray\n        Test features.\n\n    Returns\n    -------\n    np.ndarray\n        Array of predicted probabilities.\n    \"\"\"\n    return model.predict_proba(X_test)[:, 1]"}
{"task_id": 273, "completion_id": 0, "solution": "import math\ndef best_split(feature, target):\n    features = [feature]\n    target_counts = Counter(target)\n    if len(set(features)) == 1:\n        min_feature_value = min(features)\n        max_feature_value = max(features)\n        threshold = (min_feature_value + max_feature_value) / 2\n        entropy = sum([target_counts.get(x, 0.0) for x in set(features)]) / n\n        entropy -= len(set(features)) / n * math.log(len(set(features)) / n, 2)\n        if threshold >= min_entropy:\n            return (threshold, entropy)\n        else:\n            return (None, 0.0)\n    threshold = -math.inf\n    while True:\n        info_gain = 0.0\n        for i in range(len(features)):\n            if i == 0:\n                left_threshold = math.inf\n                right_threshold = -math.inf\n            else:\n                left_threshold = features[i - 1]\n                right_threshold = features[i]\n            current_entropy = sum([target_counts.get(x, 0.0) for x in set([left_threshold, right_threshold])])\n            current_information_gain = info_gain + log(math.log(n + 1) / log(current_entropy, 2))\n            if current_information_gain > info_gain:\n                break\n            elif current_information_gain == info_gain:\n                threshold = max(threshold, current_threshold)\n            else:\n                info_gain = current_information_gain\n        if threshold != -math.inf:\n            break\n    return (threshold, info_gain)"}
{"task_id": 286, "completion_id": 0, "solution": "import random\nimport numpy as np\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    return [[float(random.random()) for _ in range(shape[0])] for _ in range(shape[1])]\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    return [[float(random.random()) for _ in range(shape[0])] for _ in range(shape[1])]\ndef random_uniform_init(shape: Tuple[int, int], low: float=0.0, high: float=1.0, seed: int=42) -> List[List[float]]:\n    rng = random.Random(seed)\n    return [[rng.uniform(low=low, high=high) for _ in range(shape[0])] for _ in range(shape[1])]\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    if name == 'zeros_init':\n        return zeros_init\n    elif name == 'ones_init':\n        return ones_init\n    elif name == 'random_uniform_init':\n        return random_uniform_init\n    else:\n        raise ValueError(f\"Invalid initialization function. '{name}'\")"}
{"task_id": 287, "completion_id": 0, "solution": "import numpy as np\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    if stride[0] != pool_shape[0] or stride[1] != pool_shape[1]:\n        raise ValueError('Strides must match the window dimensions')\n    num_steps_x = stride[0]\n    num_steps_y = stride[1]\n    pooled_output = np.zeros_like(X)\n    for x in range(num_steps_x):\n        for y in range(num_steps_y):\n            pooled_x_slice = slice(x * pool_shape[0], min((x + 1) * pool_shape[0], pool_shape[0]))\n            pooled_y_slice = slice(y * pool_shape[1], min((y + 1) * pool_shape[1], pool_shape[1]))\n            accumulated_value = np.sum(X[pooled_x_slice, pooled_y_slice])\n            pooled_output[pooled_x_slice, pooled_y_slice] += accumulated_value / (num_steps_x * num_steps_y)\n    grad_input = np.zeros_like(X)\n    for x in range(num_steps_x):\n        for y in range(num_steps_y):\n            pooled_x_slice = slice(x * pool_shape[0], min((x + 1) * pool_shape[0], pool_shape[0]))\n            pooled_y_slice = slice(y * pool_shape[1], min((y + 1) * pool_shape[1], pool_shape[1]))\n            grad_pooled_x = np.sum(accumulated_value * X[pooled_x_slice, :][:, pooled_y_slice], axis=0)\n            grad_pooled_y = np.sum(accumulated_value * X[:, pooled_x_slice, :][:, pooled_y_slice], axis=0)\n            grad_input[pooled_x_slice, pooled_y_slice] += grad_pooled_x / num_steps_x + grad_pooled_y / num_steps_y\n    return (pooled_output.tolist(), grad_input.tolist())"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    value: object\ndef compare_trees(tree_a, tree_b):\n\n    def check_equivalence(node_a, node_b):\n        if isinstance(node_a, Leaf) and isinstance(node_b, Leaf):\n            return node_a.value == node_b.value\n        elif not isinstance(node_a, Leaf) or not isinstance(node_b, Leaf):\n            return False\n        if node_a.feature != node_b.feature:\n            return False\n        if node_a.threshold != node_b.threshold:\n            return False\n        if not check_equivalence(node_a.left, node_b.left) or not check_equivalence(node_a.right, node_b.right):\n            return False\n        return True\n    if type(tree_a) != Leaf and type(tree_b) != Leaf:\n        raise ValueError('Both trees must be of type Leaf')\n    if tree_a.feature != tree_b.feature or tree_a.threshold != tree_b.threshold:\n        return False\n    return check_equivalence(tree_a, tree_b)"}
{"task_id": 292, "completion_id": 0, "solution": ""}
{"task_id": 294, "completion_id": 0, "solution": "from typing import Any, Dict, List, Tuple\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    if not isinstance(G, dict):\n        raise TypeError('Input G must be a dictionary')\n    adj_list = {}\n    for (key, value) in G.items():\n        if 'is_directed' in key:\n            continue\n        adj_list[key] = []\n        outgoing_edges = [(v, w) for (v, _, _) in value]\n        outgoing_edges.sort(key=lambda x: x[1])\n        for (u, v, w) in outgoing_edges:\n            adj_list[u].append((v, w))\n    return adj_list"}
{"task_id": 296, "completion_id": 0, "solution": "import numpy as np\ndef is_binary(x):\n    \"\"\"\n    Check if a NumPy array contains only binary values (0 or 1).\n    \n    Parameters:\n    x (np.ndarray or array-like): Input array to validate.\n    \n    Returns:\n    bool: True if all elements are 0 or 1, False otherwise.\n    \"\"\"\n    return np.all((x == 0) | (x == 1))"}
{"task_id": 298, "completion_id": 0, "solution": "import math\nfrom collections import Counter, defaultdict\nfrom typing import List\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n    prefix_counts = [Counter() for _ in range(N + 1)]\n    padded_corpus = [i + '<bol>' for i in corpus] + [i + '<eol>' for i in corpus]\n    for sentence in padded_corpus:\n        words = sentence.split()\n        for (i, word) in enumerate(words):\n            prefix_counts[i][len(word)] += 1\n    mle_probs = []\n    for prefix_size in range(N + 1):\n        p = [0.0 for _ in range(len(padded_corpus))]\n        for sentence in padded_corpus:\n            words = sorted(sentence.split())\n            for (i, word) in enumerate(words):\n                prev_word = words[i - prefix_size] if i > 0 else ''\n                curr_word = words[i - prefix_size + 1] if i < len(words) else ''\n                if prev_word != '' and curr_word == '':\n                    break\n                elif prev_word != '' and curr_word == '':\n                    continue\n                p[i] += (prefix_counts[prefix_size][i] - prefix_counts[prefix_size][prev_word]) * math.log(prefix_counts[prefix_size][i] / prefix_counts[prefix_size][prev_word])\n        normalized_p = [p[i] / sum(p) for i in range(N)]\n        log_likelihood = math.log(sum(normalized_p))\n        mle_probs.append(log_likelihood)\n    log_sum = sum(mle_probs) + float('inf')\n    return round(log_sum, 4) if any((n == 0 for n in mle_probs)) else -float('inf')\ncorpus = ['a b a', 'b a b']\nN = 1"}
{"task_id": 302, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.sparse.linalg import eigs\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    n_samples = X.shape[0]\n    W = np.diag(1 / np.sqrt(np.sum(X ** 2, axis=1)))\n    L = np.dot(W.T, W)\n    D_inv_L = np.linalg.inv(L)\n    (eig_vals, eig_vecs) = np.linalg.eigh(D_inv_L)\n    sorted_indices = eig_vals.argsort()[::-1]\n    sorted_eigvals = eig_vals[sorted_indices]\n    sorted_eigvecs = eig_vecs[:, sorted_indices]\n    top_k_eigvals = sorted_eigvals[:k]\n    top_k_eigvecs = sorted_eigvecs[:, :k]\n    init_centroids = np.random.rand(n_samples, k)\n    for i in range(k):\n        distances = np.dot(top_k_eigvecs[i], X)\n        cluster_labels = np.argmin(distances, axis=1)\n        new_centroids = np.copy(init_centroids)\n        for j in range(n_samples):\n            if np.all(cluster_labels[j] == -1):\n                new_centroids[j] += np.random.rand(1)[0]\n        if np.allclose(cluster_labels, np.unique(new_centroids)):\n            break\n    return np.array([cluster_labels[index] for index in np.arange(n_samples)])"}
{"task_id": 303, "completion_id": 0, "solution": ""}
{"task_id": 304, "completion_id": 0, "solution": "import numpy as np\ndef bayesian_posterior_mean(X: np.ndarray, y: np.ndarray, alpha: float=1.0, beta: float=1.0, mu=0.0, V=None, fit_intercept: bool=True) -> np.ndarray:\n    if V is None:\n        S_inv = np.eye(X.shape[1])\n    elif isinstance(V, np.ndarray):\n        V = V * np.eye(X.shape[1])\n        S_inv = X.T @ V @ X + np.eye(S_inv.shape[0])\n    else:\n        if len(V.shape) == 1:\n            V = np.atleast_2d(V).T\n        if V.shape != (X.shape[1],):\n            raise ValueError('V must have the same dimensions as X.')\n        V = np.tile(V, (X.shape[1] + 1, 1))\n        S_inv = x0.dot(x1)\n    post_mean = np.zeros(len(y))\n    if fit_intercept:\n        post_mean += np.ones(len(post_mean))\n    post_mean -= np.dot(np.dot(X.T, V), mu)\n    post_mean /= np.linalg.norm(X.T.dot(X) / V)\n    return post_mean.tolist()"}
{"task_id": 308, "completion_id": 0, "solution": "import numpy as np\ndef mel_filterbank(N: int, n_filters: int=20, fs: int=44000, min_freq: int=0, max_freq: int | None=None, normalize: bool=True) -> list[list[float]]:\n    \"\"\"\n    Build a Mel filterbank transformation matrix.\n    \n    Args:\n        N (int): Discrete Fourier transform length.\n        n_filters (int): Number of filters in the filterbank.\n        fs (int): Sampling rate, in Hz.\n        min_freq (int): Minimum frequency (Hz).\n        max_freq (int or None): Maximum frequency (Hz), or None if not specified.\n        normalize (bool): Whether to scale each filter by a factor of 1/f(i+2)-1.\n    \n    Returns:\n        A list of lists representing the filterbank structure.\n    \"\"\"\n    freqs = np.linspace(min_freq, max_freq, n_filters + 2)\n    mels = [mel * 1000.0 / 700 for mel in freqs]\n    return [[np.arctan(np.tanh(mel - min_freq) / (2 * fs)) for mel in freq] for freq in freqs]"}
{"task_id": 312, "completion_id": 0, "solution": "import numpy as np\ndef blackman_harris(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if not isinstance(window_len, int) or window_len < 1:\n        raise ValueError('Window length must be a positive integer')\n    if symmetric:\n        coeff = [0.35875, -0.48829, 0.14128, -0.01168]\n    else:\n        coeff = [0.35875, 0.48829, 0.14128, 0.01168][:window_len]\n    return round(np.array(coeff), 4)\nwindow_len = 10"}
{"task_id": 313, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    total_sum = sum(np.sum((np.array([item[d]]) for item in gamma)))\n    psi_d = psi(d)\n    psi_total = psi(total_sum)\n    return round(psi_d - psi_total, 4)\ndef psi(d):\n    \"\"\"Calculate the digamma function.\"\"\"\n    if d < len(gamma) / 2:\n        return psi_1(d)\n    else:\n        return psi(d - len(gamma) / 2) / (d - len(gamma) / 2) + 1 / (2 * d ** 2)\ndef psi_1(n):\n    \"\"\"Calculate the digamma function for the first half of the Gamma array.\"\"\"\n    if n <= 0:\n        return math.log(-math.inf)\n    elif n == 1:\n        return 0\n    else:\n        i = 1\n        while i * i <= n:\n            i += 1\n        return -i * (math.log(i) + math.log((1 + math.sqrt(i)) / (1 - math.sqrt(i)))) / 2\ngamma = [[0.1, 0.2], [0.3, 0.4]]\nd = 0"}
{"task_id": 317, "completion_id": 0, "solution": "from typing import List, Tuple, Dict\nfrom typing import List, Tuple, Dict"}
{"task_id": 318, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    alpha = []\n    for _ in range(n_estimators):\n        gradient = []\n        for i in range(len(y_train)):\n            h_i = lambda x: np.sign(y_train[i] - x)\n            gradient.append(np.dot(-np.ones_like(h_i(x)), h_i(x)))\n        alpha += np.log(gradient / np.sum(gradient) + 1e-08)\n        y_pred = np.sign(np.dot(alpha, H(x_test)))\n        alpha.append(2 * np.mean(alpha) - np.mean(alpha))\n        if not any(y_pred == X_test):\n            break\n    return [1 if y_pred == 1 else -1 for y_pred in Y_train]\nY_train = np.array([-1, 1])\nX_test = np.random.rand(100, 1)"}
{"task_id": 329, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 331, "completion_id": 0, "solution": "import numpy as np\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    if len(predictions.shape) != 2 or len(oob_mask.shape) != 2:\n        raise ValueError('Input must be a 2D array')\n    n_samples = y_true.shape[0]\n    n_estimators = predictions.shape[1]\n    full_predictions = np.zeros((n_samples, n_estimators))\n    for i in range(n_samples):\n        mask = oob_mask[i]\n        full_predictions[i, mask] = predictions[i, mask]\n    oob_mse_val = np.sqrt(np.mean((full_predictions - y_true) ** 2))\n    return oob_mse_val\ny_true = [1, 2, 3, 4, 5]\npredictions = [[0.1, 0.4, 0.9], [0.6, 0.7, 0.8], [0.3, 0.6, 0.5], [0.5, 0.2, 0.1]]\noob_mask = [[True, False, True, False], [False, True, False, True], [True, True, True, False], [False, True, False, False]]"}
{"task_id": 332, "completion_id": 0, "solution": "from collections import defaultdict\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> defaultdict:\n    result = defaultdict(lambda : {'support': 0, 'children': {}})\n    for transaction in transactions:\n        processed_items = set()\n        for item in transaction:\n            if item not in result['children']:\n                result['children'][item] = {}\n            processed_items.add(item)\n        processed_items = sorted(processed_items, key=lambda x: (-result['children'][item]['support'], x))\n        current_item = next(iter(processed_items), None)\n        while current_item:\n            support = result['children'][current_item]['support']\n            if support < min_support:\n                break\n            result['children'][current_item]['support'] += 1\n            current_item = result['children'][current_item][f'{current_item}_child']\n    return result"}
{"task_id": 336, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(X: list[list[int | float]], Y: list[list[int | float]]) -> list[list[float]]:\n    if len(X[0]) != len(Y):\n        return -1\n    (n, m) = (len(X), len(Y))\n    if not isinstance(X, list) or not isinstance(Y, list):\n        raise ValueError('X and Y must be provided as lists')\n    if m == 0:\n        return np.zeros((n, n))\n    K = [[0 for _ in range(m)] for _ in range(n)]\n    for i in range(n):\n        for j in range(m):\n            X_i = X[i]\n            Y_j = Y[j]\n            inner_product = np.dot(X_i, Y_j) + c0\n            K[i][j] = inner_product\n    return K\nX = [[1, 2, 3], [4, 5, 6]]\nY = [[7, 8, 9], [10, 11, 12]]"}
{"task_id": 340, "completion_id": 0, "solution": "import numpy as np\nimport numpy.random as rng\ndef adam_update(param: np.ndarray, grad: np.ndarray, t: int, m: np.ndarray, v: np.ndarray, lr: float=0.001, beta1: float=0.9, beta2: float=0.999, eps: float=1e-08):\n    \"\"\"Perform **one** Adam optimisation step.\"\"\"\n    if not isinstance(param, np.ndarray):\n        param = np.array(param)\n    if not isinstance(t, int):\n        raise ValueError('Time step must be an integer.')\n    m_t = np.zeros_like(param)\n    v_t = np.zeros_like(param)\n    param_t = np.copy(param)\n    lr_t = lr\n    beta1_t = beta1\n    beta2_t = beta2\n    for _ in range(t):\n        g = grad\n        m_t_t = beta1_t * m_t + (1 - beta1_t) * g\n        v_t_t = beta2_t * v_t + (1 - beta2_t) * g ** 2\n        m_t = m_t_t / np.sqrt(v_t_t + eps)\n        v_t = v_t_t / np.sqrt(m_t_t + eps)\n        param_t -= lr_t * m_t_t / (np.sqrt(v_t_t) + eps)\n        print(f'Updated {param_t}, new m: {m_t_t}, new v: {v_t_t}')\n    return (param_t, m_t, v_t)"}
{"task_id": 343, "completion_id": 0, "solution": "import numpy as np\nimport numpy as NP\ndef cross_entropy_update(theta_samples: np.ndarray, rewards: list[np.ndarray] | np.ndarray, retain_prcnt: float=0.5) -> tuple[list[np.float64], list[np.float64]]:\n    \"\"\"\n    Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float, optional\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list[np.float64], list[np.float64]]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    probs = np.cumsum(NP.ravel(np.exp(rewards)), axis=1)\n    retained_num = int(probs[..., :1] * retain_prcnt) if type(probs) == np.ndarray else retaint_prcnt\n    selected_samples = theta_samples[RP[nums]][:, :]\n    mean = selected_samples.mean(axis=1)\n    variances = selected_samples.var(axis=1)\n    return (mean.tolist(), variances.tolist())"}
{"task_id": 353, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nimport numpy as np\ndef entropy(labels):\n    if not labels:\n        return np.round(0.0, 5)\n    counts = Counter(labels)\n    if len(counts) == 1:\n        return 0.0\n    entropy = sum([-count * math.log2(count) for count in counts.values()])\n    return round(entropy, 5)\nlabels = [1, 2, 3, 4, 1, 2, 3]"}
{"task_id": 354, "completion_id": 0, "solution": "import numpy as np\ndef conv2D(X: np.ndarray, W: np.ndarray, stride: int, pad: str, dilation: int=0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\"\"\"\n    if pad == 'same':\n        num_rows = np.ceil((X.shape[1] - W.shape[1]) / (W.shape[2] + 2 * dilation))\n        num_cols = np.ceil((X.shape[2] - W.shape[2]) / (W.shape[3] + 2 * dilation))\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        (num_rows, num_cols) = pad\n    else:\n        raise ValueError(\"Invalid padding specification 'pad'. Must be either 'same', a tuple of two ints or a number.\")\n    padded_X = pad_image(X, W.shape[2], pad)\n    padded_W = pad_image(W, W.shape[2] + 2 * dilation, W.shape[3] + 2 * dilation)\n    reshaped_X = X.reshape(-1, W.shape[1], W.shape[2], W.shape[3])\n    conv_result = np.einsum('ij, klm -> jkl', padded_X, padded_W)\n    return conv_result.tolist()"}
{"task_id": 355, "completion_id": 0, "solution": "import numpy as np\ndef pca_transform(data: np.ndarray, n_components: int, solver: str='svd') -> tuple[list[list[float]], list[float]]:\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    if n_components == 0:\n        n_components = len(standardized_data[0])\n    elif n_components > len(standardized_data[0]):\n        n_components = min(len(standardized_data[0]), n_components)\n    if solver == 'svd':\n        eigenvectors = np.linalg.svd(standardized_data, full_matrices=False)\n        scores = eigenvectors[:, :n_components]\n    else:\n        cov_matrix = np.cov(standardized_data.T)\n        (eigenvalues, _) = np.linalg.eig(cov_matrix)\n        sorted_eigenvalues_index = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_eigenvalues_index]\n        sorted_eigenvectors = np.array([np.sqrt(eigenvalues[i]) for i in range(n_components)])\n        scores = sorted_eigenvectors.dot(standardized_data)\n    signs = np.sign(scores[:, n_components - 1])\n    scores = np.round(np.mean(scores, axis=1), 4)\n    explained_variance_ratio = [sum((index < n_components for index in range(scores.shape[1]))) / scores.shape[1] for index in range(scores.shape[1])]\n    return (scores, explained_variance_ratio)\ndata = np.random.rand(100, 2)"}
{"task_id": 356, "completion_id": 0, "solution": "def leaf_predict(leaf: 'Leaf', classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf (no rounding necessary).\n    \"\"\"\n    if classifier:\n        return max(classpath)\n    else:\n        return leaf.value[0]"}
{"task_id": 357, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, K: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"\n    Cluster 'data' into 'K' groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals and the original indices.\n    \"\"\"\n\n    def calculate_closest_centroid(sample, centroids):\n        distances = np.sqrt(np.sum((sample - centroid) ** 2, axis=1))\n        return (np.argmin(distances), np.argmin([distance / np.linalg.norm(centroid - sample) for (distance, centroid) in zip(distances, centroids)]))\n    centroids = np.random.choice(data.shape[0], size=(K,), replace=False)\n    while True:\n        closest_indices = []\n        for _ in range(max_iters):\n            closest_indices.append(calculate_closest_centroid(data[closest_indices, :], centroids)[0])\n        new_centroids = [data[i][closest_indices].mean(axis=0) for i in closest_indices]\n        if np.allclose(new_centroids, centroids, atol=1e-06):\n            break\n        centroids = new_centroids\n    return [[x, y] for (x, y) in zip(new_centroids[::2], new_centroids[1::2])]"}
{"task_id": 362, "completion_id": 0, "solution": "import numpy as np\ndef row_stochastic_matrix(data: list[list[int | float]]) -> list[list[float]]:\n    data_array = np.array(data)\n    for row in data_array.T:\n        if np.sum(row) == 0:\n            return -1\n    row_sum = np.round(np.sum(data_array.T), 4)\n    normalized_row_data = data_array / row_sum\n    return normalized_row_data.tolist()\ndata = [[0.8, 0.9, 0.2], [0.7, 0.6, 0.3], [0.5, 0.4, 0.2]]"}
{"task_id": 363, "completion_id": 0, "solution": "def is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    container = []\n    for space in [env.action_space, env.observation_space]:\n        if isinstance(space, Box):\n            container.append(tuple_action)\n        elif isinstance(space, Tuple):\n            container.append(tuple_obs)\n        else:\n            container.append(False)\n    return tuple(container)"}
{"task_id": 369, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"\n    Aggregate the outputs of GradientBoosting trees.\n    \n    Parameters:\n    - updates: A list of arrays where each array contains the predictions of \n               one tree's output for all samples.\n    - learning_rate: A positive float that scales every tree's output.\n    - regression: A boolean indicating whether the problem is regression or multi-class classification.\n    \n    Returns:\n    - An array of float values representing the aggregated predictions for either regression or classification.\n    \"\"\"\n    if len(updates[0].shape) == 1:\n        updated_array = [update.copy() for update in updates]\n    else:\n        updated_array = updates\n    running_prediction = np.zeros(len(updated_array[0]))\n    for (i, update) in enumerate(updated_array):\n        running_prediction += learning_rate * update\n        if regression:\n            running_prediction = round(running_prediction, 4)\n        else:\n            running_prediction = np.exp(update) / np.sum(np.exp(update))\n            running_prediction = np.argmax(running_prediction, axis=1)\n    return running_prediction.ravel()"}
{"task_id": 371, "completion_id": 0, "solution": "import numpy as np\ndef conv1D(X, W, stride=1, pad=(0,), dilation=0):\n    ksize = W.shape[-1] - 1\n    stride_tensor = np.array(stride) if isinstance(stride, int) else stride\n    pad_val = np.array(pad) if isinstance(pad, int) else pad\n    dilation_tensor = np.array(dilation) if isinstance(dilation, int) else dilation\n    w = np.pad(W, ((0, 0), (pad_val, pad_val)), mode='constant', constant_values=(0,))\n    conv_result = np.convolve(X, w, mode='same')\n    result_list = [result[:, :ksize].tolist()] if isinstance(result, np.ndarray) else []\n    return result_list\nX = np.random.randn(5, 64, 128).astype(np.float32)\nW = np.random.randn(17, 64, 32).astype(np.float32)\nstride = 3\npad = (1,)\ndilation = 1"}
{"task_id": 373, "completion_id": 0, "solution": "import numpy as np\ndef gini(y: list[int]) -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if not y:\n        return 0.0\n    n_classes = len(set(y))\n    n_samples = sum(y)\n    if n_classes == 0:\n        return 0.0\n    probabilities = [float(num) / n_samples for num in y]\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 4)"}
{"task_id": 374, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float=0.01, n_iterations: int=10000) -> tuple[list[float], float]:\n    (m, n) = X.shape\n    if not isinstance(m, int) or not isinstance(n, int):\n        return -1\n    weights = np.zeros((n, 1))\n    bias = 0\n    for _ in range(n_iterations):\n        dw = 2 / m * X.T.dot(y - np.dot(X, weights))\n        db = 2 / m * np.sum(y - np.dot(X, weights))\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        if np.any(weights == 0):\n            print('Warning: Model parameters may have become zero during the iterations.')\n    return ([np.squeeze(p) for p in weights], np.squeeze(bias))"}
{"task_id": 376, "completion_id": 0, "solution": "from typing import List\nimport numpy as np\ndef is_symmetric(X: List[List[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\"\"\"\n\n    def check_symmetry(row_i, col_j):\n        return abs(np.abs(X[row_i][col_j] - X[col_j][row_i]) < 1e-08) and abs(np.abs(X[row_i][col_j] + X[col_j][row_i]) < 1e-08)\n    if len(X) != len(X[0]):\n        return False\n    for i in range(len(X)):\n        for j in range(len(X[i])):\n            if not check_symmetry(i, j):\n                return False\n    return True"}
{"task_id": 377, "completion_id": 0, "solution": "import numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Calculates the negative gradient of the logistic loss function for a given\n    prediction score f and true label y.\n    \n    Args:\n        y (list): A list of integers representing the true labels (0 or 1).\n        f (list): A list of float64 numbers representing the predicted scores.\n        \n    Returns:\n        list: A list of floats representing the negative gradient of the\n              logistic loss function.\n    \"\"\"\n    y = np.array(y)\n    f = np.array(f)\n    n_g = (f * y).mean()\n    return [round(n_g, 4)]"}
{"task_id": 380, "completion_id": 0, "solution": "import numpy as np\ndef selu(x, order=0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n\n    def elu(x, alpha):\n        return max(0, x - np.exp(-x / alpha))\n\n    def scaled_elu(x):\n        return alpha * elu(np.clip(x, -np.inf, np.inf))\n    if isinstance(x, (int, float)):\n        return [scaled_elu(alpha) if x > 0 else alpha ** (x - 1) for _ in range(len(x))]\n    elif isinstance(x, (list, np.ndarray)):\n        return scaled_elu(alpha)([elu(xi, alpha) for xi in x])\n    else:\n        raise TypeError('Input x must be either a number or a list/tuple')"}
{"task_id": 387, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float=0.1) -> list[float]:\n    training_data = zip(x, y)\n    sorted_x = sorted(training_data, key=lambda item: item[0])\n    pred = [np.mean(y)]\n    num_iterations = len(sorted_x)\n    for i in range(num_iterations):\n        new_pred = [np.mean(y)]\n        for j in range(1, len(sorted_x)):\n            if j % 2 == 0:\n                residuals = y - sorted_x[j][0]\n                min_value = sorted_x[j][0]\n                max_value = sorted_x[j - 1][0]\n                mid_point = sorted_x[j // 2][0]\n                if mid_point < min_value or mid_point > max_value:\n                    mid_point = sorted_x[j // 2 + 1][0]\n                predicted_mid = np.mean(residuals[(mid_point <= sorted_x[j][0]) & (sorted_x[j][0] <= mid_point)])\n                new_pred.append(predicted_mid)\n            else:\n                residuals = y - sorted_x[j][0]\n                min_value = sorted_x[j][0]\n                max_value = sorted_x[j - 1][0]\n                mid_point = sorted_x[(j - 1) // 2][0]\n                if mid_point < min_value or mid_point > max_value:\n                    mid_point = sorted_x[(j - 1) // 2 + 1][0]\n                predicted_mid = np.mean(residuals[(mid_point <= sorted_x[j][0]) & (sorted_x[j][0] <= mid_point)])\n                new_pred.append(predicted_mid)\n        stump_predictions = [(1 - learning_rate) * (pred[i] + learning_rate * (new_pred[i] - pred[i])) for i in range(len(new_pred))]\n        pred = stump_predictions\n    return list(np.round(pred, 4))"}
{"task_id": 394, "completion_id": 0, "solution": "import numpy as np\ndef rmsprop_update(w: np.ndarray, grad: np.ndarray, Eg: np.ndarray | None=None, learning_rate: float=0.01, rho: float=0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients, by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    w_next = np.zeros_like(w)\n    if Eq(Eg, None):\n        Eg_next = 0\n    else:\n        E_g = rho * Eq(Eg, 0) + (1 - rho) * grad ** 2\n        Eg_next = Eq(Eg) + learning_rate / np.sqrt(Eg_next) * grad\n    w_next[:] = w - Eq(w) / Eq(Eg_next)\n    return (w_next.tolist(), Eg_next.tolist())\nw = np.array([1.0, 2.0])\ngrad = np.array([-2.0, 3.0])"}
{"task_id": 398, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 411, "completion_id": 0, "solution": "import numpy as np\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    if len(observations) != len(actions):\n        return {'action_dim': len(actions), 'obs_dim': len(observations)}\n    unique_actions = set([np.ravel(x) for x in actions])\n    unique_observations = set([np.ravel(y) for y in observations])\n    n_actions_per_dim = []\n    n_obs_per_dim = []\n    for (dim, arr) in zip(*[((a, b) for a in unique_actions for b in unique_observations)], ((a, b) for a in unique_actions for b in unique_observations)):\n        n_actions_per_dim.append(len(arr))\n        n_obs_per_dim.append(len(np.unique(b)))\n    action_ids = sorted(set(unique_actions))\n    obs_ids = sorted(set(unique_observations))\n    stats = {'action_dim': len(unique_actions), 'obs_dim': len(unique_observations), 'n_actions_per_dim': n_actions_per_dim, 'n_obs_per_dim': n_obs_per_dim, 'action_ids': action_ids, 'obs_ids': obs_ids}\n    return stats\nactions = [(0, 1), (1, 0), (1, 1)]\nobservations = [10.0, 11.5, 12.0]"}
{"task_id": 413, "completion_id": 0, "solution": "import numpy as np\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int=10, learning_rate: float=0.1, max_depth: int=3) -> list[float]:\n    predictions = []\n    for x in X_test:\n        y_pred = np.zeros_like(x)\n        depth = 0\n        learning_rate_2 = learning_rate / 10\n        y_pred[depth] = x.mean()\n        while True:\n            if y_pred.sum() <= learning_rate_2 * max_depth:\n                break\n            else:\n                depth += 1\n                y_pred = ((y_pred - x.mean()) ** 2).mean(axis=-1)\n        predictions.append(y_pred)\n    predictions = [int(round(p)) for p in predictions]\n    return predictions\nX_train = np.random.rand(100, 5)\ny_train = np.random.rand(100)\nX_test = np.random.rand(10, 5)\npredictions = gbdt_regression(X_train, y_train, X_test, n_estimators=10, learning_rate=0.01, max_depth=2)"}
{"task_id": 416, "completion_id": 0, "solution": "import numpy as np\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Calculates the probability density function (PDF) of a multivariate Gaussian (normal) distribution\n    given the sample data points 'X', the mean vector 'mean' and the covariance matrix 'cov'.\n    \n    Parameters:\n        X (numpy.ndarray): A numpy array representing the sample data points.\n        mean (numpy.ndarray): A numpy array representing the mean vector.\n        cov (numpy.ndarray): A numpy array representing the covariance matrix of the normal distribution.\n        \n    Returns:\n        result (list): A list where each element represents the PDF value for the corresponding input point,\n                       rounded to four decimal places.\n    \"\"\"\n    if len(mean.shape) != 1 or len(mean.shape) != len(cov.shape):\n        raise ValueError(\"The 'mean' and 'cov' arrays must be 1D vectors.\")\n    (n_samples, _) = X.shape\n    pdf_values = []\n    for x_i in X:\n        term_1 = np.exp(-0.5 * (x_i - mean) ** 2 / np.diag(cov))\n        term_2 = math.sqrt(np.linalg.det(cov))\n        result = np.nan_to_num(term_1 / term_2)\n        pdf_values.append(round(result, 4))\n    return pdf_values"}
{"task_id": 419, "completion_id": 0, "solution": "import numpy as np\ndef bayesian_linear_regression_pred(X: np.ndarray, y: np.ndarray, X_new: np.ndarray, mu: float | np.ndarray=0.0, sigma: float=1.0, V: float | np.ndarray | None=None, fit_intercept: bool=True) -> list[float]:\n    if fit_intercept:\n        X_new = np.hstack((np.ones(X.shape[0]), X_new))\n    transformed_X = np.c_[np.ones_like(X.shape[0]), X]\n    prior_distribution = np.linalg.inv(np.dot(V.transpose(), V))\n    posterior_mean = (np.dot(transformed_X.T, X_new) + prior_distribution) @ np.dot(mu, transformed_X)\n    return posterior_mean.tolist()"}
{"task_id": 423, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List"}
{"task_id": 428, "completion_id": 0, "solution": "from numpy import *\ndef hann(window_len: int, symmetric: bool=False) -> list[float]:\n    if window_len <= 0:\n        return []\n    elif window_len == 1:\n        return [1]\n    hann_window = zeros(int((2 * window_len - 1) / 2))\n    for i in range(len(hann_window)):\n        if i % 2 == 0:\n            hann_index = floor(i / 2)\n            hann_coefficient = cos(2 * math.pi * hann_index / (2 * window_len - 1)) - 0.5\n            hann_window[i] = hann_coefficient\n    hann_window = hann_window[::2] + hann_window[1::2]\n    hann_window = hann_window.tolist()\n    return hann_window"}
{"task_id": 433, "completion_id": 0, "solution": ""}
{"task_id": 435, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators: int=10, learning_rate: float=0.1) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    if len(y.shape) != 1:\n        raise ValueError('Y array must have one column')\n    (num_samples, num_features) = X.shape\n    initial_prediction = np.mean(y)\n    output = [initial_prediction]\n    for _ in range(n_estimators):\n        residuals = y - np.dot(X.T, X)\n        coefficients = np.linalg.lstsq(X, residuals)[0]\n        weak_learner_prediction = np.dot(coefficients, X)\n        updated_prediction = initial_prediction + learning_rate * weak_learner_prediction\n        output.append(updated_prediction)\n    return output\nX = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\ny = [1.5, 2.5, 3.5]"}
{"task_id": 437, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x, order: int=0):\n    if isinstance(x, np.ndarray):\n        x_list = [np.array(item).tolist() if isinstance(item, np.ndarray) else item for item in x]\n        flat_x = np.concatenate([item.flatten() for item in x_list])\n        sigmas = np.vectorize(lambda v: 1 / (1 + np.exp(-v)))([flat_x])\n        sigmas_deltas = np.vectorize(lambda v: (1 - sigmas(v)) * sigmas(v), signature='(),I', noalias=True)([flat_x])\n        sigmas_deltas = [round(np.sum(sigmas_deltas[:, i] * sigmas(deltas[:, i])), 4) for (i, deltas) in enumerate(zip(*sigmas_deltas[::-1]))]\n    elif isinstance(x, dict):\n        x_values = list(x.values())\n        x_flat = np.array(list(x.keys()))\n        x_list = [np.array(item).tolist() if isinstance(item, np.ndarray) else item for item in x_values]\n        flat_x = np.concatenate([item.flatten() for item in x_list])\n        sigmas = lambda x: 1 / (1 + np.exp(-x))\n        sigmas_deltas = lambda x: (1 - sigmas(x)) * sigmas(x)\n        sigmas_deltas = [round(np.sum(sigmas_deltas[:, i] * sigmas_deltas(deltas[:, i])), 4) for (i, deltas) in enumerate(zip(*sigmas_deltas[::-1]))]\n    elif isinstance(x, float) or isinstance(x, int):\n        sigmas = lambda x: 1 / (1 + np.exp(-x))\n        sigmas_deltas = lambda x: (1 - sigmas(x)) * sigmas(x)\n        sigmas_deltas = [round(np.sum(sigmas_deltas[:, i] * sigmas_deltas(deltas[:, i])), 4) for (i, deltas) in enumerate(zip(*sigmas_deltas[::-1]))]\n    else:\n        sigmas = lambda x: x\n        sigmas_deltas = lambda x: x\n        sigmas_deltas = [round(np.sum(sigmas_deltas[:, i] * sigmas_deltas(deltas[:, i])), 4) for (i, deltas) in enumerate(zip(*sigmas_deltas[::-1]))]\n    if order == 0:\n        return sigmas(x)\n    elif order == 1:\n        return sigmas_deltas(x)\n    elif order == 2:\n        return sigmas_deltas(x)"}
{"task_id": 438, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None=None) -> list[list[int]]:\n    if num_classes is not None and len(num_classes) < max(y) + 1:\n        return [-1]\n    num_columns = num_classes if num_classes is not None else max(y) + 1\n    y_encoded = [np.zeros(num_columns)] * len(y)\n    for (i, label) in enumerate(y):\n        y_encoded[i][label] = 1\n    return y_encoded.tolist()"}
{"task_id": 439, "completion_id": 0, "solution": "from collections import defaultdict\nimport operator\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    freq_items = {}\n    for transaction in transactions:\n        for item in set(transaction):\n            if item not in freq_items:\n                freq_items[item] = {'freq': 0}\n            freq_items[item]['freq'] += 1\n    supported_freq = [item for (item, _) in sorted(freq_items.items(), key=lambda x: (-x['freq'], x[0])) if x['freq'] >= min_sup]\n    result = []\n    seen = {tuple(sorted((v, k))): True for (k, v) in freq_items.items()}\n    while supported_freq:\n        next_supports = []\n        for sup in supported_freq:\n            candidate = frozenset(sup)\n            if candidate not in seen or seen[candidate]:\n                continue\n            for prev_supp in supported_freq:\n                if tuple(prev_supp - frozenset(sup)) in seen:\n                    break\n            else:\n                next_supports.append(sup)\n        result.extend(next_supports)\n        seen.update({tuple(sorted((v, k))) for (k, v) in freq_items.items()})\n    return result"}
{"task_id": 440, "completion_id": 0, "solution": "import numpy as np\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    flattened_predictions = [row_flat for sublist in predictions for row_flat in sublist]\n    overall_prediction = np.mean(flattened_predictions, axis=0)\n    max_abs_values = np.abs(overall_prediction)\n    max_indices = np.argwhere(max_abs_values == np.max(max_abs_values))\n    aggregated_probabilities = []\n    for (i, idx) in enumerate(max_indices):\n        if len(idx) > 0:\n            aggregated_probabilities.append(np.mean(flattened_predictions[idx], axis=0))\n    return (aggregated_probabilities, max_indices.tolist())"}
{"task_id": 444, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, sigma: float | list[float] | None=None) -> list[list[float]]:\n    \"\"\"\n    Compute the Radial Basis Function (RBF) kernel matrix.\n\n    Args:\n        X (list[list[int | float]]): Two dimensional arrays or lists representing the features of both sets of data.\n            Each row represents a set of feature values in the form [feature_1, feature_2, ...].\n        Y (list[list[int | float]]|None): Optional, the second list where the RBF kernel will be calculated against.\n            Defaults to None.\n        sigma (float|list[float]|None): An optional scale parameter for the RBF kernel. If none, defaults to sqrt(C/2).\n\n    Returns:\n        list[list[float]]: A list of length (N,M) or (N,N) representing the RBF kernel matrix. Entries are the results of applying the\n                           RBF kernel formula.\n    \"\"\"\n    if Y is None:\n        Y = []\n    if not isinstance(sigma, (int, float)) or len(Y) != len(X):\n        raise ValueError('sigma must be a positive scalar or a list of scalars.')\n    n_features = len(X[0])\n    if sigma is None:\n        sigma = np.sqrt(n_features / 2)\n    elif not isinstance(sigma, (int, float)):\n        raise ValueError('sigma must be a positive scalar or a list of scalars.')\n    rbf_matrix = []\n    for i in range(len(X)):\n        current_row = []\n        for j in range(n_features):\n            x_i = X[i][j]\n            y_j = Y[j][j]\n            normalized_x = x_i / np.linalg.norm(x_i)\n            squared_difference = (normalized_x - y_j) ** 2\n            squared_difference = np.exp(-0.5 * squared_difference * sigma ** 2)\n            current_row.append(squared_difference)\n        rbf_matrix.append(current_row)\n    return rbf_matrix"}
{"task_id": 446, "completion_id": 0, "solution": "import numpy as np\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"\n    Finds the best single-level split on a dataset using information gain.\n\n    Parameters:\n    - X: A 2D list (or numpy array) representing the dataset, where each row represents a data point.\n    - y: A 1D list (or numpy array) representing the target variable.\n\n    Returns:\n    - Tuple containing:\n      - best_feature_index: Index of the feature used for splitting.\n      - best_threshold: Threshold value used for splitting.\n      - left_majority_label: Label with the largest frequency among the samples in the left child.\n      - right_majority_label: Label with the largest frequency among the samples in the right child.\n    \"\"\"\n    if not isinstance(X, list) or any((not isinstance(row, list) for row in X)):\n        raise ValueError('Input data must contain lists.')\n    X = np.array(X)\n    y = np.array(y)\n    best_gain = -np.inf\n    best_split = None\n    best_indices = None\n    for i in range(len(X[0])):\n        for j in range(i + 1, len(X[0])):\n            features = X[:, [i, j]]\n            sorted_indices = np.argsort(y)[::-1]\n            while True:\n                threshold = np.median(features[j])\n                threshold_indices = sorted_indices[y == threshold]\n                if len(threshold_indices) < 2:\n                    continue\n                (left_count, right_count) = (np.bincount(threshold_indices).sum(), np.bincount(y[sorted_indices]).sum())\n                gain = features[j].mean() - left_count / float(np.sum(y)) * features[i].mean() - right_count / float(np.sum(y)) * features[j].mean()\n                if gain > best_gain:\n                    best_gain = gain\n                    best_indices = {j: threshold}\n                    best_split = (i, threshold)\n                if len(threshold_indices) == 2:\n                    break\n            if best_indices is not None:\n                break\n    return (tuple(best_indices.items()), best_gain, best_indices.get(best_split, None), best_indices.get(best_split, None))"}
{"task_id": 452, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\nfrom typing import *\nfrom sklearn.preprocessing import LabelEncoder\ndef divide_on_feature(X: np.ndarray | List[List[int]], feature_i: int, threshold: int) -> List[List[str]]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    subset_1 = [item for item in X if item[feature_i] >= threshold]\n    subset_2 = [item for item in X if item[feature_i] == threshold]\n    return [subset_1, subset_2]"}
{"task_id": 453, "completion_id": 0, "solution": "import numpy as np\ndef update_q_values(Q: 'np.ndarray', Q_next: 'np.ndarray', actions: list[int], rewards: list[float], dones: list[bool], gamma: float) -> list[list[float]]:\n    if len(Q.shape) != 2 or len(Q_next.shape) != 2:\n        return [-1]\n    non_terminal_mask = ~dones\n    y_current = Q * non_terminal_mask\n    y_next = Q_next * non_terminal_mask\n    y = np.where(dones, Q_next.max() * gamma, y_current).sum(axis=1)\n    return [y.tolist()]"}
{"task_id": 458, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None=None) -> list[list[int]]:\n    if isinstance(num_classes, int):\n        return [[label == i][i] for i in range(num_classes)]\n    elif num_classes is None:\n        return [list(range(max(labels) + 1))]\n    unique_labels = np.unique(labels)\n    labels_matrix = np.zeros((len(unique_labels), num_classes))\n    labels_matrix[np.arange(len(unique_labels)), unique_labels] = 1\n    return labels_matrix.tolist()"}
{"task_id": 461, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(x: list[float], y: list[float], n_estimators: int=200, learning_rate: float=0.5) -> list[float]:\n    \"\"\"\n    Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"\n    y_hat = []\n    while True:\n        y_hat.append(np.mean(y))\n        indices = np.random.permutation(len(y))\n        training_indices = indices[:-n_estimators]\n        validation_indices = indices[-n_estimators:]\n        for (i, j) in zip(training_indices, validation_indices):\n            training_x = x[i:i + n_estimators + 1]\n            training_y = y[i:i + n_estimators + 1]\n            val_error = sum((training_y - y_hat[j]) ** 2)\n            training_error_terms = [val_error]\n            prediction_term = learning_rate / len(training_x) * (y_hat[j] - training_y[j])\n            training_error_terms.append(val_error)\n            y_hat[y_hat == y_hat[j]] += prediction_term\n        if np.allclose(np.array(y_hat), np.array(y)):\n            break\n    return np.round(y_hat, 4).tolist()\nx = [-1, 0, 2, 3, 4]\ny = [1, 2, 3, 8, 9]"}
{"task_id": 471, "completion_id": 0, "solution": "import numpy as np\ndef expected_sarsa_update(q_table: list[list[float]], state: int, action: int, reward: float, next_state: int | None, epsilon: float, lr: float, gamma: float) -> list[list[float]]:\n    expect_next_state = expected_for_sarsa(next_state=next_state, epsilon=epsilon)\n    updated_q_values = 0\n    for i in range(len(q_table[0])):\n        contribution = reward\n        if not next_state == None:\n            contribution += q_table[next_state][action]\n        updated_q_values += expect_next_state[i] * q_table[state][action]\n    q_table[state][action] += lr * (updated_q_values - q_table[state][action])\n    return q_table\ndef expected_for_sarsa(next_state: int, epsilon: float) -> float:\n    num_actions = len(q_table[0])\n    if random.random() < epsilon:\n        return q_table[next_state][random.choice(range(num_actions))]\n    else:\n        pi_a = [sum((q_table[next_state][j] * exp for (j, exp) in enumerate(state))) for _ in range(num_actions)]\n        max_pi = max(pi_a)\n        return pi_a.index(max_pi) / len(pi_a)\nq_table = [[0.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 50.0]]"}
{"task_id": 474, "completion_id": 0, "solution": "import numpy as np\nimport random\nimport numpy as np\nimport random\ndef generate_data(m: int, n_train: int, n_val: int, interval: float, seed: int | None=None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to the nearest 4\u1d57\u02b0 decimal place.\n    \"\"\"\n    random.seed(seed)\n    X_train = []\n    X_val = []\n    Y_train = []\n    Y_val = []\n    min_coords = [i / 2 * 10 for i in range(10)]\n    max_coords = [i / 2 + 0.9 for i in range(10)]\n    for _ in range(n_train):\n        x = random.uniform(*min_coords)\n        y = random.uniform(*max_coords)\n        X_train.append([x, y])\n    for _ in range(n_val):\n        x = random.uniform(*min_coords)\n        y = random.uniform(*max_coords)\n        X_val.append([x, y])\n    (X_train, X_val) = (X_train[:n_train], X_val[:n_val])\n    (Y_train, Y_val) = (Y_train[:n_train], Y_val[:n_val])\n    return (np.array(X_train), np.array(X_val), np.array(Y_train), np.array(Y_val))\nn_train = 400\nn_val = 100"}
{"task_id": 475, "completion_id": 0, "solution": "import numpy as np\ndef adam_update(w, grad, m_prev=None, v_prev=None, t=1, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-08):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    m_t = grad if m_prev is None else b1 * m_prev + (1 - b1) * grad\n    v_t = v_prev if v_prev is not None else b2 * v_prev + (1 - b2) * grad ** 2\n    m\u0302_t = m_t / (1 - b1 ** t) ** (1 / (learning_rate - 1))\n    v\u0302_t = v_t / (1 - b2 ** t) ** (1 / (learning_rate - 1))\n    w_new = w - learning_rate * m\u0302_t / np.sqrt(v\u0302_t + eps)\n    return (w_new, m\u0302_t, v\u0302_t)"}
{"task_id": 479, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Optional\nimport numpy as np\ndef epsilon_greedy_bandit(rewards: List[List[float]], epsilon: float=0.05, ev_prior: float=0.5, seed: Optional[int]=None) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    n_arms = len(rewards[0])\n    v_estimates = [ev_prior] * n_arms\n    pulls = [0] * n_arms\n    while True:\n        print(f'Time step {np.arange(n_arms)}:')\n        current_v = v_estimates.copy()\n        for t in range(len(rewards)):\n            u = np.random.uniform(0, 1)\n            if u < epsilon:\n                selected_arm = np.argmax(current_v)\n            else:\n                selected_arm = max(range(n_arms), key=lambda k: (-current_v[k], k))\n            reward = rewards[t][selected_arm]\n            current_v[selected_arm] = current_v[selected_arm] + (reward - current_v[selected_arm]) / pulls[selected_arm]\n        if np.allclose(v_estimates, v_estimates[-1]):\n            break\n        pulls = np.concatenate(pulls[:-1])\n    return round(np.array(v_estimates).astype(float), 4)"}
{"task_id": 481, "completion_id": 0, "solution": "import numpy as np\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"\n    Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        ids start from 0 and increase consecutively in discovery order.\n    \"\"\"\n    if len(data) == 0:\n        return []\n    n_samples = data.shape[0]\n    cluster_id = [-1] * n_samples\n    cluster_labels = [0] * n_samples\n    centers = []\n    for i in range(n_samples):\n        x = data[i, 0]\n        y = data[i, 1]\n        center = np.array([x, y])\n        cluster_labels[i] = centers.index(center)\n        centers.append(cluster_labels[i])\n    for _ in range(5):\n        changes = False\n        new_centers = []\n        for i in range(n_samples):\n            count_neighbors = np.count_nonzero(data[i].reshape(-1, 1) <= eps) - 1\n            neighbors = set()\n            for j in range(i + 1, n_samples):\n                if j < n_samples:\n                    neighbor = data[j][i]\n                    if neighbor != data[j][0]:\n                        neighbors.add(neighbor)\n            for neighbor in list(neighbors):\n                if (counts := sum(((neighbor > eps) & (data[j][0] < eps) for j in range(n_samples)))):\n                    counts -= count_neighbors // 2\n            if count_neighbors >= min_samples and counts == 0:\n                new_center = np.mean(np.where(counts == 1, data[i], 0))\n            else:\n                new_center = centers[i]\n            if data[i] > eps:\n                change_label = cluster_labels[i]\n            elif new_center > eps:\n                change_label = cluster_labels[i] + 1\n            else:\n                change_label = -1\n            if change_label != cluster_labels[i]:\n                changes = True\n                new_centers.append(change_label)\n            cluster_labels[i] = new_center\n        if changes:\n            centers = new_centers\n    result = []\n    for (i, label) in enumerate(cluster_labels):\n        if label != -1:\n            result.append(label)\n    return result"}
{"task_id": 482, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y, num_classes=None):\n    y_ones = np.eye(num_classes)[y]\n    return y_ones"}
{"task_id": 485, "completion_id": 0, "solution": "import numpy as np\ndef regression_tree_predict(X_train: list[float | int], y_train: list[float | int], X_test: list[float | int], max_depth: int=3) -> list[float]:\n    \"\"\"\n    Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n\n    def calculate_sse(current_split_point):\n        total_error = 0\n        for i in range(1, len(y_train)):\n            error = sum((y_train[i] - Y_tree[i]) ** 2)\n            if i != len(y_train) - 1:\n                error += sum((Y_tree[i + 1] - Y_tree[i]) ** 2)\n            total_error += error\n        return total_error / float(len(y_train) - 1)\n    predictions = [None] * len(X_train)\n\n    def recursive_splitting(current_split_point):\n        y_values_in_left_subtree = []\n        y_values_in_right_subtree = []\n        for j in range(1, len(X_train)):\n            if X_train[j] > current_split_point:\n                y_values_in_right_subtree.append(y_train[j])\n            else:\n                y_values_in_left_subtree.append(y_train[j])\n        if len(y_train) == 1:\n            Y_tree = [np.mean(y_train)]\n        elif len(y_values_in_left_subtree) < 2:\n            Y_tree = [np.mean(y_values_in_left_subtree)]\n        else:\n            mid_index = len(y_values_in_left_subtree) // 2\n            Y_tree[0] = np.mean(y_values_in_left_subtree[:mid_index])\n            Y_tree[1] = np.mean(y_values_in_right_subtree[mid_index:])\n        if len(y_train) == 2:\n            predictions[0] = calculate_sse(current_split_point)\n        else:\n            recursive_splitting(min(current_split_point - 1, x[1])) if current_split_point - 1 >= min(x[1], X_train[-1]) else None\n            recursive_splitting(max(current_split_point + 1, x[1])) if current_split_point + 1 <= X_train[-1] else None\n        return Y_tree\n    X_train = np.array(X_train)\n    Y_tree = np.array([None] * len(X_train))\n    recursive_splitting(max(X_train[-1]))\n    return predictions.tolist()"}
{"task_id": 490, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    if len(x) == 0:\n        return []\n    n = len(x)\n    diag_list = [[x[i] for i in range(n)] for _ in range(n)]\n    return diag_list.tolist()"}
{"task_id": 491, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nimport itertools\nimport operator\ndef fp_growth(transactions, min_sup):\n    counts = defaultdict(Counter)\n    for transactions in transactions:\n        for item in transactions:\n            counts[item].update(((key,) for key in transactions if key == item))\n    filtered_transactions = {item: tuple(sorted(count.items(), reverse=True)) for (item, count) in counts.items() if count.size() >= min_sup}\n    tree = {}\n\n    def _fp(tree, k, item):\n        if k < len(filtered_transactions) - 1:\n            _fp(tree, k + 1, item)\n        else:\n            tree[k] = item\n    _fp(tree, 0, '')\n\n    def generate_combinations(combination):\n        for i in range(len(combination)):\n            combination[i] = tuple(sorted(combination[i]))\n            yield combination\n    combinations = []\n    for pair in itertools.product(*filtered_transactions.values()):\n        sorted_pairs = sorted(pair)\n        comb = tuple((sorted(pairs) for pairs in combinations_for(sorted_pairs)))\n        combinations.append(comb)\n    while True:\n        new_tree = defaultdict(dict)\n        for comb in combinations:\n            new_key = tuple(sorted(comb[len(filtered_transactions):]))\n            if new_key not in tree or comb[:len(new_key)] != new_key:\n                for item in filtered_transactions[new_key]:\n                    new_tree[len(filtered_transactions)].setdefault(item, []).extend(comb)\n        tree = new_tree\n        max_unique_items = counter(tree).most_common()[0][1]\n        if max_unique_items == min_sup:\n            break\n    return [(k, tuple(sorted(v))) for (k, v) in sorted(tree.items()) if len(k) > 1]"}
{"task_id": 492, "completion_id": 0, "solution": "import numpy as np\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\"\"\"\n    if degree < 0:\n        return -1\n    polynomial_matrix = []\n    for i in range(len(X)):\n        current_row = []\n        for j in range(len(X[0])):\n            product = 1\n            for k in range(j):\n                product *= X[i][j] ** (deg - k)\n            current_row.append(product)\n        sorted_row = sorted(current_row, key=lambda x: len(str(x)))\n        polynomial_matrix.append(sorted_row)\n    return polynomial_matrix\nX = [[1, 2], [3, 4], [5, 6]]\ndegree = 3"}
{"task_id": 493, "completion_id": 0, "solution": "import numpy as np\ndef mse(y):\n    if len(y) == 0:\n        return -1\n    try:\n        mean_y = np.mean(y)\n    except Exception:\n        return -1\n    sum_squared_diffs = np.sum((y - mean_y) ** 2)\n    mse_value = sum_squared_diffs / len(y)\n    return round(mse_value, 4)"}
{"task_id": 496, "completion_id": 0, "solution": "import numpy as np\ndef apply_affine(x, slope=1.0, intercept=0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    y = slope * x + intercept\n    grad = [slope]\n    grad2 = [0]\n    for i in range(1, len(x)):\n        grad.append((slope - gradient) / x[i])\n        grad2.append(gradient * 2)\n    return (y, grad, grad2)"}
{"task_id": 499, "completion_id": 0, "solution": "import numpy as np\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    standardized_data = data - np.mean(data, axis=0)\n    cov_matrix = np.cov(standardized_data.T)\n    (e_values, e_vectors) = np.linalg.eig(cov_matrix)\n    eigenvectors_sorted = e_vectors[:, ::-1]\n    eigenvectors_sorted = eigenvectors_sorted[eigenvalues_sorted > 0].T\n    transformed_data = np.dot(eigenvectors_sorted[:k], standardized_data)\n    return round(transformed_data, 4)"}
{"task_id": 500, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 505, "completion_id": 0, "solution": "import numpy as np\ndef contrastive_divergence(X: np.ndarray, W: np.ndarray, hbias: np.ndarray, vbias: np.ndarray, learning_rate: float, k: int) -> list[list[float]]:\n    if X.size == 0:\n        return []\n    pos_grad = np.dot(vbias.T, np.dot(W, X))\n    neg_grad = np.dot(np.dot(hbias.T, X), np.sign(np.dot(W, X)))\n    for _ in range(k):\n        new_W = W + learning_rate * (pos_grad - neg_grad) / X.shape[0]\n        W_new = np.clip(new_W, 1e-08, 1 - 1e-08)\n        pos_grad = np.dot(vbias.T, np.dot(W_new, X))\n        neg_grad = np.dot(np.dot(np.sign(np.dot(W_new, X)), X), np.sign(np.dot(W_new, X)))\n    return W_new.tolist()\nX = np.array([[0.2, 0.7], [0.8, 0.1]])\nW = np.array([[0.6, 0.2], [0.4, 0.7]])"}
{"task_id": 509, "completion_id": 0, "solution": "import numpy as np\ndef chebyshev(x: list[float] | 'np.ndarray', y: list[float] | 'np.ndarray') -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n    \n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    max_diff = max((abs(xi - yi) for (xi, yi) in zip(x, y)))\n    round_result = round(max_diff, 4)\n    return round_result"}
{"task_id": 510, "completion_id": 0, "solution": "import numpy as np\ndef vae_loss(y: np.ndarray, y_pred: np.ndarray, t_mean: np.ndarray, t_log_var: np.ndarray) -> float:\n    reconst_loss = np.mean(np.log(y_pred) if np.any(y_pred > 0) else np.log(1 - y_pred))\n    kl_term = -0.5 * np.sum(1 + np.log(t_mean) - np.square(t_log_var)) / t_mean.shape[0]\n    total_loss = reconst_loss + kl_term\n    result = round(total_loss, 6)\n    return result"}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    if len(w) != len(V[0]):\n        raise ValueError(\"The length of 'w' and 'V' vectors must match.\")\n    y_pred = []\n    for sample in X:\n        y_square_diff_sum = np.sum(np.square(sample - np.mean(sample)))\n        dot_product_sum = np.dot(sample, V)\n        y_pred.append(y_square_diff_sum + 0.5 * dot_product_sum)\n        y_pred = np.round(y_pred, decimals=4)\n    return y_pred"}
{"task_id": 517, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    if isinstance(z, (int, float)):\n        return np.log(1 + np.exp(-z))\n    else:\n        z_array = np.array(z).astype(float)\n        sigmoid_values = [np.log(1 + np.exp(-val)) for val in z_array]\n        return np.round(np.mean(sigmoid_values, axis=0), decimals=4)"}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Callable\nimport numpy as np\nfrom typing import Callable\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is \n    larger than one, the array is assumed to be one-hot encoded and is replaced \n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n\n    def wrapper(actual: np.ndarray, predicted: np.ndarray) -> np.ndarray:\n        actual = actual.flatten()\n        if len(actual.shape) != 1:\n            raise ValueError('Actual array must have one dimension.')\n        predicted = predicted.flatten()\n        if len(predicted.shape) != 1:\n            raise ValueError('Predicted array must have one dimension.')\n        argmax_result = np.argmax(predicted, axis=1)\n        hided_array = np.zeros(len(actual), dtype=np.int)\n        hided_array[np.arange(len(actual)), argmax_result.flatten()] = 1\n        return hided_array\n    return wrapper"}
{"task_id": 520, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, list):\n        return [sigmoid_grad(item) for item in z]\n    elif isinstance(z, (int, float)):\n        return np.where(z <= 0, 1 - z, 1 - z)\n    else:\n        return (z * (1 - z)).round(4)\nz = np.array([0.5, 0.8, -0.7])"}
{"task_id": 528, "completion_id": 0, "solution": "import numpy as np\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int=100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is classified with `sign( w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b ) ` where `sign(z)` returns **1** when *z* \u2265 0 and **-1** otherwise.  \n    The grid spans the bounding box of *X* and contains *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n            If missing, *X* will default to [[x\u2081 , x\u2082] for i in range(len(X))].\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    if not all([len(row) == len(X[0]) for row in X]):\n        raise ValueError('All rows of X must have equal number of elements.')\n    X = np.array(X)\n    x1_min = min(X[:, 0])\n    x1_max = max(X[:, 0])\n    x2_min = min(X[:, 1])\n    x2_max = max(X[:, 1])\n    x1_plot = np.linspace(x1_min, x1_max, grid_n).reshape(grid_n, 1)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    y1_plot = np.zeros((grid_n, grid_n))\n    y2_plot = np.zeros((grid_n, grid_n))\n    for i in range(grid_n):\n        for j in range(grid_n):\n            x1_plot[i, j] = x1_plot[i, j] + (i + j) / grid_n * (x1_max - x1_min) / grid_n\n            x2_plot[i, j] = x2_plot[i, j] + (j + i) / grid_n * (x2_max - x2_min) / grid_n\n    y1_plot[grid_n // 2, :] = 0\n    y2_plot[:, grid_n // 2] = 0\n    pred_labels = np.sign(np.dot(W.T, np.column_stack((x1_plot.flatten(), x2_plot.flatten())) + b)).flatten()\n    return np.c_[y1_plot.flatten(), y2_plot.flatten(), pred_labels]"}
{"task_id": 537, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    mu_0 = []\n    mu_1 = []\n    sigma_0 = []\n    sigma_1 = []\n    for i in range(len(y_train)):\n        y_class = y_train[i]\n        if y_class == 0:\n            mu_0.append(X_train[y_train == 0].mean(axis=0))\n            sigma_0.append(np.sqrt(X_train[y_train == 0].std(axis=0)))\n        else:\n            mu_1.append(X_train[y_train == 1].mean(axis=0))\n            sigma_1.append(np.sqrt(X_train[y_train == 1].std(axis=0)))\n    p0_tpr = np.zeros(len(mu_0)) / len(y_train)\n    p1_tpr = np.ones(len(mu_1)) / len(y_train)\n    for i in range(len(p0_tpr)):\n        p0_tpr[i] += (y_train[i] == mu_0[i]) * ((y_train[i] == mu_0[i]) + (X_test[i] - mu_0[i]) ** 2 > 0)\n        p1_tpr[i] += (y_train[i] == mu_1[i]) * ((y_train[i] == mu_1[i]) + (X_test[i] - mu_1[i]) ** 2 > 0)\n    results = np.where(p0_tpr >= p1_tpr, 0, 1)\n    epsilon = 1e-09\n    results += [epsilon] if np.any(results) else [0]\n    return results.tolist()\ny_train = np.random.randint(0, 2, size=100)\nX_test = np.random.rand(20, 5)"}
{"task_id": 539, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Compute the binary cross-entropy loss between the predicted probabilities and the ground truth labels.\n\n    Parameters:\n    - AL: A NumPy array of shape (1, m), where m is the number of samples.\n    - Y: A NumPy array of shape (m,), where each element corresponds to the true class of each sample.\n    \n    Returns:\n    - The computed loss value which is a float between -15 and 15, inclusive.\n    \"\"\"\n    max_prob = 1 - 1e-15\n    min_prob = 1e-15\n    AL_sum = np.sum(AL)\n    if AL_sum == 0:\n        raise ValueError('All predictions must have at least one true positive.')\n    mask = Y != 0\n    AL -= AL[mask].mean()\n    losses = -(AL * np.log(Y[mask]) + (1 - Y[mask]).clip(min=0) * np.log(1 - Y[mask]))\n    loss = np.nansum(losses, axis=0)\n    loss = loss / AL_sum\n    loss = np.clip(loss, min_prob, max_prob)\n    return np.squeeze(loss)"}
{"task_id": 552, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 555, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 561, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef gmm_em(data: np.ndarray, k: int, max_iter: int=200, epsilon: float=0.0001) -> list[int]:\n    \"\"\"\n    Perform Expectation-Maximization (EM) on a Gaussian Mixture Model (GMM). \n    Initialize K Gaussian components, repeat E-step and M-step until reaching convergence.\n    \n    Parameters:\n    - data (numpy.ndarray): 2D array of shape (n_samples, n_features) representing the dataset.\n    - k (int): Number of Gaussian components.\n    - max_iter (int): Maximum number of iterations allowed for EM algorithm.\n    - epsilon (float): Tolerance for stopping criteria in E-step.\n    \n    Returns:\n    - list[int]: A list of length `n_samples` indicating the final cluster label of each sample.\n    \"\"\"\n    (m, n) = data.shape\n    prior_mean = np.zeros((k, n))\n    prior_covariance = np.zeros((k, k, n, n))\n    for _ in range(k):\n        (current_mean, current_covariance) = initialize_gmm(data)\n        while True:\n            log_probabilities = compute_log_probabilities(current_mean, current_covariance)\n            update_parameters(log_probabilities, prior_covariance)\n            (new_prior_mean, new_prior_covariance) = update_means_covariance(current_mean, current_covariance, log_probabilities)\n            updated_log_probabilities = compute_log_probabilities(new_prior_mean, new_prior_covariance)\n            if np.linalg.norm(updated_log_probabilities - log_probabilities) < epsilon:\n                break\n        assignments = assign_samples_to_component(updated_log_probabilities, prior_mean, prior_covariance)\n        return assignments\n\n    def initialize_gmm(data):\n        \"\"\"\n        Initialize the means and covariances of the GMM.\n        \"\"\"\n        (n_samples, _) = data.shape\n        num_components = len(np.unique(data[:, 0]))\n        means = np.mean(data, axis=0)\n        covariances = np.cov(data.T, rowvar=False)\n        prior_mean = np.array([means[0] for _ in range(num_components)])\n        prior_covariance = np.array(covariances[0])\n        return (means, covariances)\n\n    def compute_log_probabilities(current_mean, current_covariance):\n        \"\"\"\n        Compute the log-likelihood of the current state given the previous ones.\n        \"\"\"\n        batch_size = current_mean.shape[0]\n        log_probabilities = np.zeros(batch_size)\n        for i in range(batch_size):\n            (mean, covariance) = (current_mean[i], current_covariance[i])\n            log_probabilities += -0.5 * np.log(2 * np.pi) + np.sum(np.square(mean - np.mean(data[batch_size * i:batch_size * (i + 1)])))\n        return log_probabilities\n\n    def update_parameters(log_probabilities, prior_covariance):\n        \"\"\"\n        Update the means and covariances based on the computed log-likelihoods.\n        \"\"\"\n        (num_samples, batch_size) = log_probabilities.shape\n        num_components = len(np.unique(data[:, 0]))\n        for i in np.arange(num_components):\n            weighted_probabilities = np.exp(log_probabilities[i:batch_size * num_components + i].reshape(-1, 1, 1, 1))\n            updated_mean = weighted_probabilities / np.sum(weighted_probabilities)\n            updated_var = weight(mean=updated_mean, var=prior_covariance)\n            prior_mean[i:i + num_components] = updated_mean\n            prior_covariance[i:i + num_components] = updated_var\n\n    def assign_samples_to_component(updated_log_probabilities, prior_mean, prior_covariance):\n        \"\"\"\n        Assign samples to the component with the highest probability.\n        \"\"\"\n        (num_samples, batch_size) = updated_log_probabilities.shape\n        sorted_indices = np.argsort(updated_log_probabilities)[::-1][:num_samples]\n        assignments = [sorted_indices[i] for i in range(num_samples)]\n        return assignments\n    return assignments"}
{"task_id": 562, "completion_id": 0, "solution": "from sklearn.cluster import SpectralClustering\nimport numpy as np\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma=2.0, method='unnormalized') -> list[int]:\n    \"\"\"\n    Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    X = np.array(data)\n    if method == 'normalized':\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0)\n        X = (X - mean) / std\n    sim_matrix = np.exp(-gamma * np.dot(X.T, X))\n    pairwise_squared_distance = np.sum(sim_matrix ** 2, axis=-1)\n    similarity_matrix = sim_matrix / pairwise_squared_distance\n    D_inv_sqrt = np.eye(len(X)) + gamma * pairwise_squared_distance\n    L = np.diag(D_inv_sqrt) @ similarity_matrix\n    sc_model = SpectralClustering(n_clusters=n_cluster, affinity='precomputed', assign_labels='first')\n    cluster_labels = sc_model.fit_predict(L)\n    new_label_map = {idx: idx for idx in range(len(Y))}\n    res = [new_label_map[label] for label in cluster_labels]\n    return res"}
{"task_id": 563, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_dual_train(X_data: np.ndarray, y_data: np.ndarray, eta: float=1.0, max_iter: int=1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is a Python list. Round every number to 4 decimal places.\n    \"\"\"\n    (N, _) = X_data.shape\n    w = []\n    b = 0\n    for i in range(N):\n        X_data_i = X_data[i]\n        y_data_i = y_data[i]\n        g = sum((eta * alpha * X_data_i[j] * X_data_j[j] for (j, alpha) in enumerate(alpha) if y_data_j[j] == -y_data_i))\n        if g > 0:\n            alpha_i = 1 / g\n            b += theta * y_data_i\n            w.append([alpha_i])\n        else:\n            alpha_i = -eta / g\n            b += theta * -y_data_i\n            w.append([alpha_i])\n    w = [np.array(v).tolist() if isinstance(v, list) else v for v in w]\n    return (tuple(w), b)"}
{"task_id": 564, "completion_id": 0, "solution": "from typing import List, Tuple\ndef predict_sample(root: 'Node', features: list | tuple, task_type: str) -> int:\n    if task_type == 'classification':\n        return root.get_leaf()\n    elif task_type == 'regression':\n        return min(root.x, root.s)"}
{"task_id": 565, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    entropy_values = np.sum(X * np.log2(np.expand_dims(X, axis=0)), axis=0)\n    sorted_indices = np.argsort(-entropy_values)[::-1]\n    best_feature_index = int(sorted_indices[0][0])\n    info_gain = -np.cumsum(entropy_values)\n    return (best_feature_index, info_gain[-1])"}
{"task_id": 566, "completion_id": 0, "solution": "from typing import List\nimport numpy as np\nimport math\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> List[int]:\n    total_paths = 0\n    num_tree_splits = 0\n    min_path_length = float('inf')\n    threshold_min = 1 + math.sqrt(math.log(n_trees, 2))\n    threshold_max = 1 + math.sqrt(math.log(n_trees, 2)) - 1\n    if abs(threshold_max - threshold_min) > epsilon:\n        split_indices = np.array_split(range(len(data)), len(data) // 2)\n        left_data = data[split_indices]\n        right_data = data[~split_indices]\n        n_left = len(left_data)\n        n_right = len(right_data)\n        isolated_left = train_isolation_forest(left_data, n_trees, sample_size, threshold_min, epsilon)\n        isolated_right = train_isolation_forest(right_data, n_trees, sample_size, threshold_max, epsilon)\n        paths_left = compute_paths(isolated_left, n_left, threshold_min)\n        paths_right = compute_paths(isolated_right, n_right, threshold_max)\n        avg_path_left = np.mean(paths_left)\n        avg_path_right = np.mean(paths_right)\n        threshold_min = avg_path_left\n        threshold_max = avg_path_right\n        num_tree_splits += len(split_indices)\n        threshold_min /= 2\n        return detect_outliers(isolated_left, n_trees, sample_size, epsilon) + detect_outliers(isolated_right, n_trees, sample_size, epsilon)"}
{"task_id": 568, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\nimport numpy as np\nimport math\nfrom collections import defaultdict\nimport numpy as np\ndef maxent_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, epsilon: float=0.001, n_iter: int=100):\n    \"\"\"\n    Build the empirical distributions needed for the Generalized Iterative Scaling (GIS)\n    maximum entropy learning process.\n\n    Parameters:\n    - X_train (np.ndarray): Training samples, shape (num_samples_train, num_features).\n    - y_train (np.ndarray): Corresponding labels, shape (num_samples_train,).\n\n    Returns:\n    - weights (np.ndarray): Weights matrix where each entry [i, j] represents the weight\n      of feature index i and label j.\n    \"\"\"\n    W = np.zeros((len(X_train), len(y_train)))\n    for i in range(len(X_train)):\n        for j in range(len(y_train)):\n            if j != y_train[i]:\n                W[i][j] += 1\n    W /= np.sum(W, axis=1, keepdims=True)\n    for _ in range(n_iter):\n        new_weights = np.zeros_like(W)\n        for i in range(len(X_train)):\n            temp_sum = 0\n            for j in range(len(y_train)):\n                if j != y_train[i]:\n                    weight = W[i][j]\n                    new_weight = weight\n                    for k in range(len(X_train)):\n                        if X_train[k] == X_train[i] and y_train[j] == y_train[k]:\n                            new_weight *= W[k][j]\n                    if new_weight > 0:\n                        temp_sum += 1\n                    else:\n                        break\n            new_weights[i] = new_weight / temp_sum\n        W = new_weights\n    predictions = []\n    for x_test in X_test:\n        prediction = np.argmax(np.exp(np.dot(w.T, x_test)), axis=1)\n        predictions.append(prediction.item())\n    return predictions"}
{"task_id": 569, "completion_id": 0, "solution": "from typing import List, Set\nimport collections\ndef statistic_key_word(data: List[Tuple[Any, Iterable[str]]], cut_off: int | None=None) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    word_freqs = collections.Counter((words for (_, words) in data))\n    frequencies = sorted(word_freqs.items(), key=lambda x: -x[1])[:cut_off]\n    freq_set = {word.lower() for word in word_freqs}\n    for (word, freq) in frequencies:\n        freq /= len(data)\n        freq *= 0.0001\n    result = []\n    for (word, freq) in frequencies:\n        result.append((word, freq))\n    return (result, freq_set)\ndata = [('text1', ['word1', 'word2']), ('text2', ['another_word1', 'another_word2'])]"}
{"task_id": 571, "completion_id": 0, "solution": "import numpy as np\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    w_list = [0] * X.shape[1]\n    b = np.sum(alpha * y)\n    for i in range(X.shape[1]):\n        for j in range(X.shape[0]):\n            if y[j] == -1:\n                w_list[i] += X[j, i] * alpha[j]\n            else:\n                w_list[i] -= X[j, i] * alpha[j]\n    w_list = [round(item, 4) for item in w_list]\n    b = round(b, 4)\n    return (w_list, b)"}
{"task_id": 572, "completion_id": 0, "solution": "import numpy as np\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Calculate the Local Outlier Factor (LOF) for each sample in the specified\n    neighborhood size 'k'. Returns a tuple containing the scores for each sample\n    sorted by their LOF value and then indices of all samples with LOF > epsilon.\n\n    Parameters:\n    - data: A numpy array representing the dataset.\n    - k: An integer indicating the neighborhood size.\n    - epsilon: A float indicating the tolerance level.\n\n    Returns:\n    - A tuple containing two lists: one sorted by LOF values in descending order,\n      and another list of indices corresponding to samples having LOF > epsilon.\n    \"\"\"\n\n    def euclidean_distance(point1, point2):\n        \"\"\"\n        Compute the Euclidean distance between two points in n-dimensional space.\n        \"\"\"\n        return ((point1 - point2) ** 2).sum() ** 0.5\n\n    def get_neighbors(point, neighbor_count):\n        \"\"\"\n        Get the neighbors of a given point within a specified radius from it in terms of their distances to the point itself.\n        \"\"\"\n        return [point[i] for i in range(neighbor_count)]\n\n    def compute_lof_score(points, epsilon, k):\n        \"\"\"\n        Compute the Local Outlier Factor (LOF) for each sample in the specified neighborhood size 'k'.\n        Return a dictionary mapping each sample's index back to its LOF score.\n        \"\"\"\n        lofs = {}\n        for p in points:\n            distances = {get_neighbors(p, k)[i]: k / sum(distances.values()) for (i, dist) in enumerate(euclidean_distance(p, points))}\n            distances.pop(distance[0], None)\n            lofs[p] = max(distances, key=lambda x: abs(x))\n        return lofs\n    num_samples = len(data)\n    indices = []\n    scores = []\n    for p in data:\n        lof_scores = compute_lof_score([data[i] for i in range(num_samples)], epsilon, k)\n        scores.append(lof_scores)\n        indices.append(list(lof_scores.keys()))\n    sorted_indices = sorted(indices, key=lambda x: scores[x])\n    filtered_indices = [sorted_indices[i] for i in indices if scores[i] > epsilon]\n    return (tuple(sorted_indices), filtered_indices)"}
{"task_id": 574, "completion_id": 0, "solution": "import numpy as np\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None=None) -> tuple:\n    \"\"\"\n    Generate synthetic 2-D data for m classes.\n\n    Parameters:\n    - m : int\n        Number of classes.\n    - n_train : int\n        Points per class in the training set.\n    - n_val : int\n        Points per class in the validation set.\n    - seed : int | None, optional\n        Random seed for reproducibility. If None, uses the global random state.\n\n    Returns:\n    - Tuple of NumPy arrays (X_train, X_val, y_train, y_val).\n    \"\"\"\n    base = 10.0 * (m // 2)\n    (x_min, y_min) = (base + 5.0, base + 10.0)\n    (x_max, y_max) = (base + 9.0, base + 11.0)\n    X_train = np.zeros((m * n_train, 2), dtype=float)\n    Y_train = np.zeros((m * n_train,), dtype=int)\n    X_val = np.zeros((m * n_val, 2), dtype=float)\n    Y_val = np.zeros((m * n_val,), dtype=int)\n    for i in range(m):\n        x = base + np.random.uniform(x_min, x_max)\n        y = base + np.random.uniform(y_min, y_max)\n        y[i] = (i + 1) % m\n        X_train[i * n_train:(i + 1) * n_train, :] = [x, y]\n        Y_train[i * n_train:] = y\n        val_indices = np.random.randint(0, n_val, size=n_val, dtype=np.int64)\n        X_val[val_indices, :] = [np.nan, np.nan]\n        Y_val[val_indices] = y[val_indices]\n    X_train = X_train.round(4).astype(float)\n    X_val = X_val.round(4).astype(float)\n    Y_train = Y_train.round(4).astype(int)\n    Y_val = Y_val.round(4).astype(int)\n    return (X_train, X_val, Y_train, Y_val)"}
{"task_id": 579, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(X: np.ndarray, k: int, max_iters: int=100) -> tuple[list[int], list[list[float]]]:\n    if not isinstance(k, int) or k < 1 or k > X.shape[1]:\n        raise ValueError('k must be a positive integer between 1 and the number of dimensions in X')\n    if not isinstance(max_iters, int) or max_iters <= 0:\n        raise ValueError('max_iters must be a positive integer')\n    if X.shape[0] != X.shape[1]:\n        raise ValueError('X must be a square matrix')\n    labels = []\n    centroids = []\n    for _ in range(max_iters):\n        current_labels = [np.argmin(np.linalg.norm(X[:, i] - centroid, axis=1)) for (i, centroid) in enumerate(centroids)]\n        unique_indices = set()\n        for (index, value) in enumerate(current_labels):\n            if value in unique_indices:\n                continue\n            else:\n                unique_indices.add(value)\n            labels.append(index)\n            centroids.append(np.mean(X[:, unique_indices], axis=0))\n        if len(set(labels)) == len(labels):\n            break\n        centroids = labels.copy()\n    return (labels, centroids.tolist())"}
{"task_id": 581, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int, l1_penalty: float, l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"\n    Fits a linear model with Elastic-Net regularisation.\n\n    Parameters:\n    - X: A 2-D NumPy array of shape (m, n) containing the input features.\n    - y: A 1-D NumPy array of length m containing the target values.\n    - learning_rate: Step size for gradient descent (\u03b1).\n    - iterations: Number of optimisation steps to perform.\n    - l1_penalty: L1 regularisation strength (\u03bb\u2081).\n    - l2_penalty: L2 regularisation strength (\u03bb\u2082).\n    \n    Returns:\n    A tuple (weights, bias), where weights is a list of length n and bias is a float.\n    Each number in the tuple is rounded to 4 decimal places.\n    \"\"\"\n    n = len(X[0])\n    m = len(y)\n    weights = [0.0] * n\n    bias = 0.0\n    for _ in range(iterations):\n        predictions = np.dot(X, weights) + bias\n        residuals = y - predictions\n        grad_l1 = -(residuals @ weights.T * 1.0 / m) / m * -l1_penalty\n        grad_b = -np.sum(residuals) / m\n        weights -= learning_rate * (grad_l1 + grad_b)\n        bias += learning_rate * residuals.mean()\n    return (weights, round(bias, 4))"}
{"task_id": 591, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 595, "completion_id": 0, "solution": "import numpy as np\ndef kmeans_centroids(X: 'np.ndarray', k: int, max_iters: int=100) -> list[list[float]]:\n    \"\"\"\n    Implement K-Means Clustering Algorithm from Scratch\n    \n    Parameters:\n    X (np.ndarray): A 2-D numpy array representing the data points.\n    k (int): The number of clusters to form.\n    max_iters (int): Maximum number of iterations for K-Means optimization.\n    \n    Returns:\n    result (List[List[float]]): A list of clusters with their corresponding centroids.\n    \"\"\"\n    centroids = X[np.random.choice(len(X), size=k, replace=False)]\n    result = [[centroids[0]], [centroids[0]]]\n    for _ in range(max_iters):\n        distances = np.linalg.norm(X - centroids, axis=1)\n        new_centroids = np.mean(X, axis=0)\n        if np.all(np.isclose(distances, distances)):\n            break\n        assignments = np.argmin(np.sum((X - new_centroids) ** 2, axis=1))\n        centroids = np.array([new_centroids[assignments], new_centroids[assignments]])\n    return [list(i) for i in centroids.tolist()]"}
