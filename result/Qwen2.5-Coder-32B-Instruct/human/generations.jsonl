{"task_id": 1, "completion_id": 0, "completion": "To solve the problem of computing the dot product of a matrix and a vector, we need to ensure that the number of columns in the matrix matches the length of the vector. If this condition is met, we can proceed to compute the dot product for each row of the matrix with the vector. Here's how you can implement this in Python:\n\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the matrix and vector dimensions are compatible\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result vector\n    result = []\n    \n    # Iterate over each row in the matrix\n    for row in a:\n        # Compute the dot product of the current row with the vector\n        dot_product = sum(x * y for x, y in zip(row, b))\n        # Append the result to the result vector\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\n# matrix = [[1, 2, 3], [4, 5, 6]]\n# vector = [7, 8, 9]\n# result = matrix_dot_vector(matrix, vector)\n# print(result)  # Output: [50, 122]\n```\n\n### Explanation:\n1. **Dimension Check**: We first check if the matrix `a` or vector `b` is empty, or if the number of columns in `a` (i.e., `len(a[0])`) does not match the length of `b`. If any of these conditions are true, we return `-1` to indicate that the operation is not valid.\n\n2. **Dot Product Calculation**: If the dimensions are compatible, we initialize an empty list `result` to store the dot products of each row with the vector.\n\n3. **Iterate and Compute**: We iterate over each row in the matrix `a`. For each row, we compute the dot product by multiplying corresponding elements of the row and the vector, and then summing these products. This is done using a generator expression within the `sum()` function.\n\n4. **Store Results**: The computed dot product for each row is appended to the `result` list.\n\n5. **Return Result**: Finally, we return the `result` list, which contains the dot products of each row of the matrix with the vector.\n\nThis function efficiently handles the matrix-vector dot product operation and ensures that the input dimensions are compatible before performing any calculations."}
{"task_id": 2, "completion_id": 0, "completion": "Certainly! To compute the transpose of a given matrix, you need to swap the rows with columns. Here's how you can implement this in Python:\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if the matrix is empty\n    if not a or not a[0]:\n        return []\n    \n    # Use list comprehension to transpose the matrix\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    \n    return transposed\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\ntransposed_matrix = transpose_matrix(matrix)\nprint(transposed_matrix)\n```\n\n### Explanation:\n- The function `transpose_matrix` takes a matrix `a` as input, which is a list of lists containing integers or floats.\n- It first checks if the matrix is empty or if the first row is empty. If so, it returns an empty list.\n- It then uses a nested list comprehension to create the transposed matrix. The outer list comprehension iterates over the columns of the original matrix, and the inner list comprehension iterates over the rows, effectively swapping rows with columns.\n- Finally, it returns the transposed matrix.\n\nThis function will work for any rectangular matrix, not just square matrices."}
{"task_id": 3, "completion_id": 0, "completion": "To solve the problem of reshaping a given matrix into a specified shape, we can leverage the capabilities of the NumPy library. NumPy provides a convenient function called `reshape` that can be used to change the shape of an array. If the reshaping is not possible (i.e., the total number of elements does not match the product of the new shape dimensions), NumPy will raise a `ValueError`. We can catch this exception and return an empty list in such cases.\n\nHere's the implementation of the `reshape_matrix` function:\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    try:\n        # Convert the input list to a NumPy array\n        np_array = np.array(a)\n        \n        # Reshape the array to the new shape\n        reshaped_array = np_array.reshape(new_shape)\n        \n        # Convert the reshaped NumPy array back to a list of lists\n        return reshaped_array.tolist()\n    except ValueError:\n        # If reshaping is not possible, return an empty list\n        return []\n\n# Example usage:\n# matrix = [[1, 2, 3], [4, 5, 6]]\n# new_shape = (3, 2)\n# print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2], [3, 4], [5, 6]]\n```\n\n### Explanation:\n1. **Convert Input to NumPy Array**: We first convert the input list `a` into a NumPy array using `np.array(a)`.\n2. **Reshape the Array**: We attempt to reshape the NumPy array to the specified `new_shape` using `np_array.reshape(new_shape)`.\n3. **Convert Back to List**: If the reshaping is successful, we convert the reshaped NumPy array back to a list of lists using the `tolist()` method.\n4. **Handle Errors**: If the reshaping is not possible (due to a mismatch in the total number of elements), a `ValueError` is raised. We catch this exception and return an empty list `[]`.\n\nThis approach ensures that the function handles both successful reshaping and cases where reshaping is not feasible."}
{"task_id": 4, "completion_id": 0, "completion": "Certainly! Below is the Python function `calculate_matrix_mean` that calculates the mean of a matrix either by row or by column, based on the specified mode.\n\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if not matrix or not matrix[0]:\n        raise ValueError(\"Matrix must not be empty and must have at least one row and one column.\")\n    \n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n    \n    if mode == 'row':\n        # Calculate mean for each row\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        # Calculate mean for each column\n        num_columns = len(matrix[0])\n        means = [sum(matrix[row][col] for row in range(len(matrix))) / len(matrix) for col in range(num_columns)]\n    \n    return means\n\n# Example usage:\n# matrix = [\n#     [1, 2, 3],\n#     [4, 5, 6],\n#     [7, 8, 9]\n# ]\n# print(calculate_matrix_mean(matrix, 'row'))    # Output: [2.0, 5.0, 8.0]\n# print(calculate_matrix_mean(matrix, 'column')) # Output: [4.0, 5.0, 6.0]\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the matrix is empty or if it has no columns, raising a `ValueError` if so. It also checks if the mode is either 'row' or 'column'.\n- **Row Mean Calculation**: If the mode is 'row', it calculates the mean for each row by summing the elements of the row and dividing by the number of elements in the row.\n- **Column Mean Calculation**: If the mode is 'column', it calculates the mean for each column by summing the elements of the column across all rows and dividing by the number of rows.\n- **Return Value**: The function returns a list of means, either for rows or columns, depending on the specified mode."}
{"task_id": 5, "completion_id": 0, "completion": "Certainly! Below is the Python function that performs scalar multiplication on a matrix. The function takes a matrix (a list of lists of integers or floats) and a scalar (an integer or float), and returns the resulting matrix after multiplication.\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    # Create a new matrix to store the result\n    result = []\n    \n    # Iterate over each row in the matrix\n    for row in matrix:\n        # Multiply each element in the row by the scalar and add to the result\n        result.append([element * scalar for element in row])\n    \n    return result\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nscalar = 3\n\nresult = scalar_multiply(matrix, scalar)\nprint(result)\n```\n\n### Explanation:\n- The function `scalar_multiply` takes two parameters: `matrix` and `scalar`.\n- It initializes an empty list `result` to store the resulting matrix.\n- It iterates over each row in the input `matrix`.\n- For each row, it creates a new list where each element is the product of the original element and the `scalar`.\n- This new list (row) is then appended to the `result`.\n- Finally, the function returns the `result` matrix.\n\nYou can test the function with different matrices and scalars to ensure it works as expected."}
{"task_id": 6, "completion_id": 0, "completion": "To calculate the eigenvalues of a 2x2 matrix, we can use the characteristic equation of the matrix. For a 2x2 matrix:\n\n\\[ A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\]\n\nThe characteristic equation is given by:\n\n\\[ \\text{det}(A - \\lambda I) = 0 \\]\n\nWhere \\( I \\) is the identity matrix and \\( \\lambda \\) represents the eigenvalues. This results in the quadratic equation:\n\n\\[ \\lambda^2 - (a + d)\\lambda + (ad - bc) = 0 \\]\n\nThe solutions to this quadratic equation are the eigenvalues of the matrix. We can solve this using the quadratic formula:\n\n\\[ \\lambda = \\frac{(a + d) \\pm \\sqrt{(a + d)^2 - 4(ad - bc)}}{2} \\]\n\nHere's the Python function to calculate the eigenvalues:\n\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the trace and determinant\n    trace = a + d\n    determinant = a * d - b * c\n    \n    # Calculate the discriminant\n    discriminant = trace**2 - 4 * determinant\n    \n    # Calculate the two eigenvalues\n    eigenvalue1 = (trace + math.sqrt(discriminant)) / 2\n    eigenvalue2 = (trace - math.sqrt(discriminant)) / 2\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage:\n# matrix = [[2, 1], [1, 2]]\n# print(calculate_eigenvalues(matrix))  # Output: [3.0, 1.0]\n```\n\nThis function first extracts the elements of the matrix, calculates the trace and determinant, and then uses these to find the eigenvalues using the quadratic formula. Finally, it returns the eigenvalues sorted from highest to lowest."}
{"task_id": 7, "completion_id": 0, "completion": "To solve this problem, we need to perform the matrix transformation \\( T^{-1}AS \\) where \\( T \\) and \\( S \\) are invertible matrices. Here's a step-by-step breakdown of the solution:\n\n1. **Validate Invertibility**: We need to check if matrices \\( T \\) and \\( S \\) are invertible. A matrix is invertible if its determinant is non-zero.\n\n2. **Compute Inverses**: If \\( T \\) and \\( S \\) are invertible, compute their inverses \\( T^{-1} \\) and \\( S^{-1} \\).\n\n3. **Perform Transformation**: Compute the product \\( T^{-1}AS \\).\n\n4. **Round Results**: Round all elements of the resulting matrix to the nearest 4th decimal place.\n\n5. **Return Result**: Convert the resulting matrix to a Python list using `tolist()`.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert input lists to numpy arrays\n    A = np.array(A, dtype=float)\n    T = np.array(T, dtype=float)\n    S = np.array(S, dtype=float)\n    \n    # Check if T and S are square matrices\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible by checking their determinants\n    det_T = np.linalg.det(T)\n    det_S = np.linalg.det(S)\n    \n    if det_T == 0 or det_S == 0:\n        return -1\n    \n    # Compute the inverses of T and S\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    \n    # Perform the transformation T^{-1}AS\n    result = T_inv @ A @ S\n    \n    # Round the result to the nearest 4th decimal place\n    result_rounded = np.round(result, 4)\n    \n    # Convert the result to a list\n    result_list = result_rounded.tolist()\n    \n    return result_list\n\n# Example usage:\n# A = [[1, 2], [3, 4]]\n# T = [[1, 0], [0, 1]]\n# S = [[1, 0], [0, 1]]\n# print(transform_matrix(A, T, S))\n```\n\n### Explanation:\n- **Conversion to Numpy Arrays**: We convert the input lists to numpy arrays for easier matrix operations.\n- **Square Matrix Check**: We ensure that \\( T \\) and \\( S \\) are square matrices since only square matrices can be invertible.\n- **Determinant Check**: We check the determinants of \\( T \\) and \\( S \\) to ensure they are non-zero, indicating invertibility.\n- **Matrix Inversion and Multiplication**: We compute the inverses and perform the matrix multiplication \\( T^{-1}AS \\).\n- **Rounding and Conversion**: We round the resulting matrix to 4 decimal places and convert it back to a list for the final output.\n\nThis function will return the transformed matrix if \\( T \\) and \\( S \\) are invertible, otherwise, it will return -1."}
{"task_id": 8, "completion_id": 0, "completion": "To calculate the inverse of a 2x2 matrix, we can use the formula for the inverse of a matrix:\n\nGiven a 2x2 matrix:\n\\[ A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\]\n\nThe inverse of matrix \\( A \\), denoted as \\( A^{-1} \\), is given by:\n\\[ A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix} \\]\n\nThe term \\( ad - bc \\) is the determinant of the matrix. If the determinant is zero, the matrix is not invertible.\n\nHere's the Python function to calculate the inverse of a 2x2 matrix:\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    determinant = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if determinant == 0:\n        return None\n    \n    # Calculate the inverse\n    inverse = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n    \n    return inverse\n\n# Example usage:\n# matrix = [[1, 2], [3, 4]]\n# print(inverse_2x2(matrix))  # Output: [[-2.0, 1.0], [1.5, -0.5]]\n```\n\nThis function first extracts the elements of the matrix, calculates the determinant, checks if the matrix is invertible, and if so, computes and returns the inverse matrix. If the matrix is not invertible, it returns `None`."}
{"task_id": 9, "completion_id": 0, "completion": "To multiply two matrices, the number of columns in the first matrix must be equal to the number of rows in the second matrix. If this condition is not met, the matrices cannot be multiplied, and the function should return -1. Here's how you can implement the `matrixmul` function:\n\n```python\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]]:\n    # Get the dimensions of the matrices\n    rows_a, cols_a = len(a), len(a[0])\n    rows_b, cols_b = len(b), len(b[0])\n    \n    # Check if the matrices can be multiplied\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\n# a = [[1, 2, 3], [4, 5, 6]]\n# b = [[7, 8], [9, 10], [11, 12]]\n# print(matrixmul(a, b))  # Output: [[58, 64], [139, 154]]\n```\n\n### Explanation:\n1. **Dimension Check**: The function first checks if the number of columns in matrix `a` is equal to the number of rows in matrix `b`. If not, it returns `-1` because matrix multiplication is not possible.\n2. **Result Matrix Initialization**: A result matrix is initialized with zeros. The dimensions of this matrix are determined by the number of rows in `a` and the number of columns in `b`.\n3. **Matrix Multiplication**: The function uses three nested loops to perform the matrix multiplication:\n   - The outer loop iterates over the rows of matrix `a`.\n   - The middle loop iterates over the columns of matrix `b`.\n   - The innermost loop performs the dot product of the row from `a` and the column from `b`, accumulating the result in the appropriate position in the result matrix.\n\nThis implementation ensures that the function correctly multiplies two matrices if possible, or returns `-1` if the matrices cannot be multiplied."}
{"task_id": 10, "completion_id": 0, "completion": "To calculate the covariance matrix for a given set of vectors, we need to follow these steps:\n\n1. **Center the Data**: Subtract the mean of each feature from each observation of that feature.\n2. **Compute the Covariance**: Use the centered data to compute the covariance between each pair of features.\n\nThe covariance between two features \\(X\\) and \\(Y\\) is given by:\n\\[ \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\]\nwhere \\(n\\) is the number of observations, \\(X_i\\) and \\(Y_i\\) are individual observations, and \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the means of \\(X\\) and \\(Y\\), respectively.\n\nHere's the implementation of the function along with test cases:\n\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    \n    # Convert the list of lists to a numpy array for easier manipulation\n    data = np.array(vectors)\n    \n    # Number of features\n    num_features = data.shape[0]\n    \n    # Number of observations\n    num_observations = data.shape[1]\n    \n    # Calculate the mean of each feature\n    means = np.mean(data, axis=1, keepdims=True)\n    \n    # Center the data\n    centered_data = data - means\n    \n    # Calculate the covariance matrix\n    covariance_matrix = (centered_data @ centered_data.T) / (num_observations - 1)\n    \n    # Convert the result back to a list of lists\n    return covariance_matrix.tolist()\n\n# Test cases\ndef test_calculate_covariance_matrix():\n    # Test case 1: Simple 2x2 matrix\n    vectors1 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    expected1 = [\n        [0.6666666666666666, 0.6666666666666666],\n        [0.6666666666666666, 0.6666666666666666]\n    ]\n    result1 = calculate_covariance_matrix(vectors1)\n    assert np.allclose(result1, expected1), f\"Test case 1 failed: {result1}\"\n\n    # Test case 2: 3x3 matrix\n    vectors2 = [\n        [1, 2, 3, 4],\n        [2, 3, 4, 5],\n        [3, 4, 5, 6]\n    ]\n    expected2 = [\n        [1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25]\n    ]\n    result2 = calculate_covariance_matrix(vectors2)\n    assert np.allclose(result2, expected2), f\"Test case 2 failed: {result2}\"\n\n    # Test case 3: Single feature\n    vectors3 = [\n        [1, 2, 3, 4, 5]\n    ]\n    expected3 = [\n        [2.0]\n    ]\n    result3 = calculate_covariance_matrix(vectors3)\n    assert np.allclose(result3, expected3), f\"Test case 3 failed: {result3}\"\n\n    # Test case 4: No variation in one feature\n    vectors4 = [\n        [1, 1, 1, 1],\n        [2, 3, 4, 5]\n    ]\n    expected4 = [\n        [0.0, 0.0],\n        [0.0, 1.6666666666666667]\n    ]\n    result4 = calculate_covariance_matrix(vectors4)\n    assert np.allclose(result4, expected4), f\"Test case 4 failed: {result4}\"\n\n    print(\"All test cases passed!\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\n### Explanation:\n- **Numpy**: We use numpy for efficient numerical operations.\n- **Centering**: We subtract the mean of each feature from its observations.\n- **Covariance Calculation**: We use matrix multiplication to compute the covariance matrix.\n- **Test Cases**: We include several test cases to ensure the function works correctly for different scenarios, including edge cases like a single feature or no variation in a feature."}
{"task_id": 11, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Jacobi method to solve a system of linear equations given by \\( Ax = b \\). The function iterates \\( n \\) times, rounding each intermediate solution to four decimal places, and returns the approximate solution \\( x \\) as a Python list.\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    # Get the number of equations\n    N = len(b)\n    \n    # Initialize the solution vector with zeros\n    x = np.zeros(N)\n    \n    # Perform n iterations\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            # Calculate the sum of the other terms\n            s = sum(A[i, j] * x[j] for j in range(N) if j != i)\n            # Update the value of x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        \n        # Round each element of the new solution to 4 decimal places\n        x_new = np.round(x_new, 4)\n        \n        # Update the solution vector\n        x = x_new\n    \n    # Convert the final solution to a list and return\n    return x.tolist()\n\n# Example usage:\n# A = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]])\n# b = np.array([4, 7, 3])\n# n = 10\n# print(solve_jacobi(A, b, n))\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the solution vector \\( x \\) with zeros.\n2. **Iteration**: For each iteration, we compute a new solution vector \\( x_{\\text{new}} \\).\n3. **Update Rule**: For each equation \\( i \\), we compute the sum of the contributions from all other variables and solve for \\( x_i \\) using the Jacobi update formula:\n   \\[\n   x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)} \\right)\n   \\]\n4. **Rounding**: After computing the new values, we round each element to four decimal places.\n5. **Return**: After \\( n \\) iterations, we convert the final solution vector to a list and return it.\n\nThis implementation assumes that the matrix \\( A \\) is diagonally dominant, which is a common requirement for the Jacobi method to converge."}
{"task_id": 12, "completion_id": 0, "completion": "Certainly! To perform Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method, we need to iteratively find the singular values and vectors. The Jacobian method involves finding the eigenvalues and eigenvectors of the matrix \\( A^T A \\) (or \\( A A^T \\)), which are related to the singular values and right (or left) singular vectors of \\( A \\).\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Ensure the input is a 2x2 matrix\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n    \n    # Compute A^T * A\n    ATA = np.dot(A.T, A)\n    \n    # Initial guess for the eigenvector\n    v = np.array([1.0, 0.0])\n    \n    # Tolerance for convergence\n    tol = 1e-10\n    \n    # Maximum number of iterations\n    max_iter = 1000\n    \n    # Power iteration to find the dominant eigenvector\n    for _ in range(max_iter):\n        v_new = np.dot(ATA, v)\n        v_new /= np.linalg.norm(v_new)\n        if np.linalg.norm(v_new - v) < tol:\n            break\n        v = v_new\n    \n    # The dominant eigenvalue is the square of the largest singular value\n    sigma1_squared = np.dot(v.T, np.dot(ATA, v))\n    sigma1 = np.sqrt(sigma1_squared)\n    \n    # The second singular value is the square root of the trace of ATA minus the largest singular value squared\n    trace_ATA = np.trace(ATA)\n    sigma2_squared = trace_ATA - sigma1_squared\n    sigma2 = np.sqrt(sigma2_squared)\n    \n    # Sort singular values in descending order\n    if sigma1 < sigma2:\n        sigma1, sigma2 = sigma2, sigma1\n        v = np.dot(ATA, v) / np.linalg.norm(np.dot(ATA, v))\n    \n    # Compute the left singular vectors\n    u1 = np.dot(A, v) / sigma1\n    u2 = np.array([-v[1], v[0]])  # Orthogonal to u1\n    u2 /= np.linalg.norm(u2)\n    \n    # Ensure the correct orientation of u2\n    if np.dot(np.dot(A, v), u2) < 0:\n        u2 *= -1\n    \n    # Form the U and V matrices\n    U = np.column_stack((u1, u2))\n    V = np.column_stack((v, u2))\n    \n    # Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    V = np.round(V, 4)\n    sigma1 = np.round(sigma1, 4)\n    sigma2 = np.round(sigma2, 4)\n    \n    return U, np.array([sigma1, sigma2]), V.T\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nU, S, Vt = svd_2x2_singular_values(A)\nprint(\"U:\\n\", U)\nprint(\"S:\\n\", S)\nprint(\"V^T:\\n\", Vt)\n```\n\n### Explanation:\n1. **Matrix \\( A^T A \\)**: We compute \\( A^T A \\) to find the eigenvalues and eigenvectors.\n2. **Power Iteration**: We use the power iteration method to find the dominant eigenvector of \\( A^T A \\).\n3. **Singular Values**: The square roots of the eigenvalues of \\( A^T A \\) are the singular values of \\( A \\).\n4. **Singular Vectors**: The eigenvectors of \\( A^T A \\) are the right singular vectors (columns of \\( V \\)). The left singular vectors (columns of \\( U \\)) are computed using \\( A \\) and the right singular vectors.\n5. **Orthogonality**: We ensure the orthogonality of the singular vectors.\n6. **Rounding**: All results are rounded to the nearest 4th decimal place.\n\nThis code will give you the SVD of a 2x2 matrix using the Jacobian method without relying on NumPy's built-in SVD function."}
{"task_id": 13, "completion_id": 0, "completion": "To calculate the determinant of a 4x4 matrix using Laplace's Expansion, we can recursively compute the determinants of the 3x3 minor matrices. Laplace's Expansion involves selecting a row or column, computing the cofactor for each element in that row or column, and then summing the products of the elements with their respective cofactors.\n\nHere's how you can implement this in Python:\n\n```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    def minor(matrix: list[list[int|float]], i: int, j: int) -> list[list[int|float]]:\n        \"\"\"Return the minor of the matrix after removing the i-th row and j-th column.\"\"\"\n        return [row[:j] + row[j+1:] for row in (matrix[:i] + matrix[i+1:])]\n\n    def determinant(matrix: list[list[int|float]]) -> float:\n        \"\"\"Recursively compute the determinant of a matrix.\"\"\"\n        if len(matrix) == 1:\n            return matrix[0][0]\n        elif len(matrix) == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        else:\n            det = 0\n            for c in range(len(matrix)):\n                det += ((-1) ** c) * matrix[0][c] * determinant(minor(matrix, 0, c))\n            return det\n\n    return determinant(matrix)\n\n# Example usage:\nmatrix = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix))  # Output should be 0, as this is a singular matrix\n```\n\n### Explanation:\n\n1. **Minor Calculation**: The `minor` function creates a new matrix by removing the specified row `i` and column `j` from the original matrix. This is used to compute the minor matrices needed for the cofactor expansion.\n\n2. **Base Cases**:\n   - For a 1x1 matrix, the determinant is simply the single element.\n   - For a 2x2 matrix, the determinant is calculated using the formula: `ad - bc`.\n\n3. **Recursive Case**:\n   - For larger matrices (3x3 and above), the determinant is computed by expanding along the first row. For each element in the first row, compute the cofactor (which is the element multiplied by the determinant of its minor matrix, with a sign based on its position).\n   - Sum these products to get the determinant of the matrix.\n\nThis implementation is recursive and handles the computation of determinants for 3x3 minor matrices as part of the 4x4 determinant calculation."}
{"task_id": 14, "completion_id": 0, "completion": "To solve the linear regression problem using the normal equation, we need to find the coefficients \\(\\theta\\) that minimize the cost function. The normal equation for linear regression is given by:\n\n\\[\n\\theta = (X^T X)^{-1} X^T y\n\\]\n\nWhere:\n- \\(X\\) is the matrix of features (with an additional column of ones for the intercept term).\n- \\(y\\) is the vector of target values.\n- \\(\\theta\\) is the vector of coefficients we want to find.\n\nHere's how you can implement this in Python:\n\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays for easier manipulation\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    \n    # Calculate the normal equation\n    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n    \n    # Round the result to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the result back to a list\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4]]\n# y = [3, 5, 7]\n# print(linear_regression_normal_equation(X, y))\n```\n\n### Explanation:\n1. **Convert Inputs**: We first convert the input lists to numpy arrays for easier matrix operations.\n2. **Add Intercept Term**: We add a column of ones to the feature matrix \\(X\\) to account for the intercept term in the linear model.\n3. **Normal Equation**: We compute the coefficients using the normal equation formula.\n4. **Rounding**: We round the resulting coefficients to four decimal places.\n5. **Return**: Finally, we convert the numpy array back to a list before returning it.\n\nThis function will correctly compute the linear regression coefficients using the normal equation and round them to the specified precision."}
{"task_id": 15, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `linear_regression_gradient_descent` function using NumPy. This function performs linear regression using gradient descent and returns the coefficients rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    # Initialize coefficients to zeros\n    m, n = X.shape\n    theta = np.zeros(n)\n    \n    # Perform gradient descent\n    for _ in range(iterations):\n        # Calculate the predictions\n        predictions = X.dot(theta)\n        \n        # Calculate the errors\n        errors = predictions - y\n        \n        # Update the coefficients\n        gradient = (1 / m) * X.T.dot(errors)\n        theta -= alpha * gradient\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert to list and return\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = np.array([[1, 1], [1, 2], [1, 3]])\n# y = np.array([1, 2, 3])\n# alpha = 0.01\n# iterations = 1000\n# print(linear_regression_gradient_descent(X, y, alpha, iterations))\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the coefficients `theta` to zeros. The shape of `theta` is determined by the number of features `n` in `X`.\n\n2. **Gradient Descent Loop**: We iterate for the specified number of iterations. In each iteration:\n   - We compute the predictions using the current coefficients.\n   - We calculate the errors between the predictions and the actual target values `y`.\n   - We compute the gradient of the cost function with respect to the coefficients.\n   - We update the coefficients by moving in the direction that reduces the cost function, scaled by the learning rate `alpha`.\n\n3. **Rounding**: After the iterations, we round the coefficients to four decimal places using `np.round`.\n\n4. **Return**: Finally, we convert the rounded coefficients to a list using `tolist()` and return it.\n\nThis function should work for any valid input arrays `X` and `y`, provided they are NumPy arrays and `X` includes a column of ones for the intercept term."}
{"task_id": 16, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `feature_scaling` function that performs both standardization and min-max normalization on a given 2D NumPy array. The results are rounded to the nearest 4th decimal and returned as Python lists.\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    \n    # Min-Max Normalization\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    \n    # Rounding to the nearest 4th decimal\n    standardized_data_rounded = np.round(standardized_data, 4)\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4)\n    \n    # Converting to list\n    standardized_data_list = standardized_data_rounded.tolist()\n    min_max_normalized_data_list = min_max_normalized_data_rounded.tolist()\n    \n    return standardized_data_list, min_max_normalized_data_list\n\n# Example usage:\n# data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# standardized, min_max_normalized = feature_scaling(data)\n# print(\"Standardized:\", standardized)\n# print(\"Min-Max Normalized:\", min_max_normalized)\n```\n\n### Explanation:\n1. **Standardization**:\n   - Calculate the mean and standard deviation for each feature (column).\n   - Subtract the mean and divide by the standard deviation for each element in the dataset.\n\n2. **Min-Max Normalization**:\n   - Find the minimum and maximum values for each feature.\n   - Subtract the minimum value and divide by the range (max - min) for each element in the dataset.\n\n3. **Rounding**:\n   - Use `np.round` to round the results to the nearest 4th decimal place.\n\n4. **Conversion to List**:\n   - Convert the NumPy arrays to Python lists using the `tolist()` method.\n\nThis function will handle any 2D NumPy array input and return the scaled data as specified."}
{"task_id": 17, "completion_id": 0, "completion": "Certainly! Below is the implementation of the k-Means clustering algorithm in Python, following the specified requirements:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the nearest centroid\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Update centroids to be the mean of the points assigned to them\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = np.round(centroids, 4)\n    \n    # Convert the numpy array back to a list of tuples\n    return [tuple(centroid) for centroid in final_centroids]\n\n# Example usage:\n# points = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\n# k = 2\n# initial_centroids = [(1.0, 2.0), (5.0, 8.0)]\n# max_iterations = 100\n# print(k_means_clustering(points, k, initial_centroids, max_iterations))\n```\n\n### Explanation:\n1. **Initialization**: The function starts by converting the input lists of points and initial centroids into NumPy arrays for efficient computation.\n2. **Assignment Step**: For each point, the algorithm calculates the distance to each centroid and assigns the point to the nearest centroid.\n3. **Update Step**: The centroids are updated to be the mean of all points assigned to them.\n4. **Convergence Check**: The algorithm checks if the centroids have changed. If not, it breaks out of the loop, indicating convergence.\n5. **Rounding**: The final centroids are rounded to the nearest fourth decimal place.\n6. **Output**: The centroids are converted back to a list of tuples before being returned.\n\nThis implementation ensures that the k-Means clustering is performed efficiently and the results are formatted as required."}
{"task_id": 18, "completion_id": 0, "completion": "Certainly! Below is a Python function that implements K-Fold Cross-Validation. This function will divide the dataset into `k` folds and return a list of train-test indices for each fold. The function also includes options to shuffle the data before splitting and to set a random seed for reproducibility.\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: np.ndarray, the feature dataset.\n    - y: np.ndarray, the target dataset.\n    - k: int, the number of folds.\n    - shuffle: bool, whether to shuffle the data before splitting.\n    - random_seed: int, optional, seed for random number generator for reproducibility.\n\n    Returns:\n    - A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if k < 2:\n        raise ValueError(\"Number of folds k must be at least 2.\")\n    \n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n    \n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    \n    fold_size = n_samples // k\n    remainder = n_samples % k\n    \n    folds = []\n    start = 0\n    \n    for i in range(k):\n        # Determine the end index for the current fold\n        end = start + fold_size + (1 if i < remainder else 0)\n        \n        # Create the test indices for the current fold\n        test_indices = indices[start:end]\n        \n        # Create the train indices for the current fold\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        \n        # Append the train-test indices as a tuple to the folds list\n        folds.append((train_indices, test_indices))\n        \n        # Update the start index for the next fold\n        start = end\n    \n    return folds\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n# y = np.array([0, 1, 0, 1, 0, 1])\n# folds = k_fold_cross_validation(X, y, k=3, shuffle=True, random_seed=42)\n# for train_idx, test_idx in folds:\n#     print(\"Train:\", train_idx, \"Test:\", test_idx)\n```\n\n### Explanation:\n- **Parameters**:\n  - `X`: The feature dataset.\n  - `y`: The target dataset.\n  - `k`: The number of folds.\n  - `shuffle`: Whether to shuffle the data before splitting.\n  - `random_seed`: Optional seed for the random number generator to ensure reproducibility.\n\n- **Process**:\n  - First, it checks if the number of folds `k` is at least 2 and if `X` and `y` have the same number of samples.\n  - It calculates the size of each fold and any remainder to handle cases where the dataset size is not perfectly divisible by `k`.\n  - It shuffles the indices if `shuffle` is `True`.\n  - It iterates over the number of folds, calculating the start and end indices for each fold, and then constructs the train and test indices.\n  - It returns a list of tuples, where each tuple contains the train and test indices for a fold.\n\nThis function should work well for most datasets and is flexible enough to handle different configurations of `k`, shuffling, and random seeds."}
{"task_id": 19, "completion_id": 0, "completion": "Certainly! Below is a Python function that implements Principal Component Analysis (PCA) from scratch. The function standardizes the input data, computes the covariance matrix, finds the eigenvalues and eigenvectors, and returns the top `k` principal components.\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the data\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    \n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Compute the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n    \n    # Step 4: Sort the eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Select the top k eigenvectors (principal components)\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Step 6: Round the principal components to the nearest 4th decimal\n    principal_components_rounded = np.round(principal_components, 4)\n    \n    # Convert to list of lists for the final output\n    return principal_components_rounded.tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [3, 4], [5, 6]])\n# k = 1\n# print(pca(data, k))\n```\n\n### Explanation:\n1. **Standardization**: The data is standardized by subtracting the mean and dividing by the standard deviation for each feature. This ensures that each feature contributes equally to the analysis.\n2. **Covariance Matrix**: The covariance matrix is computed to understand how the different features vary together.\n3. **Eigenvalues and Eigenvectors**: The eigenvalues and eigenvectors of the covariance matrix are computed. The eigenvectors represent the directions of maximum variance, and the eigenvalues represent the magnitude of this variance.\n4. **Sorting**: The eigenvalues and eigenvectors are sorted in descending order of eigenvalues to identify the principal components.\n5. **Selection**: The top `k` eigenvectors are selected as the principal components.\n6. **Rounding**: The principal components are rounded to the nearest 4th decimal for precision.\n\nThis function will return the principal components as a list of lists, rounded to four decimal places."}
{"task_id": 20, "completion_id": 0, "completion": "To implement a decision tree learning algorithm using recursive binary splitting based on entropy and information gain, we need to follow these steps:\n\n1. **Calculate Entropy**: Entropy is a measure of the impurity or randomness in the data. For a given set of examples, it is calculated as:\n   \\[\n   \\text{Entropy}(S) = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\n   \\]\n   where \\( p_i \\) is the proportion of examples that belong to class \\( i \\).\n\n2. **Calculate Information Gain**: Information gain is the reduction in entropy achieved by splitting the dataset on a particular attribute. It is calculated as:\n   \\[\n   \\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)\n   \\]\n   where \\( S_v \\) is the subset of \\( S \\) for which attribute \\( A \\) has value \\( v \\).\n\n3. **Recursive Splitting**: Recursively split the dataset on the attribute that provides the highest information gain until all examples in a node belong to the same class or there are no more attributes to split on.\n\n4. **Base Cases**: If all examples in a node belong to the same class, return a leaf node with that class. If there are no attributes left to split on, return a leaf node with the majority class of the examples.\n\nHere is the implementation of the decision tree learning algorithm:\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    def entropy(examples):\n        if not examples:\n            return 0\n        label_counts = Counter(example[target_attr] for example in examples)\n        total = len(examples)\n        return -sum((count / total) * math.log2(count / total) for count in label_counts.values())\n\n    def information_gain(examples, attribute):\n        total_entropy = entropy(examples)\n        value_counts = Counter(example[attribute] for example in examples)\n        total = len(examples)\n        weighted_entropy = 0\n        for value in value_counts:\n            subset = [example for example in examples if example[attribute] == value]\n            weighted_entropy += (len(subset) / total) * entropy(subset)\n        return total_entropy - weighted_entropy\n\n    def majority_class(examples):\n        label_counts = Counter(example[target_attr] for example in examples)\n        return label_counts.most_common(1)[0][0]\n\n    if not examples:\n        return {}\n\n    # If all examples have the same target value, return a leaf node with that value\n    first_label = examples[0][target_attr]\n    if all(example[target_attr] == first_label for example in examples):\n        return first_label\n\n    # If no attributes are left, return a leaf node with the majority class\n    if not attributes:\n        return majority_class(examples)\n\n    # Choose the attribute with the highest information gain\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr))\n\n    # Create a subtree for each value of the best attribute\n    tree = {best_attribute: {}}\n    for value in set(example[best_attribute] for example in examples):\n        subset = [example for example in examples if example[best_attribute] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n\n    return tree\n\n# Example usage:\n# examples = [\n#     {'color': 'red', 'size': 'small', 'label': 'apple'},\n#     {'color': 'red', 'size': 'large', 'label': 'cherry'},\n#     {'color': 'green', 'size': 'small', 'label': 'grape'},\n#     {'color': 'green', 'size': 'large', 'label': 'kiwi'}\n# ]\n# attributes = ['color', 'size']\n# target_attr = 'label'\n# tree = learn_decision_tree(examples, attributes, target_attr)\n# print(tree)\n```\n\nThis code defines a function `learn_decision_tree` that constructs a decision tree using the given examples, attributes, and target attribute. The tree is represented as a nested dictionary, where each key is an attribute, and each value is either a subtree or a class label."}
{"task_id": 21, "completion_id": 0, "completion": "To implement a deterministic version of the Pegasos algorithm for a kernel SVM, we need to follow these steps:\n\n1. **Kernel Function**: Define the kernel functions for both linear and RBF (Radial Basis Function) kernels.\n2. **Initialization**: Initialize the alpha coefficients and bias.\n3. **Update Rule**: For each iteration, update the alpha coefficients and bias using all samples.\n4. **Return Values**: Return the alpha coefficients and bias rounded to the nearest 4th decimal.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples, n_features = data.shape\n    \n    # Initialize alpha coefficients and bias\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    \n    # Define kernel functions\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n    \n    def rbf_kernel(x1, x2, sigma=sigma):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n    \n    # Choose the kernel function\n    if kernel == 'linear':\n        kernel_func = linear_kernel\n    elif kernel == 'rbf':\n        kernel_func = rbf_kernel\n    else:\n        raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'rbf'.\")\n    \n    # Pegasos algorithm\n    for t in range(1, iterations + 1):\n        # Learning rate\n        eta_t = 1 / (lambda_val * t)\n        \n        # Compute the gradient\n        sum_alpha_y_k = np.zeros(n_samples)\n        for i in range(n_samples):\n            for j in range(n_samples):\n                sum_alpha_y_k[i] += alphas[j] * labels[j] * kernel_func(data[i], data[j])\n        \n        # Update rule\n        for i in range(n_samples):\n            if labels[i] * (sum_alpha_y_k[i] + bias) < 1:\n                alphas[i] = (1 - eta_t * lambda_val) * alphas[i] + eta_t * labels[i]\n            else:\n                alphas[i] = (1 - eta_t * lambda_val) * alphas[i]\n        \n        # Update bias\n        sum_alpha_y = np.sum(alphas * labels)\n        bias = bias + eta_t * sum_alpha_y\n    \n    # Round the results to the nearest 4th decimal\n    alphas_rounded = np.round(alphas, 4)\n    bias_rounded = np.round(bias, 4)\n    \n    return alphas_rounded.tolist(), bias_rounded.tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])\n# labels = np.array([1, 1, 1, -1, -1])\n# alphas, bias = pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100)\n# print(\"Alphas:\", alphas)\n# print(\"Bias:\", bias)\n```\n\n### Explanation:\n- **Kernel Functions**: We define two kernel functions, `linear_kernel` and `rbf_kernel`, based on the input parameter `kernel`.\n- **Initialization**: We initialize `alphas` to zero and `bias` to zero.\n- **Gradient Computation**: For each sample, we compute the sum of `alpha * y * K(x_i, x_j)` for all samples `j`.\n- **Update Rule**: We update `alphas` and `bias` based on the Pegasos update rule. If the condition `y_i * (sum_alpha_y_k[i] + bias) < 1` is met, we update `alpha_i` accordingly.\n- **Rounding**: Finally, we round the `alphas` and `bias` to the nearest 4th decimal and return them as lists.\n\nThis implementation ensures that all samples are used in every iteration, as required by the deterministic version of the Pegasos algorithm."}
{"task_id": 22, "completion_id": 0, "completion": "Certainly! The sigmoid activation function is defined as:\n\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n\nHere's how you can implement this function in Python, ensuring the output is rounded to four decimal places:\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    # Calculate the sigmoid value\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    # Return the value rounded to four decimal places\n    return round(sigmoid_value, 4)\n\n# Example usage:\n# print(sigmoid(0))  # Output should be 0.5000\n# print(sigmoid(1))  # Output should be 0.7311\n# print(sigmoid(-1)) # Output should be 0.2689\n```\n\nThis function takes a float `z` as input, computes the sigmoid of `z`, and returns the result rounded to four decimal places. You can test the function with different values of `z` to see how it behaves."}
{"task_id": 23, "completion_id": 0, "completion": "Certainly! The softmax function is commonly used in machine learning, particularly in the output layer of neural networks for multi-class classification problems. It converts a list of scores (logits) into probabilities by exponentiating each score and then normalizing by the sum of these exponentiated values.\n\nHere's how you can implement the softmax function in Python:\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    # Calculate the maximum score to improve numerical stability\n    max_score = max(scores)\n    \n    # Exponentiate each score minus the max score for numerical stability\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    \n    # Calculate the sum of the exponentiated scores\n    sum_exp_scores = sum(exp_scores)\n    \n    # Calculate the softmax values\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    \n    # Round each value to four decimal places\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    \n    return softmax_values_rounded\n\n# Example usage:\n# scores = [2.0, 1.0, 0.1]\n# print(softmax(scores))  # Output: [0.6590, 0.2424, 0.0986]\n```\n\n### Explanation:\n1. **Numerical Stability**: Subtracting the maximum score from each score before exponentiation helps prevent overflow issues when the scores are large.\n2. **Exponentiation**: Each adjusted score is exponentiated.\n3. **Normalization**: The exponentiated scores are normalized by dividing by their sum to ensure they sum to 1.\n4. **Rounding**: Each softmax value is rounded to four decimal places for the desired precision.\n\nThis implementation should work efficiently and accurately for typical use cases involving the softmax function."}
{"task_id": 24, "completion_id": 0, "completion": "To solve this task, we need to implement a function that simulates a single neuron with a sigmoid activation function. The function will take in feature vectors, true binary labels, weights, and a bias. It will compute the predicted probabilities using the sigmoid function and calculate the mean squared error (MSE) between these predictions and the true labels.\n\nHere's the step-by-step implementation:\n\n1. **Sigmoid Function**: This function will take a weighted sum of inputs and return a value between 0 and 1.\n2. **Weighted Sum Calculation**: For each feature vector, compute the weighted sum of features and the bias.\n3. **Predicted Probabilities**: Apply the sigmoid function to the weighted sums to get the predicted probabilities.\n4. **Mean Squared Error Calculation**: Compute the MSE between the predicted probabilities and the true labels.\n5. **Rounding**: Ensure all results are rounded to four decimal places.\n6. **Return Values**: Return the predicted probabilities as a list and the MSE as a float.\n\nHere's the complete code:\n\n```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    def sigmoid(x: float) -> float:\n        return 1 / (1 + math.exp(-x))\n    \n    predicted_probabilities = []\n    mse = 0.0\n    \n    for feature_vector, label in zip(features, labels):\n        # Calculate the weighted sum\n        weighted_sum = sum(w * x for w, x in zip(weights, feature_vector)) + bias\n        # Apply sigmoid function to get the predicted probability\n        predicted_probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(predicted_probability)\n        # Calculate the squared error for this example\n        mse += (predicted_probability - label) ** 2\n    \n    # Calculate the mean squared error\n    mse /= len(labels)\n    \n    # Round the predicted probabilities and MSE to four decimal places\n    predicted_probabilities = [round(p, 4) for p in predicted_probabilities]\n    mse = round(mse, 4)\n    \n    return predicted_probabilities, mse\n\n# Example usage:\n# features = [[0.1, 0.2], [0.3, 0.4]]\n# labels = [0, 1]\n# weights = [0.5, 0.6]\n# bias = 0.1\n# print(single_neuron_model(features, labels, weights, bias))\n```\n\n### Explanation:\n- **Sigmoid Function**: `sigmoid(x)` computes the sigmoid of `x`.\n- **Weighted Sum**: For each feature vector, we compute the dot product of the weights and the feature vector, then add the bias.\n- **Predicted Probabilities**: The sigmoid of the weighted sum gives the predicted probability for each example.\n- **Mean Squared Error**: We accumulate the squared differences between predicted probabilities and true labels, then divide by the number of examples to get the MSE.\n- **Rounding**: We use list comprehensions to round the predicted probabilities and the MSE to four decimal places.\n- **Return**: The function returns the list of predicted probabilities and the MSE.\n\nThis function should work for any valid input of feature vectors, labels, weights, and bias, provided they are of compatible dimensions."}
{"task_id": 25, "completion_id": 0, "completion": "Certainly! Below is the implementation of a single neuron with sigmoid activation and backpropagation to update the weights and bias. The function takes the specified inputs and returns the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\ndef mean_squared_error(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n\n    for epoch in range(epochs):\n        # Forward pass\n        linear_combination = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_combination)\n        \n        # Calculate MSE\n        mse = mean_squared_error(labels, predictions)\n        mse_values.append(round(mse, 4))\n        \n        # Backward pass\n        error = predictions - labels\n        d_loss_d_pred = 2 * error / len(labels)\n        d_pred_d_z = sigmoid_derivative(predictions)\n        d_loss_d_z = d_loss_d_pred * d_pred_d_z\n        \n        # Update weights and bias\n        d_loss_d_weights = np.dot(features.T, d_loss_d_z)\n        d_loss_d_bias = np.sum(d_loss_d_z)\n        \n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n\n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\n# features = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# labels = np.array([0, 1, 1, 0])\n# initial_weights = np.array([0.1, 0.2])\n# initial_bias = 0.3\n# learning_rate = 0.1\n# epochs = 1000\n# updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n# print(\"Updated Weights:\", updated_weights)\n# print(\"Updated Bias:\", updated_bias)\n# print(\"MSE Values:\", mse_values)\n```\n\n### Explanation:\n1. **Sigmoid Activation**: The `sigmoid` function is used to introduce non-linearity.\n2. **Sigmoid Derivative**: The `sigmoid_derivative` function is used in the backpropagation step to compute the gradient.\n3. **Mean Squared Error (MSE)**: The `mean_squared_error` function calculates the loss.\n4. **Forward Pass**: Computes the linear combination of inputs and weights, applies the sigmoid activation, and calculates the predictions.\n5. **Backward Pass**: Computes the error, gradients, and updates the weights and bias using gradient descent.\n6. **Rounding**: All results are rounded to four decimal places as required.\n\nThis function should work for any binary classification problem with a single neuron."}
{"task_id": 26, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `Value` class that supports basic autograd operations: addition, multiplication, and ReLU activation. This class will handle scalar values and compute gradients through automatic differentiation.\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        # Topological sort all of the children in the graph\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # Go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example usage:\n# a = Value(2.0)\n# b = Value(-3.0)\n# c = Value(10.0)\n# d = a * b + c\n# d.relu()\n# d.backward()\n# print(a)  # Should print: Value(data=2.0, grad=-3.0)\n# print(b)  # Should print: Value(data=-3.0, grad=-6.0)\n# print(c)  # Should print: Value(data=10.0, grad=1.0)\n```\n\n### Explanation:\n- **Initialization**: The `__init__` method initializes the data, gradient, and sets up the backward function and previous nodes.\n- **Addition (`__add__`)**: This method creates a new `Value` object representing the sum of two `Value` objects. It also defines the backward pass to compute gradients.\n- **Multiplication (`__mul__`)**: This method creates a new `Value` object representing the product of two `Value` objects. It also defines the backward pass to compute gradients.\n- **ReLU Activation (`relu`)**: This method applies the ReLU activation function and defines the backward pass to compute gradients.\n- **Backward Pass (`backward`)**: This method performs the backward pass to compute gradients for all nodes in the computational graph using topological sorting.\n- **String Representation (`__repr__`)**: This method provides a string representation of the `Value` object for easy debugging and display.\n\nThis implementation should correctly handle scalar values and compute gradients for the specified operations."}
{"task_id": 27, "completion_id": 0, "completion": "To solve this problem, we need to compute the transformation matrix \\( P \\) that transforms vectors from basis \\( B \\) to basis \\( C \\). The transformation matrix \\( P \\) can be found using the following steps:\n\n1. **Form the matrix \\( B \\)**: This matrix is formed by placing the basis vectors of \\( B \\) as columns.\n2. **Form the matrix \\( C \\)**: Similarly, this matrix is formed by placing the basis vectors of \\( C \\) as columns.\n3. **Compute the inverse of matrix \\( B \\)**: This is necessary to change the basis from \\( B \\) to the standard basis.\n4. **Compute the transformation matrix \\( P \\)**: The transformation matrix \\( P \\) is given by \\( P = C \\cdot B^{-1} \\).\n\nHere is the Python function to perform these steps:\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the input lists to numpy arrays\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    \n    # Compute the inverse of B_matrix\n    B_inv = np.linalg.inv(B_matrix)\n    \n    # Compute the transformation matrix P\n    P = C_matrix @ B_inv\n    \n    # Round the result to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert the result to a list and return\n    return P_rounded.tolist()\n\n# Example usage:\n# B = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n# C = [[1, 1, 1], [1, 0, 1], [1, 1, 0]]\n# print(transform_basis(B, C))\n```\n\n### Explanation:\n- **Matrix Formation**: We form matrices \\( B \\) and \\( C \\) using the input basis vectors.\n- **Inverse Calculation**: We calculate the inverse of matrix \\( B \\) using `np.linalg.inv()`.\n- **Matrix Multiplication**: We compute the transformation matrix \\( P \\) by multiplying matrix \\( C \\) with the inverse of matrix \\( B \\).\n- **Rounding**: We round the resulting matrix to four decimal places using `np.round()`.\n- **Conversion to List**: Finally, we convert the numpy array to a list using `tolist()` for the required output format.\n\nThis function will correctly compute the transformation matrix from basis \\( B \\) to basis \\( C \\) and return it as a list of lists, rounded to four decimal places."}
{"task_id": 28, "completion_id": 0, "completion": "Certainly! Below is the Python function that computes the Singular Value Decomposition (SVD) of a 2x2 matrix using the method based on eigendecomposition of \\(A^T A\\). The function returns the matrices \\(U\\), \\(S\\), and \\(V\\) as lists rounded to the nearest 4th decimal.\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Ensure A is a 2x2 matrix\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n    \n    # Compute A^T A\n    A_T_A = np.dot(A.T, A)\n    \n    # Perform eigendecomposition on A^T A\n    eigenvalues, eigenvectors = np.linalg.eig(A_T_A)\n    \n    # Sort eigenvalues and eigenvectors in descending order of eigenvalues\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Compute singular values as the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Construct the diagonal matrix S\n    S = np.diag(singular_values)\n    \n    # Use the eigenvectors of A^T A as the columns of V\n    V = eigenvectors\n    \n    # Compute U by normalizing A v_i for each non-zero singular value \u03c3_i\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n        else:\n            U[:, i] = np.zeros(2)\n    \n    # Ensure U is an orthogonal matrix\n    U, _ = np.linalg.qr(U)\n    \n    # Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert to lists\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    \n    return U_list, S_list, V_list\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\n### Explanation:\n1. **Compute \\(A^T A\\)**: This matrix is symmetric and positive semi-definite.\n2. **Eigendecomposition**: We find the eigenvalues and eigenvectors of \\(A^T A\\).\n3. **Singular Values**: The singular values are the square roots of the eigenvalues.\n4. **Matrix \\(V\\)**: The columns of \\(V\\) are the eigenvectors of \\(A^T A\\).\n5. **Matrix \\(U\\)**: For each non-zero singular value, compute \\(u_i = \\frac{A v_i}{\\sigma_i}\\) and normalize it. If a singular value is zero, the corresponding column in \\(U\\) can be any vector orthogonal to the previous columns.\n6. **Orthogonalize \\(U\\)**: Ensure \\(U\\) is orthogonal using QR decomposition.\n7. **Rounding and Conversion**: Round the matrices to 4 decimal places and convert them to lists for the final output.\n\nThis function should work correctly for any 2x2 matrix input."}
{"task_id": 29, "completion_id": 0, "completion": "Certainly! Below is the Python function `shuffle_data` that performs a random shuffle of the samples in two numpy arrays `X` and `y`, while maintaining the corresponding order between them. The function includes an optional `seed` parameter for reproducibility and returns the shuffled data as a list of lists using numpy's `tolist()` method.\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    # Set the random seed for reproducibility if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Get the number of samples\n    num_samples = X.shape[0]\n    \n    # Generate a random permutation of indices\n    indices = np.random.permutation(num_samples)\n    \n    # Shuffle X and y using the same permutation\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Convert the shuffled numpy arrays to lists\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n    \n    # Return the shuffled data as a list of lists\n    return X_shuffled_list, y_shuffled_list\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([0, 1, 0])\n# X_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\n# print(X_shuffled)\n# print(y_shuffled)\n```\n\n### Explanation:\n- **Setting the Seed**: If a `seed` is provided, `np.random.seed(seed)` is called to ensure that the random permutation is reproducible.\n- **Generating Permutation**: `np.random.permutation(num_samples)` generates a random permutation of indices from `0` to `num_samples - 1`.\n- **Shuffling**: The arrays `X` and `y` are indexed using the generated permutation to shuffle them in the same order.\n- **Conversion to List**: The shuffled numpy arrays are converted to lists using the `tolist()` method.\n- **Return**: The function returns the shuffled `X` and `y` as lists of lists.\n\nThis function ensures that the correspondence between `X` and `y` is maintained after shuffling."}
{"task_id": 30, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `batch_iterator` function that meets the specified requirements. This function will yield batches of the specified size from the numpy arrays `X` and `y` (if provided). Each batch will be returned as a Python list after reshaping using numpy's `tolist()` method.\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the dataset.\n    \n    Parameters:\n    - X: numpy array, the input data.\n    - y: numpy array, optional, the target data.\n    - batch_size: int, the size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (X_batch, y_batch) as lists.\n    - If y is not provided, yields X_batch as a list.\n    \"\"\"\n    assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n    if y is not None:\n        assert isinstance(y, np.ndarray), \"y must be a numpy array\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n    \n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)  # Shuffle the indices to randomize the batches\n    \n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        \n        X_batch = X[batch_indices].tolist()\n        \n        if y is not None:\n            y_batch = y[batch_indices].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# y = np.array([0, 1, 0, 1])\n# for batch in batch_iterator(X, y, batch_size=2):\n#     print(batch)\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if `X` and `y` (if provided) are numpy arrays and if they have the same number of samples.\n- **Shuffling**: The indices of the samples are shuffled to ensure that the batches are random.\n- **Batch Generation**: The function iterates over the indices in steps of `batch_size`, extracts the corresponding samples, and converts them to lists using `tolist()`.\n- **Yielding**: If `y` is provided, it yields tuples of `(X_batch, y_batch)`. Otherwise, it yields `X_batch` only.\n\nThis implementation ensures that the batches are randomly selected and returned in the required format."}
{"task_id": 31, "completion_id": 0, "completion": "Certainly! Below is the Python function `divide_on_feature` that divides a dataset based on whether the value of a specified feature is greater than or equal to a given threshold. The function returns two subsets of the dataset as Python lists after reshaping using NumPy's `tolist()` method.\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on the feature at index feature_i\n    and the given threshold.\n\n    Parameters:\n    X (np.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value for division.\n\n    Returns:\n    tuple: Two lists, the first with samples where the feature value is >= threshold,\n           and the second with samples where the feature value is < threshold.\n    \"\"\"\n    # Convert X to a NumPy array if it isn't already\n    X = np.array(X)\n    \n    # Create boolean masks for the two subsets\n    mask_greater_equal = X[:, feature_i] >= threshold\n    mask_less = X[:, feature_i] < threshold\n    \n    # Use the masks to create the two subsets\n    subset_greater_equal = X[mask_greater_equal]\n    subset_less = X[mask_less]\n    \n    # Convert the subsets to lists\n    subset_greater_equal_list = subset_greater_equal.tolist()\n    subset_less_list = subset_less.tolist()\n    \n    return subset_greater_equal_list, subset_less_list\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# feature_i = 0\n# threshold = 4\n# subset_ge, subset_lt = divide_on_feature(X, feature_i, threshold)\n# print(\"Subset with feature >= threshold:\", subset_ge)\n# print(\"Subset with feature < threshold:\", subset_lt)\n```\n\n### Explanation:\n- The function `divide_on_feature` takes three parameters: `X` (the dataset), `feature_i` (the index of the feature to use for division), and `threshold` (the threshold value).\n- It first ensures that `X` is a NumPy array.\n- It then creates two boolean masks: `mask_greater_equal` for samples where the feature value is greater than or equal to the threshold, and `mask_less` for samples where the feature value is less than the threshold.\n- Using these masks, it extracts the two subsets from `X`.\n- Finally, it converts these subsets to Python lists using the `tolist()` method and returns them as a tuple."}
{"task_id": 32, "completion_id": 0, "completion": "To generate polynomial features for a given dataset, we can use the `combinations_with_replacement` function from the `itertools` module to create all possible combinations of the input features up to the specified degree. For each combination, we compute the product of the features raised to the powers specified by the combination. Here's how you can implement this:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    n_samples, n_features = X.shape\n    # Generate all combinations of features with replacement up to the given degree\n    combinations = combinations_with_replacement(range(n_features), degree)\n    \n    # Initialize a list to store the polynomial features\n    polynomial_feature_list = []\n    \n    # Iterate over each combination\n    for combo in combinations:\n        # Compute the product of features for the current combination\n        feature_product = np.ones(n_samples)\n        for power, index in enumerate(combo):\n            feature_product *= X[:, index] ** (power + 1)\n        polynomial_feature_list.append(feature_product)\n    \n    # Stack all polynomial features into a single array\n    polynomial_features_array = np.column_stack(polynomial_feature_list)\n    \n    # Add the bias term (constant term) which is 1 for all samples\n    polynomial_features_array = np.hstack((np.ones((n_samples, 1)), polynomial_features_array))\n    \n    # Return the result as a list after reshaping\n    return polynomial_features_array.tolist()\n\n# Example usage:\n# X = np.array([[0, 1], [2, 3], [4, 5]])\n# degree = 2\n# print(polynomial_features(X, degree))\n```\n\n### Explanation:\n1. **Combinations Generation**: We use `combinations_with_replacement` to generate all possible combinations of feature indices up to the specified degree. This includes combinations like (0, 0), (0, 1), (1, 1), etc., for a 2-feature dataset.\n\n2. **Feature Product Calculation**: For each combination, we calculate the product of the features raised to the powers specified by the combination. For example, for the combination (0, 1), we compute `X[:, 0] * X[:, 1]`.\n\n3. **Bias Term**: We add a bias term (a column of ones) to the polynomial features to account for the constant term in the polynomial.\n\n4. **Output**: The resulting polynomial features are returned as a list after converting the numpy array to a list using `tolist()`.\n\nThis function will generate all polynomial features up to the specified degree, including interaction terms and the bias term."}
{"task_id": 33, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `get_random_subsets` function that generates random subsets of a given dataset. The function takes into account whether to sample with or without replacement based on the `replacements` parameter.\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Get the number of samples in the dataset\n    n_samples = X.shape[0]\n    \n    # Initialize a list to store the subsets\n    subsets = []\n    \n    for _ in range(n_subsets):\n        # Randomly sample indices with or without replacement\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        \n        # Create the subset using the sampled indices\n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        # Append the subset as a tuple to the list\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([0, 1, 0])\n# subsets = get_random_subsets(X, y, n_subsets=2, replacements=True)\n# print(subsets)\n```\n\n### Explanation:\n- **Random Seed**: The function sets a random seed for reproducibility, ensuring that the random subsets generated are the same across different runs if the same seed is used.\n- **Sampling**: The `np.random.choice` function is used to sample indices. The `replace` parameter is set based on the `replacements` argument to determine whether sampling is done with or without replacement.\n- **Subsets Creation**: For each subset, the function creates `X_subset` and `y_subset` using the sampled indices and appends them as a tuple to the `subsets` list.\n- **Output**: Each subset is converted to a list using `tolist()` before being added to the final list of subsets, as per the output constraints.\n\nThis function should work efficiently for generating random subsets of a dataset as specified."}
{"task_id": 34, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `to_categorical` function that performs one-hot encoding of nominal values from a 1D numpy array. If `n_col` is not provided, it will be determined based on the unique values in the input array.\n\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    # Ensure x is a numpy array\n    x = np.array(x)\n    \n    # Determine the number of columns if not provided\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Initialize the one-hot encoded array with zeros\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    \n    # Set the appropriate indices to 1\n    one_hot_encoded[np.arange(x.size), x] = 1\n    \n    # Convert to list and return\n    return one_hot_encoded.tolist()\n\n# Example usage:\n# x = np.array([0, 1, 2, 1])\n# print(to_categorical(x))  # Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]\n```\n\n### Explanation:\n1. **Input Handling**: The function first ensures that the input `x` is a numpy array.\n2. **Determine `n_col`**: If `n_col` is not provided, it calculates the number of columns as the maximum value in `x` plus one. This assumes that the values in `x` are non-negative integers starting from 0.\n3. **One-Hot Encoding**: It initializes a zero matrix of shape `(len(x), n_col)`. It then sets the appropriate indices to 1 to create the one-hot encoded representation.\n4. **Output**: Finally, it converts the resulting numpy array to a list using `tolist()` and returns it.\n\nThis function should work efficiently for the given task and constraints."}
{"task_id": 35, "completion_id": 0, "completion": "Certainly! Below is the Python function that converts a 1D numpy array into a diagonal matrix and returns it as a Python list using numpy's `tolist()` method.\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    # Create a diagonal matrix from the 1D array\n    diagonal_matrix = np.diag(x)\n    # Convert the diagonal matrix to a list\n    return diagonal_matrix.tolist()\n\n# Example usage:\n# x = np.array([1, 2, 3])\n# print(make_diagonal(x))\n# Output: [[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n```\n\n### Explanation:\n1. **Import numpy**: We start by importing the numpy library, which is essential for handling arrays and matrices.\n2. **Function Definition**: The function `make_diagonal` takes a 1D numpy array `x` as input.\n3. **Create Diagonal Matrix**: We use `np.diag(x)` to create a 2D diagonal matrix from the 1D array `x`. The `np.diag` function, when given a 1D array, returns a 2D array with the elements of the 1D array on the diagonal and zeros elsewhere.\n4. **Convert to List**: We then convert the resulting 2D numpy array to a Python list using the `tolist()` method.\n5. **Return the List**: Finally, the function returns the list representation of the diagonal matrix.\n\nYou can test the function with different 1D numpy arrays to see how it works."}
{"task_id": 36, "completion_id": 0, "completion": "Certainly! Below is the Python function to calculate the accuracy score of a model's predictions. The function takes two 1D numpy arrays, `y_true` and `y_pred`, and returns the accuracy score rounded to the nearest 4th decimal.\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    # Ensure the input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if the input arrays have the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n    \n    # Calculate the accuracy score\n    accuracy = correct_predictions / len(y_true)\n    \n    # Return the accuracy score rounded to the nearest 4th decimal\n    return round(accuracy, 4)\n\n# Example usage:\n# y_true = np.array([0, 1, 2, 2, 0])\n# y_pred = np.array([0, 0, 2, 2, 1])\n# print(accuracy_score(y_true, y_pred))  # Output: 0.6000\n```\n\nThis function first ensures that the inputs are numpy arrays and checks if they have the same length. It then calculates the number of correct predictions and computes the accuracy score by dividing the number of correct predictions by the total number of predictions. Finally, it returns the accuracy score rounded to four decimal places."}
{"task_id": 37, "completion_id": 0, "completion": "To calculate the correlation matrix for a given dataset, we can use the `numpy` library, which provides a convenient function `numpy.corrcoef` to compute the correlation matrix. This function can handle both the cases where we have a single dataset (X) or two datasets (X and Y).\n\nHere's how you can implement the `calculate_correlation_matrix` function:\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array representing the first dataset.\n    Y (numpy.ndarray, optional): A 2D numpy array representing the second dataset.\n    \n    Returns:\n    list: The correlation matrix as a 2D list rounded to the nearest 4th decimal.\n    \"\"\"\n    if Y is None:\n        # Calculate the correlation matrix of X with itself\n        corr_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        # Calculate the correlation matrix between X and Y\n        # Stack X and Y horizontally to compute the correlation between all columns\n        combined = np.hstack((X, Y))\n        corr_matrix = np.corrcoef(combined, rowvar=False)\n    \n    # Round the correlation matrix to the nearest 4th decimal\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    \n    # Convert the numpy array to a list\n    corr_matrix_list = corr_matrix_rounded.tolist()\n    \n    return corr_matrix_list\n\n# Example usage:\n# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# Y = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n# print(calculate_correlation_matrix(X, Y))\n```\n\n### Explanation:\n- **Parameters**:\n  - `X`: A 2D numpy array representing the first dataset.\n  - `Y`: An optional 2D numpy array representing the second dataset.\n  \n- **Logic**:\n  - If `Y` is not provided, the function calculates the correlation matrix of `X` with itself using `np.corrcoef(X, rowvar=False)`.\n  - If `Y` is provided, the function combines `X` and `Y` horizontally using `np.hstack((X, Y))` and then calculates the correlation matrix of the combined dataset.\n  \n- **Rounding**:\n  - The correlation matrix is rounded to the nearest 4th decimal using `np.round(corr_matrix, 4)`.\n  \n- **Output**:\n  - The function returns the correlation matrix as a 2D list using `tolist()`.\n\nThis function should work efficiently for the given task and constraints."}
{"task_id": 38, "completion_id": 0, "completion": "To implement the AdaBoost fit method, we need to follow these steps:\n\n1. **Initialize Sample Weights**: Start with equal weights for all samples.\n2. **Iterate Over Classifiers**: For each classifier, find the best feature and threshold to split the data.\n3. **Calculate Error**: Compute the weighted error for the chosen feature and threshold.\n4. **Update Classifier Weight**: Calculate the weight of the classifier based on its error.\n5. **Update Sample Weights**: Adjust the sample weights based on the classifier's performance.\n6. **Normalize Sample Weights**: Ensure the sample weights sum to 1.\n7. **Store Classifier Parameters**: Keep track of the feature, threshold, and weight of each classifier.\n\nHere's the implementation of the `adaboost_fit` function:\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n\n    for _ in range(n_clf):\n        best_feature = None\n        best_threshold = None\n        best_error = float('inf')\n        best_polarity = None\n\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.ones(n_samples) * polarity\n                    predictions[X[:, feature] < threshold] = -polarity\n\n                    weighted_error = np.sum(sample_weights[y != predictions])\n\n                    if weighted_error > 0.5:\n                        weighted_error = 1 - weighted_error\n                        polarity = -polarity\n\n                    if weighted_error < best_error:\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_error = weighted_error\n                        best_polarity = polarity\n\n        classifier_weight = 0.5 * math.log((1 - best_error) / best_error)\n        sample_weights *= np.exp(-classifier_weight * best_polarity * y * (X[:, best_feature] < best_threshold))\n        sample_weights /= np.sum(sample_weights)\n\n        classifiers.append({\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'polarity': best_polarity,\n            'weight': round(classifier_weight, 4)\n        })\n\n    return classifiers\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([1, 1, -1, -1])\n# classifiers = adaboost_fit(X, y, 3)\n# print(classifiers)\n```\n\n### Explanation:\n- **Sample Weights Initialization**: We start with equal weights for all samples.\n- **Feature and Threshold Selection**: For each feature, we consider all unique values as potential thresholds. We also consider both polarities (1 and -1) to determine the direction of the split.\n- **Error Calculation**: The weighted error is calculated for each combination of feature, threshold, and polarity. If the error is greater than 0.5, we flip the polarity and adjust the error accordingly.\n- **Classifier Weight**: The weight of the classifier is calculated using the formula \\( \\frac{1}{2} \\log \\left( \\frac{1 - \\text{error}}{\\text{error}} \\right) \\).\n- **Sample Weight Update**: The sample weights are updated based on the classifier's performance and then normalized.\n- **Classifier Storage**: Each classifier's feature, threshold, polarity, and weight are stored in a list.\n\nThis implementation ensures that all results are rounded to the nearest 4th decimal as required."}
{"task_id": 39, "completion_id": 0, "completion": "To implement the log-softmax function, we need to follow these steps:\n\n1. Convert the input list to a numpy array for efficient computation.\n2. Compute the maximum value in the array to ensure numerical stability.\n3. Subtract the maximum value from each element in the array to prevent overflow when computing the exponential.\n4. Compute the softmax values using the adjusted scores.\n5. Take the natural logarithm of the softmax values to get the log-softmax.\n6. Round the results to the nearest 4th decimal.\n7. Convert the numpy array back to a list using the `tolist()` method.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the input list to a numpy array\n    scores = np.array(scores)\n    \n    # Compute the maximum value in the array for numerical stability\n    max_score = np.max(scores)\n    \n    # Subtract the maximum value from each element\n    adjusted_scores = scores - max_score\n    \n    # Compute the exponential of the adjusted scores\n    exp_scores = np.exp(adjusted_scores)\n    \n    # Compute the sum of the exponentials\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Compute the softmax values\n    softmax_values = exp_scores / sum_exp_scores\n    \n    # Compute the log-softmax values\n    log_softmax_values = np.log(softmax_values)\n    \n    # Round the results to the nearest 4th decimal\n    log_softmax_values_rounded = np.round(log_softmax_values, 4)\n    \n    # Convert the numpy array back to a list\n    return log_softmax_values_rounded.tolist()\n\n# Example usage:\n# scores = [1.0, 2.0, 3.0]\n# print(log_softmax(scores))\n```\n\nThis function will compute the log-softmax of the input scores, ensuring numerical stability and rounding the results to four decimal places."}
{"task_id": 40, "completion_id": 0, "completion": "To implement the `Dense` class, we need to follow the structure and requirements provided. We'll start by implementing the `initialize` method to set up the weights and biases. Then, we'll implement the `parameters` method to count the trainable parameters. The `forward_pass` method will compute the output of the layer, and the `backward_pass` method will handle the gradient computation and parameter updates. Finally, the `output_shape` method will return the shape of the output.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Gradient w.r.t weights\n        W_grad = np.dot(self.layer_input.T, accum_grad)\n        # Gradient w.r.t bias\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        # Gradient w.r.t input\n        input_grad = np.dot(accum_grad, self.W.T)\n\n        # Update weights and biases if the layer is trainable\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, W_grad)\n            self.w0 = self.w0_opt.update(self.w0, w0_grad)\n\n        return input_grad\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example optimizer class for demonstration purposes\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, w, grad):\n        return w - self.learning_rate * grad\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a dense layer with 10 units and input shape of 5\n    dense_layer = Dense(n_units=10, input_shape=(5,))\n    # Initialize the layer with an SGD optimizer\n    dense_layer.initialize(SGD(learning_rate=0.01))\n    # Example input\n    X = np.random.rand(1, 5)\n    # Forward pass\n    output = dense_layer.forward_pass(X)\n    print(\"Output:\", np.round(output, 4).tolist())\n    # Example accumulated gradient (same shape as output)\n    accum_grad = np.random.rand(1, 10)\n    # Backward pass\n    input_grad = dense_layer.backward_pass(accum_grad)\n    print(\"Input Gradient:\", np.round(input_grad, 4).tolist())\n```\n\n### Explanation:\n1. **Initialization (`initialize`)**:\n   - We initialize the weights `W` using a uniform distribution with a limit of `1 / sqrt(input_shape[0])`.\n   - The biases `w0` are initialized to zero.\n   - We also initialize optimizers for `W` and `w0` using the provided optimizer.\n\n2. **Parameter Count (`parameters`)**:\n   - We calculate the total number of trainable parameters by summing the number of elements in `W` and `w0`.\n\n3. **Forward Pass (`forward_pass`)**:\n   - We compute the output by performing a dot product between the input `X` and the weight matrix `W`, and then adding the bias `w0`.\n\n4. **Backward Pass (`backward_pass`)**:\n   - We calculate the gradient with respect to the weights, biases, and input.\n   - If the layer is trainable, we update the weights and biases using the optimizer's update rule.\n\n5. **Output Shape (`output_shape`)**:\n   - We return the shape of the output, which is `(self.n_units,)`.\n\nThis implementation ensures that the `Dense` class functions correctly within a neural network framework, and all results are rounded to the nearest 4th decimal as required."}
{"task_id": 41, "completion_id": 0, "completion": "To implement a simple 2D convolutional layer, we need to follow these steps:\n\n1. **Padding**: Add padding to the input matrix. Padding is added to the borders of the input matrix to control the spatial size of the output volume.\n2. **Convolution**: Slide the kernel over the input matrix, applying the dot product at each position, and summing the results.\n3. **Stride**: Move the kernel by the specified stride after each operation.\n4. **Rounding**: Round the results to the nearest 4th decimal.\n5. **Return**: Convert the result to a Python list using `tolist()`.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Get dimensions\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate output dimensions\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    \n    # Initialize output matrix\n    output = np.zeros((output_height, output_width))\n    \n    # Pad the input matrix\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    \n    # Perform convolution\n    for i in range(output_height):\n        for j in range(output_width):\n            # Extract the region from the input matrix\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            # Apply the kernel (dot product)\n            output[i, j] = np.sum(region * kernel)\n    \n    # Round to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Convert to list\n    return output.tolist()\n\n# Example usage:\n# input_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# kernel = np.array([[1, 0], [0, 1]])\n# padding = 1\n# stride = 1\n# print(simple_conv2d(input_matrix, kernel, padding, stride))\n```\n\n### Explanation:\n- **Padding**: We use `np.pad` to add padding to the input matrix. The padding is added symmetrically to all sides.\n- **Output Dimensions**: The output dimensions are calculated based on the input dimensions, kernel dimensions, padding, and stride.\n- **Convolution**: We iterate over each position in the output matrix, extract the corresponding region from the padded input, and compute the dot product with the kernel.\n- **Rounding**: We use `np.round` to round the output values to the nearest 4th decimal.\n- **Return**: Finally, we convert the output matrix to a list using `tolist()`.\n\nThis function should work for any valid input matrix, kernel, padding, and stride values."}
{"task_id": 42, "completion_id": 0, "completion": "Certainly! Below is the implementation of the ReLU activation function in Python:\n\n```python\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\n```\n\nThis function takes a single float `z` as input and returns `z` if it is greater than 0. Otherwise, it returns 0, as per the definition of the ReLU function."}
{"task_id": 43, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Ridge Regression loss function as per your requirements:\n\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Calculate the predictions\n    y_pred = X @ w\n    \n    # Calculate the Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Calculate the regularization term\n    regularization = alpha * np.sum(w ** 2)\n    \n    # Calculate the Ridge loss\n    ridge_loss_value = mse + regularization\n    \n    # Return the Ridge loss rounded to the nearest 4th decimal\n    return round(ridge_loss_value, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# w = np.array([0.1, 0.2])\n# y_true = np.array([3, 7, 11])\n# alpha = 0.5\n# print(ridge_loss(X, w, y_true, alpha))\n```\n\n### Explanation:\n1. **Predictions Calculation**: The predicted values `y_pred` are calculated by taking the dot product of the feature matrix `X` and the coefficient vector `w`.\n2. **Mean Squared Error (MSE)**: The MSE is computed as the average of the squared differences between the true labels `y_true` and the predicted labels `y_pred`.\n3. **Regularization Term**: The regularization term is calculated as `alpha * sum(w^2)`, where `alpha` is the regularization parameter and `w` is the coefficient vector.\n4. **Ridge Loss**: The Ridge loss is the sum of the MSE and the regularization term.\n5. **Rounding**: The final result is rounded to the nearest 4th decimal place as specified.\n\nYou can test the function with different inputs to ensure it works correctly."}
{"task_id": 44, "completion_id": 0, "completion": "Certainly! The Leaky ReLU activation function is a variant of the ReLU function that allows a small, non-zero gradient when the unit is not active (i.e., for negative inputs). Here's how you can implement it in Python:\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z\n```\n\n### Explanation:\n- The function `leaky_relu` takes two parameters: `z` (the input value) and `alpha` (the slope for negative inputs, with a default value of 0.01).\n- If `z` is greater than or equal to 0, the function returns `z` (similar to the standard ReLU function).\n- If `z` is less than 0, the function returns `alpha * z`, allowing a small gradient to flow through the network, which helps in mitigating the \"dying ReLU\" problem."}
{"task_id": 45, "completion_id": 0, "completion": "Certainly! Below is the Python function `kernel_function` that computes the linear kernel between two input vectors `x1` and `x2` using the dot product:\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    # Ensure the inputs are numpy arrays\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n    \n    # Compute the dot product\n    return np.dot(x1, x2)\n\n# Example usage:\n# x1 = np.array([1, 2, 3])\n# x2 = np.array([4, 5, 6])\n# print(kernel_function(x1, x2))  # Output: 32\n```\n\nThis function first ensures that the inputs are numpy arrays, then computes and returns their dot product, which is the definition of the linear kernel."}
{"task_id": 46, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `precision` function using numpy arrays. This function calculates the precision metric based on the true and predicted binary labels.\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    # Ensure y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (TP)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate false positives (FP)\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate precision\n    if tp + fp == 0:\n        return 0.0  # To handle the case where there are no positive predictions\n    \n    precision_score = tp / (tp + fp)\n    return precision_score\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0, 1])\n# y_pred = np.array([1, 0, 0, 1, 0, 1])\n# print(precision(y_true, y_pred))  # Output should be 0.75\n```\n\n### Explanation:\n- **True Positives (TP):** These are the cases where the true label is 1 and the predicted label is also 1.\n- **False Positives (FP):** These are the cases where the true label is 0 but the predicted label is 1.\n- **Precision:** It is calculated as the ratio of true positives to the sum of true positives and false positives. This metric gives an idea of how many of the predicted positive instances are actually positive.\n\nThe function first converts the input arrays to numpy arrays to ensure compatibility and then calculates the number of true positives and false positives. Finally, it computes the precision score, handling the edge case where there are no positive predictions to avoid division by zero."}
{"task_id": 47, "completion_id": 0, "completion": "To implement the three variants of gradient descent (Stochastic Gradient Descent, Batch Gradient Descent, and Mini-Batch Gradient Descent) using Mean Squared Error (MSE) as the loss function, we need to follow these steps:\n\n1. **Initialize Parameters**: We start with the given weights.\n2. **Compute the Gradient**: For each variant, we compute the gradient of the MSE loss with respect to the weights.\n3. **Update Weights**: Adjust the weights using the computed gradient and the learning rate.\n4. **Iterate**: Repeat the process for a specified number of iterations.\n5. **Return the Weights**: After all iterations, return the final weights rounded to the nearest 4th decimal.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    # Convert inputs to numpy arrays for easier manipulation\n    X = np.array(X)\n    y = np.array(y)\n    weights = np.array(weights)\n    \n    n_samples, n_features = X.shape\n    \n    for _ in range(n_iterations):\n        if method == 'batch':\n            # Batch Gradient Descent\n            y_pred = X.dot(weights)\n            error = y_pred - y\n            gradient = (2 / n_samples) * X.T.dot(error)\n            weights -= learning_rate * gradient\n        \n        elif method == 'stochastic':\n            # Stochastic Gradient Descent\n            for i in range(n_samples):\n                xi = X[i:i+1]\n                yi = y[i:i+1]\n                y_pred = xi.dot(weights)\n                error = y_pred - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient\n        \n        elif method == 'mini_batch':\n            # Mini-Batch Gradient Descent\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i+batch_size]\n                yi = y[i:i+batch_size]\n                y_pred = xi.dot(weights)\n                error = y_pred - yi\n                gradient = (2 / batch_size) * xi.T.dot(error)\n                weights -= learning_rate * gradient\n        \n        else:\n            raise ValueError(\"Invalid method. Choose 'batch', 'stochastic', or 'mini_batch'.\")\n    \n    # Return the weights rounded to the nearest 4th decimal\n    return weights.round(4).tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n# y = [3, 5, 7, 9]\n# weights = [0, 0]\n# learning_rate = 0.01\n# n_iterations = 1000\n# batch_size = 2\n# method = 'mini_batch'\n# print(gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method))\n```\n\n### Explanation:\n- **Batch Gradient Descent**: Computes the gradient using the entire dataset and updates the weights once per iteration.\n- **Stochastic Gradient Descent**: Computes the gradient using one sample at a time and updates the weights after each sample.\n- **Mini-Batch Gradient Descent**: Computes the gradient using a small batch of samples and updates the weights after each batch.\n\nThis implementation ensures that the weights are updated correctly for each variant and returns the final weights rounded to four decimal places."}
{"task_id": 48, "completion_id": 0, "completion": "To implement the Reduced Row Echelon Form (RREF) of a matrix, we can use Gaussian elimination with back substitution. The process involves several steps:\n\n1. **Forward Elimination**: Transform the matrix into an upper triangular form.\n2. **Back Substitution**: Normalize the rows and eliminate any non-zero entries above the leading 1s.\n\nHere's the implementation of the `rref` function using NumPy:\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    # Convert the input matrix to a NumPy array for easier manipulation\n    A = np.array(matrix, dtype=float)\n    rows, cols = A.shape\n    \n    # Initialize the row index\n    row = 0\n    \n    for col in range(cols):\n        # Find the pivot row for the current column\n        if row >= rows:\n            break\n        \n        # Find the first non-zero element in the current column starting from the current row\n        pivot_row = row\n        while pivot_row < rows and A[pivot_row, col] == 0:\n            pivot_row += 1\n        \n        # If a non-zero element is found, swap the current row with the pivot row\n        if pivot_row < rows:\n            A[[row, pivot_row]] = A[[pivot_row, row]]\n            \n            # Normalize the pivot row by dividing by the pivot element\n            pivot = A[row, col]\n            A[row] /= pivot\n            \n            # Eliminate all other entries in the current column\n            for other_row in range(rows):\n                if other_row != row:\n                    factor = A[other_row, col]\n                    A[other_row] -= factor * A[row]\n            \n            # Move to the next row\n            row += 1\n    \n    # Convert the result back to a list\n    return A.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, -1, -4],\n    [2, 3, -1, -11],\n    [-2, 0, -3, 22]\n]\n\nprint(rref(matrix))\n```\n\n### Explanation:\n\n1. **Initialization**: Convert the input matrix to a NumPy array for easier manipulation and determine the number of rows and columns.\n2. **Pivot Selection**: For each column, find the first non-zero element (pivot) starting from the current row. If a non-zero element is found, swap the current row with the pivot row.\n3. **Normalization**: Divide the pivot row by the pivot element to make it 1.\n4. **Elimination**: For all other rows, eliminate the entry in the current column by subtracting a multiple of the pivot row.\n5. **Move to Next Row**: After processing the current column, move to the next row and repeat the process.\n6. **Output**: Convert the final matrix to a list using `tolist()` and return it.\n\nThis implementation handles the cases where rows can be entirely zero and where diagonal entries can be zero if the matrix is reducible."}
{"task_id": 49, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Adam optimization algorithm in Python. The function `adam_optimizer` takes the specified parameters and returns the optimized parameters rounded to the nearest 4th decimal place.\n\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    # Initialize parameters\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)  # First moment estimate\n    v = np.zeros_like(x)  # Second moment estimate\n    t = 0  # Time step\n\n    for t in range(1, num_iterations + 1):\n        # Compute the gradient\n        g = grad(x)\n        \n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n        \n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1 ** t)\n        \n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update parameters\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Return the optimized parameters rounded to the nearest 4th decimal\n    return x.round(4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# Initial parameters\nx0 = [1.0, 1.0]\n\n# Run the Adam optimizer\noptimized_params = adam_optimizer(f, grad, x0, num_iterations=1000)\nprint(optimized_params)\n```\n\n### Explanation:\n- **Initialization**: We start by initializing the parameters `x`, the first moment estimate `m`, and the second moment estimate `v` to zero vectors of the same shape as `x0`.\n- **Loop through iterations**: For each iteration, we:\n  - Compute the gradient of the function at the current parameters.\n  - Update the first and second moment estimates using the given decay rates `beta1` and `beta2`.\n  - Compute the bias-corrected estimates of the first and second moments.\n  - Update the parameters using the Adam update rule.\n- **Return**: Finally, we return the optimized parameters rounded to the nearest 4th decimal place using `round(4)` and convert them to a list using `tolist()`.\n\nThis implementation should work for any differentiable function `f` and its gradient `grad` provided as input."}
{"task_id": 50, "completion_id": 0, "completion": "To implement Lasso Regression using Gradient Descent, we need to iteratively update the weights and bias by computing the gradients of the loss function with respect to these parameters. The loss function for Lasso Regression includes both the mean squared error (MSE) and the L1 regularization term.\n\nHere's the step-by-step implementation:\n\n1. **Initialize weights and bias**: Start with random values for weights and zero for bias.\n2. **Compute the gradient of the loss function**: This includes the gradient of the MSE and the gradient of the L1 regularization term.\n3. **Update weights and bias**: Use the computed gradients to update the weights and bias.\n4. **Check for convergence**: Stop the iterations if the change in the loss function is below a specified tolerance level or if the maximum number of iterations is reached.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Number of samples and features\n    n, p = X.shape\n    \n    # Initialize weights and bias\n    w = np.zeros(p)\n    b = 0.0\n    \n    # Precompute some values for efficiency\n    X_transpose = X.T\n    \n    # Function to compute the L1 penalty gradient\n    def l1_penalty_gradient(w):\n        return alpha * np.sign(w)\n    \n    # Gradient Descent\n    for iteration in range(max_iter):\n        # Predicted values\n        y_pred = np.dot(X, w) + b\n        \n        # Compute the gradient of the MSE\n        mse_gradient_w = (1/n) * np.dot(X_transpose, (y_pred - y))\n        mse_gradient_b = (1/n) * np.sum(y_pred - y)\n        \n        # Compute the gradient of the L1 penalty\n        l1_grad_w = l1_penalty_gradient(w)\n        \n        # Update weights and bias\n        w -= learning_rate * (mse_gradient_w + l1_grad_w)\n        b -= learning_rate * mse_gradient_b\n        \n        # Compute the loss for convergence check\n        loss = (1/(2*n)) * np.sum((y - (np.dot(X, w) + b))**2) + alpha * np.sum(np.abs(w))\n        \n        # Check for convergence\n        if iteration > 0 and np.abs(loss - prev_loss) < tol:\n            break\n        \n        prev_loss = loss\n    \n    # Return the weights and bias rounded to 4 decimal places\n    return w.round(4).tolist(), round(b, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([3, 5, 7])\n# w, b = l1_regularization_gradient_descent(X, y, alpha=0.1, learning_rate=0.01, max_iter=1000, tol=1e-4)\n# print(\"Weights:\", w)\n# print(\"Bias:\", b)\n```\n\n### Explanation:\n- **Initialization**: We start with zero weights and zero bias.\n- **Gradient Calculation**: The gradient of the MSE is calculated as `(1/n) * X^T * (y_pred - y)`, and the gradient of the L1 penalty is `alpha * sign(w)`.\n- **Weight Update**: We update the weights and bias using the computed gradients and the learning rate.\n- **Convergence Check**: We check if the change in loss is below the tolerance level to stop the iterations early if possible.\n\nThis implementation should work for any dataset provided in the form of `X` (features) and `y` (target values)."}
{"task_id": 51, "completion_id": 0, "completion": "To implement the Optimal String Alignment (OSA) distance, we need to consider the four allowed operations: insertion, deletion, substitution, and transposition of adjacent characters. The OSA distance is a variation of the Levenshtein distance that includes the transposition operation.\n\nHere's how you can implement the OSA distance in Python:\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    # Create a matrix to store distances\n    m, n = len(source), len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    \n    # Initialize the matrix\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    # Compute the OSA distance\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            \n            dp[i][j] = min(\n                dp[i - 1][j] + 1,    # Deletion\n                dp[i][j - 1] + 1,    # Insertion\n                dp[i - 1][j - 1] + cost  # Substitution\n            )\n            \n            # Check for transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    \n    return dp[m][n]\n\n# Example usage:\nprint(OSA(\"caper\", \"acer\"))  # Output: 2\n```\n\n### Explanation:\n1. **Matrix Initialization**: We create a matrix `dp` where `dp[i][j]` represents the OSA distance between the first `i` characters of `source` and the first `j` characters of `target`. The matrix is initialized such that `dp[i][0] = i` and `dp[0][j] = j`, representing the cost of converting a string to an empty string by deletions or insertions, respectively.\n\n2. **Filling the Matrix**: We iterate over each character of `source` and `target`. For each pair of characters, we calculate the cost of substitution (0 if the characters are the same, 1 otherwise). We then compute the minimum cost of the three possible operations: deletion, insertion, and substitution.\n\n3. **Transposition Check**: If the current characters and the previous characters in both strings can be transposed to match, we also consider the cost of this transposition operation.\n\n4. **Result**: The value at `dp[m][n]` gives the OSA distance between the two strings.\n\nThis implementation efficiently computes the OSA distance using dynamic programming, ensuring that all possible edit operations are considered."}
{"task_id": 52, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `recall` function in Python, which calculates the recall metric for a binary classification setting:\n\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert inputs to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate False Negatives (FN)\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Recall\n    # Recall = TP / (TP + FN)\n    # If TP + FN is zero, recall should be 0.0 to avoid division by zero\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    else:\n        recall_value = TP / denominator\n    \n    # Return the recall value rounded to three decimal places\n    return round(recall_value, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0, 1]\n# y_pred = [1, 0, 0, 1, 0, 1]\n# print(recall(y_true, y_pred))  # Output should be 0.833\n```\n\n### Explanation:\n- **True Positives (TP):** These are the cases where the true label is 1 and the predicted label is also 1.\n- **False Negatives (FN):** These are the cases where the true label is 1 but the predicted label is 0.\n- **Recall:** It is calculated as the ratio of True Positives to the sum of True Positives and False Negatives. This metric indicates the proportion of actual positive cases that were correctly identified by the model.\n- **Handling Division by Zero:** If there are no actual positive cases (`TP + FN == 0`), the recall is defined to be 0.0 to avoid division by zero errors."}
{"task_id": 53, "completion_id": 0, "completion": "To implement the self-attention mechanism, we need to follow these steps:\n\n1. **Linear Transformations**: Apply linear transformations to the input using the weight matrices \\( W_q \\), \\( W_k \\), and \\( W_v \\) to get the query, key, and value matrices.\n2. **Scaled Dot-Product Attention**: Compute the attention scores using the dot product of the query and key matrices, scale them by the square root of the dimension of the keys, and apply a softmax function to get the attention weights.\n3. **Weighted Sum**: Use the attention weights to compute a weighted sum of the value matrix.\n4. **Output**: Return the result as a numpy array rounded to the nearest 4th decimal and converted to a list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Step 1: Linear transformations\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Step 2: Scaled dot-product attention\n    d_k = K.shape[-1]  # Dimension of keys\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    \n    # Step 3: Weighted sum\n    output = np.dot(attention_weights, V)\n    \n    # Step 4: Output\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n\n# Example usage:\n# X = np.random.rand(3, 4)  # Example input matrix (batch size 3, sequence length 4)\n# W_q = np.random.rand(4, 4)  # Example weight matrix for queries\n# W_k = np.random.rand(4, 4)  # Example weight matrix for keys\n# W_v = np.random.rand(4, 4)  # Example weight matrix for values\n# result = self_attention(X, W_q, W_k, W_v)\n# print(result)\n```\n\n### Explanation:\n- **Linear Transformations**: We compute the query, key, and value matrices by multiplying the input matrix \\( X \\) with the respective weight matrices \\( W_q \\), \\( W_k \\), and \\( W_v \\).\n- **Scaled Dot-Product Attention**: The attention scores are computed as the dot product of the query and key matrices, scaled by the square root of the dimension of the keys. The softmax function is applied to these scores to get the attention weights.\n- **Weighted Sum**: The output is computed as the dot product of the attention weights and the value matrix.\n- **Output**: The result is rounded to the nearest 4th decimal and converted to a list for the final output.\n\nThis implementation assumes that the input matrix \\( X \\) has a shape of \\((\\text{batch size}, \\text{sequence length}, \\text{embedding dimension})\\) and the weight matrices \\( W_q \\), \\( W_k \\), and \\( W_v \\) have a shape of \\((\\text{embedding dimension}, \\text{model dimension})\\). Adjust the dimensions as necessary for your specific use case."}
{"task_id": 54, "completion_id": 0, "completion": "Certainly! Below is the implementation of a simple RNN cell in Python. This function processes a sequence of input vectors and produces the final hidden state using the tanh activation function.\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state with the initial hidden state\n    h = initial_hidden_state\n    \n    # Process each input in the sequence\n    for x in input_sequence:\n        # Update the hidden state using the RNN formula\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    \n    # Return the final hidden state rounded to four decimal places\n    return h.round(4).tolist()\n\n# Example usage:\n# input_sequence = [[0.1, 0.2], [0.3, 0.4]]\n# initial_hidden_state = [0.0, 0.0]\n# Wx = [[0.5, 0.6], [0.7, 0.8]]\n# Wh = [[0.9, 1.0], [1.1, 1.2]]\n# b = [0.1, 0.2]\n# print(rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b))\n```\n\n### Explanation:\n- **Inputs**:\n  - `input_sequence`: A list of input vectors, where each vector is a list of floats.\n  - `initial_hidden_state`: The initial hidden state vector, a list of floats.\n  - `Wx`: The weight matrix for input-to-hidden connections.\n  - `Wh`: The weight matrix for hidden-to-hidden connections.\n  - `b`: The bias vector.\n\n- **Process**:\n  - Convert all inputs to numpy arrays for efficient computation.\n  - Initialize the hidden state `h` with the `initial_hidden_state`.\n  - For each input vector `x` in the `input_sequence`, update the hidden state using the RNN formula: \n    \\[\n    h = \\tanh(W_x \\cdot x + W_h \\cdot h + b)\n    \\]\n  - After processing all inputs, round the final hidden state to four decimal places and convert it back to a list using `tolist()`.\n\nThis function will return the final hidden state after processing the entire sequence, rounded to four decimal places."}
{"task_id": 55, "completion_id": 0, "completion": "To implement the `translate_object` function, we need to apply a 2D translation matrix to each point in the list. A 2D translation matrix can be represented as:\n\n\\[ \nT = \\begin{bmatrix} \n1 & 0 & tx \\\\ \n0 & 1 & ty \\\\ \n0 & 0 & 1 \n\\end{bmatrix} \n\\]\n\nHowever, since we are dealing with 2D points, we can represent each point as a homogeneous coordinate \\([x, y, 1]\\) to apply the translation matrix. The translation can be done by matrix multiplication:\n\n\\[ \n\\begin{bmatrix} \nx' \\\\ \ny' \\\\ \n1 \n\\end{bmatrix} \n= \n\\begin{bmatrix} \n1 & 0 & tx \\\\ \n0 & 1 & ty \\\\ \n0 & 0 & 1 \n\\end{bmatrix} \n\\cdot \n\\begin{bmatrix} \nx \\\\ \ny \\\\ \n1 \n\\end{bmatrix} \n\\]\n\nThis results in:\n\n\\[ \nx' = x + tx \\\\\ny' = y + ty \n\\]\n\nWe will use NumPy to handle the matrix operations efficiently. Here's the implementation:\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Create the translation matrix\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Convert the list of points to a homogeneous coordinate matrix\n    # Add a 1 to each point to make it [x, y, 1]\n    homogeneous_points = np.array([point + [1] for point in points])\n    \n    # Transpose the matrix to make it easier to multiply\n    homogeneous_points = homogeneous_points.T\n    \n    # Apply the translation matrix\n    translated_points = np.dot(translation_matrix, homogeneous_points)\n    \n    # Convert back to Cartesian coordinates by removing the last row\n    translated_points = translated_points[:2, :].T\n    \n    # Convert the result back to a list of lists\n    return translated_points.tolist()\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)  # Output: [[3, 5], [5, 7], [7, 9]]\n```\n\nThis function first constructs the translation matrix, converts the input points to homogeneous coordinates, applies the translation, and then converts the result back to a list of Cartesian coordinates."}
{"task_id": 56, "completion_id": 0, "completion": "Certainly! To compute the Kullback-Leibler (KL) divergence between two normal distributions \\( P \\sim N(\\mu_P, \\sigma_P^2) \\) and \\( Q \\sim N(\\mu_Q, \\sigma_Q^2) \\), we can use the following formula:\n\n\\[\nD_{KL}(P \\parallel Q) = \\log \\left( \\frac{\\sigma_Q}{\\sigma_P} \\right) + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2 \\sigma_Q^2} - \\frac{1}{2}\n\\]\n\nHere's the implementation of the function in Python:\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    # Calculate the KL divergence using the formula\n    kl_divergence = np.log(sigma_q / sigma_p) + \\\n                    (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - \\\n                    0.5\n    return kl_divergence\n\n# Example usage:\n# mu_p, sigma_p = 0, 1  # Parameters for distribution P\n# mu_q, sigma_q = 1, 2  # Parameters for distribution Q\n# print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n```\n\nThis function takes the means and standard deviations of the two normal distributions as input and returns the KL divergence. The formula used ensures that the divergence is computed correctly for normal distributions."}
{"task_id": 57, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Gauss-Seidel method for solving a system of linear equations. The function `gauss_seidel` takes a square matrix `A`, a right-hand side vector `b`, the number of iterations `n`, and an optional initial guess `x_ini`. It returns the approximated solution vector `x` after performing the specified number of iterations, rounded to the nearest 4th decimal.\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Get the number of equations\n    N = len(b)\n    \n    # Initialize the solution vector with zeros or the provided initial guess\n    if x_ini is None:\n        x = np.zeros(N, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform the Gauss-Seidel iterations\n    for _ in range(n):\n        x_new = np.zeros(N, dtype=float)\n        for i in range(N):\n            # Calculate the sum of the known values\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i+1:], x[i+1:])\n            # Update the solution for the current variable\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        \n        # Update the solution vector\n        x = x_new\n    \n    # Round the results to the nearest 4th decimal and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# A = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\n# b = [4, 7, 3]\n# n = 10\n# print(gauss_seidel(A, b, n))\n```\n\n### Explanation:\n- **Initialization**: The function first converts `A` and `b` to numpy arrays for easier manipulation. It then determines the number of equations `N` from the length of `b`.\n- **Initial Guess**: If no initial guess `x_ini` is provided, the solution vector `x` is initialized to a vector of zeros. Otherwise, it is initialized to the provided guess.\n- **Iterations**: The function performs `n` iterations of the Gauss-Seidel method. In each iteration, it updates each component of the solution vector `x` using the most recent values available.\n- **Rounding and Output**: After completing the iterations, the solution vector `x` is rounded to the nearest 4th decimal and converted to a list before being returned.\n\nThis implementation assumes that the matrix `A` is diagonally dominant or that the method will converge for the given problem."}
{"task_id": 58, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Gaussian Elimination method with partial pivoting to solve a system of linear equations \\( Ax = b \\). The solution vector \\( x \\) is returned as a Python list rounded to the nearest 4th decimal place.\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    n = len(b)\n    \n    # Augment A with b\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    \n    # Perform Gaussian Elimination with partial pivoting\n    for i in range(n):\n        # Find the pivot row\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        \n        # Swap the current row with the pivot row\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        \n        # Make all rows below the current row 0 in the current column\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    \n    # Backward substitution to find the solution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i+1:n], x[i+1:n])) / Ab[i, i]\n    \n    # Round the solution to the nearest 4th decimal place\n    x = np.round(x, 4)\n    \n    # Convert the solution to a list\n    return x.tolist()\n\n# Example usage:\n# A = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\n# b = [1, -2, 0]\n# print(gaussian_elimination(A, b))\n```\n\n### Explanation:\n1. **Augmentation**: The matrix \\( A \\) is augmented with the vector \\( b \\) to form a single matrix \\( Ab \\).\n2. **Partial Pivoting**: For each column, the row with the largest absolute value in the current column is swapped with the current row to improve numerical stability.\n3. **Forward Elimination**: The matrix is transformed into an upper triangular form by eliminating the entries below the pivot.\n4. **Backward Substitution**: The solution vector \\( x \\) is computed by solving the upper triangular system.\n5. **Rounding**: The solution is rounded to the nearest 4th decimal place.\n6. **Output**: The solution is returned as a Python list.\n\nThis implementation should work for any square matrix \\( A \\) and vector \\( b \\) that form a consistent system of linear equations."}
{"task_id": 59, "completion_id": 0, "completion": "To implement the LSTM network, we need to follow the standard LSTM equations for each time step. The LSTM cell processes the input sequence one step at a time, updating the hidden state and cell state using the forget gate, input gate, candidate cell state, and output gate.\n\nHere's the complete implementation of the `LSTM` class with the `forward` method:\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        # x is a list of input vectors, each of shape (input_size, 1)\n        # initial_hidden_state and initial_cell_state are of shape (hidden_size, 1)\n        \n        hidden_states = []\n        h = initial_hidden_state\n        c = initial_cell_state\n        \n        for input_vector in x:\n            # Concatenate input and hidden state\n            combined = np.vstack((input_vector, h))\n            \n            # Compute gates and candidate cell state\n            f = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            i = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            c_hat = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            o = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            \n            # Update cell state and hidden state\n            c = f * c + i * c_hat\n            h = o * np.tanh(c)\n            \n            # Store the hidden state for this time step\n            hidden_states.append(h.copy())\n        \n        # Round the results to the nearest 4th decimal\n        hidden_states = [hs.round(4).tolist() for hs in hidden_states]\n        final_hidden_state = h.round(4).tolist()\n        final_cell_state = c.round(4).tolist()\n        \n        return hidden_states, final_hidden_state, final_cell_state\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n```\n\n### Explanation:\n1. **Initialization**: The weights and biases for the forget gate (`Wf`, `bf`), input gate (`Wi`, `bi`), candidate cell state (`Wc`, `bc`), and output gate (`Wo`, `bo`) are initialized randomly and to zero, respectively.\n\n2. **Forward Method**:\n   - The method takes a sequence of input vectors `x`, an initial hidden state `initial_hidden_state`, and an initial cell state `initial_cell_state`.\n   - For each input vector in the sequence, it computes the forget gate (`f`), input gate (`i`), candidate cell state (`c_hat`), and output gate (`o`).\n   - The cell state (`c`) and hidden state (`h`) are updated using these gates.\n   - The hidden states at each time step are stored in a list.\n   - The results are rounded to the nearest 4th decimal and converted to a list using `tolist()`.\n\n3. **Sigmoid Function**: A helper function to compute the sigmoid activation.\n\nThis implementation should correctly process a sequence of inputs and return the hidden states, final hidden state, and final cell state as specified."}
{"task_id": 60, "completion_id": 0, "completion": "To implement the TF-IDF (Term Frequency-Inverse Document Frequency) scoring system, we need to follow these steps:\n\n1. **Term Frequency (TF):** Calculate the frequency of each term in a document relative to the total number of terms in that document.\n2. **Inverse Document Frequency (IDF):** Calculate the inverse of the frequency of a term across all documents in the corpus. This helps to reduce the weight of common terms.\n3. **TF-IDF Calculation:** Multiply the TF and IDF for each term in the query against each document in the corpus.\n4. **Handling Edge Cases:** Ensure the implementation handles cases like empty documents, terms not present in the corpus, and division by zero.\n\nHere's the implementation of the `compute_tf_idf` function:\n\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"The corpus is empty.\")\n    \n    # Number of documents in the corpus\n    num_docs = len(corpus)\n    \n    # Dictionary to store the document frequency of each term\n    df = defaultdict(int)\n    \n    # Calculate document frequency for each term\n    for doc in corpus:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            df[term] += 1\n    \n    # Function to calculate TF for a term in a document\n    def term_frequency(term, doc):\n        return doc.count(term) / len(doc) if doc else 0.0\n    \n    # Function to calculate IDF for a term\n    def inverse_document_frequency(term):\n        return np.log((num_docs + 1) / (df[term] + 1)) + 1\n    \n    # List to store TF-IDF scores for each document\n    tf_idf_scores = []\n    \n    # Calculate TF-IDF scores for each document\n    for doc in corpus:\n        doc_scores = []\n        for term in query:\n            tf = term_frequency(term, doc)\n            idf = inverse_document_frequency(term)\n            tf_idf = tf * idf\n            doc_scores.append(tf_idf)\n        tf_idf_scores.append(doc_scores)\n    \n    # Convert to numpy array and round to 4 decimal places\n    tf_idf_scores = np.array(tf_idf_scores).round(4)\n    \n    # Convert to list\n    return tf_idf_scores.tolist()\n\n# Example usage:\n# corpus = [[\"this\", \"is\", \"a\", \"sample\", \"document\"], [\"this\", \"document\", \"is\", \"another\", \"example\"]]\n# query = [\"this\", \"document\", \"example\"]\n# print(compute_tf_idf(corpus, query))\n```\n\n### Explanation:\n1. **Document Frequency (DF):** We use a `defaultdict` to count how many documents contain each term.\n2. **Term Frequency (TF):** For each term in the document, we count its occurrences and divide by the total number of terms in the document.\n3. **Inverse Document Frequency (IDF):** We use the formula `log((N + 1) / (df(term) + 1)) + 1` to avoid division by zero and handle terms not present in any document.\n4. **TF-IDF Calculation:** For each term in the query, we compute the TF-IDF score for each document and store it in a list.\n5. **Rounding and Conversion:** We use `numpy` to handle the rounding and conversion to a list.\n\nThis implementation ensures that the function is robust and handles various edge cases effectively."}
{"task_id": 61, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `f_score` function that calculates the F-Score for a binary classification task. The function takes into account the true labels, predicted labels, and a beta parameter to adjust the importance of precision and recall.\n\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Ensure y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives, False Positives, and False Negatives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    # Return F-Score rounded to three decimal places\n    return round(f_score_value, 3)\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0, 1])\n# y_pred = np.array([1, 0, 0, 1, 0, 1])\n# beta = 1.0\n# print(f_score(y_true, y_pred, beta))  # Output: 0.857\n```\n\n### Explanation:\n- **True Positives (TP)**: The number of positive samples correctly predicted as positive.\n- **False Positives (FP)**: The number of negative samples incorrectly predicted as positive.\n- **False Negatives (FN)**: The number of positive samples incorrectly predicted as negative.\n- **Precision**: The ratio of true positive predictions to the total predicted positives.\n- **Recall**: The ratio of true positive predictions to the total actual positives.\n- **F-Score**: A weighted harmonic mean of precision and recall, adjusted by the beta parameter. When `beta=1`, it computes the F1-Score.\n\nThe function handles edge cases where precision or recall might be zero to avoid division by zero errors."}
{"task_id": 62, "completion_id": 0, "completion": "Certainly! Below is the implementation of a simple RNN with backpropagation through time (BPTT) in Python. The code includes the initialization of weights, the forward pass, and the backward pass with BPTT.\n\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Parameters:\n        - input_sequence: A list of input vectors, each of shape (input_size, 1).\n        \n        Returns:\n        - outputs: A list of output vectors, each of shape (output_size, 1).\n        - last_inputs: A list of input vectors used in the forward pass.\n        - last_hiddens: A list of hidden states used in the forward pass.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n\n        for x in input_sequence:\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x)\n            last_hiddens.append(h)\n            h_prev = h\n\n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \n        Parameters:\n        - input_sequence: A list of input vectors, each of shape (input_size, 1).\n        - expected_output: A list of expected output vectors, each of shape (output_size, 1).\n        - outputs: A list of output vectors from the forward pass, each of shape (output_size, 1).\n        - last_inputs: A list of input vectors used in the forward pass.\n        - last_hiddens: A list of hidden states used in the forward pass.\n        - learning_rate: The learning rate for weight updates.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t]\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh  # tanh derivative\n\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t-1].T) if t > 0 else np.zeros_like(dW_hh)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Update weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n# Example usage:\n# input_size = 10\n# hidden_size = 20\n# output_size = 10\n# rnn = SimpleRNN(input_size, hidden_size, output_size)\n# input_sequence = [np.random.randn(input_size, 1) for _ in range(5)]\n# expected_output = [np.random.randn(output_size, 1) for _ in range(5)]\n# outputs, last_inputs, last_hiddens = rnn.forward(input_sequence)\n# rnn.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01)\n```\n\n### Explanation:\n1. **Initialization**: The RNN is initialized with random weights and zero biases.\n2. **Forward Pass**:\n   - The hidden state is updated using the current input and the previous hidden state.\n   - The output is computed using the current hidden state.\n   - The hidden states and inputs are stored for use in the backward pass.\n3. **Backward Pass (BPTT)**:\n   - The error is computed as the difference between the predicted output and the expected output.\n   - Gradients are computed for the weights and biases using the chain rule.\n   - The weights and biases are updated using the computed gradients and the learning rate.\n\nThis implementation uses the tanh activation function for the hidden layer and a linear activation function for the output layer. The loss function is 1/2 * Mean Squared Error (MSE)."}
{"task_id": 63, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Conjugate Gradient (CG) method for solving a linear system \\( Ax = b \\) where \\( A \\) is a symmetric, positive-definite matrix.\n\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    # Initialize the solution vector\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    \n    # Initial residual\n    r = b - A @ x\n    # Initial search direction\n    p = r\n    # Initial residual norm squared\n    r_norm_squared = np.dot(r, r)\n    \n    for i in range(n):\n        # Matrix-vector product\n        Ap = A @ p\n        # Step size\n        alpha = r_norm_squared / np.dot(p, Ap)\n        # Update the solution\n        x = x + alpha * p\n        # Update the residual\n        r_new = r - alpha * Ap\n        # New residual norm squared\n        r_new_norm_squared = np.dot(r_new, r_new)\n        # Check for convergence\n        if np.sqrt(r_new_norm_squared) < tol:\n            break\n        # Update the search direction\n        beta = r_new_norm_squared / r_norm_squared\n        p = r_new + beta * p\n        # Update the residual norm squared\n        r_norm_squared = r_new_norm_squared\n    \n    # Return the solution rounded to 8 decimal places\n    return x.round(8).tolist()\n\n# Example usage:\n# A = np.array([[4, 1], [1, 3]])\n# b = np.array([1, 2])\n# x = conjugate_gradient(A, b, n=100)\n# print(x)\n```\n\n### Explanation:\n1. **Initialization**:\n   - The solution vector `x` is initialized to zero or to the provided initial guess `x0`.\n   - The initial residual `r` is computed as \\( r = b - Ax \\).\n   - The initial search direction `p` is set to the initial residual `r`.\n   - The initial residual norm squared `r_norm_squared` is computed as \\( r^T r \\).\n\n2. **Iteration**:\n   - For each iteration, the matrix-vector product \\( Ap \\) is computed.\n   - The step size `alpha` is determined as \\( \\alpha = \\frac{r^T r}{p^T Ap} \\).\n   - The solution vector `x` is updated as \\( x = x + \\alpha p \\).\n   - The residual `r` is updated as \\( r = r - \\alpha Ap \\).\n   - The new residual norm squared `r_new_norm_squared` is computed.\n   - If the norm of the new residual is less than the tolerance `tol`, the iteration stops.\n   - The search direction `p` is updated using the formula \\( p = r + \\beta p \\), where \\( \\beta = \\frac{r_{\\text{new}}^T r_{\\text{new}}}{r^T r} \\).\n\n3. **Termination**:\n   - The solution vector `x` is rounded to 8 decimal places and converted to a list before being returned.\n\nThis implementation efficiently solves the linear system using the Conjugate Gradient method, which is particularly well-suited for large, sparse, symmetric, positive-definite matrices."}
{"task_id": 64, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `gini_impurity` function in Python, which calculates the Gini Impurity for a list of class labels:\n\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    \n    # Calculate the number of samples\n    n_samples = len(y)\n    \n    # Calculate the frequency of each class\n    class_counts = np.bincount(y)\n    \n    # Calculate the Gini Impurity\n    gini = 1.0\n    for count in class_counts:\n        p_class = count / n_samples\n        gini -= p_class ** 2\n    \n    return round(gini, 3)\n\n# Example usage:\n# y = [0, 1, 1, 0, 1]\n# print(gini_impurity(y))  # Output: 0.480\n```\n\n### Explanation:\n1. **Input Check**: The function first checks if the input list `y` is empty. If it is, the Gini Impurity is `0.0` because there are no classes to measure impurity over.\n2. **Sample Count**: It calculates the total number of samples in the list `y`.\n3. **Class Frequency**: It uses `np.bincount` to count the occurrences of each class in the list. This function returns an array where the index represents the class label and the value at that index represents the count of that class.\n4. **Gini Impurity Calculation**: The Gini Impurity is calculated using the formula:\n   \\[\n   Gini = 1 - \\sum_{i=1}^{n} (p_i)^2\n   \\]\n   where \\( p_i \\) is the probability of class \\( i \\) in the dataset.\n5. **Rounding**: The result is rounded to three decimal places for precision.\n\nThis function can be used to evaluate the impurity of a node in a decision tree, helping to determine the best feature to split on."}
{"task_id": 65, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `compressed_row_sparse_matrix` function that converts a given dense matrix into the Compressed Row Sparse (CSR) format:\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]  # Start with 0 to indicate the start of the first row\n\n    for row in dense_matrix:\n        current_row_non_zeros = 0\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                current_row_non_zeros += 1\n        # Update the row pointer with the cumulative count of non-zero elements\n        row_pointer.append(row_pointer[-1] + current_row_non_zeros)\n\n    return values, column_indices, row_pointer\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [4, 0, 0, 0],\n    [0, 0, 0, 5],\n    [0, 6, 0, 0]\n]\n\nvalues, column_indices, row_pointer = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", column_indices)\nprint(\"Row Pointer:\", row_pointer)\n```\n\n### Explanation:\n- **Values Array**: This list stores all the non-zero elements of the matrix in row-major order.\n- **Column Indices Array**: This list stores the column indices corresponding to each non-zero element in the `values` array.\n- **Row Pointer Array**: This list stores the cumulative number of non-zero elements up to each row. The length of this list is one more than the number of rows in the matrix, with the first element being 0 to indicate the start of the first row.\n\n### Example:\nFor the given `dense_matrix`:\n```\n[\n    [0, 0, 3, 0],\n    [4, 0, 0, 0],\n    [0, 0, 0, 5],\n    [0, 6, 0, 0]\n]\n```\nThe output will be:\n- **Values**: `[3, 4, 5, 6]`\n- **Column Indices**: `[2, 0, 3, 1]`\n- **Row Pointer**: `[0, 1, 2, 3, 4]`\n\nThis indicates that:\n- The first row has one non-zero element (3) at column index 2.\n- The second row has one non-zero element (4) at column index 0.\n- The third row has one non-zero element (5) at column index 3.\n- The fourth row has one non-zero element (6) at column index 1."}
{"task_id": 66, "completion_id": 0, "completion": "To implement the orthogonal projection of a vector `v` onto another vector `L`, we can use the formula for the projection of `v` onto `L`:\n\n\\[ \\text{proj}_L v = \\left( \\frac{v \\cdot L}{L \\cdot L} \\right) L \\]\n\nWhere:\n- \\( v \\cdot L \\) is the dot product of vectors `v` and `L`.\n- \\( L \\cdot L \\) is the dot product of vector `L` with itself.\n\nHere's the implementation of the function `orthogonal_projection`:\n\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_product_vL = sum(x * y for x, y in zip(v, L))\n    \n    # Calculate the dot product of L with itself\n    dot_product_LL = sum(x * x for x in L)\n    \n    # Calculate the scalar projection factor\n    scalar_projection = dot_product_vL / dot_product_LL\n    \n    # Calculate the projection vector\n    projection = [scalar_projection * x for x in L]\n    \n    # Round each component of the projection vector to three decimal places\n    projection_rounded = [round(x, 3) for x in projection]\n    \n    return projection_rounded\n\n# Example usage:\n# v = [3, 4]\n# L = [1, 0]\n# print(orthogonal_projection(v, L))  # Output: [3.0, 0.0]\n```\n\n### Explanation:\n1. **Dot Product Calculation**: We calculate the dot product of `v` and `L` using a generator expression inside the `sum` function.\n2. **Dot Product of `L` with Itself**: Similarly, we calculate the dot product of `L` with itself.\n3. **Scalar Projection**: The scalar projection is the ratio of the dot product of `v` and `L` to the dot product of `L` with itself.\n4. **Projection Vector**: We multiply each component of `L` by the scalar projection to get the projection vector.\n5. **Rounding**: Finally, we round each component of the projection vector to three decimal places for the output.\n\nThis function will correctly compute the orthogonal projection of `v` onto `L` and return the result as a list of rounded values."}
{"task_id": 67, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `compressed_col_sparse_matrix` function that converts a dense matrix into its Compressed Column Sparse (CSC) representation:\n\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n\n    return values, row_indices, column_pointer\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [4, 0, 0, 0],\n    [0, 5, 0, 6],\n    [0, 0, 0, 0]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", column_pointer)\n```\n\n### Explanation:\n- **Values**: This list stores all the non-zero elements of the matrix in column-major order.\n- **Row Indices**: This list stores the row indices corresponding to each non-zero value in the `values` list.\n- **Column Pointer**: This list indicates the starting index in the `values` list for each column. The length of this list is `num_cols + 1`, where the last element is the total number of non-zero elements.\n\n### Example:\nFor the given `dense_matrix`:\n```\n[\n    [0, 0, 3, 0],\n    [4, 0, 0, 0],\n    [0, 5, 0, 6],\n    [0, 0, 0, 0]\n]\n```\nThe output will be:\n- **Values**: `[3, 4, 5, 6]`\n- **Row Indices**: `[2, 0, 1, 2]`\n- **Column Pointer**: `[0, 0, 1, 2, 4]`\n\nThis indicates that:\n- Column 0 has no non-zero elements.\n- Column 1 has one non-zero element starting at index 0 in `values`.\n- Column 2 has two non-zero elements starting at index 1 in `values`.\n- Column 3 has two non-zero elements starting at index 2 in `values`."}
{"task_id": 68, "completion_id": 0, "completion": "To find the column space (or image) of a matrix \\( A \\), we can use the concept of row echelon form (REF) or reduced row echelon form (RREF). The columns of the original matrix that correspond to the pivot columns in the RREF form a basis for the column space.\n\nHere's how we can implement the `matrix_image` function:\n\n1. Convert the matrix \\( A \\) to its reduced row echelon form (RREF).\n2. Identify the pivot columns in the RREF.\n3. Extract the corresponding columns from the original matrix \\( A \\) to form the basis for the column space.\n\nWe'll use the `scipy.linalg` library to compute the RREF of the matrix. Here's the complete implementation:\n\n```python\nimport numpy as np\nfrom scipy.linalg import lu\n\ndef matrix_image(A):\n    # Convert A to a numpy array if it isn't already\n    A = np.array(A, dtype=float)\n    \n    # Perform LU decomposition with partial pivoting\n    P, L, U = lu(A)\n    \n    # U is in row echelon form, we need to find the pivot columns\n    # A column in U is a pivot column if it has a leading 1\n    m, n = U.shape\n    pivot_columns = []\n    for col in range(n):\n        if np.isclose(U[:, col][np.argmax(U[:, col])], 1.0):\n            pivot_columns.append(col)\n    \n    # Extract the corresponding columns from the original matrix A\n    basis = A[:, pivot_columns]\n    \n    # Round the result to 8 decimal places and convert to list\n    return np.round(basis, 8).tolist()\n\n# Example usage:\n# A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# print(matrix_image(A))\n```\n\n### Explanation:\n\n- **LU Decomposition**: We use LU decomposition with partial pivoting to transform the matrix \\( A \\) into an upper triangular matrix \\( U \\). The pivot columns in \\( U \\) correspond to the linearly independent columns in \\( A \\).\n  \n- **Identifying Pivot Columns**: We check each column of \\( U \\) to see if it contains a leading 1 (after accounting for numerical precision using `np.isclose`).\n\n- **Extracting Basis Vectors**: The columns of \\( A \\) that correspond to the pivot columns in \\( U \\) form a basis for the column space of \\( A \\).\n\n- **Rounding and Conversion**: The resulting basis vectors are rounded to 8 decimal places and converted to a list for the final output.\n\nThis approach ensures that we correctly identify the basis vectors for the column space of the matrix \\( A \\)."}
{"task_id": 69, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `r_squared` function that calculates the R-squared value given arrays of true values `y_true` and predicted values `y_pred`. The result is rounded to three decimal places.\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    # Convert inputs to numpy arrays for efficient computation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the mean of the true values\n    y_mean = np.mean(y_true)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - y_mean) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    # Return R-squared rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\n# y_true = [3, -0.5, 2, 7]\n# y_pred = [2.5, 0.0, 2, 8]\n# print(r_squared(y_true, y_pred))  # Output should be 0.948\n```\n\n### Explanation:\n- **Total Sum of Squares (TSS):** This measures the total variance in the dependent variable. It is calculated as the sum of the squared differences between each true value and the mean of the true values.\n- **Residual Sum of Squares (RSS):** This measures the variance in the dependent variable that is not explained by the model. It is calculated as the sum of the squared differences between each true value and the corresponding predicted value.\n- **R-squared:** This is calculated as \\(1 - \\frac{\\text{RSS}}{\\text{TSS}}\\). It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n\nThe function returns the R-squared value rounded to three decimal places as required."}
{"task_id": 70, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `calculate_brightness` function that meets the specified requirements:\n\n```python\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img or not img[0]:\n        return -1\n    \n    # Check for inconsistent row lengths\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    \n    # Initialize total pixel value and count\n    total_pixel_value = 0\n    pixel_count = 0\n    \n    # Iterate over each pixel in the image\n    for row in img:\n        for pixel in row:\n            # Check if the pixel value is within the valid range\n            if not (0 <= pixel <= 255):\n                return -1\n            total_pixel_value += pixel\n            pixel_count += 1\n    \n    # Calculate the average brightness\n    average_brightness = total_pixel_value / pixel_count\n    \n    # Return the average brightness rounded to two decimal places\n    return round(average_brightness, 2)\n\n# Example usage:\n# img = [\n#     [100, 150, 200],\n#     [50, 100, 150],\n#     [0, 50, 100]\n# ]\n# print(calculate_brightness(img))  # Output: 104.17\n```\n\n### Explanation:\n1. **Empty Image Check**: The function first checks if the image matrix is empty or if the first row is empty. If so, it returns `-1`.\n2. **Inconsistent Row Lengths**: It checks if all rows have the same length. If any row has a different length, it returns `-1`.\n3. **Pixel Value Validation**: It iterates through each pixel to ensure all values are within the range 0 to 255. If any value is outside this range, it returns `-1`.\n4. **Average Brightness Calculation**: It calculates the total sum of all pixel values and divides by the total number of pixels to get the average brightness. The result is rounded to two decimal places before being returned."}
{"task_id": 71, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `rmse` function that calculates the Root Mean Square Error (RMSE) between the actual values (`y_true`) and the predicted values (`y_pred`). The function includes error handling for mismatched array shapes, empty arrays, and invalid input types.\n\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if inputs are numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Both y_true and y_pred must be numpy arrays.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"y_true and y_pred must not be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # Calculate RMSE\n    n = y_true.size\n    residuals = y_true - y_pred\n    mse = np.mean(residuals ** 2)\n    rmse_value = np.sqrt(mse)\n    \n    # Return RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(rmse(y_true, y_pred))  # Output: 0.316\n```\n\n### Explanation:\n1. **Input Validation**:\n   - The function first checks if both `y_true` and `y_pred` are numpy arrays. If not, it raises a `ValueError`.\n   - It then checks if either of the arrays is empty and raises a `ValueError` if so.\n   - It checks if the shapes of the two arrays match. If not, it raises a `ValueError`.\n\n2. **RMSE Calculation**:\n   - The number of observations `n` is determined by the size of the arrays.\n   - The residuals (differences between actual and predicted values) are calculated.\n   - The Mean Squared Error (MSE) is computed as the mean of the squared residuals.\n   - The RMSE is the square root of the MSE.\n\n3. **Output**:\n   - The RMSE value is rounded to three decimal places before being returned.\n\nThis implementation ensures that the function is robust and handles various edge cases effectively."}
{"task_id": 72, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `jaccard_index` function that calculates the Jaccard Index for binary classification tasks. The function handles edge cases such as no overlap and arrays containing only zeros.\n\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Ensure the inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection and union of the two arrays\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    \n    # Handle the edge case where the union is zero (both arrays are all zeros)\n    if union == 0:\n        return 1.0  # If both are all zeros, we consider them perfectly overlapping\n    \n    # Calculate the Jaccard Index\n    jaccard_idx = intersection / union\n    \n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard_idx, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 0, 1]\n# y_pred = [1, 0, 0, 0, 1]\n# print(jaccard_index(y_true, y_pred))  # Output: 0.667\n```\n\n### Explanation:\n1. **Input Conversion**: The function first ensures that the inputs `y_true` and `y_pred` are numpy arrays for efficient computation.\n2. **Intersection and Union Calculation**:\n   - The intersection is calculated using `np.logical_and(y_true, y_pred).sum()`, which counts the number of positions where both arrays have a value of 1.\n   - The union is calculated using `np.logical_or(y_true, y_pred).sum()`, which counts the number of positions where at least one of the arrays has a value of 1.\n3. **Edge Case Handling**: If the union is zero (both arrays are all zeros), the function returns 1.0, indicating perfect overlap since there are no differing elements.\n4. **Jaccard Index Calculation**: The Jaccard Index is computed as the ratio of the intersection to the union.\n5. **Rounding**: The result is rounded to three decimal places before being returned.\n\nThis function should work efficiently for binary classification tasks and handle the specified edge cases appropriately."}
{"task_id": 73, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `dice_score` function in Python, which calculates the Dice Score for binary classification. The function handles edge cases, such as when there are no true or predicted positives, by returning a Dice Score of 0 in such scenarios.\n\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    # Ensure the inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if the inputs have the same length\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # Calculate the intersection of y_true and y_pred\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the sum of y_true and y_pred\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    \n    # Calculate the Dice Score\n    if sum_true == 0 and sum_pred == 0:\n        # If both are empty, we consider it a perfect match\n        return 1.0\n    elif sum_true == 0 or sum_pred == 0:\n        # If one of them is empty and the other is not, the Dice Score is 0\n        return 0.0\n    else:\n        dice_score_value = (2 * intersection) / (sum_true + sum_pred)\n        return round(dice_score_value, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0]\n# y_pred = [1, 1, 1, 0, 0]\n# print(dice_score(y_true, y_pred))  # Output: 0.5\n```\n\n### Explanation:\n- **Intersection Calculation**: The intersection of `y_true` and `y_pred` is calculated using element-wise multiplication (`y_true * y_pred`), which results in a binary array where elements are 1 only if both `y_true` and `y_pred` are 1 at that position. The sum of this array gives the number of true positives.\n- **Sum Calculation**: The sum of `y_true` and `y_pred` gives the total number of true and predicted positives, respectively.\n- **Dice Score Calculation**: The Dice Score is computed using the formula provided. If both `y_true` and `y_pred` are empty (i.e., all zeros), the function returns 1.0, indicating a perfect match. If one of them is empty and the other is not, the function returns 0.0, indicating no overlap.\n- **Rounding**: The result is rounded to three decimal places as specified.\n\nThis function should work efficiently for binary classification tasks where `y_true` and `y_pred` are binary arrays of the same length."}
{"task_id": 74, "completion_id": 0, "completion": "To implement the `create_row_hv` function, we need to follow these steps:\n\n1. **Generate Hypervectors for Feature Names**: For each feature in the row, generate a hypervector representing the feature name. This can be done using a random seed specific to the feature name.\n\n2. **Generate Hypervectors for Feature Values**: For each feature value, generate a hypervector using the same seed as the feature name to ensure reproducibility.\n\n3. **Bind Feature Name and Value Hypervectors**: Use a binding operation (such as element-wise multiplication followed by normalization) to combine the hypervectors of the feature name and its value.\n\n4. **Bundle All Feature Hypervectors**: Combine all the bound hypervectors from step 3 to form a composite hypervector for the entire row. This can be done using a bundling operation (such as element-wise addition followed by normalization).\n\n5. **Return the Composite Hypervector**: Convert the final composite hypervector to a list before returning it.\n\nHere's the implementation of the `create_row_hv` function:\n\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    def generate_hv(seed, dim):\n        \"\"\"Generate a random hypervector with a given seed.\"\"\"\n        np.random.seed(seed)\n        hv = np.random.choice([-1, 1], size=dim)\n        return hv\n    \n    def bind_hv(hv1, hv2):\n        \"\"\"Bind two hypervectors using element-wise multiplication and normalization.\"\"\"\n        bound_hv = hv1 * hv2\n        bound_hv /= np.linalg.norm(bound_hv)\n        return bound_hv\n    \n    def bundle_hvs(hvs):\n        \"\"\"Bundle a list of hypervectors using element-wise addition and normalization.\"\"\"\n        bundled_hv = np.sum(hvs, axis=0)\n        bundled_hv /= np.linalg.norm(bundled_hv)\n        return bundled_hv\n    \n    # List to store the bound hypervectors for each feature\n    bound_hvs = []\n    \n    for feature, value in row.items():\n        # Generate hypervector for the feature name\n        feature_seed = random_seeds[feature]\n        feature_hv = generate_hv(feature_seed, dim)\n        \n        # Generate hypervector for the feature value using the same seed\n        value_seed = hash(value) ^ feature_seed  # Combine value and feature seed for uniqueness\n        value_hv = generate_hv(value_seed, dim)\n        \n        # Bind the feature name and value hypervectors\n        bound_hv = bind_hv(feature_hv, value_hv)\n        \n        # Add the bound hypervector to the list\n        bound_hvs.append(bound_hv)\n    \n    # Bundle all the bound hypervectors to form the composite hypervector for the row\n    composite_hv = bundle_hvs(bound_hvs)\n    \n    # Convert the composite hypervector to a list and return\n    return composite_hv.tolist()\n```\n\n### Explanation:\n- **generate_hv**: This helper function generates a random hypervector of a specified dimension using a given seed.\n- **bind_hv**: This helper function binds two hypervectors using element-wise multiplication and normalizes the result.\n- **bundle_hvs**: This helper function bundles a list of hypervectors using element-wise addition and normalizes the result.\n- **Main Logic**: For each feature in the row, we generate hypervectors for both the feature name and its value, bind them, and then bundle all the bound hypervectors to form the composite hypervector for the row.\n\nThis implementation ensures that the hypervectors are generated in a reproducible manner using the provided seeds and that the composite hypervector accurately represents the entire row."}
{"task_id": 75, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `confusion_matrix` function that generates a confusion matrix for a binary classification problem. The confusion matrix is represented as a 2x2 list of lists, where the rows correspond to the actual classes and the columns correspond to the predicted classes.\n\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Initialize the confusion matrix\n    # Rows: Actual, Columns: Predicted\n    # [[TN, FP], [FN, TP]]\n    cm = [[0, 0], [0, 0]]\n    \n    for y_true, y_pred in data:\n        if y_true == 0 and y_pred == 0:\n            cm[0][0] += 1  # True Negative\n        elif y_true == 0 and y_pred == 1:\n            cm[0][1] += 1  # False Positive\n        elif y_true == 1 and y_pred == 0:\n            cm[1][0] += 1  # False Negative\n        elif y_true == 1 and y_pred == 1:\n            cm[1][1] += 1  # True Positive\n    \n    return cm\n\n# Example usage:\ndata = [\n    [0, 0],  # True Negative\n    [0, 1],  # False Positive\n    [1, 0],  # False Negative\n    [1, 1],  # True Positive\n    [0, 0],  # True Negative\n    [1, 1],  # True Positive\n]\n\nprint(confusion_matrix(data))\n# Output: [[2, 1], [1, 2]]\n```\n\n### Explanation:\n- **True Negative (TN):** The actual label is 0, and the predicted label is 0.\n- **False Positive (FP):** The actual label is 0, but the predicted label is 1.\n- **False Negative (FN):** The actual label is 1, but the predicted label is 0.\n- **True Positive (TP):** The actual label is 1, and the predicted label is 1.\n\nThe function iterates over each pair of `[y_true, y_pred]` in the input data and updates the appropriate cell in the confusion matrix based on the values of `y_true` and `y_pred`."}
{"task_id": 76, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `cosine_similarity` function using NumPy. This function calculates the cosine similarity between two vectors, ensuring that the vectors have the same shape and are not empty or zero magnitude.\n\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Check if the vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Vectors must have the same shape.\")\n    \n    # Check if the vectors are not empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Vectors cannot be empty.\")\n    \n    # Calculate the dot product of the vectors\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitude (norm) of each vector\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    \n    # Check if the magnitudes are not zero\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError(\"Vectors cannot have zero magnitude.\")\n    \n    # Calculate the cosine similarity\n    cosine_sim = dot_product / (norm_v1 * norm_v2)\n    \n    # Return the cosine similarity rounded to three decimal places\n    return round(cosine_sim, 3)\n\n# Example usage:\n# v1 = np.array([1, 2, 3])\n# v2 = np.array([4, 5, 6])\n# print(cosine_similarity(v1, v2))  # Output: 0.974\n```\n\n### Explanation:\n1. **Shape Check**: The function first checks if the input vectors `v1` and `v2` have the same shape. If not, it raises a `ValueError`.\n2. **Empty Check**: It checks if either of the vectors is empty. If so, it raises a `ValueError`.\n3. **Dot Product**: The dot product of the two vectors is calculated using `np.dot`.\n4. **Magnitude Calculation**: The magnitudes (or norms) of the vectors are calculated using `np.linalg.norm`.\n5. **Zero Magnitude Check**: It checks if either vector has a zero magnitude, which would make the cosine similarity undefined. If so, it raises a `ValueError`.\n6. **Cosine Similarity Calculation**: The cosine similarity is calculated by dividing the dot product by the product of the magnitudes.\n7. **Rounding**: The result is rounded to three decimal places before being returned.\n\nThis function should work correctly for any valid input vectors that meet the constraints specified."}
{"task_id": 77, "completion_id": 0, "completion": "To implement the `performance_metrics` function, we need to calculate several performance metrics for a binary classification model. Here's a step-by-step breakdown of how to compute each metric:\n\n1. **Confusion Matrix**: This is a 2x2 matrix that contains the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n2. **Accuracy**: This is the ratio of correctly predicted instances to the total instances.\n3. **F1 Score**: This is the harmonic mean of precision and recall.\n4. **Specificity**: This is the ratio of true negatives to the total actual negatives.\n5. **Negative Predictive Value (NPV)**: This is the ratio of true negatives to the total predicted negatives.\n\nLet's implement the function:\n\n```python\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    # Ensure the input lists are of the same length\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    \n    # Ensure all elements are either 0 or 1\n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    \n    # Initialize counters for TP, TN, FP, FN\n    TP = FP = TN = FN = 0\n    \n    # Calculate TP, TN, FP, FN\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 1 and p == 0:\n            FN += 1\n    \n    # Confusion Matrix\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    \n    # Accuracy\n    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0.0\n    \n    # Precision\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    \n    # Recall (Sensitivity)\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n    \n    # F1 Score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    # Specificity\n    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n    \n    # Negative Predictive Value\n    negative_predictive_value = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n    \n    # Return the results rounded to three decimal places\n    return (\n        confusion_matrix,\n        round(accuracy, 3),\n        round(f1_score, 3),\n        round(specificity, 3),\n        round(negative_predictive_value, 3)\n    )\n\n# Example usage:\n# actual = [1, 0, 1, 1, 0, 1]\n# predicted = [1, 0, 0, 1, 0, 1]\n# print(performance_metrics(actual, predicted))\n```\n\nThis function first checks if the input lists are valid, then calculates the necessary counts for the confusion matrix, and finally computes each of the required metrics. The results are rounded to three decimal places as specified."}
{"task_id": 78, "completion_id": 0, "completion": "Certainly! Below is the Python function `descriptive_statistics` that calculates the specified descriptive statistics for a given dataset. The function uses NumPy for efficient computation and handles the rounding of the results as specified.\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    # Convert data to a NumPy array if it isn't already\n    data = np.array(data)\n    \n    # Calculate mean\n    mean = np.mean(data)\n    \n    # Calculate median\n    median = np.median(data)\n    \n    # Calculate mode\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0]  # stats.mode returns a ModeResult object\n    \n    # Calculate variance\n    variance = np.var(data, ddof=0)  # Population variance\n    \n    # Calculate standard deviation\n    standard_deviation = np.std(data, ddof=0)  # Population standard deviation\n    \n    # Calculate percentiles\n    percentiles = np.percentile(data, [25, 50, 75])\n    percentile_25 = percentiles[0]\n    percentile_50 = percentiles[1]\n    percentile_75 = percentiles[2]\n    \n    # Calculate interquartile range (IQR)\n    iqr = percentile_75 - percentile_25\n    \n    # Create the result dictionary\n    result = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': round(mode, 4),\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(iqr, 4)\n    }\n    \n    return result\n\n# Example usage:\n# data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]\n# print(descriptive_statistics(data))\n```\n\n### Explanation:\n- **Mean**: Calculated using `np.mean`.\n- **Median**: Calculated using `np.median`.\n- **Mode**: Calculated using `stats.mode` from the `scipy` library, which returns a `ModeResult` object. We extract the mode value using `mode_result.mode[0]`.\n- **Variance**: Calculated using `np.var` with `ddof=0` to get the population variance.\n- **Standard Deviation**: Calculated using `np.std` with `ddof=0` to get the population standard deviation.\n- **Percentiles**: Calculated using `np.percentile` for the 25th, 50th, and 75th percentiles.\n- **Interquartile Range (IQR)**: Calculated as the difference between the 75th and 25th percentiles.\n\nThis function will return a dictionary with all the specified statistics, rounded to four decimal places where necessary."}
{"task_id": 79, "completion_id": 0, "completion": "Certainly! Below is the Python function that calculates the probability of achieving exactly \\( k \\) successes in \\( n \\) independent Bernoulli trials, each with a probability \\( p \\) of success, using the Binomial distribution formula. The result is rounded to five decimal places.\n\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binom_coeff = math.comb(n, k)\n    \n    # Calculate the probability using the binomial distribution formula\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# n = 10, k = 3, p = 0.5\n# print(binomial_probability(10, 3, 0.5))  # Output: 0.11719\n```\n\n### Explanation:\n- **Binomial Coefficient**: The function uses `math.comb(n, k)` to calculate the binomial coefficient, which is the number of ways to choose \\( k \\) successes from \\( n \\) trials.\n- **Probability Calculation**: The probability is calculated using the formula:\n  \\[\n  P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1-p)^{n-k}\n  \\]\n- **Rounding**: The result is rounded to five decimal places using the `round()` function to meet the output constraints."}
{"task_id": 80, "completion_id": 0, "completion": "Certainly! Below is the Python function that calculates the probability density function (PDF) of the normal distribution using the given formula. The function takes three parameters: `x` (the value at which the PDF is evaluated), `mean` (the mean of the distribution), and `std_dev` (the standard deviation of the distribution). The result is rounded to 5 decimal places.\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be greater than 0.\")\n    \n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    \n    return round(pdf_value, 5)\n\n# Example usage:\n# print(normal_pdf(0, 0, 1))  # Should print the PDF of standard normal distribution at x=0\n```\n\n### Explanation:\n- **Coefficient**: The term `1 / (std_dev * math.sqrt(2 * math.pi))` is the normalization factor that ensures the total area under the curve is 1.\n- **Exponent**: The term `-0.5 * ((x - mean) / std_dev) ** 2` is the exponent part of the Gaussian function.\n- **PDF Value**: The product of the coefficient and the exponential term gives the PDF value at `x`.\n- **Rounding**: The result is rounded to 5 decimal places for precision.\n\nThis function also includes a check to ensure that the standard deviation is greater than 0, as a non-positive standard deviation is not valid for a normal distribution."}
{"task_id": 81, "completion_id": 0, "completion": "Certainly! Below is the Python function that calculates the probability of observing exactly \\( k \\) events in a fixed interval using the Poisson distribution formula. The function takes \\( k \\) (number of events) and \\( \\lambda \\) (mean rate of occurrences) as inputs and returns the probability rounded to 5 decimal places.\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or not isinstance(k, int):\n        raise ValueError(\"k must be a non-negative integer.\")\n    if lam <= 0:\n        raise ValueError(\"lam must be a positive number.\")\n    \n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(probability, 5)\n\n# Example usage:\n# print(poisson_probability(3, 2.5))  # Example call to the function\n```\n\n### Explanation:\n- **Poisson Distribution Formula**: The formula for the Poisson distribution is:\n  \\[\n  P(X = k) = \\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!}\n  \\]\n  where \\( e \\) is the base of the natural logarithm, \\( \\lambda \\) is the average rate of occurrences, and \\( k \\) is the number of events.\n\n- **Function Parameters**:\n  - `k`: The number of events you want to calculate the probability for. It must be a non-negative integer.\n  - `lam`: The average rate (mean) of occurrences in the interval. It must be a positive number.\n\n- **Error Handling**: The function checks if `k` is a non-negative integer and if `lam` is a positive number, raising a `ValueError` if these conditions are not met.\n\n- **Rounding**: The result is rounded to 5 decimal places for precision.\n\nYou can test the function with different values of `k` and `lam` to see how it works."}
{"task_id": 82, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `calculate_contrast` function that calculates the contrast of a grayscale image using the difference between the maximum and minimum pixel values.\n\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError(\"The input image must be a 2D array (grayscale image).\")\n    \n    if not (0 <= img.min() <= 255 and 0 <= img.max() <= 255):\n        raise ValueError(\"Pixel values must be between 0 and 255.\")\n    \n    max_pixel = img.max()\n    min_pixel = img.min()\n    \n    contrast = max_pixel - min_pixel\n    return contrast\n\n# Example usage:\n# img = np.array([[0, 50, 100], [150, 200, 255]])\n# print(calculate_contrast(img))  # Output: 255\n```\n\n### Explanation:\n- The function `calculate_contrast` takes a 2D NumPy array `img` as input, which represents a grayscale image.\n- It first checks if the input is a 2D array. If not, it raises a `ValueError`.\n- It then checks if the pixel values are within the valid range (0 to 255). If not, it raises a `ValueError`.\n- The function calculates the maximum and minimum pixel values in the image using `img.max()` and `img.min()`.\n- The contrast is computed as the difference between the maximum and minimum pixel values.\n- Finally, the function returns the calculated contrast.\n\nThis function assumes that the input image is a valid grayscale image with pixel values in the range [0, 255]."}
{"task_id": 83, "completion_id": 0, "completion": "Certainly! Below is the complete Python function to calculate the dot product of two vectors using NumPy:\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n        \n    Raises:\n        ValueError: If the input vectors are not 1D or do not have the same length.\n    \"\"\"\n    # Check if both inputs are 1D arrays\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D arrays.\")\n    \n    # Check if both vectors have the same length\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError(\"Both vectors must have the same length.\")\n    \n    # Calculate the dot product using numpy's dot function\n    dot_product = np.dot(vec1, vec2)\n    \n    return dot_product\n\n# Example usage:\n# vec1 = np.array([1, 2, 3])\n# vec2 = np.array([4, 5, 6])\n# print(calculate_dot_product(vec1, vec2))  # Output: 32\n```\n\n### Explanation:\n- The function `calculate_dot_product` takes two arguments, `vec1` and `vec2`, which are expected to be 1D NumPy arrays.\n- It first checks if both inputs are 1D arrays. If not, it raises a `ValueError`.\n- It then checks if both vectors have the same length. If not, it raises a `ValueError`.\n- If the inputs are valid, it calculates the dot product using `np.dot` and returns the result.\n- The function also includes a docstring that describes the purpose of the function, its arguments, return value, and potential exceptions."}
{"task_id": 84, "completion_id": 0, "completion": "To perform a Phi Transformation that maps input features into a higher-dimensional space by generating polynomial features, we can use the `PolynomialFeatures` class from the `sklearn.preprocessing` module. This class is designed to generate a new feature matrix consisting of all polynomial combinations of the features with a degree less than or equal to the specified degree.\n\nHere's how you can implement the `phi_transform` function:\n\n```python\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Convert the input data into a 2D array as required by PolynomialFeatures\n    data_array = np.array(data).reshape(-1, 1)\n    \n    # Initialize PolynomialFeatures with the specified degree\n    poly = PolynomialFeatures(degree, include_bias=False)\n    \n    # Fit and transform the data\n    transformed_data = poly.fit_transform(data_array)\n    \n    # Round the transformed data to 8 decimal places\n    transformed_data_rounded = np.round(transformed_data, 8)\n    \n    # Convert the result to a list of lists and return\n    return transformed_data_rounded.tolist()\n\n# Example usage:\n# data = [1.0, 2.0, 3.0]\n# degree = 2\n# print(phi_transform(data, degree))\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the degree is less than 0. If so, it returns an empty list as specified.\n- **Data Reshaping**: The input data is reshaped into a 2D array because `PolynomialFeatures` expects a 2D array where each row is a sample and each column is a feature.\n- **Polynomial Features**: We use `PolynomialFeatures` to generate the polynomial features. The `include_bias=False` parameter ensures that the constant term (bias) is not included in the output.\n- **Rounding**: The transformed data is rounded to 8 decimal places to meet the output constraints.\n- **Output**: The transformed data is converted back to a list of lists before being returned.\n\nThis function will correctly transform the input data into a higher-dimensional space using polynomial features, allowing models to capture nonlinear relationships in the data."}
{"task_id": 85, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Positional Encoding layer for Transformers as per your requirements. The function calculates the positional encodings using sine and cosine functions and returns the result as a list of lists with `dtype` `np.float16`.\n\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Initialize the positional encoding matrix\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    \n    # Calculate the positional encodings\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            angle = np.float16(pos / (10000 ** (i / d_model)))\n            pos_enc[pos, i] = np.sin(angle)\n            if i + 1 < d_model:  # Check to avoid index out of bounds\n                pos_enc[pos, i + 1] = np.cos(angle)\n    \n    # Convert to list and return\n    return pos_enc.tolist()\n\n# Example usage:\n# print(pos_encoding(5, 10))\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if `position` is 0 or `d_model` is less than or equal to 0. If so, it returns -1.\n- **Positional Encoding Calculation**:\n  - A zero matrix of shape `(position, d_model)` is initialized with `dtype` `np.float16`.\n  - For each position `pos` from 0 to `position - 1`, and for each dimension `i` from 0 to `d_model - 1` in steps of 2:\n    - The angle is calculated as `pos / (10000 ** (i / d_model))`.\n    - The sine of the angle is assigned to the even-indexed dimension (`i`).\n    - The cosine of the angle is assigned to the next odd-indexed dimension (`i + 1`), if it exists.\n- **Output**: The resulting matrix is converted to a list of lists using `tolist()` and returned.\n\nThis implementation ensures that the positional encodings are calculated correctly and efficiently, adhering to the constraints provided."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n    else:\n        return 0  # Good fit\n\n# Example usage:\n# print(model_fit_quality(0.9, 0.6))  # Output: 1 (Overfitting)\n# print(model_fit_quality(0.5, 0.4))  # Output: -1 (Underfitting)\n# print(model_fit_quality(0.8, 0.75)) # Output: 0 (Good fit)\n```\n\nThis function checks the conditions for overfitting, underfitting, and a good fit based on the provided training and test accuracy values. It returns `1` for overfitting, `-1` for underfitting, and `0` for a good fit."}
{"task_id": 87, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Adam optimizer update step function as per your requirements. The function handles both scalar and array inputs, and it includes bias correction for the moving averages.\n\n```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    \n    # Update biased first raw moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round the results to 5 decimal places and convert to list\n    updated_parameter = parameter.round(5).tolist()\n    updated_m = m.round(5).tolist()\n    updated_v = v.round(5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n\n# Example usage:\n# parameter = [0.0, 0.0]\n# grad = [0.1, 0.2]\n# m = [0.0, 0.0]\n# v = [0.0, 0.0]\n# t = 1\n# updated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\n# print(updated_parameter, updated_m, updated_v)\n```\n\nThis function will correctly update the parameters using the Adam optimization algorithm, including bias correction for the moving averages. The results are rounded to 5 decimal places and converted to lists before being returned."}
{"task_id": 88, "completion_id": 0, "completion": "To implement a simplified GPT-2-like text generation function, we need to follow the architecture components mentioned in the task description. We'll use the provided `load_encoder_hparams_and_params` function to get the necessary components: a dummy encoder, hyperparameters, and model parameters. Here's how you can implement the `gen_text` function:\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),  # Word token embeddings\n        \"wpe\": np.random.rand(1024, 10),  # Positional embeddings\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(variance + eps)\n    return g * x + b\n\ndef multi_head_attention(q, k, v, n_head):\n    batch_size, seq_len, d_model = q.shape\n    d_k = d_model // n_head\n\n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n\n    output = np.matmul(attention_weights, v)\n    output = output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return output\n\ndef feed_forward(x, d_model, d_ff):\n    x = np.maximum(0, np.dot(x, np.random.rand(d_model, d_ff)) + np.random.rand(d_ff))\n    x = np.dot(x, np.random.rand(d_ff, d_model)) + np.random.rand(d_model)\n    return x\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    n_ctx = hparams[\"n_ctx\"]\n    n_head = hparams[\"n_head\"]\n    wte = params[\"wte\"]\n    wpe = params[\"wpe\"]\n    ln_f_g = params[\"ln_f\"][\"g\"]\n    ln_f_b = params[\"ln_f\"][\"b\"]\n\n    # Encode the prompt\n    input_ids = encoder.encode(prompt)\n    input_ids = input_ids[-n_ctx:]  # Truncate to context length\n\n    # Generate tokens\n    for _ in range(n_tokens_to_generate):\n        # Prepare input embeddings\n        token_embeddings = wte[input_ids]\n        positions = np.arange(len(input_ids))\n        positional_embeddings = wpe[positions]\n        x = token_embeddings + positional_embeddings\n\n        # Multi-head attention\n        q = k = v = x\n        x = multi_head_attention(q, k, v, n_head)\n\n        # Feed-forward network\n        x = feed_forward(x, x.shape[-1], 20)  # Using 20 as a dummy d_ff\n\n        # Layer normalization\n        x = layer_norm(x, ln_f_g, ln_f_b)\n\n        # Predict the next token\n        logits = np.dot(x[-1], wte.T)  # Using token embeddings as a simple linear layer\n        next_token_id = np.argmax(logits)\n\n        # Append the predicted token to the input_ids\n        input_ids.append(next_token_id)\n\n        # Truncate to context length\n        input_ids = input_ids[-n_ctx:]\n\n    # Decode the generated tokens\n    generated_text = encoder.decode(input_ids)\n    return generated_text\n\n# Example usage\nprint(gen_text(\"hello world\", n_tokens_to_generate=10))\n```\n\n### Explanation:\n1. **Token and Positional Embeddings**: We use the provided `wte` and `wpe` to get the embeddings for the input tokens and their positions.\n2. **Multi-head Attention**: We implement a simple multi-head attention mechanism. Note that in a real GPT-2, the query, key, and value matrices are learned parameters, but here we use the same matrix for simplicity.\n3. **Feed-Forward Network**: A simple feed-forward network is implemented using ReLU activation.\n4. **Layer Normalization**: We apply layer normalization to stabilize the training process.\n5. **Text Generation**: We iteratively predict the next token by taking the last token's embedding, passing it through the model, and selecting the token with the highest probability.\n\nThis implementation is highly simplified and does not include many optimizations and features of a full GPT-2 model, such as learned query, key, and value matrices, more sophisticated feed-forward networks, and better parameter initialization. However, it should give you a basic understanding of how text generation works in models like GPT-2."}
{"task_id": 89, "completion_id": 0, "completion": "To implement the simplified self-attention mechanism for the Pattern Weaver's Code, we need to follow these steps:\n\n1. **Softmax Function**: Implement the softmax function to convert raw scores into probabilities.\n2. **Self-Attention Mechanism**:\n   - Compute the raw attention scores between each pair of crystals.\n   - Apply the softmax function to these scores to get the attention weights.\n   - Use these weights to compute the weighted sum of the crystal values, which represents the enhanced pattern for each crystal.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Subtract the max value for numerical stability\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=-1, keepdims=True)\n    \n    # Convert crystal values to a numpy array for easier manipulation\n    crystal_values = np.array(crystal_values).reshape(n, 1)\n    \n    # Initialize a matrix to hold the raw attention scores\n    attention_scores = np.zeros((n, n))\n    \n    # Compute the raw attention scores\n    for i in range(n):\n        for j in range(n):\n            # Here we use a simple dot product as the score function\n            # This is a simplified version of the self-attention mechanism\n            score = np.dot(crystal_values[i].T, crystal_values[j]) / np.sqrt(dimension)\n            attention_scores[i, j] = score\n    \n    # Apply softmax to get the attention weights\n    attention_weights = softmax(attention_scores)\n    \n    # Compute the final weighted pattern for each crystal\n    weighted_patterns = np.dot(attention_weights, crystal_values)\n    \n    # Flatten the result and round to 4 decimal places\n    result = [round(value, 4) for value in weighted_patterns.flatten()]\n    \n    return result\n\n# Example usage:\nn = 3\ncrystal_values = [1.0, 2.0, 3.0]\ndimension = 1\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\n### Explanation:\n\n- **Softmax Function**: The softmax function is implemented to handle numerical stability by subtracting the maximum value from the input array before exponentiation.\n- **Attention Scores**: We compute the raw attention scores using a dot product between each pair of crystal values, normalized by the square root of the dimension. This normalization is a common practice in self-attention mechanisms to prevent the dot product from becoming too large.\n- **Attention Weights**: The softmax function is applied to the attention scores to convert them into weights that sum to 1 for each crystal.\n- **Weighted Patterns**: The final weighted pattern for each crystal is computed by taking the dot product of the attention weights and the crystal values.\n- **Output**: The result is a list of floats, each representing the enhanced pattern for a crystal, rounded to four decimal places.\n\nThis implementation provides a basic self-attention mechanism tailored to the problem described."}
{"task_id": 90, "completion_id": 0, "completion": "To implement the BM25 ranking function, we need to follow these steps:\n\n1. **Calculate the Document Frequency (DF):** This is the number of documents that contain a given term.\n2. **Calculate the Inverse Document Frequency (IDF):** This is a measure of how important a term is. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.\n3. **Calculate the Term Frequency (TF):** This is the frequency of a term in a document.\n4. **Apply the BM25 formula:** The BM25 score for a document is calculated using the formula:\n   \\[\n   \\text{BM25}(d, q) = \\sum_{i \\in q} \\text{idf}(i) \\times \\left( \\frac{(k_1 + 1) \\times \\text{tf}(i, d)}{k_1 \\times \\left(1 - b + b \\times \\frac{|d|}{\\text{avgdl}}\\right) + \\text{tf}(i, d)} \\right)\n   \\]\n   where:\n   - \\( \\text{idf}(i) \\) is the inverse document frequency of term \\( i \\).\n   - \\( \\text{tf}(i, d) \\) is the term frequency of term \\( i \\) in document \\( d \\).\n   - \\( k_1 \\) and \\( b \\) are free parameters.\n   - \\( |d| \\) is the length of document \\( d \\).\n   - \\( \\text{avgdl} \\) is the average document length in the corpus.\n\nHere's the implementation of the BM25 ranking function:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Convert the corpus and query to lowercase to ensure case insensitivity\n    corpus = [doc.lower().split() for doc in corpus]\n    query = query.lower().split()\n    \n    # Calculate the number of documents\n    N = len(corpus)\n    \n    # Calculate the average document length\n    avgdl = sum(len(doc) for doc in corpus) / N\n    \n    # Calculate the document frequency for each term in the query\n    df = Counter()\n    for doc in corpus:\n        unique_terms = set(doc)\n        df.update(unique_terms)\n    \n    # Calculate the IDF for each term in the query\n    idf = {}\n    for term in query:\n        if term in df:\n            idf[term] = np.log((N - df[term] + 0.5) / (df[term] + 0.5))\n        else:\n            idf[term] = 0.0\n    \n    # Calculate the BM25 score for each document\n    scores = []\n    for doc in corpus:\n        score = 0.0\n        doc_length = len(doc)\n        term_freq = Counter(doc)\n        \n        for term in query:\n            if term in term_freq:\n                tf = term_freq[term]\n                score += idf[term] * ((k1 + 1) * tf) / (k1 * (1 - b + b * (doc_length / avgdl)) + tf)\n        \n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage:\ncorpus = [\n    \"the quick brown fox jumps over the lazy dog\",\n    \"never jump over the lazy dog quickly\",\n    \"a quick brown dog outpaces a quick fox\"\n]\nquery = \"quick brown fox\"\nprint(calculate_bm25_scores(corpus, query))\n```\n\nThis code will output the BM25 scores for each document in the corpus with respect to the given query, rounded to three decimal places."}
{"task_id": 91, "completion_id": 0, "completion": "To calculate the F1 score, we need to first compute the precision and recall. The F1 score is the harmonic mean of precision and recall, and it is defined as:\n\n\\[ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n\nWhere:\n- Precision is the ratio of true positive predictions to the total predicted positives.\n- Recall is the ratio of true positive predictions to the total actual positives.\n\nHere's how you can implement the `calculate_f1_score` function:\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    # Initialize counters\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    # Calculate true positives, false positives, and false negatives\n    for true, pred in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            true_positives += 1\n        elif true == 0 and pred == 1:\n            false_positives += 1\n        elif true == 1 and pred == 0:\n            false_negatives += 1\n    \n    # Calculate precision and recall\n    if true_positives + false_positives == 0:\n        precision = 0\n    else:\n        precision = true_positives / (true_positives + false_positives)\n    \n    if true_positives + false_negatives == 0:\n        recall = 0\n    else:\n        recall = true_positives / (true_positives + false_negatives)\n    \n    # Calculate F1 score\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    \n    return round(f1_score, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0, 1]\n# y_pred = [1, 0, 0, 1, 0, 1]\n# print(calculate_f1_score(y_true, y_pred))  # Output: 0.857\n```\n\nThis function first checks if the lengths of `y_true` and `y_pred` are the same. It then calculates the number of true positives, false positives, and false negatives. Using these, it computes precision and recall, and finally, the F1 score. The result is rounded to three decimal places."}
{"task_id": 92, "completion_id": 0, "completion": "To solve this problem, we need to follow the steps outlined in the task description. We'll start by subtracting the known daily fluctuation from the consumption data, perform linear regression on the detrended data, predict the base consumption for day 15, add back the fluctuation for day 15, and finally apply a 5% safety margin. Here's how you can implement this in Python:\n\n```python\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point.\n    days = np.arange(1, 11)\n    fluctuations = 10 * np.sin(2 * PI * days / 10)\n    detrended_data = np.array(consumption_data) - fluctuations\n    \n    # Step 2: Perform linear regression on the detrended data.\n    X = days.reshape(-1, 1)  # Reshape for sklearn\n    model = LinearRegression()\n    model.fit(X, detrended_data)\n    \n    # Step 3: Predict day 15's base consumption.\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)[0]\n    \n    # Step 4: Add the day 15 fluctuation back.\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    \n    # Step 5: Round, then add a 5% safety margin (rounded up).\n    final_consumption = math.ceil(predicted_consumption_day_15 * 1.05)\n    \n    # Step 6: Return the final integer.\n    return final_consumption\n\n# Example usage:\nconsumption_data = [120, 130, 140, 150, 160, 170, 180, 190, 200, 210]  # Example data\nprint(power_grid_forecast(consumption_data))\n```\n\n### Explanation:\n\n1. **Subtract Fluctuation**: We calculate the fluctuation for each day using the formula \\( f(i) = 10 \\times \\sin(2\\pi i / 10) \\) and subtract it from the consumption data to get the detrended data.\n\n2. **Linear Regression**: We use `LinearRegression` from `sklearn` to fit a line to the detrended data. The days are used as the independent variable.\n\n3. **Predict Day 15**: We predict the base consumption for day 15 using the fitted linear model.\n\n4. **Add Fluctuation**: We calculate the fluctuation for day 15 and add it back to the predicted base consumption.\n\n5. **Safety Margin**: We apply a 5% safety margin by multiplying the result by 1.05 and rounding up to the nearest integer using `math.ceil`.\n\n6. **Return Result**: Finally, we return the calculated integer value as the predicted power consumption for day 15 with the safety margin."}
{"task_id": 93, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Mean Absolute Error (MAE) function in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Both y_true and y_pred must be numpy arrays.\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    return round(mean_absolute_error, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(mae(y_true, y_pred))  # Output: 0.375\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if both `y_true` and `y_pred` are NumPy arrays and if they have the same shape. If not, it raises a `ValueError`.\n- **Absolute Errors**: It calculates the absolute differences between the true and predicted values using `np.abs(y_true - y_pred)`.\n- **Mean Calculation**: It computes the mean of these absolute errors using `np.mean()`.\n- **Rounding**: The result is rounded to three decimal places using the `round()` function before being returned.\n\nThis function will give you the Mean Absolute Error between two sets of values, rounded to three decimal places."}
{"task_id": 94, "completion_id": 0, "completion": "To implement the multi-head attention mechanism, we need to break down the process into three main functions: `compute_qkv`, `self_attention`, and `multi_head_attention`. Here's how you can implement these functions:\n\n1. **`compute_qkv`**: This function will compute the Query (Q), Key (K), and Value (V) matrices by multiplying the input matrix `X` with the respective weight matrices `W_q`, `W_k`, and `W_v`.\n\n2. **`self_attention`**: This function will compute the self-attention scores for a single head. It involves calculating the dot product of Q and K, applying a scaling factor, and then using a softmax function to get the attention weights. These weights are then used to compute the weighted sum of the values.\n\n3. **`multi_head_attention`**: This function will orchestrate the multi-head attention process. It will split the Q, K, and V matrices into multiple heads, compute the self-attention for each head, and then concatenate the results.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = K.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = softmax(scores, axis=-1)\n    output = attention_weights @ V\n    return output\n\ndef softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / e_x.sum(axis=axis, keepdims=True)\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    batch_size, seq_len, d_model = X.shape\n    d_k = d_v = d_model // n_heads\n    \n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Split Q, K, V into heads\n    Q = Q.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n    K = K.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n    V = V.reshape(batch_size, seq_len, n_heads, d_v).transpose(0, 2, 1, 3)\n    \n    # Compute self-attention for each head\n    head_outputs = []\n    for i in range(n_heads):\n        head_output = self_attention(Q[:, i], K[:, i], V[:, i])\n        head_outputs.append(head_output)\n    \n    # Concatenate all head outputs\n    output = np.concatenate(head_outputs, axis=-1)\n    \n    # Reshape to original shape\n    output = output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    \n    # Round to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Convert to list\n    return output.tolist()\n\n# Example usage:\n# X = np.random.rand(2, 5, 12)  # Batch size 2, sequence length 5, model dimension 12\n# W_q = np.random.rand(12, 12)\n# W_k = np.random.rand(12, 12)\n# W_v = np.random.rand(12, 12)\n# n_heads = 3\n# result = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n# print(result)\n```\n\n### Explanation:\n- **`compute_qkv`**: Computes the Q, K, and V matrices by matrix multiplication.\n- **`self_attention`**: Computes the attention scores, applies softmax, and computes the weighted sum of values.\n- **`multi_head_attention`**: Splits the input into heads, computes self-attention for each head, concatenates the results, and reshapes back to the original form. The output is rounded to four decimal places and converted to a list.\n\nThis implementation assumes that the input dimensions are compatible with the number of heads and that the model dimension is divisible by the number of heads."}
{"task_id": 95, "completion_id": 0, "completion": "To calculate the Phi coefficient, we need to use a contingency table for the two binary variables. The Phi coefficient is essentially the Pearson correlation coefficient for two binary variables and is calculated using the formula:\n\n\\[\n\\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{1\\cdot}n_{0\\cdot})(n_{\\cdot1}n_{\\cdot0})}}\n\\]\n\nWhere:\n- \\( n_{11} \\) is the count of both variables being 1.\n- \\( n_{00} \\) is the count of both variables being 0.\n- \\( n_{10} \\) is the count of the first variable being 1 and the second being 0.\n- \\( n_{01} \\) is the count of the first variable being 0 and the second being 1.\n- \\( n_{1\\cdot} \\) is the total count of the first variable being 1.\n- \\( n_{0\\cdot} \\) is the total count of the first variable being 0.\n- \\( n_{\\cdot1} \\) is the total count of the second variable being 1.\n- \\( n_{\\cdot0} \\) is the total count of the second variable being 0.\n\nHere's the implementation of the `phi_corr` function:\n\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"The input lists must have the same length.\")\n    \n    n11 = sum(1 for xi, yi in zip(x, y) if xi == 1 and yi == 1)\n    n00 = sum(1 for xi, yi in zip(x, y) if xi == 0 and yi == 0)\n    n10 = sum(1 for xi, yi in zip(x, y) if xi == 1 and yi == 0)\n    n01 = sum(1 for xi, yi in zip(x, y) if xi == 0 and yi == 1)\n    \n    n1_ = n11 + n10\n    n0_ = n00 + n01\n    n_1 = n11 + n01\n    n_0 = n10 + n00\n    \n    numerator = n11 * n00 - n10 * n01\n    denominator = (n1_ * n0_ * n_1 * n_0) ** 0.5\n    \n    if denominator == 0:\n        return 0.0  # To handle the case where any of the margins are zero\n    \n    phi = numerator / denominator\n    return round(phi, 4)\n\n# Example usage:\n# x = [1, 0, 1, 1, 0]\n# y = [1, 1, 0, 1, 0]\n# print(phi_corr(x, y))  # Output: 0.3333\n```\n\nThis function first checks if the input lists are of the same length. It then calculates the necessary counts from the contingency table and uses them to compute the Phi coefficient. The result is rounded to four decimal places before being returned."}
{"task_id": 96, "completion_id": 0, "completion": "Certainly! The Hard Sigmoid activation function is a piecewise linear approximation of the standard sigmoid function. It is defined as:\n\n\\[ \\text{Hard Sigmoid}(x) = \\max(0, \\min(1, 0.2 \\cdot x + 0.5)) \\]\n\nThis function is computationally efficient because it involves only multiplication, addition, and clamping operations. Here's how you can implement it in Python:\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))\n```\n\nThis function first computes \\(0.2 \\cdot x + 0.5\\), then clamps the result to be between 0 and 1 using the `max` and `min` functions."}
{"task_id": 97, "completion_id": 0, "completion": "Certainly! Below is the implementation of the ELU (Exponential Linear Unit) activation function in Python, following the provided specifications:\n\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage:\n# print(elu(1.0))  # Output: 1.0\n# print(elu(-1.0)) # Output: -0.6321\n```\n\n### Explanation:\n- The ELU function is defined such that for positive inputs (`x >= 0`), the output is the input itself (`x`).\n- For negative inputs (`x < 0`), the output is computed as `alpha * (exp(x) - 1)`, where `exp(x)` is the exponential of `x`.\n- The result is rounded to four decimal places to meet the output constraints."}
{"task_id": 98, "completion_id": 0, "completion": "Certainly! Below is the implementation of the PReLU (Parametric ReLU) activation function in Python:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n```\n\n### Explanation:\n- The PReLU function is defined such that for positive input values (`x > 0`), it behaves like the standard ReLU function, returning the input value itself.\n- For negative input values (`x <= 0`), it returns `alpha * x`, where `alpha` is a learnable parameter that determines the slope of the function for negative inputs. The default value for `alpha` is set to 0.25, but it can be adjusted as needed."}
{"task_id": 99, "completion_id": 0, "completion": "To implement the Softplus activation function, we need to compute the value of \\(\\log(1 + e^x)\\). This function is a smooth approximation of the ReLU function and is often used in neural networks. However, directly computing \\(e^x\\) for large positive values of \\(x\\) can lead to numerical overflow, while for very negative values, it can lead to underflow issues.\n\nTo handle these edge cases, we can use the following mathematical identities:\n\n1. For large \\(x\\), \\(e^x\\) becomes very large, and \\(\\log(1 + e^x) \\approx x\\).\n2. For very negative \\(x\\), \\(e^x\\) becomes very small, and \\(\\log(1 + e^x) \\approx \\log(e^x) = x\\).\n\nUsing these identities, we can write a more numerically stable version of the Softplus function.\n\nHere's the implementation:\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:  # Threshold to prevent overflow\n        return round(x, 4)\n    elif x < -20:  # Threshold to prevent underflow\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Example usage:\n# print(softplus(10))  # Should be close to 10\n# print(softplus(-10)) # Should be close to 0\n# print(softplus(0))   # Should be close to 0.6931\n```\n\n### Explanation:\n- For \\(x > 20\\), we approximate \\(\\log(1 + e^x) \\approx x\\) to avoid overflow.\n- For \\(x < -20\\), we approximate \\(\\log(1 + e^x) \\approx e^x\\) to avoid underflow.\n- For values of \\(x\\) in between, we compute \\(\\log(1 + e^x)\\) directly.\n\nThis approach ensures that the function is numerically stable and handles edge cases effectively. The results are rounded to four decimal places as required."}
{"task_id": 100, "completion_id": 0, "completion": "Certainly! The Softsign activation function is defined as:\n\n\\[ \\text{Softsign}(x) = \\frac{x}{1 + |x|} \\]\n\nThis function maps any real-valued number into the range (-1, 1). Here's how you can implement it in Python:\n\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n\n# Example usage:\n# print(softsign(2.5))  # Output: 0.7143\n# print(softsign(-2.5)) # Output: -0.7143\n```\n\nThis function calculates the Softsign of the input `x` and returns the result rounded to four decimal places. You can test it with different values to see how it behaves."}
{"task_id": 101, "completion_id": 0, "completion": "To implement the GRPO (Group Relative Policy Optimization) objective function, we need to follow the steps outlined in the description. The GRPO objective function is designed to optimize policy parameters in reinforcement learning by considering the likelihood ratios, advantage estimates, and applying a clipping mechanism along with a KL divergence penalty.\n\nHere's the implementation of the GRPO objective function:\n\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Calculate the clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Calculate the GRPO objective components\n    term1 = rhos * A\n    term2 = clipped_rhos * A\n    \n    # Calculate the minimum of the two terms\n    L_clip = np.minimum(term1, term2)\n    \n    # Calculate the KL divergence between the old policy and the reference policy\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Calculate the GRPO objective\n    grpo_obj = np.mean(L_clip) - beta * kl_divergence\n    \n    # Return the result rounded to the nearest 6th decimal\n    return round(grpo_obj, 6)\n\n# Example usage:\n# rhos = [1.1, 0.9, 1.2, 0.8]\n# A = [0.5, -0.3, 0.2, -0.1]\n# pi_theta_old = [0.25, 0.25, 0.25, 0.25]\n# pi_theta_ref = [0.3, 0.2, 0.25, 0.25]\n# print(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref))\n```\n\n### Explanation:\n1. **Likelihood Ratios (`rhos`)**: These are the ratios of the new policy probabilities to the old policy probabilities.\n2. **Advantage Estimates (`A`)**: These are the estimated advantages for each action.\n3. **Old Policy Probabilities (`pi_theta_old`)**: These are the probabilities of actions under the old policy.\n4. **Reference Policy Probabilities (`pi_theta_ref`)**: These are the probabilities of actions under the reference policy.\n5. **Clipping Parameter (`epsilon`)**: This parameter is used to clip the likelihood ratios to ensure stability.\n6. **KL Divergence Penalty Coefficient (`beta`)**: This coefficient scales the KL divergence penalty.\n\nThe function computes the clipped objective term and subtracts the KL divergence penalty scaled by `beta`. The result is then rounded to the nearest 6th decimal place as required."}
{"task_id": 102, "completion_id": 0, "completion": "Certainly! The Swish activation function is defined as:\n\n\\[ \\text{Swish}(x) = x \\cdot \\sigma(x) \\]\n\nwhere \\(\\sigma(x)\\) is the sigmoid function, given by:\n\n\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\]\n\nHere's how you can implement the Swish activation function in Python, ensuring the result is rounded to the nearest 4th decimal:\n\n```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)\n\n# Example usage:\n# print(swish(1.0))  # This should print the Swish value for 1.0, rounded to 4 decimal places\n```\n\nThis function calculates the Swish value for a given input `x` and rounds the result to four decimal places as specified. You can test the function with different input values to see how it performs."}
{"task_id": 103, "completion_id": 0, "completion": "Certainly! Below is the implementation of the SELU activation function in Python, ensuring numerical stability and rounding the result to the nearest 4th decimal place.\n\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage:\n# print(selu(1.0))  # Example input\n# print(selu(-1.0)) # Example input\n```\n\n### Explanation:\n- **Parameters**:\n  - `alpha` and `scale` are the predefined constants for the SELU function.\n- **Function Logic**:\n  - If the input `x` is greater than 0, the function returns `scale * x`.\n  - If the input `x` is less than or equal to 0, the function returns `scale * alpha * (exp(x) - 1)`.\n- **Rounding**:\n  - The result is rounded to the nearest 4th decimal place using Python's built-in `round()` function.\n\nThis implementation ensures that the SELU function is computed correctly and efficiently, with the output being numerically stable and formatted as required."}
{"task_id": 104, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `predict_logistic` function for binary classification using Logistic Regression. This function computes the class probabilities using the sigmoid function and returns binary predictions based on a threshold of 0.5.\n\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    linear_combination = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    \n    # Convert probabilities to binary predictions based on a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Convert the numpy array to a list and return\n    return predictions.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# weights = np.array([0.5, -0.5])\n# bias = 0.1\n# print(predict_logistic(X, weights, bias))\n```\n\n### Explanation:\n1. **Linear Combination**: We compute the linear combination of the input features `X` and the weights `weights` by taking the dot product and adding the bias.\n2. **Sigmoid Function**: The sigmoid function is applied to the linear combination to get the probabilities of the positive class.\n3. **Binary Prediction**: We convert these probabilities to binary predictions by setting a threshold of 0.5. If the probability is greater than or equal to 0.5, the prediction is 1 (positive class); otherwise, it is 0 (negative class).\n4. **Return**: Finally, we convert the numpy array of predictions to a list using `tolist()` and return it.\n\nThis function can be used to make predictions on new data given the trained weights and bias from a logistic regression model."}
{"task_id": 105, "completion_id": 0, "completion": "To implement a gradient descent-based training algorithm for Softmax regression, we need to follow these steps:\n\n1. **Initialize Parameters**: Start with random initial values for the parameters.\n2. **Compute Softmax Probabilities**: For each training example, compute the softmax probabilities.\n3. **Compute Cross Entropy Loss**: Calculate the loss using the cross-entropy loss function.\n4. **Compute Gradients**: Calculate the gradient of the loss with respect to the parameters.\n5. **Update Parameters**: Use the gradients to update the parameters.\n6. **Collect Loss Values**: Store the loss values for each iteration.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Number of classes\n    C = np.max(y) + 1\n    # Number of features\n    M = X.shape[1]\n    \n    # Initialize parameters randomly\n    B = np.random.randn(C, M)\n    \n    # Convert y to one-hot encoding\n    y_one_hot = np.eye(C)[y]\n    \n    # List to store loss values\n    losses = []\n    \n    for _ in range(iterations):\n        # Compute the scores\n        scores = np.dot(X, B.T)\n        \n        # Compute the softmax probabilities\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        \n        # Compute the cross-entropy loss\n        loss = -np.sum(y_one_hot * np.log(probs)) / X.shape[0]\n        losses.append(round(loss, 4))\n        \n        # Compute the gradient\n        dscores = probs - y_one_hot\n        dW = np.dot(dscores.T, X) / X.shape[0]\n        \n        # Update the parameters\n        B -= learning_rate * dW\n    \n    # Return the parameters and losses rounded to 4 decimal places\n    return B.round(4).tolist(), losses\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([0, 1, 0])\n# learning_rate = 0.01\n# iterations = 100\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n# print(\"Optimized Coefficients:\", B)\n# print(\"Collected Losses:\", losses)\n```\n\n### Explanation:\n- **Initialization**: We initialize the parameters `B` with random values.\n- **One-Hot Encoding**: We convert the target labels `y` into one-hot encoded vectors.\n- **Softmax Calculation**: We compute the softmax probabilities for each class.\n- **Loss Calculation**: We calculate the cross-entropy loss.\n- **Gradient Calculation**: We compute the gradient of the loss with respect to the parameters.\n- **Parameter Update**: We update the parameters using the computed gradients.\n- **Loss Collection**: We store the loss values for each iteration.\n\nThis implementation ensures that the results are rounded to the nearest 4th decimal place and returned as Python lists."}
{"task_id": 106, "completion_id": 0, "completion": "Certainly! Below is the implementation of a gradient descent-based training algorithm for logistic regression. The function `train_logreg` computes the model parameters using Binary Cross Entropy loss and returns the optimized coefficients along with the collected loss values over iterations, rounded to the 4th decimal.\n\n```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add a column of ones to X for the intercept term\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    \n    # Initialize weights (coefficients) to zeros\n    weights = np.zeros(X.shape[1])\n    \n    # List to store loss values\n    loss_values = []\n    \n    # Sigmoid function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    # Binary Cross Entropy Loss\n    def binary_cross_entropy(y_true, y_pred):\n        epsilon = 1e-15  # To prevent log(0)\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        # Calculate the predicted probabilities\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        \n        # Calculate the loss\n        loss = binary_cross_entropy(y, y_pred)\n        loss_values.append(round(loss, 4))\n        \n        # Calculate the gradient\n        gradient = np.dot(X.T, (y_pred - y)) / y.size\n        \n        # Update the weights\n        weights -= learning_rate * gradient\n    \n    # Return the optimized weights and loss values\n    return weights.tolist(), loss_values\n\n# Example usage:\n# X = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n# y = np.array([0, 1, 1, 0])\n# learning_rate = 0.1\n# iterations = 1000\n# coefficients, losses = train_logreg(X, y, learning_rate, iterations)\n# print(\"Coefficients:\", coefficients)\n# print(\"Losses:\", losses)\n```\n\n### Explanation:\n1. **Data Preparation**: We add a column of ones to the input features `X` to account for the intercept term in the logistic regression model.\n2. **Initialization**: We initialize the weights (coefficients) to zeros.\n3. **Sigmoid Function**: This function is used to map predicted values to probabilities.\n4. **Binary Cross Entropy Loss**: This function calculates the loss between the true labels and the predicted probabilities.\n5. **Gradient Descent**: We iteratively update the weights by moving in the direction that reduces the loss, using the gradient of the loss with respect to the weights.\n6. **Loss Collection**: We collect the loss values at each iteration and round them to the 4th decimal.\n7. **Return Values**: The function returns the optimized weights and the list of loss values.\n\nThis implementation should work efficiently for binary classification problems using logistic regression with gradient descent."}
{"task_id": 107, "completion_id": 0, "completion": "To implement masked self-attention, we need to follow these steps:\n\n1. Compute the Query (Q), Key (K), and Value (V) matrices from the input matrix \\( X \\) using the weight matrices \\( W_q \\), \\( W_k \\), and \\( W_v \\).\n2. Compute the attention scores using the dot product of Q and the transpose of K.\n3. Apply the mask to the attention scores to prevent the model from attending to future tokens.\n4. Apply a softmax function to the masked attention scores to get the attention weights.\n5. Compute the weighted sum of the values using the attention weights.\n6. Return the result as a Python list after reshaping using numpy's `tolist()` method.\n\nHere is the complete implementation:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute the attention scores\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply the mask\n    scores = scores + mask * -1e9  # Large negative value for masked positions\n    \n    # Apply softmax to get attention weights\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of the values\n    output = np.dot(attention_weights, V)\n    \n    # Return the result as a Python list\n    return output.tolist()\n\n# Example usage:\n# Let's assume we have a batch of sequences with 3 tokens each and a hidden dimension of 4\nX = np.random.rand(2, 3, 4)  # Batch size of 2, sequence length of 3, hidden dimension of 4\nW_q = np.random.rand(4, 4)   # Weight matrix for Q\nW_k = np.random.rand(4, 4)   # Weight matrix for K\nW_v = np.random.rand(4, 4)   # Weight matrix for V\n\n# Create a mask for the sequence length 3\n# The mask should be -1e9 for positions we want to mask and 0 otherwise\n# For example, a mask for the first sequence to only attend to the first token:\nmask = np.array([\n    [0, -1e9, -1e9],  # First sequence\n    [0, 0, -1e9]      # Second sequence\n])\n\n# Compute Q, K, V\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n# Compute masked self-attention\nresult = masked_attention(Q, K, V, mask)\nprint(result)\n```\n\n### Explanation:\n- **Query (Q), Key (K), Value (V) Matrices**: These are computed by multiplying the input matrix \\( X \\) with the respective weight matrices \\( W_q \\), \\( W_k \\), and \\( W_v \\).\n- **Attention Scores**: These are computed as the dot product of \\( Q \\) and the transpose of \\( K \\), scaled by the square root of the dimension of \\( K \\) to prevent the dot product from growing too large.\n- **Mask Application**: The mask is added to the attention scores to ensure that the model does not attend to future tokens. Positions to be masked are set to a very large negative value.\n- **Softmax**: The softmax function is applied to the masked scores to get the attention weights.\n- **Weighted Sum**: The final output is computed as the weighted sum of the values \\( V \\) using the attention weights.\n- **Output**: The result is converted to a Python list using `tolist()` for the final output."}
{"task_id": 108, "completion_id": 0, "completion": "To measure the disorder in a basket of apples based on their colors, we can use the concept of entropy from information theory. Entropy is a measure of the uncertainty or randomness in a system. In this context, it will increase as the variety of colors increases and will be zero if all apples are the same color.\n\nHere's how we can implement the `disorder` function using entropy:\n\n```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    \n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the entropy\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    \n    # Return the entropy rounded to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Example usage:\nprint(disorder([0, 0, 0, 0]))          # Output: 0.0\nprint(disorder([1, 1, 0, 0]))          # Output: 1.0\nprint(disorder([0, 1, 2, 3]))          # Output: 2.0\nprint(disorder([0, 0, 1, 1, 2, 2, 3, 3]))  # Output: 1.75\n```\n\n### Explanation:\n- **Counter**: We use `collections.Counter` to count the occurrences of each color in the list.\n- **Entropy Calculation**: For each unique color, we calculate its probability (count of that color divided by total number of apples) and then compute the entropy using the formula: \\(-\\sum (p_i \\cdot \\log_2(p_i))\\), where \\(p_i\\) is the probability of color \\(i\\).\n- **Rounding**: The result is rounded to the nearest 4th decimal place as required.\n\nThis function satisfies the conditions specified in the task:\n- It returns 0 for a list with all identical elements.\n- It returns a higher value for lists with more variety in colors."}
{"task_id": 109, "completion_id": 0, "completion": "To implement Layer Normalization for sequence data, we need to follow these steps:\n\n1. **Compute the Mean and Variance**: For each sequence in the batch, compute the mean and variance across the feature dimension.\n2. **Normalize the Data**: Subtract the mean and divide by the square root of the variance plus a small constant (epsilon) to avoid division by zero.\n3. **Scale and Shift**: Multiply the normalized data by a scaling parameter (gamma) and add a shifting parameter (beta).\n4. **Round and Convert**: Round the result to 5 decimal places and convert it to a list before returning.\n\nHere's the implementation of the `layer_normalization` function:\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Ensure the input dimensions are correct\n    batch_size, seq_length, feature_dim = X.shape\n    assert gamma.shape == (feature_dim,), \"Gamma must have the same shape as the feature dimension\"\n    assert beta.shape == (feature_dim,), \"Beta must have the same shape as the feature dimension\"\n    \n    # Compute the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the data\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Scale and shift\n    X_normalized = gamma * X_normalized + beta\n    \n    # Round to 5 decimal places and convert to list\n    return X_normalized.round(5).tolist()\n\n# Example usage:\n# X = np.random.rand(2, 3, 4)  # Batch size 2, sequence length 3, feature dimension 4\n# gamma = np.random.rand(4)\n# beta = np.random.rand(4)\n# print(layer_normalization(X, gamma, beta))\n```\n\n### Explanation:\n- **Mean and Variance Calculation**: We use `np.mean` and `np.var` with `axis=-1` to compute the mean and variance across the feature dimension for each sequence in the batch. The `keepdims=True` argument ensures that the result has the same number of dimensions as the input, which is necessary for broadcasting.\n- **Normalization**: The normalization step involves subtracting the mean and dividing by the standard deviation (square root of variance) plus a small constant `epsilon` to prevent division by zero.\n- **Scaling and Shifting**: The normalized data is then scaled by `gamma` and shifted by `beta`.\n- **Rounding and Conversion**: Finally, the result is rounded to 5 decimal places and converted to a list for the output.\n\nThis function should work for any 3D input tensor `X` with the appropriate shapes for `gamma` and `beta`."}
{"task_id": 110, "completion_id": 0, "completion": "To compute the METEOR score, we need to consider several components: unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation. The METEOR score is a metric that evaluates the quality of machine translation by considering these factors.\n\nHere's a step-by-step implementation of the METEOR score function in Python:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # Tokenize the reference and candidate\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    \n    # Calculate unigram matches\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    \n    # Find matches\n    matches = 0\n    for word in ref_counter:\n        if word in cand_counter:\n            matches += min(ref_counter[word], cand_counter[word])\n    \n    # Calculate precision and recall\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    \n    # Calculate F-mean\n    if precision + recall > 0:\n        f_mean = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    else:\n        f_mean = 0\n    \n    # Calculate fragmentation penalty\n    # This is a simplified version of the fragmentation penalty\n    # In practice, this would involve more complex calculations\n    # Here, we use a basic approach to demonstrate the concept\n    ref_chunks = list(ref_counter.elements())\n    cand_chunks = list(cand_counter.elements())\n    \n    # Calculate the number of chunks\n    ref_chunk_count = len(ref_chunks)\n    cand_chunk_count = len(cand_chunks)\n    \n    # Calculate the number of matching chunks\n    matching_chunks = 0\n    for i in range(min(ref_chunk_count, cand_chunk_count)):\n        if ref_chunks[i] == cand_chunks[i]:\n            matching_chunks += 1\n    \n    # Calculate the fragmentation penalty\n    if ref_chunk_count > 0:\n        fragmentation_penalty = matching_chunks / ref_chunk_count\n    else:\n        fragmentation_penalty = 0\n    \n    # Calculate the final METEOR score\n    meteor = (1 - gamma) * f_mean + gamma * fragmentation_penalty\n    \n    # Apply the alpha penalty for length difference\n    length_penalty = np.exp(-abs(len(ref_tokens) - len(cand_tokens)) / 10)\n    meteor *= alpha * length_penalty\n    \n    return round(meteor, 3)\n\n# Example usage:\nreference = \"the cat sat on the mat\"\ncandidate = \"the cat is on the mat\"\nprint(meteor_score(reference, candidate))\n```\n\n### Explanation:\n1. **Tokenization**: The reference and candidate sentences are split into tokens (words).\n2. **Unigram Matches**: We count the number of matching words between the reference and candidate.\n3. **Precision and Recall**: These are calculated based on the number of matches and the total number of words in the candidate and reference, respectively.\n4. **F-mean**: This is a weighted harmonic mean of precision and recall.\n5. **Fragmentation Penalty**: This is a simplified version of the penalty for word order fragmentation. In practice, this would involve more complex calculations to account for the order of words.\n6. **Length Penalty**: This penalizes translations that are significantly longer or shorter than the reference.\n7. **Final METEOR Score**: The final score is a combination of the F-mean, fragmentation penalty, and length penalty.\n\nThis implementation is a simplified version of the METEOR score calculation. The fragmentation penalty is particularly simplified, and in a full implementation, you would need to consider more complex measures of word order similarity."}
{"task_id": 111, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `compute_pmi` function that calculates the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, their individual counts, and the total number of samples. The result is rounded to three decimal places.\n\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    # Calculate the expected joint probability under independence\n    expected_joint_prob = (total_counts_x / total_samples) * (total_counts_y / total_samples)\n    \n    # Calculate the actual joint probability\n    actual_joint_prob = joint_counts / total_samples\n    \n    # Calculate PMI\n    if expected_joint_prob == 0 or actual_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    \n    # Return PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\n# joint_counts = 10\n# total_counts_x = 100\n# total_counts_y = 50\n# total_samples = 1000\n# print(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n```\n\n### Explanation:\n- **Expected Joint Probability**: This is calculated as the product of the individual probabilities of the two events, assuming they are independent.\n- **Actual Joint Probability**: This is simply the joint count divided by the total number of samples.\n- **PMI Calculation**: The PMI is the logarithm (base 2) of the ratio of the actual joint probability to the expected joint probability. It quantifies how much more (or less) likely the two events are to occur together compared to what would be expected if they were independent.\n- **Handling Edge Cases**: If either the expected or actual joint probability is zero, the PMI is set to 0.0 to avoid division by zero or logarithm of zero errors.\n\nThis function will return the PMI value rounded to three decimal places as required."}
{"task_id": 112, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `min_max` function that performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1]. The function returns a list of floats rounded to 4 decimal places.\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    # Avoid division by zero if all values are the same\n    if min_val == max_val:\n        return [0.0] * len(x)\n    \n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]\n\n# Example usage:\n# data = [10, 20, 30, 40, 50]\n# print(min_max(data))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n```\n\n### Explanation:\n- **Input Check**: The function first checks if the input list `x` is empty. If it is, it returns an empty list.\n- **Find Min and Max**: It calculates the minimum and maximum values in the list.\n- **Normalization**: It applies the Min-Max normalization formula to each element in the list: \\((\\text{value} - \\text{min}) / (\\text{max} - \\text{min})\\).\n- **Division by Zero**: If all values in the list are the same, the function returns a list of zeros to avoid division by zero.\n- **Rounding**: Each normalized value is rounded to 4 decimal places before returning the final list."}
{"task_id": 113, "completion_id": 0, "completion": "To implement a simple residual block with a shortcut connection using NumPy, we need to follow these steps:\n\n1. **Matrix Multiplication**: Apply the first weight matrix `w1` to the input `x`.\n2. **ReLU Activation**: Apply the ReLU function to the result of the first matrix multiplication.\n3. **Second Matrix Multiplication**: Apply the second weight matrix `w2` to the result of the first ReLU activation.\n4. **Shortcut Connection**: Add the original input `x` to the result of the second matrix multiplication.\n5. **Final ReLU Activation**: Apply the ReLU function to the result of the shortcut connection.\n6. **Output**: Round the final result to 4 decimal places and convert it to a list.\n\nHere's the implementation of the `residual_block` function:\n\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # First matrix multiplication and ReLU activation\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    \n    # Second matrix multiplication\n    z2 = np.dot(a1, w2)\n    \n    # Shortcut connection and final ReLU activation\n    z3 = z2 + x\n    a3 = np.maximum(0, z3)\n    \n    # Round to 4 decimal places and convert to list\n    return a3.round(4).tolist()\n\n# Example usage:\n# x = np.array([1.0, 2.0, 3.0])\n# w1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n# w2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\n# print(residual_block(x, w1, w2))\n```\n\n### Explanation:\n- **Matrix Multiplication**: `np.dot(x, w1)` and `np.dot(a1, w2)` are used to perform matrix multiplications.\n- **ReLU Activation**: `np.maximum(0, z1)` and `np.maximum(0, z3)` apply the ReLU function, which outputs the maximum of 0 and the input value.\n- **Shortcut Connection**: `z3 = z2 + x` adds the original input `x` to the result of the second matrix multiplication.\n- **Rounding and Conversion**: `a3.round(4).tolist()` rounds the final output to 4 decimal places and converts it to a list for the final output.\n\nThis function assumes that the dimensions of `x`, `w1`, and `w2` are compatible for matrix multiplication. Specifically, if `x` is of shape `(n,)`, then `w1` should be of shape `(n, m)` and `w2` should be of shape `(m, n)` to ensure the operations are valid."}
{"task_id": 114, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `global_avg_pool` function that performs Global Average Pooling on a 3D NumPy array:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    # Check if the input is a 3D array\n    if x.ndim != 3:\n        raise ValueError(\"Input must be a 3D array of shape (height, width, channels).\")\n    \n    # Calculate the average across the height and width dimensions\n    avg_pooled = np.mean(x, axis=(0, 1))\n    \n    return avg_pooled\n\n# Example usage:\n# feature_maps = np.random.rand(10, 10, 3)  # Example input with shape (10, 10, 3)\n# result = global_avg_pool(feature_maps)\n# print(result)  # Output should be of shape (3,)\n```\n\n### Explanation:\n- The function `global_avg_pool` takes a 3D NumPy array `x` as input, which represents feature maps from a convolutional layer.\n- It first checks if the input is a 3D array. If not, it raises a `ValueError`.\n- It then computes the mean of the array across the first two dimensions (height and width) using `np.mean(x, axis=(0, 1))`. This operation effectively performs Global Average Pooling, resulting in a 1D array where each element is the average of the corresponding feature map.\n- The function returns this 1D array."}
{"task_id": 115, "completion_id": 0, "completion": "To implement Batch Normalization for a 4D NumPy array in the BCHW format, we need to follow these steps:\n\n1. **Reshape the Input**: Since we want to normalize across the batch and spatial dimensions for each channel, we need to reshape the input array to combine the batch, height, and width dimensions into a single dimension, while keeping the channels separate.\n\n2. **Compute Mean and Variance**: Calculate the mean and variance of the reshaped array along the new combined dimension (which corresponds to the batch and spatial dimensions).\n\n3. **Normalize**: Subtract the mean and divide by the square root of the variance plus a small epsilon value to ensure numerical stability.\n\n4. **Scale and Shift**: Apply the scale (gamma) and shift (beta) parameters to the normalized values.\n\n5. **Reshape Back**: Reshape the normalized array back to its original BCHW format.\n\n6. **Round and Convert to List**: Finally, round the result to 4 decimal places and convert it to a list before returning.\n\nHere's the implementation of the `batch_normalization` function:\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # X: (batch, channels, height, width)\n    # gamma: (channels,)\n    # beta: (channels,)\n    \n    # Step 1: Reshape X to (channels, batch * height * width)\n    N, C, H, W = X.shape\n    X_reshaped = X.transpose(1, 0, 2, 3).reshape(C, -1)\n    \n    # Step 2: Compute mean and variance along the batch * height * width dimension\n    mean = np.mean(X_reshaped, axis=1, keepdims=True)\n    var = np.var(X_reshaped, axis=1, keepdims=True)\n    \n    # Step 3: Normalize\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    \n    # Step 4: Scale and shift\n    X_scaled_shifted = gamma[:, np.newaxis] * X_normalized + beta[:, np.newaxis]\n    \n    # Step 5: Reshape back to (batch, channels, height, width)\n    X_normalized_reshaped = X_scaled_shifted.reshape(C, N, H, W).transpose(1, 0, 2, 3)\n    \n    # Step 6: Round to 4 decimal places and convert to list\n    result = np.round(X_normalized_reshaped, 4).tolist()\n    \n    return result\n```\n\n### Explanation:\n- **Reshape**: We use `transpose` to bring the channels to the front and then reshape to combine the batch, height, and width dimensions.\n- **Mean and Variance**: These are computed along the new combined dimension.\n- **Normalization**: We subtract the mean and divide by the standard deviation (variance + epsilon).\n- **Scaling and Shifting**: We apply the gamma and beta parameters.\n- **Reshape Back**: We reshape the array back to its original BCHW format.\n- **Rounding and Conversion**: We round the final result to 4 decimal places and convert it to a list for the output."}
{"task_id": 116, "completion_id": 0, "completion": "To compute the derivative of a polynomial term of the form \\( c \\cdot x^n \\), we can use the power rule of differentiation. The power rule states that the derivative of \\( x^n \\) is \\( n \\cdot x^{n-1} \\). Therefore, the derivative of \\( c \\cdot x^n \\) is \\( c \\cdot n \\cdot x^{n-1} \\).\n\nHere's how you can implement this in Python:\n\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    # Calculate the derivative using the power rule\n    derivative = c * n * (x ** (n - 1))\n    # Return the result rounded to 4 decimal places\n    return round(derivative, 4)\n\n# Example usage:\n# The derivative of 3 * x^2 at x = 2 is 3 * 2 * (2^1) = 12\nprint(poly_term_derivative(3, 2, 2))  # Output: 12.0\n```\n\n### Explanation:\n- **Parameters**:\n  - `c`: The coefficient of the polynomial term.\n  - `x`: The point at which to evaluate the derivative.\n  - `n`: The exponent of the polynomial term.\n  \n- **Computation**:\n  - The derivative is calculated as \\( c \\cdot n \\cdot x^{n-1} \\).\n  - The result is rounded to 4 decimal places using the `round()` function to meet the output constraints.\n\nThis function will correctly compute the derivative of a polynomial term at a given point and return the result rounded to four decimal places."}
{"task_id": 117, "completion_id": 0, "completion": "To compute an orthonormal basis for a subspace spanned by a list of 2D vectors using the Gram-Schmidt process, we need to follow these steps:\n\n1. **Normalize the first vector**: This vector will be the first vector in our orthonormal basis.\n2. **Orthogonalize subsequent vectors**: For each subsequent vector, subtract its projection onto the previously computed orthonormal vectors.\n3. **Normalize the orthogonalized vectors**: Convert each orthogonalized vector into a unit vector.\n4. **Check for linear independence**: Use a tolerance value to determine if a vector is linearly independent of the previously computed vectors.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    # Convert input list of lists to a numpy array for easier manipulation\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_vectors = []\n\n    for v in vectors:\n        # Start with the current vector\n        u = v.copy()\n        \n        # Subtract the projection of u onto each of the previously computed orthonormal vectors\n        for w in orthonormal_vectors:\n            u -= np.dot(u, w) * w\n        \n        # Check if the resulting vector is linearly independent\n        if np.linalg.norm(u) > tol:\n            # Normalize the vector to make it a unit vector\n            u = u / np.linalg.norm(u)\n            # Add the orthonormal vector to the list\n            orthonormal_vectors.append(u)\n    \n    # Convert each vector to a list and round to 4 decimal places\n    orthonormal_vectors = [v.tolist() for v in np.round(orthonormal_vectors, 4)]\n    \n    return orthonormal_vectors\n\n# Example usage:\n# vectors = [[1, 0], [1, 1], [2, 2]]\n# print(orthonormal_basis(vectors))\n```\n\n### Explanation:\n\n- **Normalization**: The first vector is normalized to create the first orthonormal vector.\n- **Orthogonalization**: For each subsequent vector, we subtract its projection onto each of the previously computed orthonormal vectors. This ensures that the new vector is orthogonal to all previously computed vectors.\n- **Linear Independence Check**: We check if the norm of the resulting vector is greater than the tolerance. If it is, the vector is added to the orthonormal basis.\n- **Rounding and Conversion**: Finally, we round each vector to 4 decimal places and convert it to a list for the output.\n\nThis function will return a list of orthonormal vectors that span the same subspace as the input vectors, rounded to 4 decimal places."}
{"task_id": 118, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `cross_product` function that computes the cross product of two 3-dimensional vectors using NumPy. The result is rounded to 4 decimal places and converted to a list before being returned.\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Ensure the input vectors are numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product\n    result = np.cross(a, b)\n    \n    # Round the result to 4 decimal places and convert to list\n    result_rounded = np.round(result, 4).tolist()\n    \n    return result_rounded\n\n# Example usage:\n# a = [1, 2, 3]\n# b = [4, 5, 6]\n# print(cross_product(a, b))  # Output: [-3.0, 6.0, -3.0]\n```\n\nThis function first converts the input vectors to NumPy arrays to leverage the `np.cross` function, which efficiently computes the cross product. The result is then rounded to 4 decimal places using `np.round` and converted to a list using `tolist()` before being returned."}
{"task_id": 119, "completion_id": 0, "completion": "To solve a system of linear equations using Cramer's Rule, we need to follow these steps:\n\n1. Calculate the determinant of the coefficient matrix \\( A \\).\n2. If the determinant is zero, the system does not have a unique solution, and we should return -1.\n3. For each variable \\( x_i \\), replace the \\( i \\)-th column of \\( A \\) with the vector \\( b \\) to form a new matrix \\( A_i \\).\n4. Calculate the determinant of each \\( A_i \\).\n5. The solution for \\( x_i \\) is given by \\( x_i = \\frac{\\det(A_i)}{\\det(A)} \\).\n6. Return the solution vector \\( x \\) rounded to the nearest 4th decimal.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, return -1\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Iterate over each column to form A_i and calculate x_i\n    for i in range(len(b)):\n        # Create a copy of A\n        A_i = np.copy(A)\n        # Replace the i-th column with b\n        A_i[:, i] = b\n        # Calculate the determinant of A_i\n        det_A_i = np.linalg.det(A_i)\n        # Calculate x_i\n        x_i = det_A_i / det_A\n        # Append x_i to the solution vector\n        x.append(x_i)\n    \n    # Round the solution to the nearest 4th decimal and convert to list\n    x = np.round(x, 4).tolist()\n    \n    return x\n\n# Example usage:\n# A = [[2, 1], [3, 4]]\n# b = [1, 2]\n# print(cramers_rule(A, b))  # Output: [0.2857, 0.1429]\n```\n\nThis function will correctly solve the system of linear equations using Cramer's Rule and handle cases where the system does not have a unique solution. The results are rounded to the nearest 4th decimal as required."}
{"task_id": 120, "completion_id": 0, "completion": "To implement the Bhattacharyya distance between two discrete probability distributions, we need to follow these steps:\n\n1. **Check Input Validity**: Ensure both lists `p` and `q` are of the same length and are not empty. If they are not, return `0.0`.\n2. **Calculate the Bhattacharyya Coefficient**: This is given by the sum of the square roots of the product of corresponding elements in `p` and `q`.\n3. **Compute the Bhattacharyya Distance**: The distance is defined as \\(-\\ln(\\text{Bhattacharyya Coefficient})\\).\n4. **Round the Result**: Ensure the result is rounded to 4 decimal places.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the inputs are valid\n    if len(p) != len(q) or not p or not q:\n        return 0.0\n    \n    # Calculate the Bhattacharyya Coefficient\n    bc = sum(np.sqrt(px * qx) for px, qx in zip(p, q))\n    \n    # Calculate the Bhattacharyya Distance\n    if bc == 0:\n        return 0.0  # To avoid taking the log of zero\n    bd = -np.log(bc)\n    \n    # Return the result rounded to 4 decimal places\n    return round(bd, 4)\n\n# Example usage:\n# p = [0.1, 0.2, 0.3, 0.4]\n# q = [0.4, 0.3, 0.2, 0.1]\n# print(bhattacharyya_distance(p, q))  # Output should be rounded to 4 decimal places\n```\n\n### Explanation:\n- **Input Validation**: We first check if the lengths of `p` and `q` are equal and that they are not empty. If any of these conditions fail, we return `0.0`.\n- **Bhattacharyya Coefficient**: We compute the sum of the square roots of the products of corresponding elements in `p` and `q`.\n- **Bhattacharyya Distance**: We use the formula \\(-\\ln(\\text{Bhattacharyya Coefficient})\\) to compute the distance. We handle the case where the coefficient is zero to avoid taking the logarithm of zero, which would result in an undefined value.\n- **Rounding**: Finally, we round the result to 4 decimal places before returning it."}
{"task_id": 121, "completion_id": 0, "completion": "Certainly! Below is the Python function that computes the element-wise sum of two vectors. It checks if the vectors have the same length and returns a new vector with the sums if they do. If the vectors have incompatible dimensions, it returns -1.\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for x, y in zip(a, b)]\n\n# Example usage:\n# vector_sum([1, 2, 3], [4, 5, 6]) should return [5, 7, 9]\n# vector_sum([1, 2], [3, 4, 5]) should return -1\n```\n\n### Explanation:\n- The function `vector_sum` takes two parameters, `a` and `b`, which are lists of integers or floats.\n- It first checks if the lengths of `a` and `b` are equal. If not, it returns `-1` to indicate that the vectors have incompatible dimensions.\n- If the lengths are equal, it uses a list comprehension along with the `zip` function to iterate over pairs of elements from `a` and `b`, summing each pair and creating a new list with these sums.\n- The resulting list is returned as the output."}
{"task_id": 122, "completion_id": 0, "completion": "To implement the policy gradient estimator using the REINFORCE algorithm, we need to follow these steps:\n\n1. **Compute the policy for each state**: This is done using the softmax function over the parameters `theta[s, :]` for each state `s`.\n2. **Compute the return for each time step**: The return is the sum of future rewards from a given time step.\n3. **Compute the gradient of the log-policy**: This involves computing the gradient of the log of the softmax function.\n4. **Compute the policy gradient**: Multiply the gradient of the log-policy by the return for each time step and average over all episodes.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        # Compute the return for each time step\n        G = 0\n        returns = []\n        for t in reversed(range(len(episode))):\n            s, a, r = episode[t]\n            G += r\n            returns.append(G)\n        returns.reverse()\n        \n        # Compute the gradient for this episode\n        for t, (s, a, r) in enumerate(episode):\n            # Compute the policy (softmax) for the current state\n            policy = np.exp(theta[s, :]) / np.sum(np.exp(theta[s, :]))\n            \n            # Compute the gradient of the log-policy for the chosen action\n            grad_log_policy = np.zeros(num_actions)\n            grad_log_policy[a] = 1 / policy[a] * (1 - policy[a])\n            \n            # Update the gradient\n            gradient[s, :] += grad_log_policy * returns[t]\n    \n    # Average the gradient over all episodes\n    gradient /= len(episodes)\n    \n    # Round to the nearest 4th decimal and convert to list\n    gradient = np.round(gradient, 4).tolist()\n    \n    return gradient\n```\n\n### Explanation:\n\n- **Policy Calculation**: The policy for each state is calculated using the softmax function, which ensures that the probabilities sum to 1.\n- **Return Calculation**: The return for each time step is calculated by summing up the rewards from that time step to the end of the episode.\n- **Gradient Calculation**: The gradient of the log-policy for the chosen action is computed. The gradient for the chosen action is adjusted by the factor `(1 - policy[a])` due to the derivative of the softmax function.\n- **Averaging**: The gradient is averaged over all episodes to get the final policy gradient estimate.\n- **Rounding and Conversion**: The result is rounded to the nearest 4th decimal and converted to a list for the final output.\n\nThis implementation should meet the requirements of the task."}
{"task_id": 123, "completion_id": 0, "completion": "To calculate the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer, we need to compute the floating-point operations (FLOPs) for both types of layers and then determine the savings percentage.\n\n### Steps to Calculate FLOPs:\n\n1. **Dense Layer FLOPs Calculation:**\n   - For a dense layer, each input dimension is connected to each output dimension.\n   - The number of FLOPs for a dense layer is given by: \\( \\text{FLOPs}_{\\text{dense}} = d_{\\text{in}} \\times d_{\\text{out}} \\times 2 \\) (the factor of 2 accounts for both multiplication and addition).\n\n2. **MoE Layer FLOPs Calculation:**\n   - In an MoE layer, only a subset of experts (determined by `k_active`) are active for each input.\n   - Each active expert processes the input and produces an output.\n   - The number of FLOPs for the MoE layer is given by: \\( \\text{FLOPs}_{\\text{MoE}} = k_{\\text{active}} \\times d_{\\text{in}} \\times d_{\\text{out}} \\times 2 \\).\n\n3. **Savings Calculation:**\n   - The savings percentage is calculated as: \n     \\[\n     \\text{Savings} = \\left(1 - \\frac{\\text{FLOPs}_{\\text{MoE}}}{\\text{FLOPs}_{\\text{dense}}}\\right) \\times 100\n     \\]\n\nHere is the Python function to perform these calculations:\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    # Calculate FLOPs for the dense layer\n    flops_dense = d_in * d_out * 2\n    \n    # Calculate FLOPs for the MoE layer\n    flops_moe = k_active * d_in * d_out * 2\n    \n    # Calculate the savings percentage\n    savings_percentage = (1 - (flops_moe / flops_dense)) * 100\n    \n    # Round results to the nearest 1st decimal\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    \n    return flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded\n\n# Example usage:\nn_experts = 16\nk_active = 4\nd_in = 512\nd_out = 1024\n\nflops_dense, flops_moe, savings = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Dense Layer FLOPs: {flops_dense}\")\nprint(f\"MoE Layer FLOPs: {flops_moe}\")\nprint(f\"Savings Percentage: {savings}%\")\n```\n\nThis function will compute the FLOPs for both the dense and MoE layers and then calculate the savings percentage, rounding all results to the nearest tenth."}
{"task_id": 124, "completion_id": 0, "completion": "To implement the Noisy Top-K Gating Function for Mixture-of-Experts (MoE) models, we need to follow these steps:\n\n1. **Compute the logits**: This is done by multiplying the input matrix `X` with the weight matrix `W_g`.\n2. **Add noise**: Add the pre-sampled noise `N` scaled by `W_noise` to the logits.\n3. **Apply softmax**: Apply the softmax function to the noisy logits to get the gating probabilities.\n4. **Top-K selection**: Select the top `k` gating probabilities for each input.\n5. **Normalize the top-K probabilities**: Ensure that the selected top `k` probabilities sum to 1 for each input.\n6. **Return the result**: Convert the final gating probabilities matrix to a Python list after rounding to the nearest 4th decimal.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Step 1: Compute the logits\n    logits = np.dot(X, W_g)\n    \n    # Step 2: Add noise\n    noisy_logits = logits + W_noise * N\n    \n    # Step 3: Apply softmax\n    def softmax(x):\n        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return e_x / e_x.sum(axis=-1, keepdims=True)\n    \n    gating_probs = softmax(noisy_logits)\n    \n    # Step 4: Top-K selection\n    topk_indices = np.argsort(gating_probs, axis=-1)[:, -k:]\n    topk_mask = np.zeros_like(gating_probs, dtype=bool)\n    np.put_along_axis(topk_mask, topk_indices[:, :, np.newaxis], True, axis=-1)\n    \n    # Step 5: Normalize the top-K probabilities\n    topk_gating_probs = np.where(topk_mask, gating_probs, 0)\n    topk_gating_probs /= topk_gating_probs.sum(axis=-1, keepdims=True)\n    \n    # Step 6: Return the result\n    return np.round(topk_gating_probs, 4).tolist()\n\n# Example usage:\n# X = np.random.rand(5, 10)  # 5 inputs, each with 10 features\n# W_g = np.random.rand(10, 3)  # 3 experts\n# W_noise = 1.0\n# N = np.random.randn(5, 3)  # Pre-sampled noise\n# k = 2\n# print(noisy_topk_gating(X, W_g, W_noise, N, k))\n```\n\n### Explanation:\n- **Logits Calculation**: We compute the logits by multiplying the input matrix `X` with the weight matrix `W_g`.\n- **Noise Addition**: We add the scaled noise to the logits. The noise is scaled by `W_noise`.\n- **Softmax Function**: We define a softmax function to convert logits into probabilities.\n- **Top-K Selection**: We use `np.argsort` to find the indices of the top `k` probabilities and create a mask to zero out the non-top-k probabilities.\n- **Normalization**: We normalize the top-k probabilities to ensure they sum to 1 for each input.\n- **Rounding and Conversion**: Finally, we round the probabilities to the nearest 4th decimal and convert the result to a Python list using `tolist()`.\n\nThis implementation ensures that the gating probabilities are computed correctly and efficiently, adhering to the constraints provided."}
{"task_id": 125, "completion_id": 0, "completion": "To implement a Sparse Mixture of Experts (MoE) layer with softmax gating and top-k routing, we need to follow these steps:\n\n1. **Compute Gating Scores**: Use the gating weight matrix to compute the gating scores for each expert.\n2. **Apply Softmax**: Normalize the gating scores using softmax to get the gating probabilities.\n3. **Top-k Routing**: For each input token, select the top-k experts based on the gating probabilities.\n4. **Compute Expert Outputs**: For the selected experts, compute the output using the expert weight matrices.\n5. **Aggregate Outputs**: Combine the outputs of the selected experts, weighted by the corresponding gating probabilities.\n6. **Return the Result**: Reshape and round the final output to the nearest 4th decimal and convert it to a Python list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Step 1: Compute gating scores\n    gating_scores = x @ Wg.T  # Shape: (batch_size, n_experts)\n    \n    # Step 2: Apply softmax to get gating probabilities\n    gating_probs = np.exp(gating_scores - np.max(gating_scores, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)  # Shape: (batch_size, n_experts)\n    \n    # Step 3: Top-k routing\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]  # Shape: (batch_size, top_k)\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)  # Shape: (batch_size, top_k)\n    \n    # Normalize top-k probabilities\n    top_k_probs /= np.sum(top_k_probs, axis=1, keepdims=True)  # Shape: (batch_size, top_k)\n    \n    # Step 4: Compute expert outputs\n    batch_size, input_dim = x.shape\n    output_dim = We.shape[1]\n    expert_outputs = np.zeros((batch_size, top_k, output_dim))  # Shape: (batch_size, top_k, output_dim)\n    \n    for i in range(batch_size):\n        for j, expert_idx in enumerate(top_k_indices[i]):\n            expert_outputs[i, j] = x[i] @ We[expert_idx].T  # Shape: (output_dim,)\n    \n    # Step 5: Aggregate outputs\n    weighted_outputs = expert_outputs * top_k_probs[:, :, np.newaxis]  # Shape: (batch_size, top_k, output_dim)\n    final_output = np.sum(weighted_outputs, axis=1)  # Shape: (batch_size, output_dim)\n    \n    # Step 6: Return the result\n    return np.round(final_output, 4).tolist()\n\n# Example usage:\n# x = np.random.rand(5, 10)  # Batch size 5, input dim 10\n# We = np.random.rand(20, 15)  # 20 experts, output dim 15\n# Wg = np.random.rand(10, 20)  # Input dim 10, 20 experts\n# n_experts = 20\n# top_k = 3\n# print(moe(x, We, Wg, n_experts, top_k))\n```\n\n### Explanation:\n- **Gating Scores**: We compute the gating scores by multiplying the input `x` with the gating weight matrix `Wg`.\n- **Softmax**: We apply softmax to these scores to get the gating probabilities.\n- **Top-k Routing**: We select the top-k experts for each input token based on the gating probabilities.\n- **Expert Outputs**: For each selected expert, we compute the output by multiplying the input with the corresponding expert weight matrix.\n- **Aggregation**: We aggregate the outputs of the selected experts, weighted by their respective gating probabilities.\n- **Output**: The final output is rounded to the nearest 4th decimal and converted to a Python list."}
{"task_id": 126, "completion_id": 0, "completion": "To implement Group Normalization, we need to follow these steps:\n\n1. **Reshape the Input Tensor**: Group the channels into smaller groups.\n2. **Compute Mean and Variance**: Calculate the mean and variance for each group.\n3. **Normalize**: Subtract the mean and divide by the square root of the variance plus a small epsilon to avoid division by zero.\n4. **Scale and Shift**: Apply the learned scale (gamma) and shift (beta).\n5. **Reshape Back**: Convert the normalized tensor back to its original shape.\n6. **Round and Convert to List**: Round the results to the nearest 4th decimal and convert to a Python list.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Get the shape of the input tensor\n    B, C, H, W = X.shape\n    \n    # Check if the number of channels is divisible by the number of groups\n    assert C % num_groups == 0, \"Number of channels must be divisible by the number of groups\"\n    \n    # Reshape X to (B, num_groups, C // num_groups, H, W)\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    \n    # Compute mean and variance along the group, height, and width dimensions\n    mean = X_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape X_normalized back to (B, C, H, W)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    \n    # Apply scale (gamma) and shift (beta)\n    # Reshape gamma and beta to (1, C, 1, 1) to broadcast correctly\n    gamma_reshaped = gamma.reshape(1, C, 1, 1)\n    beta_reshaped = beta.reshape(1, C, 1, 1)\n    \n    Y = gamma_reshaped * X_normalized + beta_reshaped\n    \n    # Round to the nearest 4th decimal and convert to list\n    result = np.round(Y, 4).tolist()\n    \n    return result\n```\n\n### Explanation:\n- **Reshaping**: We reshape the input tensor to group the channels. This allows us to compute statistics (mean and variance) over the grouped channels.\n- **Mean and Variance**: We compute these statistics over the grouped channels, height, and width dimensions.\n- **Normalization**: We normalize each group using the computed mean and variance.\n- **Scaling and Shifting**: We apply the learned parameters `gamma` and `beta` to scale and shift the normalized values.\n- **Rounding and Conversion**: Finally, we round the results to four decimal places and convert the numpy array to a Python list for the output.\n\nThis function should work for any valid input tensor and parameters, provided the number of channels is divisible by the number of groups."}
{"task_id": 127, "completion_id": 0, "completion": "To solve this problem, we can use a gradient descent algorithm. Gradient descent is an optimization algorithm used to find the minimum of a function by iteratively moving towards the negative of the gradient (or approximate gradient) of the function at the current point.\n\nHere's the step-by-step approach:\n\n1. **Define the function and its derivative**: We need to compute the derivative of the function \\( f(x) = x^4 - 3x^3 + 2 \\) to use in the gradient descent algorithm. The derivative \\( f'(x) \\) is \\( 4x^3 - 9x^2 \\).\n\n2. **Implement the gradient descent algorithm**: Starting from an initial point \\( x \\), we will iteratively update \\( x \\) using the formula:\n   \\[\n   x_{\\text{new}} = x_{\\text{old}} - \\text{learning\\_rate} \\times f'(x_{\\text{old}})\n   \\]\n   We will continue this process until the change in \\( x \\) is smaller than a specified tolerance or we reach the maximum number of iterations.\n\n3. **Round the result**: Finally, we will round the result to the nearest 4th decimal place.\n\nHere is the complete implementation:\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    # Define the function f(x) and its derivative f'(x)\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    def f_prime(x):\n        return 4*x**3 - 9*x**2\n    \n    # Initialize the current position\n    x = start_x\n    \n    # Perform gradient descent\n    for _ in range(max_iters):\n        # Calculate the gradient at the current position\n        gradient = f_prime(x)\n        \n        # Update the position\n        new_x = x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(new_x - x) < tolerance:\n            break\n        \n        # Update x for the next iteration\n        x = new_x\n    \n    # Return the result rounded to the nearest 4th decimal\n    return round(x, 4)\n\n# Example usage:\n# start_x = 0.0  # You can choose any starting point\n# print(find_treasure(start_x))\n```\n\n### Explanation:\n- **Function and Derivative**: We defined the function \\( f(x) \\) and its derivative \\( f'(x) \\).\n- **Gradient Descent Loop**: We iteratively update the position \\( x \\) using the gradient descent formula. The loop continues until the change in \\( x \\) is smaller than the tolerance or the maximum number of iterations is reached.\n- **Rounding**: The final result is rounded to the nearest 4th decimal place to meet the output constraints.\n\nThis implementation should help you find the value of \\( x \\) where the function \\( f(x) \\) reaches its minimum, starting from any random initial position."}
{"task_id": 128, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Dynamic Tanh (DyT) function as described. The function takes an input array `x` and three scalar parameters `alpha`, `gamma`, and `beta`. It applies the DyT transformation and returns the result as a Python list rounded to the nearest 4th decimal.\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Apply the Dynamic Tanh transformation\n    transformed = alpha * np.tanh(gamma * x + beta)\n    \n    # Round the results to the nearest 4th decimal\n    rounded_transformed = np.round(transformed, 4)\n    \n    # Convert the result to a Python list\n    result_list = rounded_transformed.tolist()\n    \n    return result_list\n\n# Example usage:\n# x = np.array([1.0, 2.0, 3.0])\n# alpha = 1.0\n# gamma = 1.0\n# beta = 0.0\n# print(dynamic_tanh(x, alpha, gamma, beta))\n```\n\n### Explanation:\n1. **Transformation**: The core of the DyT function is the transformation `alpha * np.tanh(gamma * x + beta)`. This applies a scaled and shifted tanh function to the input `x`.\n2. **Rounding**: The results are rounded to the nearest 4th decimal using `np.round(transformed, 4)`.\n3. **Conversion to List**: The rounded numpy array is converted to a Python list using the `tolist()` method.\n\nYou can test the function with different inputs to see how it behaves. The example usage at the bottom of the code snippet demonstrates how to call the function with a sample input."}
