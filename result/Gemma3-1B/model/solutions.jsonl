{"task_id": 0, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(data, alpha):\n    \"\"\"\n    Compute the closed-form solution for Ridge Regression using the L2 regularization.\n\n    Args:\n        data (np.ndarray): The input data (features).\n        alpha (float): The regularization strength.\n\n    Returns:\n        list: The weight vector (the ridge regression coefficient).\n    \"\"\"\n    X = data.T\n    n = data.shape[0]\n    det_X = np.linalg.det(X)\n    w = np.linalg.solve(X.T @ X, X.T @ data)\n    w = np.round(w, 4)\n    return w.tolist()"}
{"task_id": 1, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\nimport scipy.stats as stats\ndef glm_links(mu, lambda_val):\n    \"\"\"\n    Generates a GLM link function.\n\n    Args:\n        mu (numpy.ndarray): The mean of the Gaussian distribution.\n        lambda_val (float): The parameter to be estimated.\n\n    Returns:\n        dict: A dictionary containing the link function.\n    \"\"\"\n    return {'link': lambda_val, 'derivative': lambda_val}"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter"}
{"task_id": 7, "completion_id": 0, "solution": "import math\nfrom collections import Counter"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data, k, epsilon=0.0001):\n    \"\"\"\n    K-Means clustering algorithm.\n\n    Args:\n        data (np.ndarray): A 2D numpy array of shape (n, d) where n is the number of samples\n                         and d is the number of features.\n        k (int): The number of clusters.\n        epsilon (float): The convergence threshold.\n\n    Returns:\n        tuple: A tuple containing the cluster centers and the cluster labels.\n    \"\"\"\n    n = data.shape[0]\n    if n == 0:\n        return ([], [])\n    centroids = data[np.random.choice(n, k, replace=False)]\n    for _ in range(max(max(data), 100)):\n        distances = np.sqrt(((data - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        cluster_labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([np.mean(data[cluster_labels], axis=0), np.mean(data[cluster_labels], axis=0)])\n        if np.all(new_centroids == centroids):\n            break\n        centroids = new_centroids\n    return (centroids, cluster_labels)"}
{"task_id": 13, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\ndef best_split(X, y, threshold):\n    \"\"\"\n    Finds the best split for a tree-based model.\n\n    Args:\n        X (np.ndarray): The input features.\n        y (np.ndarray): The target variable.\n        threshold (float): The threshold for the split.\n\n    Returns:\n        tuple: A tuple containing the best split index and the corresponding\n               threshold.\n    \"\"\"\n    best_split = None\n    best_threshold = 0.0\n    for i in range(threshold + 1):\n        tree = DecisionTreeRegressor(max_depth=5)\n        tree.fit(X, y)\n        if tree.score > best_threshold:\n            best_threshold = i\n            best_split = i\n    return (best_split, best_threshold)"}
{"task_id": 20, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\nimport math"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef linear_autoencoder(X, k):\n    \"\"\"\n    Computes the reconstruction error of a linear autoencoder using SVD.\n\n    Args:\n        X (list[list[float]]): A 2D list of input data (m x n).\n        k (int): The number of latent dimensions to keep.\n\n    Returns:\n        tuple: A tuple containing the reconstructed matrix (X) and the MSE.\n    \"\"\"\n    X = np.array(X)\n    if k <= 0 or k > X.shape[0]:\n        return -1\n    try:\n        (U, s, V) = np.linalg.svd(X)\n        reconstructed_X = U[:, :k] @ V\n        mse = np.mean(X - reconstructed_X)\n        return (reconstructed_X, mse)\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef random_binary_tensor(shape, sparsity):\n    \"\"\"\n    Generates a NumPy array with a specified shape and sparsity.\n\n    Args:\n        shape (tuple): The shape of the tensor (e.g., (2, 3)).\n        sparsity (float): The probability of a value being 1.0 (0.0 to 1.0).\n\n    Returns:\n        numpy.ndarray: A NumPy array filled with 0s and 1s based on the sparsity.\n    \"\"\"\n    if 0.0 <= sparsity <= 1.0:\n        seed = random.randint(0, 1000000)\n        return np.random.rand(shape) * sparsity\n    else:\n        return -1"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\ndef to_frames(x: np.ndarray, frame_width: int, frame_stride: int) -> np.ndarray:\n    \"\"\"\n    Converts a 1D signal into overlapping frames.\n\n    Args:\n        x (np.ndarray): The input signal.\n        frame_width (int): The width of each frame.\n        frame_stride (int): The stride between frames.\n\n    Returns:\n        np.ndarray: A NumPy array with the same shape as the input,\n                     but with frames separated by the specified stride.\n    \"\"\"\n    if x.ndim != 1:\n        raise ValueError('Input must be a 1-dimensional array.')\n    n = len(x)\n    if n == 0:\n        return np.array([])\n    frames = []\n    for i in range(0, n, frame_stride):\n        frames.append(x[i:i + frame_stride])\n    return np.array(frames)"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\nimport numpy.linalg as la\ndef is_stochastic(X):\n    \"\"\"\n    Check if a matrix is stochastic.\n\n    Args:\n        X (list[list[float]]): A matrix of floats.\n\n    Returns:\n        bool: True if the matrix is stochastic, False otherwise.\n    \"\"\"\n    try:\n        np_X = np.array(X)\n        return np.all(np.isin(np_X, [0.0, 1.0]))\n    except:\n        return False"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef rms_prop(X, y):\n    \"\"\"\n    Calculates the RMS (Root Mean Squared Error) for a linear regression model.\n\n    Args:\n        X (numpy.ndarray): The design matrix (features).\n        y (numpy.ndarray): The target vector.\n\n    Returns:\n        numpy.ndarray: The learned weight vector.\n    \"\"\"\n    n = len(y)\n    if n == 0:\n        return np.zeros(n)\n    mse = np.mean((y - X ** 2) ** 2)\n    r_m_squared_error = 2 * mse\n    return np.round(r_m_squared_error, 4)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef softplus(x, deriv=False):\n    \"\"\"\n    Calculates the Softplus function for a given input.\n\n    Args:\n        x (float or numpy.ndarray): The input value.\n        deriv (bool, optional): Whether to calculate the derivative. Defaults to False.\n\n    Returns:\n        float: The Softplus value, rounded to 4 decimal places.\n    \"\"\"\n    if deriv:\n        return np.log(1 + np.exp(-x))\n    else:\n        return np.sign(x)"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nfrom collections import Counter"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 65, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef forward_algorithm(S, A, B):\n    \"\"\"\n    Implements the forward algorithm for a Hidden Markov Model.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): Transition probabilities.\n        B (list[float]): Emission probabilities.\n\n    Returns:\n        float: The likelihood of the observation sequence, rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(S, list) or len(S) != len(A) or (not isinstance(A, list)) or (len(A) != len(B)):\n        return -1\n    S = np.array(S)\n    A = np.array(A)\n    B = np.array(B)\n    for i in range(len(S)):\n        if not isinstance(S[i], (int, float)) or not isinstance(A[i], (int, float)) or (not isinstance(B[i], (int, float))):\n            return -1\n    likelihood = np.dot(A, S)\n    return round(likelihood, 4)"}
{"task_id": 70, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List"}
{"task_id": 75, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train, y_train, X_test, k):\n    \"\"\"\n    Predicts class labels for test samples using the k-NN algorithm.\n\n    Args:\n        X_train (list): List of feature vectors for training data.\n        y_train (list): List of class labels for training data.\n        X_test (list): List of feature vectors for the test data.\n        k (int): The number of nearest neighbors to consider.\n\n    Returns:\n        list: A list of predicted class labels for the test data.\n    \"\"\"\n    if not isinstance(k, int) or k <= 0:\n        return -1\n    if k > len(X_train):\n        return -1\n    distances = []\n    for i in range(len(X_train)):\n        distances.append(np.linalg.norm(X_train[i] - X_test[i]))\n    predicted_labels = []\n    for i in range(len(X_train)):\n        if distances[i] <= k:\n            predicted_labels.append(y_train[i])\n        else:\n            predicted_labels.append(-1)\n    return predicted_labels"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\nimport tensorflow as tf\ndef cross_entropy_loss(y, y_pred):\n    \"\"\"\n    Compute the unnormalised categorical cross-entropy loss.\n\n    Args:\n        y (list): One-hot encoded labels.\n        y_pred (list): Predicted probabilities.\n\n    Returns:\n        float: The cross-entropy loss.\n    \"\"\"\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    y_pred = tf.clip_by_value(y_pred, 1e-07, 1.0)\n    loss = -np.sum(y * np.log(y_pred))\n    return round(loss, 4)"}
{"task_id": 77, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2, Y):\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities.\n        Y (np.ndarray): Ground truth labels (0 or 1).\n\n    Returns:\n        float: The binary cross-entropy cost, rounded to 6 decimal places.\n    \"\"\"\n    A2 = np.clip(A2, 1e-15, 1.0)\n    return -np.sum(A2 * np.log(A2))"}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\ndef aggregate_random_forest_votes(predictions):\n    \"\"\"\n    Aggregates individual predictions from a random forest into a single list.\n\n    Args:\n        predictions (list): A list of lists, where each inner list contains the predictions\n                           for a single sample.\n\n    Returns:\n        list: A list containing the aggregated predictions.\n    \"\"\"\n    if not predictions:\n        return []\n    class_counts = Counter()\n    for prediction_list in predictions:\n        class_counts.update(prediction_list)\n    max_count = max(class_counts.values())\n    max_classes = [i for (i, count) in enumerate(class_counts.values()) if count == max_count]\n    aggregated_predictions = []\n    for class_label in max_classes:\n        aggregated_predictions.append(class_counts[class_label])\n    return aggregated_predictions"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef softplus(z):\n    \"\"\"\n    Compute the softplus of a scalar.\n\n    Args:\n        z (numpy.ndarray): A scalar NumPy array.\n\n    Returns:\n        numpy.ndarray: The softplus of the input array.\n    \"\"\"\n    return np.plus(np.log(np.exp(z) + 1), 1e-06)"}
{"task_id": 90, "completion_id": 0, "solution": ""}
{"task_id": 96, "completion_id": 0, "solution": "import math\nfrom collections import Counter"}
{"task_id": 108, "completion_id": 0, "solution": "import numpy as np\nimport scipy.linalg"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef kmeans(X, K, max_iter=100):\n    \"\"\"\n    Perform K-Means clustering with a reproducible random state.\n\n    Args:\n        X (np.ndarray): A 2D NumPy array of shape (n_samples, n_features)\n        K (int): The number of clusters.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        np.ndarray: A list of centroids, sorted by the first coordinate.\n    \"\"\"\n    np.random.seed(0)\n    (n_samples, n_features) = X.shape\n    centroids = X[np.random.choice(n_samples, K, replace=False)]\n    for i in range(max_iter):\n        distances = np.array(X)\n        distances = np.min(distances, axis=1)\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.array([centroid for (i, centroid) in enumerate(centroids) if i in cluster_assignments])\n        if np.all(new_centroids == centroids):\n            break\n        centroids = new_centroids\n    return centroids.tolist()"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nimport torch\ndef run_example(model, vocab: dict, input_text: str) -> str:\n    \"\"\"\n    Runs a model with a given input text and returns the decoded string.\n\n    Args:\n        model (torch.nn.Module): The model to run.\n        vocab (dict): A dictionary mapping characters to integers.\n        input_text (str): The input text to feed to the model.\n\n    Returns:\n        str: The decoded string from the model.\n    \"\"\"\n    input_array = torch.tensor(input_text, dtype=torch.int64)\n    output = model(input_array)\n    predicted_index = torch.argmax(output)\n    decoded_text = vocab[predicted_index]\n    return decoded_text"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, output_vocabulary, examples):\n    \"\"\"\n    Runs a model on a sequence of examples and returns a list of predicted strings.\n\n    Args:\n        model: A callable function that takes a string as input and returns a string.\n        input_vocabulary: A dictionary mapping characters to integer indices.\n        output_vocabulary: A dictionary mapping characters to integer indices.\n        examples: A list of strings to use for the example.\n\n    Returns:\n        A list of strings representing the predicted output.\n    \"\"\"\n    predictions = []\n    for example in examples:\n        try:\n            prediction = model(example)\n            predictions.append(prediction)\n        except Exception as e:\n            print(f'Error during prediction for example: {example}. Error: {e}')\n            predictions.append(None)\n    return predictions"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\nimport scipy.optimize\ndef logistic_loss_and_gradient(X, y, w):\n    \"\"\"\n    Computes the average cross-entropy loss and its gradient with respect to the weights.\n\n    Args:\n        X (np.ndarray): Feature matrix (n_samples, n_features).\n        y (np.ndarray): Target vector (n_samples,).\n        w (np.ndarray): Weight vector (n_samples,).\n\n    Returns:\n        tuple: A tuple containing the loss and the gradient.\n    \"\"\"\n    m = len(y)\n    loss = np.mean(np.log(w))\n    gradient = 1 / (m * np.log(w) + 1)\n    return (loss, gradient)"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd"}
{"task_id": 140, "completion_id": 0, "solution": "from collections import deque"}
{"task_id": 141, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X_train, y_train, X_test, k):\n    \"\"\"\n    Computes the k-Nearest Neighbors (k-NN) classifier for a test set.\n\n    Args:\n        X_train (np.ndarray): Training data (n_samples, n_features).\n        y_train (np.ndarray): Training labels (n_samples,).\n        X_test (np.ndarray): Test data (n_samples, n_features).\n        k (int): The number of nearest neighbors to consider.\n\n    Returns:\n        np.ndarray: Predicted labels for the test set.\n    \"\"\"\n    if metric == 'euclidean':\n        distances = np.linalg.norm(X_train, axis=1)\n        indices = np.argsort(distances)\n        k_nearest_indices = indices[:k]\n        predicted_labels = np.zeros(len(y_train))\n        for i in k_nearest_indices:\n            predicted_labels[y_train[i]] = y_train[i]\n        return predicted_labels\n    elif metric == 'manhattan':\n        distances = np.linalg.norm(X_train, axis=1)\n        indices = np.argsort(distances)\n        k_nearest_indices = indices[:k]\n        predicted_labels = np.zeros(len(y_train))\n        for i in k_nearest_indices:\n            predicted_labels[y_train[i]] = y_train[i]\n        return predicted_labels\n    elif metric == 'cosine':\n        distances = np.linalg.norm(X_train, axis=1)\n        indices = np.argsort(distances)\n        k_nearest_indices = indices[:k]\n        predicted_labels = np.zeros(len(y_train))\n        for i in k_nearest_indices:\n            predicted_labels[y_train[i]] = y_train[i]\n        return predicted_labels\n    else:\n        return np.array(y_train)"}
{"task_id": 146, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\ndef actor_forward(state, weights):\n    \"\"\"\n    Performs the forward pass of an actor network.\n\n    Args:\n        state (list): A one-dimensional NumPy array representing the state.\n        weights (dict): A dictionary containing the weights for the network.\n\n    Returns:\n        list: A list of floats representing the output vector.\n    \"\"\"\n    linear_1 = np.dot(state, weights['W1']) + weights['b1']\n    linear_2 = np.dot(linear_1, weights['W2']) + weights['b2']\n    relu = np.tanh(linear_2)\n    linear_3 = np.dot(relu, weights['W3']) + weights['b3']\n    tanh_relu = np.tanh(linear_3)\n    linear_4 = np.dot(tanh_relu, weights['W4']) + weights['b4']\n    return linear_4"}
{"task_id": 160, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.linalg import norm"}
{"task_id": 165, "completion_id": 0, "solution": "import numpy as np\ndef hamming_distance(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Compute the normalized Hamming distance between two 1D vectors.\n\n    Args:\n        x: A list of integers representing the first vector.\n        y: A list of integers representing the second vector.\n\n    Returns:\n        The normalized Hamming distance between the two vectors, rounded to 4 decimal places.\n        Returns -1 if the vectors have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    distance = 0\n    for i in range(len(x)):\n        if x[i] != y[i]:\n            distance += 1\n    return round(distance / len(x) * 100, 4)"}
{"task_id": 169, "completion_id": 0, "solution": "import numpy as np\ndef generate_window(window_type: str, n: int, coefficients: list[float]) -> list[float]:\n    \"\"\"\n    Generates a list of coefficients for a given window type.\n\n    Args:\n        window_type (str): The type of window to generate (e.g., \"hamming\", \"hann\", \"blackman\").\n        n (int): The number of coefficients to generate.\n        coefficients (list[float]): A list of floats representing the coefficients for the window.\n\n    Returns:\n        list[float]: A list of floats representing the coefficients.\n    \"\"\"\n    if window_type == 'hamming':\n        return np.random.rand(n)\n    elif window_type == 'hann':\n        return np.random.rand(n)\n    elif window_type == 'blackman':\n        return np.random.rand(n)\n    elif window_type == 'cosine':\n        return np.random.rand(n)\n    else:\n        return None"}
{"task_id": 171, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 176, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.ensemble import AdaBoost\ndef adaboost_predict(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Predicts class labels for a test set using AdaBoost.\n\n    Args:\n        X_train (list): Training data features.\n        y_train (list): Training data labels.\n        X_test (list): Test data features.\n        y_test (list): Test data labels.\n\n    Returns:\n        list: Predicted class labels for the test set.\n    \"\"\"\n    n_estimators = 10\n    weights = np.ones(len(X_train))\n    for i in range(n_estimators):\n        weights[i] = np.sum(y_train[i] == 1) / len(y_train)\n    y_pred = AdaBoost(weights, n_estimators=n_estimators)\n    return y_pred.predict(X_test)"}
{"task_id": 178, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 180, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nfrom typing import List\ndef knn_predict(X_train, y_train, X_test, k: int=5, task: str='classification') -> List[float]:\n    \"\"\"\n    Predicts the class label for a set of test samples using k-nearest neighbors.\n\n    Args:\n        X_train (list): Training feature vectors.\n        y_train (list): Training labels.\n        X_test (list): Test feature vectors.\n        k (int): The number of nearest neighbors to consider. Defaults to 5.\n        task (str): \"classification\" or \"regression\".\n\n    Returns:\n        list: Predicted labels for the test samples.\n    \"\"\"\n    if task == 'classification':\n        distances = [np.linalg.norm(X_train[i] - X_test[i]) for i in range(len(X_train))]\n        k_nearest_indices = np.argsort(distances)[:k]\n        neighbor_labels = [y_train[i] for i in k_nearest_indices]\n        counts = Counter(neighbor_labels)\n        most_common_label = counts.most_common(1)[0][0]\n        return most_common_label\n    elif task == 'regression':\n        distances = [np.linalg.norm(X_train[i] - X_test[i]) for i in range(len(X_train))]\n        k_nearest_indices = np.argsort(distances)[:k]\n        predicted_values = [X_train[i] * 10 ** (-distances[i]) for i in k_nearest_indices]\n        average_value = np.mean(predicted_values)\n        return average_value\n    else:\n        raise ValueError(\"Invalid task. Must be 'classification' or 'regression'.\")"}
{"task_id": 184, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\ndef decision_tree_predict(X_train, y_train, X_test, max_depth=None):\n    \"\"\"\n    Builds a decision tree classifier using the CART algorithm.\n\n    Args:\n        X_train (np.ndarray): Training data (n_samples, n_features).\n        y_train (np.ndarray): Training labels (n_samples).\n        X_test (np.ndarray): Test data (n_samples, n_features).\n        max_depth (int, optional): Maximum depth of the tree. Defaults to None.\n\n    Returns:\n        list: Predicted labels for the test data.\n    \"\"\"\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    dtree = DecisionTreeClassifier(max_depth=max_depth)\n    dtree.fit(X_train, y_train)\n    predictions = dtree.predict(X_test)\n    return predictions"}
{"task_id": 190, "completion_id": 0, "solution": "import numpy as np\ndef best_gini_split(X, y):\n    \"\"\"\n    Finds the best split index for Gini impurity.\n\n    Args:\n        X (np.ndarray): A 2D numpy array of feature values.\n        y (np.ndarray): A 1D numpy array of class labels.\n\n    Returns:\n        tuple: A tuple containing the best split index and the corresponding Gini impurity.\n    \"\"\"\n    n = len(X)\n    if n == 0:\n        return (-1, None, -1)\n    best_index = 0\n    min_gini = float('inf')\n    for i in range(n):\n        left_gini = calculate_gini(X[i], y[i])\n        right_gini = calculate_gini(X, y[i])\n        gini = (left_gini + right_gini) / 2\n        if gini < min_gini:\n            min_gini = gini\n            best_index = i\n    return (best_index, min_gini, -1)\ndef calculate_gini(x, y):\n    \"\"\"\n    Calculates the Gini impurity for a given split.\n\n    Args:\n        x (np.ndarray): A 2D numpy array of feature values.\n        y (np.ndarray): A 1D numpy array of class labels.\n\n    Returns:\n        float: The Gini impurity.\n    \"\"\"\n    n = len(x)\n    if n == 0:\n        return 0.0\n    gini = 1.0\n    for i in range(n):\n        gini -= x[i] / n * (y[i] / n)\n    return gini"}
{"task_id": 191, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef metric(name):\n    \"\"\"\n    Calculates the distance between two vectors.\n\n    Args:\n        name (str): The name of the metric.\n\n    Returns:\n        float: The distance between the two vectors.\n    \"\"\"\n    try:\n        x = np.array(name)\n        y = np.array(name)\n        if x.size == 0 or y.size == 0:\n            return -1\n        return round(np.linalg.norm(x) - np.linalg.norm(y), 4)\n    except:\n        return -1"}
{"task_id": 197, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Tuple, Union"}
{"task_id": 198, "completion_id": 0, "solution": "def update_beta(phi, corpus):\n    \"\"\"\n    Updates the beta distribution based on the given corpus.\n\n    Args:\n        phi (list): The current beta distribution.\n        corpus (list): The corpus of words.\n\n    Returns:\n        list: The updated beta distribution.\n    \"\"\"\n    beta = phi\n    for i in range(len(corpus)):\n        beta = beta + corpus[i] * 0.1\n    return beta"}
{"task_id": 202, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 216, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, rate=0.5, iterations=30):\n    \"\"\"\n    Elastic Net regression using gradient descent.\n\n    Args:\n        x_train (np.ndarray): Training data.\n        y_train (np.ndarray): Training data.\n        x_test (np.ndarray): Test data.\n        degree (int): Degree of the polynomial.\n        rate (float): Rate of regularization.\n        iterations (int): Number of iterations.\n\n    Returns:\n        np.ndarray: Predictions for the test data.\n    \"\"\"\n    try:\n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n        x_test = np.array(x_test)\n        gradient = np.zeros_like(x_train)\n        for i in range(len(x_train)):\n            gradient[i] = (y_train[i] - x_train[i]) * x_train[i] ** degree\n        loss = np.mean(np.square(gradient))\n        return gradient\n    except Exception as e:\n        print(f'Error: {e}')\n        return -1"}
{"task_id": 217, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 218, "completion_id": 0, "solution": "def blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"\n    Evaluates the outcome of a Blackjack hand.\n\n    Args:\n        player: A list of integers representing the player's hand.\n        dealer: A list of integers representing the dealer's hand.\n\n    Returns:\n        The reward for the player.\n    \"\"\"\n    player_value = sum(player)\n    dealer_value = sum(dealer)\n    if player_value == 21 and dealer_value >= 17:\n        return 1\n    elif player_value == 21:\n        return 1\n    elif player_value <= 21 and dealer_value <= 21:\n        return 1\n    else:\n        return 0.0"}
{"task_id": 221, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 222, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid function for a single input.\n\n    Args:\n        z: A scalar number.\n\n    Returns:\n        The sigmoid of z, rounded to 4 decimal places.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))"}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.0):\n    \"\"\"\n    Apply Leaky ReLU to each element of a tensor.\n\n    Args:\n        z (numpy.ndarray): The input tensor.\n        a (float): The slope for Leaky ReLU. Defaults to 0.0.\n\n    Returns:\n        numpy.ndarray: The tensor with Leaky ReLU applied.\n    \"\"\"\n    return np.where(z >= 0, z, a * z)"}
{"task_id": 226, "completion_id": 0, "solution": "import numpy as np\nimport math"}
{"task_id": 241, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 243, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 249, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state, params):\n    \"\"\"\n    Computes the forward pass of an actor-critic network.\n\n    Args:\n        state (list): The current state of the environment.\n        params (dict): A dictionary containing the parameters of the network.\n\n    Returns:\n        tuple: A tuple containing the probabilities and the value.\n    \"\"\"\n    W1 = params['W1']\n    W2 = params['W2']\n    b1 = params['b1']\n    b2 = params['b2']\n    W = params['W']\n    b = params['b']\n    actor_output = np.dot(state, W1) + b1\n    actor_output = np.clip(actor_output, 0, 1)\n    critic_output = np.dot(state, W2) + b2\n    critic_output = np.clip(critic_output, 0, 1)\n    return (actor_output, critic_output)"}
{"task_id": 253, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 256, "completion_id": 0, "solution": "import numpy as np\nfrom tensorflow.keras.backend import get_tensor_and_sub"}
{"task_id": 257, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd"}
{"task_id": 261, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"\n    Generates a Glorot normal distribution array.\n\n    Args:\n        shape: A tuple specifying the shape of the input tensor.\n\n    Returns:\n        A NumPy array of floats with the specified shape and distribution.\n    \"\"\"\n    fan_in = shape[0]\n    fan_out = shape[1]\n    fan_in_squared = fan_in ** 2\n    fan_out_squared = fan_out ** 2\n    fan_in_mean = np.mean(fan_in)\n    fan_out_mean = np.mean(fan_out)\n    fan_in_std = np.std(fan_in)\n    fan_out_std = np.std(fan_out)\n    fan_in_variance = fan_in_mean * fan_out_mean\n    fan_out_variance = fan_out_mean * fan_out_std\n    return np.random.normal(loc=fan_in_mean, scale=fan_in_variance, size=shape)"}
{"task_id": 266, "completion_id": 0, "solution": "from typing import List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"\n    Builds an adjacency list representation of an undirected graph.\n\n    Args:\n        V: A list of vertex identifiers.\n        E: A list of tuples representing edges (u, v).\n\n    Returns:\n        A list of lists representing the adjacency list.\n    \"\"\"\n    adj_list = [[] for _ in range(len(V))]\n    for (u, v) in E:\n        adj_list[u].append(v)\n        adj_list[v].append(u)\n    return adj_list"}
{"task_id": 267, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier"}
{"task_id": 273, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef best_split(feature, target):\n    \"\"\"\n    Determine the best threshold for a feature to maximize information gain.\n\n    Args:\n        feature (list): A list of numerical features.\n        target (list): A list of target labels.\n\n    Returns:\n        tuple: A tuple containing the best threshold and the information gain.\n    \"\"\"\n    if not feature or not target:\n        return (None, 0.0)\n    feature_counts = Counter(feature)\n    target_counts = Counter(target)\n    best_threshold = None\n    max_gain = 0.0\n    for threshold in range(min(feature_counts.values(), target_counts.values()), max(feature_counts.values(), target_counts.values()) + 1):\n        current_gain = 0.0\n        for (feature, count) in feature_counts.items():\n            if feature == threshold:\n                current_gain += count\n        for (target, count) in target_counts.items():\n            if target == threshold:\n                current_gain += count\n        if current_gain > max_gain:\n            max_gain = current_gain\n            best_threshold = threshold\n    return (best_threshold, max_gain)"}
{"task_id": 286, "completion_id": 0, "solution": "def get_initializer(name):\n    \"\"\"\n    Returns the initializer for a given name.\n\n    Args:\n        name (str): The name of the initializer.\n\n    Returns:\n        callable: The initializer function.\n\n    Raises:\n        ValueError: If the name is not found in the initializer.\n    \"\"\"\n    try:\n        print(f'Initializing with name: {name}')\n        return lambda : None\n    except Exception as e:\n        raise ValueError(f'Error initializing with name {name}: {e}')"}
{"task_id": 287, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\ndef compare_trees(tree_a, tree_b):\n    \"\"\"\n    Compares two trees for equivalence.\n\n    Args:\n        tree_a: The root of the first tree.\n        tree_b: The root of the second tree.\n\n    Returns:\n        True if the trees are equivalent, False otherwise.\n    \"\"\"\n    if tree_a is None and tree_b is None:\n        return True\n    if tree_a is None or tree_b is None:\n        return False\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return tree_a == tree_b\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    else:\n        return False"}
{"task_id": 292, "completion_id": 0, "solution": "import numpy as np\ndef single_point_crossover(parent1, parent2):\n    \"\"\"\n    Performs single-point crossover on two weight matrices.\n\n    Args:\n        parent1: The first weight matrix (list of lists).\n        parent2: The second weight matrix (list of lists).\n\n    Returns:\n        A tuple containing the two offspring weight matrices.\n    \"\"\"\n    if len(parent1) != len(parent2):\n        return -1\n    offspring1 = []\n    offspring2 = []\n    for i in range(len(parent1)):\n        if i < len(parent2):\n            offspring1.append(parent1[i])\n            offspring2.append(parent2[i])\n    return (tuple(offspring1), tuple(offspring2))"}
{"task_id": 294, "completion_id": 0, "solution": "from typing import Dict, List, Tuple"}
{"task_id": 296, "completion_id": 0, "solution": "import numpy as np\ndef is_binary(x):\n    \"\"\"\n    Check if a NumPy array contains only 0s and 1s.\n\n    Args:\n        x (np.ndarray): A NumPy array.\n\n    Returns:\n        bool: True if the array contains only 0s and 1s, False otherwise.\n    \"\"\"\n    return np.all(x == 0) | np.all(x == 1)"}
{"task_id": 298, "completion_id": 0, "solution": "import math\nfrom collections import Counter"}
{"task_id": 302, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(X, k):\n    \"\"\"\n    Clusters data using spectral clustering.\n\n    Args:\n        X (np.ndarray): A 2D NumPy array where each row represents a data point.\n        k (int): The desired number of clusters.\n\n    Returns:\n        list: A list of integers representing the cluster labels for each data point.\n    \"\"\"\n    n = X.shape[0]\n    if k > n:\n        raise ValueError('k cannot be greater than the number of data points.')\n    W = np.zeros((n, k))\n    for i in range(n):\n        for j in range(i, n):\n            w = X[i, j]\n            W[i, j] = 1.0 / (1.0 + w)\n    (eigenvalues, eigenvectors) = np.linalg.eig(W)\n    eigenvalues = np.sort(eigenvalues)\n    k_eigenvalues = eigenvalues[:k]\n    E = np.zeros((k, n))\n    for i in range(k):\n        E[i, :] = eigenvalues[i]\n    centroids = E.copy()\n    cluster_labels = np.zeros(n, dtype=int)\n    for i in range(n):\n        distances = np.linalg.norm(X[i] - centroids[i], axis=1)\n        cluster_labels[i] = np.argmin(distances)\n    return cluster_labels.tolist()"}
{"task_id": 303, "completion_id": 0, "solution": "def err_fmt(params: list[tuple[str, str]]) -> str:\n    \"\"\"\n    Formats a list of tuples into a single string, adhering to the specified format.\n\n    Args:\n        params: A list of tuples, where each tuple contains a string (label) and a string (value).\n\n    Returns:\n        A string formatted as described in the problem.\n    \"\"\"\n    result = ''\n    for (label, value) in params:\n        result += f'--------------------------------------------------\\n'\n        result += f'{label}: {value}\\n'\n        result += f'--------------------------------------------------\\n'\n    return result"}
{"task_id": 304, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.stats import norm"}
{"task_id": 308, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 312, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 313, "completion_id": 0, "solution": "import numpy as np\nimport math"}
{"task_id": 317, "completion_id": 0, "solution": "from typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\nfrom typing import List, Tuple\ndef err_fmt(params: List[Tuple[str, str]]) -> str:\n    \"\"\"\n    Formats a list of parameters into a detailed debug string.\n\n    Args:\n        params: A list of tuples, where each tuple contains a parameter name (str) and a parameter value (str).\n\n    Returns:\n        A string containing a detailed debug message.\n    \"\"\"\n    result = ''\n    for (param, value) in params:\n        result += f'DEBUG: {param} = {value}\\n'\n    return result"}
{"task_id": 318, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 329, "completion_id": 0, "solution": "import numpy as np\ndef dft_bins(N, fs, positive=True):\n    \"\"\"\n    Compute the centre frequency of a DFT.\n\n    Args:\n        N (int): The number of DFT coefficients.\n        fs (int): The sampling frequency in Hz.\n        positive (bool): Whether to return positive frequencies.\n\n    Returns:\n        list: A list of floats representing the frequencies in Hz.\n    \"\"\"\n    if positive:\n        return [round(np.sqrt(N), 4) for _ in range(N)]\n    else:\n        return []"}
{"task_id": 331, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd\ndef oob_mse(y_true, y_pred, oob_mask):\n    \"\"\"\n    Computes the out-of-bag (oob) mean squared error for a Random Forest.\n\n    Args:\n        y_true (np.ndarray): True target values.\n        y_pred (np.ndarray): Predicted target values.\n        oob_mask (np.ndarray): Boolean mask indicating whether each sample has an OOB prediction.\n\n    Returns:\n        float: The oob mean squared error.\n    \"\"\"\n    if oob_mask.size == 0:\n        return np.nan\n    oob_mse_value = np.mean(y_true * y_pred)\n    return round(oob_mse_value, 4)"}
{"task_id": 332, "completion_id": 0, "solution": "from collections import Counter\ndef build_fp_tree(transactions, min_support):\n    \"\"\"\n    Constructs an FP-Tree from a list of transactions.\n\n    Args:\n        transactions (list[list[str]]): A list of transactions, where each transaction\n                                       is a list of strings.\n        min_support (int): The minimum support threshold for a transaction.\n\n    Returns:\n        dict: The root of the FP-Tree.\n    \"\"\"\n    counts = Counter()\n    for transaction in transactions:\n        counts[transaction] += 1\n    sorted_transactions = sorted(counts.items(), key=lambda item: item[1], reverse=True)\n    tree = {}\n    for (transaction, count) in sorted_transactions:\n        if count == 1:\n            tree[transaction] = 1\n        else:\n            tree[transaction] = tree.get(transaction, 0) + 1\n    return tree"}
{"task_id": 336, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(X, Y):\n    \"\"\"\n    Calculates the linear kernel matrix.\n\n    Args:\n        X (list): A list of NumPy arrays representing the input vectors.\n        Y (list): A list of NumPy arrays representing the target vectors.\n\n    Returns:\n        numpy.ndarray: A NumPy array representing the kernel matrix.\n                         Returns -1 if the shapes of X and Y are incompatible.\n    \"\"\"\n    if len(X) != len(Y):\n        return -1\n    K = np.zeros((len(X), len(Y)))\n    for i in range(len(X)):\n        for j in range(len(Y)):\n            K[i, j] = X[i] * Y[j]\n    return K"}
{"task_id": 340, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 343, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd\ndef cross_entropy_update(theta_samples, rewards):\n    \"\"\"\n    Compute the mean and variance of the retained samples.\n\n    Args:\n        theta_samples (np.ndarray): A NumPy array of shape (N, D) containing the\n                                  parameter vectors.\n        rewards (np.ndarray): A NumPy array of shape (N,) containing the rewards.\n\n    Returns:\n        tuple: A tuple containing the mean and variance of the retained samples.\n    \"\"\"\n    N = theta_samples.shape[0]\n    mean = np.mean(theta_samples)\n    variance = np.var(theta_samples)\n    return (mean, variance)"}
{"task_id": 353, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nimport numpy as np\ndef entropy(labels):\n    \"\"\"\n    Compute the Shannon entropy of a one-dimensional sequence of labels.\n\n    Args:\n        labels (list or np.ndarray): A one-dimensional sequence of labels.\n\n    Returns:\n        float: The entropy rounded to 5 decimal places.\n    \"\"\"\n    if not labels:\n        return 0.0\n    if not labels:\n        return 0.0\n    counts = Counter(labels)\n    total = len(labels)\n    entropy = 0.0\n    for count in counts.values():\n        probability = float(count) / total\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 5)"}
{"task_id": 354, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 355, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd"}
{"task_id": 356, "completion_id": 0, "solution": ""}
{"task_id": 357, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data, k):\n    \"\"\"\n    K-means clustering algorithm.\n\n    Args:\n        data (np.ndarray): The input data.\n        k (int): The number of clusters.\n\n    Returns:\n        list: A list of centroids.\n    \"\"\"\n    n = data.shape[0]\n    centroids = data[np.random.choice(range(n), k, replace=False)]\n    for _ in range(10):\n        distances = np.array(data)\n        centroid_k = np.mean(distances, axis=0)\n        for i in range(k):\n            distances[np.argmin(distances, axis=0)] = centroid_k\n            distances[np.argmax(distances, axis=0)] = centroid_k\n        centroids = np.mean(distances, axis=0)\n    return centroids.tolist()"}
{"task_id": 362, "completion_id": 0, "solution": "import numpy as np\ndef row_stochastic_matrix(data: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Convert a 2D list of numbers into a stochastic matrix.\n\n    Args:\n        data (list[list[float]]): A 2D list of numbers.\n\n    Returns:\n        list[list[float]]: A stochastic matrix.\n    \"\"\"\n    try:\n        row_sum = sum(data)\n        if row_sum == 0:\n            return -1\n        result = [float(x) for x in data]\n        result = [round(x, 4) for x in result]\n        return result\n    except:\n        return [-1]"}
{"task_id": 363, "completion_id": 0, "solution": "from typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple\nfrom typing import Any, Tuple"}
{"task_id": 369, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 371, "completion_id": 0, "solution": "import numpy as np\ndef conv1D(X, W, stride, padding):\n    \"\"\"\n    Performs a 1D convolution with a kernel W and stride.\n\n    Args:\n        X (numpy.ndarray): The input data.\n        W (numpy.ndarray): The kernel.\n        stride (int): The stride of the convolution.\n        padding (int): The padding value.\n\n    Returns:\n        numpy.ndarray: The output of the convolution.\n    \"\"\"\n    in_channels = X.shape[1]\n    out_channels = W.shape[1]\n    out_length = (in_channels - padding) // stride + 1\n    out = np.zeros((out_length, out_channels))\n    for i in range(out_length):\n        for j in range(out_channels):\n            out[i, j] = np.sum(X[i:i + stride, j:j + padding])\n    return out"}
{"task_id": 373, "completion_id": 0, "solution": "import numpy as np\ndef gini(y: list[int]) -> float:\n    \"\"\"\n    Compute the Gini impurity of a discrete label sequence.\n\n    Args:\n        y: A one-dimensional list or array of integer labels.\n\n    Returns:\n        The Gini impurity of the sequence.\n    \"\"\"\n    if not y:\n        return 0.0\n    n = len(y)\n    counts = np.bincount(y)\n    return 1 - np.sum(counts) / n"}
{"task_id": 374, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 376, "completion_id": 0, "solution": "import numpy as np\ndef is_symmetric(X):\n    \"\"\"\n    Check whether the given 2-D array is symmetric.\n    \"\"\"\n    if not X:\n        return False\n    rows = len(X)\n    cols = len(X[0])\n    if rows != cols:\n        return False\n    for i in range(rows):\n        for j in range(cols):\n            if X[i][j] != X[j][i]:\n                return False\n    return True"}
{"task_id": 377, "completion_id": 0, "solution": "import numpy as np\ndef logistic_negative_gradient(y, f):\n    \"\"\"\n    Calculates the negative gradient for logistic loss.\n\n    Args:\n        y (list): A list of binary labels (0 or 1).\n        f (list): A list of current predictions (floats).\n\n    Returns:\n        list: A list of negative gradients, rounded to four decimal places.\n    \"\"\"\n    y = np.array(y)\n    f = np.array(f)\n    gradient = []\n    for i in range(len(y)):\n        if y[i] == 0:\n            gradient.append(1e-06)\n        else:\n            gradient.append(1e-06)\n    return gradient"}
{"task_id": 380, "completion_id": 0, "solution": "import numpy as np\ndef selu(x, order):\n    \"\"\"\n    Compute the SELU activation function and its derivatives.\n\n    Args:\n        x (numpy.ndarray): The input array.\n        order (int): The order of the SELU activation function.\n\n    Returns:\n        numpy.ndarray: The output array containing the SELU activation function\n                       and its derivatives.\n    \"\"\"\n    return np.selu(x, order)"}
{"task_id": 387, "completion_id": 0, "solution": ""}
{"task_id": 394, "completion_id": 0, "solution": "import numpy as np\nimport math"}
{"task_id": 398, "completion_id": 0, "solution": "import numpy as np\ndef elu(x, alpha=0.1, derivative=False):\n    \"\"\"\n    Compute the ELU activation function and its derivative.\n\n    Args:\n        x (numpy.ndarray): A 1D NumPy array of numbers.\n        alpha (float): The constant for the ELU activation function.\n        derivative (bool): Whether to compute the derivative.\n\n    Returns:\n        numpy.ndarray: A NumPy array of the same shape as x, rounded to 4 decimal places.\n    \"\"\"\n    if derivative:\n        return np.where(x >= 0, x, 0.1 * alpha + (x - 0.1) * alpha)\n    else:\n        return np.where(x >= 0, x, 0.1 * alpha + (x - 0.1) * alpha)"}
{"task_id": 411, "completion_id": 0, "solution": ""}
{"task_id": 413, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 416, "completion_id": 0, "solution": "import numpy as np\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Calculates the multivariate Gaussian PDF.\n\n    Args:\n        X (np.ndarray): A NumPy array of shape (n_samples, n_features) containing the data points.\n        mean (np.ndarray): A NumPy array of shape (n_features,) representing the mean vector.\n        cov (np.ndarray): A NumPy array of shape (n_features, n_features) representing the covariance matrix.\n\n    Returns:\n        list: A list of floats, where each element is the PDF value rounded to four decimal places.\n    \"\"\"\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    pdf = np.exp(-np.sum((X - mean) ** 2) / (2 * np.sqrt(np.sum(cov * cov)) ** 0.5))\n    result = [round(x, 4) for x in pdf]\n    return result"}
{"task_id": 419, "completion_id": 0, "solution": "import numpy as np\nimport scipy.stats as st"}
{"task_id": 423, "completion_id": 0, "solution": "import numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"\n    Calculates the Root Mean Squared Logarithmic Error (RMSLE) between actual and predicted values.\n\n    Args:\n        actual (list[float]): A list of non-negative real numbers representing the ground truth values.\n        predicted (list[float]): A list of non-negative real numbers representing the model predictions.\n\n    Returns:\n        float: The RMSLE, rounded to four decimal places.\n        Returns -1 if the input lists have different lengths or contain non-negative numbers.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    try:\n        rms_le = np.sqrt(np.mean((actual + predicted) ** 2))\n        return round(rms_le, 4)\n    except:\n        return -1"}
{"task_id": 428, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef hann(window_len: int=1):\n    \"\"\"\n    Generates a Hann window of a specified length.\n\n    Args:\n        window_len (int, optional): The length of the window. Defaults to 1.\n\n    Returns:\n        list: A list of floats representing the Hann window.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    return [round(float(i * 0.5 + 0.5) * 0.5, 4) for i in range(window_len)]"}
{"task_id": 433, "completion_id": 0, "solution": "import numpy as np\ndef glorot_fan(shape):\n    \"\"\"\n    Compute fan-out and receptive for a given shape.\n\n    Args:\n        shape: A tuple or list representing the shape of the tensor.\n\n    Returns:\n        A tuple containing the fan-out and receptive dimensions.\n    \"\"\"\n    if len(shape) == 4:\n        return (shape[0], shape[1])\n    else:\n        return (shape[0], shape[1])"}
{"task_id": 435, "completion_id": 0, "solution": "import numpy as np\ndef gradient_boosting_regressor(X, y):\n    \"\"\"\n    Implements a gradient boosting regressor with linear regression.\n\n    Args:\n        X (list[list[float]]): Training data (features, target).\n        y (list[float]): Training data (target).\n\n    Returns:\n        list[float]: Predicted predictions.\n    \"\"\"\n    if len(X) == 0 or len(y) == 0:\n        return np.array(y)\n    n_estimators = 10\n    learning_rate = 0.1\n    max_iterations = 100\n    y_pred = np.zeros(len(y))\n    for t in range(n_estimators):\n        y_pred[t] = np.mean(y)\n        for i in range(len(X)):\n            y_pred[t] += X[i] * y[i]\n    return y_pred"}
{"task_id": 437, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x, order):\n    \"\"\"\n    Compute the sigmoid function and its derivatives.\n\n    Args:\n        x (float or numpy.ndarray): The input value.\n        order (int): The order of the sigmoid function.\n\n    Returns:\n        float: The sigmoid function value.\n        list: The derivative of the sigmoid function.\n    \"\"\"\n    if order == 0:\n        return x\n    elif order == 1:\n        return x * (1 - x)\n    elif order == 2:\n        return x * (1 - x) * (1 - x)\n    else:\n        return 1"}
{"task_id": 438, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(y):\n    \"\"\"\n    Converts a list of class indices to a one-hot encoded representation.\n\n    Args:\n        y (list[int]): A list of integers representing class indices.\n\n    Returns:\n        list[list[int]]: A list of lists, where each inner list represents a row\n                         in the one-hot encoded representation.\n    \"\"\"\n    if num_classes is None:\n        num_classes = np.max(y) + 1\n    else:\n        num_classes = num_classes\n    categorical_data = np.zeros((len(y), num_classes))\n    for (i, c) in enumerate(y):\n        categorical_data[i, c] = 1\n    return categorical_data.tolist()"}
{"task_id": 439, "completion_id": 0, "solution": "import itertools"}
{"task_id": 440, "completion_id": 0, "solution": "import numpy as np\ndef aggregate_predictions(predictions):\n    \"\"\"\n    Aggregates predictions from a list of estimators into a single probability distribution.\n\n    Args:\n        predictions (list[list[float]]): A list of lists, where each inner list\n            represents an estimator with probabilities.\n\n    Returns:\n        tuple: A tuple containing the aggregated probability distribution and the\n            index of the maximal probability.\n    \"\"\"\n    total_probability = 0.0\n    max_probability = 0.0\n    max_index = -1\n    for (i, estimator) in enumerate(predictions):\n        probability = np.mean(estimator)\n        if probability > max_probability:\n            max_probability = probability\n            max_index = i\n        total_probability += probability\n    return (np.round(total_probability, 4), max_index)"}
{"task_id": 444, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 446, "completion_id": 0, "solution": ""}
{"task_id": 452, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Union"}
{"task_id": 453, "completion_id": 0, "solution": "import numpy as np\ndef update_q_values(Q):\n    \"\"\"\n    Updates the Q-values for a given batch of states.\n\n    Args:\n        Q (np.ndarray): The Q-values for the batch of states.\n\n    Returns:\n        np.ndarray: The updated Q-values.\n    \"\"\"\n    if Q.size == 0:\n        return np.array([1.0])\n    else:\n        return Q.copy()"}
{"task_id": 458, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(labels):\n    \"\"\"\n    Converts a one-dimensional list of integers to a one-dimensional list of one-hot encoded vectors.\n\n    Args:\n        labels: A list of non-negative integers.\n\n    Returns:\n        A list of lists, where each inner list represents a one-hot encoded vector.\n        Returns -1 if the number of labels is less than or equal to the number of classes.\n    \"\"\"\n    if not labels:\n        return []\n    num_labels = len(labels)\n    num_classes = max(1, num_labels)\n    categorical_data = []\n    for label in labels:\n        if label < 0:\n            return -1\n        categorical_data.append(np.zeros(num_classes))\n    return categorical_data"}
{"task_id": 461, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor"}
{"task_id": 471, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 474, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef generate_data(n_samples, n_features):\n    \"\"\"\n    Generates a 2D NumPy array of random data for binary classification.\n\n    Args:\n        n_samples (int): The number of samples to generate.\n        n_features (int): The number of features in the data.\n\n    Returns:\n        numpy.ndarray: A 2D NumPy array of random data.\n    \"\"\"\n    X = np.random.rand(n_samples, n_features)\n    return X"}
{"task_id": 475, "completion_id": 0, "solution": ""}
{"task_id": 479, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 481, "completion_id": 0, "solution": "import numpy as np\ndef dbscan(data, eps, min_samples):\n    \"\"\"\n    Clusters data points using DBSCAN.\n\n    Args:\n        data (np.ndarray): 2D NumPy array of data points.\n        eps (float): Radius of the neighborhood for DBSCAN.\n        min_samples (int): Minimum number of points required to form a core.\n\n    Returns:\n        list: A list of cluster labels for each data point.\n    \"\"\"\n    labels = []\n    for (i, point) in enumerate(data):\n        if len(data) == 0:\n            labels.append(-1)\n            continue\n        if np.linalg.norm(point - data[0]) <= eps:\n            labels.append(i)\n        else:\n            neighbors = []\n            for (j, other_point) in enumerate(data):\n                if i != j and np.linalg.norm(point - other_point) <= eps:\n                    neighbors.append(j)\n            if len(neighbors) < min_samples:\n                labels.append(-1)\n    return labels"}
{"task_id": 482, "completion_id": 0, "solution": "import numpy as np\nimport pandas as pd\ndef to_categorical(y, num_classes=None):\n    \"\"\"\n    Convert integer labels to one-hot encoded format.\n\n    Args:\n        y (list or np.ndarray): A list or NumPy array of integer labels.\n        num_classes (int, optional): The number of classes. If None,\n                                     the number of classes is inferred from\n                                     the length of y. Defaults to None.\n\n    Returns:\n        np.ndarray: A NumPy array of shape (num_classes,) containing\n                    one-hot encoded labels.\n    \"\"\"\n    y = np.array(y)\n    if num_classes is None:\n        num_classes = y.shape[0]\n    elif num_classes < 0:\n        raise ValueError('Number of classes must be non-negative.')\n    elif num_classes > y.shape[0]:\n        raise ValueError('Number of classes cannot exceed the length of the input array.')\n    else:\n        return np.eye(num_classes)[y]"}
{"task_id": 485, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 490, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x: list | tuple) -> list[list]:\n    \"\"\"Convert a one-dimensional vector into a square matrix.\n\n    Args:\n        x: A one-dimensional structure (list or tuple) containing numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix.\n    \"\"\"\n    if not x:\n        return []\n    matrix = np.array(x)\n    return matrix.tolist()"}
{"task_id": 491, "completion_id": 0, "solution": "from collections import Counter"}
{"task_id": 492, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations"}
{"task_id": 493, "completion_id": 0, "solution": "import numpy as np\ndef mse(y):\n    \"\"\"\n    Compute the mean-squared error (MSE) of a sample.\n\n    Args:\n        y (list[float]): A list of float values.\n\n    Returns:\n        float: The mean squared error rounded to four decimal places.\n               Returns -1 if the input list is empty.\n    \"\"\"\n    if not y:\n        return -1\n    return round(np.mean(np.square(y - np.mean(y)), 2), 4)"}
{"task_id": 496, "completion_id": 0, "solution": "import numpy as np\ndef apply_affine(x, slope, intercept):\n    \"\"\"\n    Apply an affine transformation to a vector and return the derivative.\n\n    Args:\n        x (list or numpy.ndarray): A 1-D list or numpy array representing the input vector.\n        slope (float): The slope of the affine transformation.\n        intercept (float): The intercept of the affine transformation.\n\n    Returns:\n        tuple: A tuple containing the activation, derivative, and second derivative.\n    \"\"\"\n    x = np.array(x)\n    (activation, derivative, second_derivative) = (np.zeros(3), 0.0, 0.0)\n    for i in range(len(x)):\n        activation[i] = slope * x[i] + intercept\n        derivative[i] = (slope * x[i] - intercept) / x[i]\n        second_derivative[i] = derivative[i] ** 2 / (2 * x[i])\n    return (activation.tolist(), derivative.tolist(), second_derivative.tolist())"}
{"task_id": 499, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 500, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport pandas as pd"}
{"task_id": 505, "completion_id": 0, "solution": "import numpy as np\ndef contrastive_divergence(X, W, V, learning_rate, k):\n    \"\"\"\n    Computes the contrastive divergence for a single batch of data.\n\n    Args:\n        X (np.ndarray): A NumPy array of shape (batch_size, n_visible) containing\n                         the visible data.\n        W (np.ndarray): A NumPy array of shape (n_hidden, n_visible) representing\n                         the hidden weights.\n        V (np.ndarray): A NumPy array of shape (n_hidden, n_visible) representing\n                         the visible weights.\n        learning_rate (float): The learning rate for the update.\n        k (int): The number of Gibbs steps.\n\n    Returns:\n        np.ndarray: A NumPy array of shape (batch_size, n_hidden) containing the\n                     updated weights.\n    \"\"\"\n    batch_size = X.shape[0]\n    n_visible = X.shape[1]\n    positive_phase = np.zeros(n_visible)\n    for i in range(k):\n        positive_phase[i] = np.sum(X[:batch_size, i] * W[:batch_size, i])\n    negative_phase = np.zeros(n_visible)\n    for i in range(k):\n        negative_phase[i] = np.sum(X[:batch_size, i] * np.linalg.inv(W[:batch_size, i]))\n    W = W + learning_rate * positive_phase * negative_phase\n    return W.tolist()"}
{"task_id": 509, "completion_id": 0, "solution": "import numpy as np\ndef chebyshev(x, y):\n    \"\"\"\n    Compute the Chebyshev distance between two vectors.\n\n    Args:\n        x (list): A list of numbers representing the first vector.\n        y (list): A list of numbers representing the second vector.\n\n    Returns:\n        float: The Chebyshev distance between the two vectors.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    return np.max(np.abs(x - y))"}
{"task_id": 510, "completion_id": 0, "solution": "import numpy as np\nimport tensorflow as tf\ndef vae_loss(y_true, y_pred, epsilon=1e-06):\n    \"\"\"\n    Computes the VAE loss (mini-batch loss) for Bernoulli-like VAEs.\n\n    Args:\n        y_true (np.ndarray): True labels (batch_size, n_features).\n        y_pred (np.ndarray): Predicted labels (batch_size, n_features).\n        epsilon (float): Small value to prevent division by zero.\n\n    Returns:\n        float: The average loss over the batch.\n    \"\"\"\n    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred)) + tf.reduce_mean(tf.keras.losses.kl(y_pred, y_true))"}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X):\n    \"\"\"\n    Calculates the prediction for a factorization machine.\n\n    Args:\n        X (list[list[float]]): A list of lists, where each inner list represents a sample.\n\n    Returns:\n        list[float]: A list of predicted values, rounded to 4 decimal places.\n    \"\"\"\n    n_samples = len(X)\n    n_features = len(X[0])\n    predictions = []\n    for i in range(n_samples):\n        sum_w = 0\n        for j in range(n_features):\n            sum_w += X[i][j]\n        prediction = sum_w / n_features\n        predictions.append(round(prediction, 4))\n    return predictions"}
{"task_id": 517, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of a scalar input.\n\n    Args:\n        z: A scalar (int or float)\n\n    Returns:\n        A float representing the sigmoid of z, rounded to 4 decimal places.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))"}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Callable\ndef unhot(func: Callable[[np.ndarray], np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Converts a one-dimensional NumPy array to a one-dimensional NumPy array.\n    \"\"\"\n    return func(np.array([]))"}
{"task_id": 520, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 528, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 537, "completion_id": 0, "solution": "import numpy as np\nimport scipy.stats as stats"}
{"task_id": 539, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(AL, Y):\n    \"\"\"\n    Computes the binary cross-entropy cost for a neural network.\n\n    Args:\n        AL (np.ndarray): The model's output probabilities (shape: (m,)).\n        Y (np.ndarray): The ground truth labels (shape: (m,)).\n\n    Returns:\n        float: The binary cross-entropy cost.\n    \"\"\"\n    AL = np.clip(AL, 1e-15, 1.0)\n    Y = np.clip(Y, 0.0, 1.0)\n    cost = -np.sum(Y * np.log(AL))\n    return cost\nAL = np.array([0.1, 0.2, 0.7])\nY = np.array([0, 1, 1])\ncost = compute_cost(AL, Y)"}
{"task_id": 552, "completion_id": 0, "solution": "import numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"\n    Compute the posterior probabilities for each state at time t.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'beta', and 'observations'.\n        observations (list[int]): A list of observation indices.\n        t (int): The time step.\n\n    Returns:\n        list[float]: A list of posterior probabilities for each state at time t.\n    \"\"\"\n    pi = hmm['pi']\n    beta = hmm['beta']\n    observations = np.array(observations)\n    if t >= len(observations):\n        return []\n    gamma = np.zeros(len(observations))\n    for i in range(len(observations)):\n        gamma[i] = np.sum(pi * beta * observations[i])\n    gamma = np.round(gamma, 4)\n    return gamma.tolist()"}
{"task_id": 555, "completion_id": 0, "solution": "import numpy as np\nfrom collections import deque\ndef dbscan(data, eps, min_samples):\n    \"\"\"\n    Performs DBSCAN-based clustering on a dataset.\n\n    Args:\n        data (np.ndarray): The input data array.\n        eps (float): The maximum distance for clustering.\n        min_samples (int): The minimum number of points within a neighborhood.\n\n    Returns:\n        list: A list of cluster IDs, corresponding to the cluster labels.\n    \"\"\"\n    n_samples = data.shape[0]\n    labels = np.zeros(n_samples, dtype=int)\n    queue = deque(range(n_samples))\n    while queue:\n        i = queue.popleft()\n        if i < min_samples:\n            neighbors = data[i]\n            for j in range(len(data)):\n                if j != i and np.linalg.norm(data[j] - data[i]) <= eps:\n                    labels[i] = 1\n                    queue.append(j)\n    return labels"}
{"task_id": 561, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef gmm_em(data: np.ndarray, k: int, max_iter: int=200, epsilon: float=0.0001) -> list[int]:\n    \"\"\"\n    Gaussian Mixture Model (GMM) via Expectation-Maximization (EM) algorithm.\n\n    Args:\n        data (np.ndarray): 2D NumPy array of shape (N, D) containing the data.\n        k (int): Number of Gaussian components.\n        max_iter (int, optional): Maximum number of iterations. Defaults to 200.\n        epsilon (float, optional): Convergence threshold. Defaults to 1e-4.\n\n    Returns:\n        list[int]: List of cluster labels for each sample.\n    \"\"\"\n    n = data.shape[0]\n    if k == 1:\n        return np.zeros(n)\n    weights = np.ones(k) / k\n    means = np.zeros((k, 1))\n    transition_matrix = np.zeros((k, k))\n    for _ in range(max_iter):\n        for i in range(k):\n            for j in range(k):\n                weights[i] += data[i, :] * transition_matrix[:, j]\n            means[i] = np.mean(data[:, i])\n        weights = weights / np.sum(weights)\n        transition_matrix = np.dot(transition_matrix, weights)\n    for i in range(k):\n        if np.sum(weights[i]) > epsilon:\n            break\n        for j in range(k):\n            if weights[j] > 0:\n                transition_matrix[:, j] += weights[j]\n            else:\n                transition_matrix[:, j] -= weights[j]\n    labels = np.zeros(n, dtype=int)\n    for i in range(n):\n        max_j = np.argmax(transition_matrix[:, i])\n        labels[i] = max_j\n    return labels"}
{"task_id": 562, "completion_id": 0, "solution": "import numpy as np\ndef spectral_clustering(data, n_clusters):\n    \"\"\"\n    Perform spectral clustering on the given data.\n\n    Args:\n        data (np.ndarray): A NumPy array of shape (N, D) containing N samples with D features.\n        n_clusters (int): The number of clusters to form.\n\n    Returns:\n        list: A list of integers representing the cluster labels.\n    \"\"\"\n    if n_clusters == 1:\n        return [0]\n    else:\n        similarity_matrix = np.zeros((data.shape[0], data.shape[1]))\n        for i in range(data.shape[0]):\n            for j in range(data.shape[1]):\n                similarity_matrix[i, j] = np.sum(data[i, :] * data[j, :])\n        laplacian = np.diag(similarity_matrix) + 1.0\n        laplacian_normalized = laplacian / np.sqrt(laplacian.shape[0] + 1e-08)\n        (eigenvalues, eigenvectors) = np.linalg.eig(laplacian_normalized)\n        k = 10\n        eigenvalues_sorted = np.sort(eigenvalues)[-k:]\n        eigenvectors_sorted = eigenvectors[:, :k]\n        labels = [i for (i, _) in enumerate(eigenvalues_sorted)]\n        return labels"}
{"task_id": 563, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 564, "completion_id": 0, "solution": "def predict_sample(root: Node, features: list, task_type: str) -> str:\n    \"\"\"\n    Predicts the value of a sample given a tree.\n\n    Args:\n        root (Node): The root of the tree.\n        features (list): A list of features.\n        task_type (str): The type of task (classification or regression).\n\n    Returns:\n        str: The predicted value of the sample.\n    \"\"\"\n    if task_type == 'classification':\n        if root.label is None:\n            return root.label\n        return root.label\n    elif task_type == 'regression':\n        if root.value is None:\n            return root.value\n        return root.value\n    else:\n        return root.label"}
{"task_id": 565, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef choose_best_feature(X, y):\n    \"\"\"\n    Return the feature index that yields maximum information gain.\n\n    Args:\n        X (np.ndarray): A 2D numpy array of shape (n_samples, n_features)\n                         containing the feature values.\n        y (np.ndarray): A 1D numpy array of shape (n_samples,) containing\n                        the class labels.\n\n    Returns:\n        tuple: A tuple containing the index of the best feature and the\n               maximum information gain.\n    \"\"\"\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    best_index = 0\n    max_gain = 0\n    for i in range(n_features):\n        gain = 0\n        for j in range(n_samples):\n            if y[j] != j:\n                gain += -1\n            else:\n                gain += 1\n        if gain > max_gain:\n            max_gain = gain\n            best_index = i\n    return (best_index, round(max_gain, 6))"}
{"task_id": 566, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef detect_outliers(data, n_trees, epsilon):\n    \"\"\"\n    Detects outliers in a dataset using Isolation Forest.\n\n    Args:\n        data (np.ndarray): A NumPy array of shape (n_samples, n_features) representing the data.\n        n_trees (int): The number of trees to build in the Isolation Forest.\n        epsilon (float): The threshold for outlier detection.\n\n    Returns:\n        list: A list of indices representing the outliers.\n    \"\"\"\n    n_samples = data.shape[0]\n    isolated_indices = []\n    for _ in range(n_trees):\n        subset = np.random.choice(n_samples, n_trees, replace=False)\n        model = IsolationForest(n_estimators=100, random_state=42)\n        model.fit(subset)\n        isolates = model.decision_function(subset)\n        for i in range(n_samples):\n            if isolates[i] > epsilon:\n                isolated_indices.append(i)\n    return isolated_indices"}
{"task_id": 568, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.stats import norm"}
{"task_id": 569, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple, Iterable, Any"}
{"task_id": 571, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Tuple\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the weight vector and bias term for a linear SVM.\n\n    Args:\n        X: A 2D NumPy array of shape (n_samples, n_features).\n        y: A 1D NumPy array of shape (n_samples,) containing the labels.\n        alpha: A 1D NumPy array of shape (n_features,) containing the Lagrange multipliers.\n\n    Returns:\n        A tuple containing the weight vector (np.ndarray) and the bias term (np.ndarray).\n        The weights and bias are rounded to 4 decimal places.\n    \"\"\"\n    w = np.round(alpha * X.flatten() + y)\n    b = np.round(alpha * y)\n    return (w, b)"}
{"task_id": 572, "completion_id": 0, "solution": "import numpy as np\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Calculates the Local Outlier Factor (LOF) for a given dataset.\n\n    Args:\n        data (np.ndarray): A 2D numpy array of sample data.\n        k (int): The number of nearest neighbors to consider.\n        epsilon (float): The tolerance for outlier detection.\n\n    Returns:\n        tuple: A tuple containing the LOF scores and a list of indices of outliers.\n    \"\"\"\n    distances = np.sqrt(np.sum((data.T - data) ** 2, axis=1))\n    k_distances = distances <= k * epsilon\n    lof = np.sum(k_distances)\n    lof = round(lof, 4)\n    outlier_indices = np.where(k_distances)[0]\n    return (lof, outlier_indices)"}
{"task_id": 574, "completion_id": 0, "solution": "import numpy as np\nimport random"}
{"task_id": 579, "completion_id": 0, "solution": "import numpy as np\nimport random\ndef kmeans(X, k, max_iters=100):\n    \"\"\"\n    K-means clustering algorithm.\n\n    Args:\n        X: A numpy array of shape (n_samples, n_features) containing the data points.\n        k: The number of clusters.\n        max_iters: The maximum number of iterations.\n\n    Returns:\n        A tuple containing the labels and the centers.\n    \"\"\"\n    n_samples = X.shape[0]\n    if k <= 0:\n        raise ValueError('k must be greater than 0')\n    centroids = X[random.sample(range(n_samples), k)]\n    for i in range(max_iters):\n        distances = np.array([np.linalg.norm(X - c, axis=1) for c in centroids])\n        labels = np.argmin(distances, axis=1)\n        centroids = np.mean(X[labels], axis=0)\n    return (labels, centroids)"}
{"task_id": 581, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt"}
{"task_id": 591, "completion_id": 0, "solution": "import numpy as np\ndef neural_net_predict(weights):\n    \"\"\"\n    Predicts the output of a neural network given a set of weights.\n\n    Args:\n        weights (list): A list of lists representing the weights of the neural network.\n\n    Returns:\n        float: The predicted output of the network.\n    \"\"\"\n    weights = np.array(weights)\n    output = np.dot(weights, x)\n    output = np.maximum(output, 0)\n    return float(output)"}
{"task_id": 595, "completion_id": 0, "solution": "import numpy as np\ndef kmeans_centroids(X, k, max_iters=100):\n    \"\"\"\n    Compute K-Means clustering using the Euclidean distance.\n\n    Args:\n        X (np.ndarray): 2D NumPy array of shape (n_samples, n_features).\n        k (int): The number of clusters.\n        max_iters (int): The maximum number of iterations.\n\n    Returns:\n        list[list[float]]: A list of centroids, sorted in ascending order.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    centroids = X.astype(float).mean(axis=0)\n    for _ in range(max_iters):\n        distances = np.sqrt(np.sum((X - centroids) ** 2, axis=1))\n        new_centroids = centroids + distances\n        if np.all(new_centroids == centroids):\n            break\n        centroids = new_centroids\n    return centroids.tolist()"}
