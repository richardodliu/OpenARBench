{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n\n    Args:\n        a: A matrix represented as a list of lists.\n        b: A vector represented as a list.\n\n    Returns:\n        A list representing the resulting vector if the operation is valid,\n        or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if not a or not b:\n        return -1\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = 0\n        for i in range(num_cols):\n            dot_product += row[i] * b[i]\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n\n    Args:\n        a: A list of lists representing the matrix.\n\n    Returns:\n        A list of lists representing the transpose of the matrix.\n    \"\"\"\n    if not a:\n        return []\n    num_rows = len(a)\n    num_cols = len(a[0])\n    transpose = [[0.0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transpose[j][i] = a[i][j]\n    return transpose\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"Reshape a given matrix into a specified shape. if it can not be reshaped return back an empty list \"[]\".\n    For example:\n    reshape_matrix([[1, 2], [3, 4]], (1, 4)) == [[1, 2, 3, 4]]\n    reshape_matrix([[1, 2], [3, 4]], (2, 2)) == [[1, 2], [3, 4]]\n    reshape_matrix([[1, 2], [3, 4]], (4, 1)) == [[1], [2], [3], [4]]\n    reshape_matrix([[1, 2], [3, 4]], (1, 5)) == []\n    \"\"\"\n    try:\n        arr = np.array(a)\n        reshaped_arr = arr.reshape(new_shape)\n        return reshaped_arr.tolist()\n    except ValueError:\n        return []\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix either by row or by column, based on a given mode.\n\n    Args:\n        matrix (list[list[float]]): The input matrix represented as a list of lists of floats.\n        mode (str): The mode of calculation. 'row' for row-wise mean, 'column' for column-wise mean.\n\n    Returns:\n        list[float]: A list of means according to the specified mode.\n                     Returns an empty list if the input matrix is empty.\n                     Returns None if the mode is invalid.\n    \"\"\"\n    if not matrix:\n        return []\n    if mode == 'row':\n        row_means = []\n        for row in matrix:\n            if row:\n                row_means.append(sum(row) / len(row))\n            else:\n                row_means.append(0.0)\n        return row_means\n    elif mode == 'column':\n        num_cols = len(matrix[0])\n        col_means = [0.0] * num_cols\n        for row in matrix:\n            for (i, val) in enumerate(row):\n                col_means[i] += val\n        for i in range(num_cols):\n            col_means[i] /= len(matrix)\n        return col_means\n    else:\n        return None\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"Multiplies a matrix by a scalar and returns the result.\n\n    Args:\n        matrix: The matrix to be multiplied.\n        scalar: The scalar to multiply the matrix by.\n\n    Returns:\n        The resulting matrix after scalar multiplication.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = [scalar * element for element in row]\n        result.append(new_row)\n    return result\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculates the eigenvalues of a 2x2 matrix.\n\n    Args:\n        matrix: A 2x2 matrix represented as a list of lists.\n\n    Returns:\n        A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input must be a 2x2 matrix.')\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    if discriminant >= 0:\n        eigenvalue1 = (trace + np.sqrt(discriminant)) / 2\n        eigenvalue2 = (trace - np.sqrt(discriminant)) / 2\n    else:\n        real_part = trace / 2\n        imaginary_part = np.sqrt(-discriminant) / 2\n        eigenvalue1 = complex(real_part, imaginary_part)\n        eigenvalue2 = complex(real_part, -imaginary_part)\n    eigenvalues = [eigenvalue1, eigenvalue2]\n    eigenvalues.sort(reverse=True)\n    return [float(ev.real) if isinstance(ev, complex) else ev for ev in eigenvalues]\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"Transforms a given matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n\n    Args:\n        A (list[list[int|float]]): The input matrix A.\n        T (list[list[int|float]]): The invertible matrix T.\n        S (list[list[int|float]]): The invertible matrix S.\n\n    Returns:\n        list[list[int|float]]: The transformed matrix T^{-1}AS, rounded to the nearest 4th decimal.\n                               Returns -1 if T or S are not invertible or if matrix dimensions are incompatible.\n    \"\"\"\n    try:\n        A = np.array(A)\n        T = np.array(T)\n        S = np.array(S)\n        if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n            return -1\n        T_inv = np.linalg.inv(T)\n        if A.shape[1] != S.shape[0]:\n            return -1\n        transformed_matrix = np.round(np.dot(np.dot(T_inv, A), S), 4)\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        return -1\n    except Exception as e:\n        return -1\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Args:\n        matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n        list[list[float]]: The inverse of the matrix, or None if the matrix is not invertible.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        return None\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse_determinant = 1 / determinant\n    inverse_matrix = [[d * inverse_determinant, -b * inverse_determinant], [-c * inverse_determinant, a * inverse_determinant]]\n    return inverse_matrix\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices. If the matrices cannot be multiplied, return -1.\n    For example:\n    matrixmul([[1, 2], [3, 4]], [[5, 6], [7, 8]]) == [[19, 22], [43, 50]]\n    matrixmul([[1, 2]], [[3], [4]]) == [[11]]\n    matrixmul([[1, 2], [3, 4]], [[5, 6, 7], [8, 9, 10]]) == [[21, 24, 27], [47, 54, 61]]\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    c = [[0 for row in range(len(b[0]))] for col in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                c[i][j] += a[i][k] * b[k][j]\n    return c\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n        vectors: A list of lists, where each inner list represents a feature\n                 with its observations.\n\n    Returns:\n        A covariance matrix as a list of lists.\n    \"\"\"\n    if not vectors:\n        return []\n    num_vectors = len(vectors)\n    num_features = len(vectors[0])\n    means = [sum(feature) / num_vectors for feature in zip(*vectors)]\n    covariance_matrix = [[0.0] * num_features for _ in range(num_features)]\n    for i in range(num_features):\n        for j in range(num_features):\n            covariance = sum(((vectors[k][i] - means[i]) * (vectors[k][j] - means[j]) for k in range(num_vectors))) / num_vectors\n            covariance_matrix[i][j] = covariance\n    return covariance_matrix\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi method.\n\n    Args:\n        A (np.ndarray): The coefficient matrix.\n        b (np.ndarray): The constant vector.\n        n (int): The number of iterations.\n\n    Returns:\n        list: The approximate solution x, rounded to four decimal places.\n    \"\"\"\n    x = np.zeros_like(b, dtype=np.float64)\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    for _ in range(n):\n        x = (b - np.dot(R, x)) / D\n        x = np.round(x, 4)\n    return x.tolist()\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method.\n\n    Args:\n        A (np.ndarray): A 2x2 NumPy array representing the matrix to decompose.\n\n    Returns:\n        tuple: A tuple containing:\n            - U (np.ndarray): A 2x2 orthogonal matrix representing the left singular vectors.\n            - S (np.ndarray): A 1D NumPy array containing the singular values in descending order.\n            - V (np.ndarray): A 2x2 orthogonal matrix representing the right singular vectors.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2')\n    AtA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(AtA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    singular_values = np.round(singular_values, 4)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        sigma = singular_values[i]\n        if sigma != 0:\n            U[:, i] = A @ V[:, i] / sigma\n        else:\n            U[:, i] = np.array([1, 0]) if i == 0 else np.array([0, 1])\n    U = np.round(U, 4)\n    V = np.round(V, 4)\n    return (U, singular_values, V)\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n        matrix: A 4x4 matrix represented as a list of lists.\n\n    Returns:\n        The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(matrix_3x3: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n\n        Args:\n            matrix_3x3: A 3x3 matrix represented as a list of lists.\n\n        Returns:\n            The determinant of the matrix.\n        \"\"\"\n        return matrix_3x3[0][0] * (matrix_3x3[1][1] * matrix_3x3[2][2] - matrix_3x3[1][2] * matrix_3x3[2][1]) - matrix_3x3[0][1] * (matrix_3x3[1][0] * matrix_3x3[2][2] - matrix_3x3[1][2] * matrix_3x3[2][0]) + matrix_3x3[0][2] * (matrix_3x3[1][0] * matrix_3x3[2][1] - matrix_3x3[1][1] * matrix_3x3[2][0])\n    determinant = 0\n    for i in range(4):\n        minor_matrix = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        determinant += (-1) ** i * matrix[0][i] * determinant_3x3(minor_matrix)\n    return determinant\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation.\n\n    Args:\n        X (list[list[float]]): The feature matrix.\n        y (list[float]): The target vector.\n\n    Returns:\n        list[float]: The coefficients of the linear regression model.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n    coefficients = np.round(coefficients, 4)\n    return coefficients.tolist()\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n\n    Args:\n        X (np.ndarray): Features with a column of ones for the intercept.\n        y (np.ndarray): Target values.\n        alpha (float): Learning rate.\n        iterations (int): Number of iterations.\n\n    Returns:\n        np.ndarray: Coefficients of the linear regression model.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X @ theta\n        error = predictions - y\n        gradient = X.T @ error / m\n        theta = theta - alpha * gradient\n    return np.round(theta.tolist(), 4)\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample\n                           and each column represents a feature.\n\n    Returns:\n        tuple: A tuple containing two 2D lists:\n               - The first list is the data scaled by standardization.\n               - The second list is the data scaled by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data = np.round(standardized_data, 4)\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    normalized_data = np.round(normalized_data, 4)\n    return (standardized_data.tolist(), normalized_data.tolist())\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Implements the k-Means clustering algorithm.\n\n    Args:\n        points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points).\n        k: An integer representing the number of clusters to form.\n        initial_centroids: A list of initial centroid points, each a tuple of coordinates.\n        max_iterations: An integer representing the maximum number of iterations to perform.\n\n    Returns:\n        A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    centroids = np.array(initial_centroids, dtype=float)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((np.array(points) - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([np.mean(np.array(points)[labels == i], axis=0) for i in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(c, 4)) for c in centroids]\n    return final_centroids\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n\n    Args:\n        X (np.ndarray): The feature matrix.\n        y (np.ndarray): The target vector.\n        k (int): The number of folds. Defaults to 5.\n        shuffle (bool): Whether to shuffle the data before splitting. Defaults to True.\n        random_seed (int): The random seed for shuffling. Defaults to None.\n\n    Returns:\n        list: A list of tuples, where each tuple contains the train and test indices for each fold.\n    \"\"\"\n    n = X.shape[0]\n    indices = np.arange(n)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    fold_size = n // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i < k - 1 else n\n        test_indices = indices[start:end]\n        train_indices = np.array([idx for idx in indices if idx not in test_indices])\n        folds.append((train_indices, test_indices))\n    return folds\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) from scratch.\n\n    Args:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample\n                           and each column represents a feature.\n        k (int): The number of principal components to return.\n\n    Returns:\n        list[list[float]]: A list of lists representing the principal components (eigenvectors).\n                           Each inner list represents an eigenvector.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    eigenvalue_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[eigenvalue_indices]\n    eigenvectors = eigenvectors[:, eigenvalue_indices]\n    principal_components = eigenvectors[:, :k].tolist()\n    rounded_principal_components = []\n    for component in principal_components:\n        rounded_component = [round(x, 4) for x in component]\n        rounded_principal_components.append(rounded_component)\n    return rounded_principal_components\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Implements the decision tree learning algorithm for classification.\n\n    Args:\n        examples: A list of examples (each example is a dict of attribute-value pairs).\n        attributes: A list of attribute names.\n        target_attr: The name of the target attribute.\n\n    Returns:\n        A nested dictionary representing the decision tree.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n    if not attributes:\n        return Counter(target_values).most_common(1)[0][0]\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    for value in get_possible_values(examples, best_attr):\n        sub_examples = [example for example in examples if example[best_attr] == value]\n        remaining_attributes = attributes[:]\n        remaining_attributes.remove(best_attr)\n        subtree = learn_decision_tree(sub_examples, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"\n    Chooses the best attribute to split on based on information gain.\n\n    Args:\n        examples: A list of examples.\n        attributes: A list of attribute names.\n        target_attr: The name of the target attribute.\n\n    Returns:\n        The name of the best attribute to split on.\n    \"\"\"\n    best_attr = None\n    max_gain = -1\n    for attr in attributes:\n        gain = information_gain(examples, attr, target_attr)\n        if gain > max_gain:\n            max_gain = gain\n            best_attr = attr\n    return best_attr\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculates the information gain of an attribute.\n\n    Args:\n        examples: A list of examples.\n        attr: The name of the attribute.\n        target_attr: The name of the target attribute.\n\n    Returns:\n        The information gain of the attribute.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    entropy_target = entropy(target_values)\n    weighted_entropy = 0\n    for value in get_possible_values(examples, attr):\n        sub_examples = [example for example in examples if example[attr] == value]\n        target_values_sub = [example[target_attr] for example in sub_examples]\n        weighted_entropy += len(sub_examples) / len(examples) * entropy(target_values_sub)\n    return entropy_target - weighted_entropy\ndef entropy(values: list) -> float:\n    \"\"\"\n    Calculates the entropy of a list of values.\n\n    Args:\n        values: A list of values.\n\n    Returns:\n        The entropy of the list of values.\n    \"\"\"\n    counts = Counter(values)\n    probabilities = [count / len(values) for count in counts.values()]\n    return -sum([p * math.log2(p) for p in probabilities])\ndef get_possible_values(examples: list[dict], attr: str) -> list:\n    \"\"\"\n    Gets the possible values of an attribute.\n\n    Args:\n        examples: A list of examples.\n        attr: The name of the attribute.\n\n    Returns:\n        A list of possible values for the attribute.\n    \"\"\"\n    return list(set([example[attr] for example in examples]))\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n\n    Args:\n        data (np.ndarray): The dataset (2D NumPy array).\n        labels (np.ndarray): The label vector (1D NumPy array).\n        kernel (str): The choice of kernel ('linear' or 'rbf').\n        lambda_val (float): The regularization parameter.\n        iterations (int): The number of iterations.\n        sigma (float): The bandwidth parameter for the RBF kernel.\n\n    Returns:\n        tuple: A tuple containing the alpha coefficients and bias.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    for _ in range(iterations):\n        for i in range(n_samples):\n            if kernel == 'linear':\n                K = np.dot(data, data[i].T)\n            elif kernel == 'rbf':\n                K = np.array([[np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2)) for j in range(n_samples)] for i in range(n_samples)])\n                K = K[i]\n            else:\n                raise ValueError(\"Invalid kernel. Choose 'linear' or 'rbf'.\")\n            prediction = np.dot(alpha, labels * K) + b\n            if labels[i] * prediction < 1:\n                alpha[i] += 1 / (lambda_val * n_samples)\n            else:\n                alpha[i] -= 1 / (lambda_val * n_samples)\n            alpha[alpha < 0] = 0\n            b += (labels[i] - prediction) / n_samples\n    alpha = np.round(alpha, 4)\n    b = np.round(b, 4)\n    return (alpha.tolist(), b)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    output = 1 / (1 + math.exp(-z))\n    return round(output, 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n        scores (list[float]): A list of scores.\n\n    Returns:\n        list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    e_scores = [math.exp(score) for score in scores]\n    sum_e_scores = sum(e_scores)\n    softmax_values = [round(e_score / sum_e_scores, 4) for e_score in e_scores]\n    return softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n        features (list[list[float]]): A list of feature vectors.\n        labels (list[int]): A list of true binary labels (0 or 1).\n        weights (list[float]): A list of weights, one for each feature.\n        bias (float): The bias of the neuron.\n\n    Returns:\n        tuple[list[float], float]: A tuple containing the predicted probabilities and the mean squared error.\n    \"\"\"\n    predicted_probabilities = []\n    for feature_vector in features:\n        weighted_sum = sum([feature * weight for (feature, weight) in zip(feature_vector, weights)]) + bias\n        activation = 1 / (1 + math.exp(-weighted_sum))\n        predicted_probabilities.append(activation)\n    squared_errors = [(prob - label) ** 2 for (prob, label) in zip(predicted_probabilities, labels)]\n    mean_squared_error = sum(squared_errors) / len(labels)\n    predicted_probabilities = [round(prob, 4) for prob in predicted_probabilities]\n    mean_squared_error = round(mean_squared_error, 4)\n    return (predicted_probabilities, mean_squared_error)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation, and implements backpropagation\n    to update the neuron's weights and bias.\n\n    Args:\n        features (np.ndarray): A 2D numpy array where each row represents a feature vector.\n        labels (np.ndarray): A 1D numpy array of true binary labels (0 or 1).\n        initial_weights (np.ndarray): A 1D numpy array of initial weights.\n        initial_bias (float): The initial bias.\n        learning_rate (float): The learning rate for gradient descent.\n        epochs (int): The number of training epochs.\n\n    Returns:\n        tuple: A tuple containing:\n            - The updated weights (np.ndarray).\n            - The updated bias (float).\n            - A list of MSE values for each epoch (list[float]).\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-z))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        dz = predictions - labels\n        dweights = np.dot(features.T, dz) / len(features)\n        dbias = np.mean(dz)\n        weights -= learning_rate * dweights\n        bias -= learning_rate * dbias\n    return (weights.tolist(), round(bias, 4), mse_values)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n\n        def _backward():\n            self.grad += (1 if self.data > 0 else 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self, grad=1.0):\n        self.grad += grad\n        for child in self._prev:\n            child._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to C.\n\n    Args:\n        B (list[list[int]]): A list of lists representing the basis vectors of basis B.\n        C (list[list[int]]): A list of lists representing the basis vectors of basis C.\n\n    Returns:\n        list[list[float]]: The transformation matrix P from basis B to C, rounded to the nearest 4th decimal.\n    \"\"\"\n    B = np.array(B)\n    C = np.array(C)\n    B_inv = np.linalg.inv(B)\n    P = np.dot(C, B_inv)\n    P = np.round(P, 4)\n    return P.tolist()\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) of a 2x2 matrix.\n\n    Args:\n        A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n        tuple: A tuple containing the matrices U, S, and V such that A = U * S * V.\n               All results are rounded to the nearest 4th decimal.\n    \"\"\"\n    ATA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A, dtype=float)\n    for i in range(len(singular_values)):\n        if singular_values[i] > 1e-10:\n            U[:, i] = A @ V[:, i] / singular_values[i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in two numpy arrays, X and y, while maintaining the\n    corresponding order between them.\n\n    Args:\n        X (numpy.ndarray): The input features array.\n        y (numpy.ndarray): The target labels array.\n        seed (int, optional): The seed for the random number generator.\n                               Defaults to None.\n\n    Returns:\n        list: A list containing the shuffled X and y arrays.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from a numpy array X and an optional numpy array y.\n\n    Args:\n        X (numpy.ndarray): The input data array.\n        y (numpy.ndarray, optional): The target data array. Defaults to None.\n        batch_size (int, optional): The batch size. Defaults to 64.\n\n    Yields:\n        list: A batch of data. If y is provided, the batch is a tuple (X_batch, y_batch).\n              Otherwise, the batch is a list containing only X_batch.\n    \"\"\"\n    n = X.shape[0]\n    for i in range(0, n, batch_size):\n        X_batch = X[i:min(i + batch_size, n)]\n        if y is not None:\n            y_batch = y[i:min(i + batch_size, n)]\n            yield [X_batch.tolist(), y_batch.tolist()]\n        else:\n            yield [X_batch.tolist()]\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Args:\n        X (numpy.ndarray): The dataset.\n        feature_i (int): The index of the feature to split on.\n        threshold (float): The threshold value.\n\n    Returns:\n        tuple: A tuple containing two lists. The first list contains samples where the feature value is greater than or equal to the threshold,\n               and the second list contains samples where the feature value is less than the threshold.\n    \"\"\"\n    X_left = []\n    X_right = []\n    for i in range(X.shape[0]):\n        if X[i, feature_i] >= threshold:\n            X_left.append(X[i].tolist())\n        else:\n            X_right.append(X[i].tolist())\n    return (X_left, X_right)\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates polynomial features for a given dataset.\n\n    Args:\n        X (numpy.ndarray): The input dataset (2D array).\n        degree (int): The maximum degree of the polynomial features.\n\n    Returns:\n        list: A new 2D numpy array with polynomial features up to the specified degree, converted to a list.\n    \"\"\"\n    n_features = X.shape[1]\n    result = []\n    for d in range(1, degree + 1):\n        for indices in combinations_with_replacement(range(n_features), d):\n            term = np.ones(X.shape[0])\n            for i in indices:\n                term *= X[:, i]\n            result.append(term)\n    return np.array(result).T.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of a given dataset.\n\n    Args:\n        X (numpy.ndarray): The 2D numpy array representing the features.\n        y (numpy.ndarray): The 1D numpy array representing the labels.\n        n_subsets (int): The number of subsets to generate.\n        replacements (bool, optional): Whether to create subsets with or without replacements.\n                                       Defaults to True.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n    Returns:\n        list: A list of n_subsets random subsets of the dataset, where each subset is a\n              tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [list(x) for x in subsets]\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Performs one-hot encoding of nominal values.\n\n    Args:\n        x (numpy.ndarray): A 1D numpy array of integer values.\n        n_col (int, optional): The number of columns for the one-hot encoded array.\n                                If not provided, it is automatically determined from the input array.\n\n    Returns:\n        list: A list representing the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    result = np.zeros((len(x), n_col))\n    result[np.arange(len(x)), x] = 1\n    return result.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n  Converts a 1D numpy array into a diagonal matrix.\n\n  Args:\n    x: A 1D numpy array.\n\n  Returns:\n    A 2D numpy array representing the diagonal matrix, returned as a python list.\n  \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Args:\n        y_true (numpy.ndarray): 1D numpy array containing the true labels.\n        y_pred (numpy.ndarray): 1D numpy array containing the predicted labels.\n\n    Returns:\n        float: The accuracy score, rounded to the nearest 4th decimal.\n    \"\"\"\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Args:\n        X (numpy.ndarray): The input dataset.\n        Y (numpy.ndarray, optional): The second dataset. If None, the correlation matrix of X with itself is calculated. Defaults to None.\n\n    Returns:\n        list: The correlation matrix as a 2D list, rounded to the nearest 4th decimal.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X = np.array(X)\n    Y = np.array(Y)\n    mean_X = np.mean(X, axis=0)\n    mean_Y = np.mean(Y, axis=0)\n    X_centered = X - mean_X\n    Y_centered = Y - mean_Y\n    covariance_matrix = np.dot(X_centered.T, Y_centered) / (X.shape[0] - 1)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    correlation_matrix = covariance_matrix / np.outer(std_X, std_Y)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the fit method for an AdaBoost classifier.\n\n    Args:\n        X (numpy.ndarray): The training data of shape (n_samples, n_features).\n        y (numpy.ndarray): The labels of shape (n_samples,).\n        n_clf (int): The number of classifiers to use.\n\n    Returns:\n        list: A list of classifiers, where each classifier is a dictionary\n              containing 'feature_index', 'threshold', and 'alpha'.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_feature_index = None\n        best_threshold = None\n        min_error = float('inf')\n        for feature_index in range(n_features):\n            thresholds = np.unique(X[:, feature_index])\n            for threshold in thresholds:\n                predictions = (X[:, feature_index] >= threshold).astype(int) * 2 - 1\n                error = np.sum(weights * (predictions != y))\n                if error < min_error:\n                    min_error = error\n                    best_feature_index = feature_index\n                    best_threshold = threshold\n        alpha = 0.5 * math.log((1 - min_error) / max(min_error, 1e-10))\n        classifiers.append({'feature_index': best_feature_index, 'threshold': round(best_threshold, 4), 'alpha': round(alpha, 4)})\n        weights *= np.exp(-alpha * y * ((X[:, best_feature_index] >= best_threshold).astype(int) * 2 - 1))\n        weights /= np.sum(weights)\n    return classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a list of scores.\n\n    Args:\n        scores (list): A 1D numpy array of scores.\n\n    Returns:\n        list: The log-softmax of the scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    log_sum_exp = np.log(np.sum(np.exp(scores - max_score)))\n    log_softmax_scores = scores - max_score - log_sum_exp\n    return np.round(log_softmax_scores.tolist(), 4)\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: Invalid weight initialization method.", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, weight_init='uniform'):\n        if weight_init == 'uniform':\n            limit = 1 / math.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n            self.w0 = np.zeros((1, self.n_units))\n        else:\n            raise ValueError('Invalid weight initialization method.')\n        self.W_opt = Optimizer(self.W)\n        self.w0_opt = Optimizer(self.w0)\n\n    def parameters(self):\n        return self.input_shape[0] * self.n_units + self.n_units\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        self.output = np.dot(X, self.W) + self.w0\n        return self.output\n\n    def backward_pass(self, accum_grad):\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W_opt.update(grad_W)\n            self.w0_opt.update(grad_w0)\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\nclass Optimizer:\n\n    def __init__(self, params, learning_rate=0.01):\n        self.params = params\n        self.learning_rate = learning_rate\n\n    def update(self, grad):\n        self.params -= self.learning_rate * grad\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Performs a 2D convolution on an input matrix.\n\n    Args:\n        input_matrix (np.ndarray): The input matrix.\n        kernel (np.ndarray): The convolutional kernel.\n        padding (int): The amount of padding to add to the input matrix.\n        stride (int): The stride of the convolution.\n\n    Returns:\n        list: The output matrix after convolution, reshaped into a list.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output_matrix[i, j] = np.sum(region * kernel)\n    return np.round(output_matrix, 4).tolist()\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n  Implements the Rectified Linear Unit (ReLU) activation function.\n\n  Args:\n    z: A float representing the input value.\n\n  Returns:\n    A float representing the ReLU of the input value.\n  \"\"\"\n    if z > 0:\n        return z\n    else:\n        return 0.0\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculates the Ridge Regression loss.\n\n    Args:\n        X (np.ndarray): Feature matrix.\n        w (np.ndarray): Coefficients.\n        y_true (np.ndarray): True labels.\n        alpha (float): Regularization parameter.\n\n    Returns:\n        float: Ridge loss.\n    \"\"\"\n    n = len(y_true)\n    y_pred = X @ w\n    mse = np.mean((y_pred - y_true) ** 2)\n    regularization_term = alpha / 2 * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization_term\n    return round(ridge_loss_value, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n        z (float): The input value.\n        alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        float|int: The value after applying the Leaky ReLU function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return alpha * z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n  Computes the linear kernel between two input vectors x1 and x2.\n\n  Args:\n    x1: A numpy array representing the first vector.\n    x2: A numpy array representing the second vector.\n\n  Returns:\n    The linear kernel value (dot product) between x1 and x2.\n  \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculates the precision metric given true and predicted binary labels.\n\n    Args:\n        y_true (numpy.ndarray): Array of true binary labels (0 or 1).\n        y_pred (numpy.ndarray): Array of predicted binary labels (0 or 1).\n\n    Returns:\n        float: The precision score.  Returns 0.0 if there are no true positives.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    else:\n        return true_positives / (true_positives + false_positives)\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent to minimize the Mean Squared Error (MSE) loss function.\n\n    Args:\n        X (numpy.ndarray): Input features.\n        y (numpy.ndarray): Target values.\n        weights (numpy.ndarray): Initial weights.\n        learning_rate (float): Learning rate.\n        n_iterations (int): Number of iterations.\n        batch_size (int): Size of the mini-batch (default is 1 for SGD).\n        method (str): Gradient descent method ('batch', 'sgd', or 'mini_batch').\n\n    Returns:\n        list: Updated weights after gradient descent, rounded to the nearest 4th decimal.\n    \"\"\"\n    n_samples = X.shape[0]\n    if method == 'batch':\n        for _ in range(n_iterations):\n            y_predicted = np.dot(X, weights)\n            error = y_predicted - y\n            gradient = np.dot(X.T, error) / n_samples\n            weights = weights - learning_rate * gradient\n    elif method == 'sgd':\n        for _ in range(n_iterations):\n            for i in range(n_samples):\n                y_predicted = np.dot(X[i], weights)\n                error = y_predicted - y[i]\n                gradient = X[i] * error\n                weights = weights - learning_rate * gradient\n    elif method == 'mini_batch':\n        for _ in range(n_iterations):\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n            y_predicted = np.dot(X_batch, weights)\n            error = y_predicted - y_batch\n            gradient = np.dot(X_batch.T, error) / batch_size\n            weights = weights - learning_rate * gradient\n    else:\n        raise ValueError(\"Invalid method. Choose 'batch', 'sgd', or 'mini_batch'.\")\n    return np.round(weights.tolist(), 4)\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n        matrix (list of lists): The input matrix.\n\n    Returns:\n        list of lists: The RREF of the input matrix.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    return A.tolist()\n        A[[i, r]] = A[[r, i]]\n        lv = A[r, lead]\n        A[r] = A[r] / lv\n        for i in range(rows):\n            if i != r:\n                lv = A[i, lead]\n                A[i] = A[i] - lv * A[r]\n        lead += 1\n    return A.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Implements the Adam (Adaptive Moment Estimation) optimization algorithm.\n\n    Args:\n        f: The objective function to be optimized.\n        grad: A function that computes the gradient of f.\n        x0: Initial parameter values.\n        learning_rate: The step size (default: 0.001).\n        beta1: Exponential decay rate for the first moment estimates (default: 0.9).\n        beta2: Exponential decay rate for the second moment estimates (default: 0.999).\n        epsilon: A small constant for numerical stability (default: 1e-8).\n        num_iterations: Number of iterations to run the optimizer (default: 1000).\n\n    Returns:\n        The optimized parameters.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for i in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x.tolist(), 4)\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent.\n\n    Args:\n        X (np.array): The input features.\n        y (np.array): The target values.\n        alpha (float): The regularization parameter.\n        learning_rate (float): The learning rate for gradient descent.\n        max_iter (int): The maximum number of iterations.\n        tol (float): The tolerance for convergence.\n\n    Returns:\n        tuple: A tuple containing the learned weights and bias.\n    \"\"\"\n    (n, p) = X.shape\n    w = np.zeros(p)\n    b = 0\n    cost_history = []\n    for i in range(max_iter):\n        y_predicted = np.dot(X, w) + b\n        error = y_predicted - y\n        dw = 1 / n * np.dot(X.T, error) + alpha / n * np.sign(w)\n        db = 1 / n * np.sum(error)\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        cost = 1 / (2 * n) * np.sum(error ** 2) + alpha / n * np.sum(np.abs(w))\n        cost_history.append(cost)\n        if i > 0 and abs(cost_history[-1] - cost_history[-2]) < tol:\n            break\n    return (np.round(w.tolist(), 4), np.round(b, 4))\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n\n    Args:\n        source (str): The first string.\n        target (str): The second string.\n\n    Returns:\n        int: The OSA distance between the two strings.\n    \"\"\"\n    n = len(source)\n    m = len(target)\n    d = np.zeros((n + 1, m + 1), dtype=int)\n    for i in range(n + 1):\n        d[i, 0] = i\n    for j in range(m + 1):\n        d[0, j] = j\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            d[i, j] = min(d[i - 1, j] + 1, d[i, j - 1] + 1, d[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                d[i, j] = min(d[i, j], d[i - 2, j - 2] + 1)\n    return d[n, m]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculates the recall metric in binary classification.\n\n    Args:\n        y_true (list): A list of true binary labels (0 or 1).\n        y_pred (list): A list of predicted binary labels (0 or 1).\n\n    Returns:\n        float: The recall value rounded to three decimal places.\n               Returns 0.0 if the denominator (TP + FN) is zero.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        return round(tp / (tp + fn), 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism.\n\n    Args:\n        X (numpy.ndarray): Input sequence of shape (seq_len, embedding_dim).\n        W_q (numpy.ndarray): Weight matrix for query projections of shape (embedding_dim, d_k).\n        W_k (numpy.ndarray): Weight matrix for key projections of shape (embedding_dim, d_k).\n        W_v (numpy.ndarray): Weight matrix for value projections of shape (embedding_dim, d_v).\n\n    Returns:\n        list: Self-attention output as a list, reshaped from a numpy array.\n    \"\"\"\n    (seq_len, embedding_dim) = X.shape\n    d_k = W_q.shape[1]\n    d_v = W_v.shape[1]\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T)\n    attention_scores = attention_scores / np.sqrt(d_k)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    output = np.round(output, 4)\n    return output.tolist()\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple Recurrent Neural Network (RNN) cell.\n\n    Args:\n        input_sequence (list[list[float]]): A sequence of input vectors.\n        initial_hidden_state (list[float]): The initial hidden state.\n        Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n        Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n        b (list[float]): The bias vector.\n\n    Returns:\n        list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    hidden_state = np.array(initial_hidden_state)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector)\n        hidden_state = np.tanh(np.dot(input_vector, Wx) + np.dot(hidden_state, Wh) + b)\n    return np.round(hidden_state.tolist(), 4)\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n        points (list): A list of [x, y] coordinates.\n        tx (float): The translation distance in the x direction.\n        ty (float): The translation distance in the y direction.\n\n    Returns:\n        list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_with_ones = np.concatenate((points_array, np.ones((points_array.shape[0], 1))), axis=1)\n    translated_points_array = np.dot(points_with_ones, translation_matrix.T)\n    translated_points = translated_points_array[:, :2]\n    return translated_points.tolist()\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculates the KL divergence between two normal distributions.\n\n    Args:\n        mu_p (float): Mean of the first normal distribution.\n        sigma_p (float): Standard deviation of the first normal distribution.\n        mu_q (float): Mean of the second normal distribution.\n        sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n        float: The KL divergence between the two normal distributions.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        return np.inf\n    kl = 0.5 * (np.log(sigma_q ** 2 / sigma_p ** 2) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / sigma_q ** 2 - 1)\n    return kl\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a system of linear equations (Ax = b) using the Gauss-Seidel method.\n\n    Args:\n        A (numpy.ndarray): A square matrix of coefficients.\n        b (numpy.ndarray): The right-hand side vector.\n        n (int): The number of iterations.\n        x_ini (numpy.ndarray, optional): An initial guess for x. Defaults to None (zeros).\n\n    Returns:\n        list: The approximated solution vector x after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(len(x)):\n            sigma = 0\n            for j in range(len(x)):\n                if j != i:\n                    sigma += A[i, j] * x[j]\n            x[i] = (b[i] - sigma) / A[i, i]\n    return np.round(x.tolist(), 4)\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Performs Gaussian Elimination with partial pivoting to solve the system (Ax = b).\n\n    Args:\n        A (numpy.ndarray): The coefficient matrix.\n        b (numpy.ndarray): The constant vector.\n\n    Returns:\n        numpy.ndarray: The solution vector (x).\n    \"\"\"\n    n = len(b)\n    A = A.astype(float)\n    b = b.astype(float)\n    Ab = np.concatenate((A, b.reshape(n, 1)), axis=1)\n    for i in range(n):\n        pivot_row = i\n        for j in range(i + 1, n):\n            if abs(Ab[j][i]) > abs(Ab[pivot_row][i]):\n                pivot_row = j\n        if pivot_row != i:\n            Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j][i] / Ab[i][i]\n            Ab[j] = Ab[j] - factor * Ab[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = Ab[i][n]\n        for j in range(i + 1, n):\n            x[i] = x[i] - Ab[i][j] * x[j]\n        x[i] = x[i] / Ab[i][i]\n    return np.round(x.tolist(), 4)\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: shapes (1,2) and (4,1) not aligned: 2 (dim 1) != 4 (dim 0)", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = x.shape[1]\n        hidden_states = []\n        cell_states = []\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(T):\n            combined = np.concatenate((x[:, t:t + 1], hidden_state), axis=0)\n            ft = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            ct_tilde = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            cell_state = ft * cell_state + it * ct_tilde\n            ot = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            hidden_state = ot * np.tanh(cell_state)\n            hidden_states.append(hidden_state)\n            cell_states.append(cell_state)\n        hidden_states_list = [np.round(h.tolist(), 4) for h in hidden_states]\n        final_hidden_state = np.round(hidden_state.tolist(), 4)\n        final_cell_state = np.round(cell_state.tolist(), 4)\n        return (hidden_states_list, final_hidden_state, final_cell_state)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n        corpus: A list of documents, where each document is a list of words.\n        query: A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n        A list of lists containing the TF-IDF scores for the query words in each document,\n        rounded to five decimal places.\n    \"\"\"\n    if not corpus:\n        return []\n    tf = []\n    for document in corpus:\n        doc_tf = {}\n        for word in document:\n            doc_tf[word] = doc_tf.get(word, 0) + 1\n        total_words = len(document)\n        doc_tf_normalized = {word: count / total_words for (word, count) in doc_tf.items()}\n        tf.append(doc_tf_normalized)\n    idf = {}\n    num_documents = len(corpus)\n    for word in set([word for doc in corpus for word in doc] + query):\n        df = sum((1 for document in corpus if word in document))\n        idf[word] = np.log(num_documents / (df + 1))\n    tf_idf_scores = []\n    for doc_tf in tf:\n        doc_scores = []\n        for word in query:\n            tf_score = doc_tf.get(word, 0)\n            idf_score = idf.get(word, 0)\n            doc_scores.append(round(tf_score * idf_score, 5))\n        tf_idf_scores.append(doc_scores)\n    return np.array(tf_idf_scores).tolist()\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fp == 0:\n        precision = 0.0\n    else:\n        precision = tp / (tp + fp)\n    if tp + fn == 0:\n        recall = 0.0\n    else:\n        recall = tp / (tp + fn)\n    if precision + recall == 0:\n        f_score = 0.0\n    else:\n        f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: 'SimpleRNN' object has no attribute 'output_size'", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = x.shape[1]\n        hiddens = np.zeros((self.hidden_size, T))\n        outputs = np.zeros((self.output_size, T))\n        last_inputs = []\n        last_hiddens = []\n        h = np.zeros((self.hidden_size, 1))\n        for t in range(T):\n            last_inputs.append(x[:, t:t + 1])\n            h = np.tanh(np.dot(self.W_xh, x[:, t:t + 1]) + np.dot(self.W_hh, h) + self.b_h)\n            last_hiddens.append(h)\n            outputs[:, t:t + 1] = np.dot(self.W_hy, h) + self.b_y\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, x, y, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = x.shape[1]\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        loss = 0\n        dh_next = np.zeros((self.hidden_size, 1))\n        for t in reversed(range(T)):\n            dy = outputs[:, t:t + 1] - y[:, t:t + 1]\n            loss += 0.5 * np.sum(dy ** 2)\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = dh * (1 - last_hiddens[t] ** 2)\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, (last_hiddens[t - 1] if t > 0 else np.zeros((self.hidden_size, 1))).T)\n            db_h += dh_raw\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss / T\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n        \"\"\"\n        T = input_sequence.shape[1]\n        hiddens = np.zeros((hidden_size, T))\n        outputs = np.zeros((W_hy.shape[0], T))\n        last_inputs = []\n        last_hiddens = []\n        h = np.zeros((hidden_size, 1))\n        for t in range(T):\n            last_inputs.append(input_sequence[:, t:t + 1])\n            h = np.tanh(np.dot(W_xh, input_sequence[:, t:t + 1]) + np.dot(W_hh, h) + b_h)\n            last_hiddens.append(h)\n            outputs[:, t:t + 1] = np.dot(W_hy, h) + b_y\n        return (outputs, last_inputs, last_hiddens)\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = input_sequence.shape[1]\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        loss = 0\n        dh_next = np.zeros((hidden_size, 1))\n        for t in reversed(range(T)):\n            dy = outputs[:, t:t + 1] - expected_output[:, t:t + 1]\n            loss += 0.5 * np.sum(dy ** 2)\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(W_hy.T, dy) + dh_next\n            dh_raw = dh * (1 - last_hiddens[t] ** 2)\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, (last_hiddens[t - 1] if t > 0 else np.zeros((hidden_size, 1))).T)\n            db_h += dh_raw\n            dh_next = np.dot(W_hh.T, dh_raw)\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n        return loss / T\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b, dtype=np.float64)\n    else:\n        x = x0.astype(np.float64)\n    r = b - A @ x\n    p = r\n    for i in range(n):\n        r_norm = np.linalg.norm(r)\n        if r_norm < tol:\n            break\n        alpha = r_norm ** 2 / (p @ A @ p)\n        x = x + alpha * p\n        r = r - alpha * A @ p\n        beta = r @ p / (p @ A @ p)\n        p = r + beta * p\n    return np.round(x.tolist(), 8)\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    n = len(y)\n    counts = np.bincount(y)\n    probabilities = counts / n\n    gini = 1.0 - np.sum(probabilities ** 2)\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [])\n    rows = len(dense_matrix)\n    cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_ptr = [0]\n    for i in range(rows):\n        for j in range(cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                col_indices.append(j)\n        row_ptr.append(len(values))\n    return (values, col_indices, row_ptr)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((v_i * L_i for (v_i, L_i) in zip(v, L)))\n    magnitude_squared = sum((L_i ** 2 for L_i in L))\n    scalar_projection = dot_product / magnitude_squared\n    projection = [scalar_projection * L_i for L_i in L]\n    projection = [round(component, 3) for component in projection]\n    return projection\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for j in range(num_cols):\n        for i in range(num_rows):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A.\n\n    Args:\n        A (numpy.ndarray): The input matrix.\n\n    Returns:\n        list: A list of basis vectors that span the column space of A,\n              rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (rows, cols) = A.shape\n    rank = 0\n    row = 0\n    for col in range(cols):\n        pivot_row = -1\n        for i in range(row, rows):\n            if abs(A[i, col]) > 1e-09:\n                pivot_row = i\n                break\n        if pivot_row != -1:\n            if pivot_row != row:\n                A[[row, pivot_row]] = A[[pivot_row, row]]\n            pivot = A[row, col]\n            A[row, :] = A[row, :] / pivot\n            for i in range(row + 1, rows):\n                factor = A[i, col]\n                A[i, :] = A[i, :] - factor * A[row, :]\n            rank += 1\n            row += 1\n    basis_vectors = []\n    for col in range(cols):\n        is_independent = True\n        for i in range(rank):\n            if abs(A[i, col]) > 1e-09:\n                is_independent = False\n                break\n        if is_independent:\n            basis_vectors.append(A[:, col])\n    basis = [A[:, i] for i in range(cols) if np.any(np.abs(A[:, i]) > 1e-09)]\n    independent_cols = []\n    for col in range(cols):\n        is_independent = True\n        for row in range(rank):\n            if abs(A[row, col]) > 1e-09:\n                is_independent = False\n                break\n        if is_independent:\n            independent_cols.append(A[:, col])\n    if not independent_cols:\n        return []\n    basis_matrix = np.array(independent_cols).T\n    return np.round(basis_matrix, 8).tolist()\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate R-squared for regression analysis.\n\n    Args:\n        y_true (np.ndarray): Array of true values.\n        y_pred (np.ndarray): Array of predicted values.\n\n    Returns:\n        float: R-squared value rounded to three decimal places.\n    \"\"\"\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n\n    Args:\n        img: A 2D matrix representing the grayscale image.\n\n    Returns:\n        The average brightness of the image rounded to two decimal places,\n        or -1 if the image is invalid.\n    \"\"\"\n    if not img:\n        return -1\n    row_len = len(img[0])\n    for row in img:\n        if len(row) != row_len:\n            return -1\n    total_brightness = 0\n    pixel_count = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    if pixel_count == 0:\n        return -1\n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Args:\n        y_true (array-like): Array of actual values.\n        y_pred (array-like): Array of predicted values.\n\n    Returns:\n        float: RMSE value rounded to three decimal places.  Returns np.nan if input is invalid.\n    \"\"\"\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        if y_true.shape != y_pred.shape:\n            return np.nan\n        if y_true.size == 0:\n            return np.nan\n        mse = np.mean((y_true - y_pred) ** 2)\n        rmse_value = np.sqrt(mse)\n        return round(rmse_value, 3)\n    except (TypeError, ValueError):\n        return np.nan\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels.\n        y_pred (np.ndarray): Array of predicted binary labels.\n\n    Returns:\n        float: The Jaccard Index, rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    if union == 0:\n        return 0.0\n    else:\n        return round(intersection / union, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score (F1-score) for binary classification.\n\n    Args:\n        y_true (np.ndarray): Array of true binary labels.\n        y_pred (np.ndarray): Array of predicted binary labels.\n\n    Returns:\n        float: The Dice Score, rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    elif sum_true == 0 or sum_pred == 0:\n        return 0.0\n    else:\n        dice = 2 * intersection / (sum_true + sum_pred)\n        return round(dice, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Creates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Args:\n        row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n        dim (int): The dimensionality of the hypervectors.\n        random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n        list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        seed = random_seeds[feature_name]\n        np.random.seed(seed)\n        feature_name_hv = np.random.rand(dim)\n        np.random.seed(seed)\n        feature_value_hv = np.random.rand(dim)\n        bound_hv = feature_name_hv * feature_value_hv\n        composite_hv = composite_hv + bound_hv\n    return composite_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n\n    Args:\n        data: A list of lists, where each inner list represents a pair\n              [y_true, y_pred] for one observation. y_true is the actual label,\n              and y_pred is the predicted label.\n\n    Returns:\n        A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (true_label, predicted_label) in data:\n        if true_label == 1 and predicted_label == 1:\n            true_positives += 1\n        elif true_label == 0 and predicted_label == 0:\n            true_negatives += 1\n        elif true_label == 0 and predicted_label == 1:\n            false_positives += 1\n        elif true_label == 1 and predicted_label == 0:\n            false_negatives += 1\n    return [[true_positives, false_negatives], [false_positives, true_negatives]]\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n        v1 (numpy.ndarray): The first vector.\n        v2 (numpy.ndarray): The second vector.\n\n    Returns:\n        float: The cosine similarity between the two vectors, rounded to three decimal places.\n               Returns 0.0 if either vector has zero magnitude.\n    \n    Raises:\n        ValueError: If the input vectors have different shapes.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        return 0.0\n    dot_product = np.dot(v1, v2)\n    cosine_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_similarity, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates performance metrics for a binary classification model.\n\n    Args:\n        actual: The actual class labels (1 for positive, 0 for negative).\n        predicted: The predicted class labels from the model.\n\n    Returns:\n        A tuple containing:\n        - confusion_matrix: A 2x2 matrix.\n        - accuracy: A float representing the accuracy of the model.\n        - f1_score: A float representing the F1 score of the model.\n        - specificity: A float representing the specificity of the model.\n        - negative_predictive_value: A float representing the negative predictive value.\n    \"\"\"\n    if not all((x in [0, 1] for x in actual)) or not all((x in [0, 1] for x in predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    if len(actual) != len(predicted):\n        raise ValueError('Actual and predicted lists must have the same length.')\n    tp = 0\n    tn = 0\n    fp = 0\n    fn = 0\n    for i in range(len(actual)):\n        if actual[i] == 1 and predicted[i] == 1:\n            tp += 1\n        elif actual[i] == 0 and predicted[i] == 0:\n            tn += 1\n        elif actual[i] == 0 and predicted[i] == 1:\n            fp += 1\n        elif actual[i] == 1 and predicted[i] == 0:\n            fn += 1\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / len(actual)\n    accuracy = round(accuracy, 3)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    f1_score = round(f1_score, 3)\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n    specificity = round(specificity, 3)\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "timed out", "solution": "import numpy as np\nimport numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates descriptive statistics for a given dataset.\n\n    Args:\n        data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n        dict: A dictionary containing the calculated descriptive statistics.\n    \"\"\"\n    data_np = np.array(data)\n    mean = np.mean(data_np)\n    median = np.median(data_np)\n    try:\n        mode_result = stats.mode(data_np)\n        mode = mode_result.mode[0]\n    except:\n        mode = np.nan\n    variance = np.var(data_np)\n    standard_deviation = np.std(data_np)\n    percentile_25 = np.percentile(data_np, 25)\n    percentile_50 = np.percentile(data_np, 50)\n    percentile_75 = np.percentile(data_np, 75)\n    interquartile_range = percentile_75 - percentile_25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if not np.isnan(mode) else np.nan, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(interquartile_range, 4)}\n    return results\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if not 0 <= p <= 1:\n        raise ValueError('Probability p must be between 0 and 1')\n    if not 0 <= k <= n:\n        raise ValueError('Number of successes k must be between 0 and n')\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if k < 0:\n        return 0\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast value, which is the difference between the maximum and minimum pixel values.\n               Returns 0 if the input image is empty.\n    \"\"\"\n    if img.size == 0:\n        return 0.0\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return float(contrast)\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    Raises:\n        ValueError: If the vectors have different lengths.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        raise ValueError('Vectors must have the same length')\n    return np.dot(vec1, vec2)\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        phi_vector = []\n        for i in range(degree + 1):\n            phi_vector.append(round(x ** i, 8))\n        transformed_data.append(phi_vector)\n    return transformed_data\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a sequence length and model dimensionality.\n\n    Args:\n        position (int): The sequence length.\n        d_model (int): The model dimensionality.\n\n    Returns:\n        list: A list of positional encodings as floats. Returns -1 if position is 0 or d_model is invalid.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    angle_rads = np.arange(d_model // 2) / (d_model / 2)\n    angle_rads = np.expand_dims(angle_rads, axis=0)\n    pos_enc = np.expand_dims(np.arange(position), axis=1)\n    pos_enc = pos_enc * angle_rads\n    pos_enc = np.sin(pos_enc)\n    pos_enc_cos = np.cos(pos_enc)\n    pos_enc = np.concatenate([pos_enc, pos_enc_cos], axis=1)\n    return pos_enc.astype(np.float16).tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: 'float' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(updated_parameter.tolist(), 5), np.round(m.tolist(), 5), np.round(v.tolist(), 5))\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Generates text based on a prompt using a simplified GPT-2-like model.\n\n    Args:\n        prompt (str): The initial text to start the generation.\n        n_tokens_to_generate (int): The number of tokens to generate.\n\n    Returns:\n        str: The generated text.\n    \"\"\"\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_tokens = encoder.encode(prompt)\n    x = input_tokens\n    generated_tokens = x[:]\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][x[-1]]\n        positional_embedding = params['wpe'][len(generated_tokens) % hparams['n_ctx']]\n        combined_embedding = token_embeddings + positional_embedding\n        attention_output = combined_embedding\n        ff_output = np.dot(params['ln_f']['g'], attention_output) + params['ln_f']['b']\n        normalized_output = ff_output\n        next_token = np.random.randint(0, len(encoder.encoder_dict))\n        generated_tokens.append(next_token)\n        x = [next_token]\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    crystal_values = np.array(crystal_values)\n    similarity_scores = np.dot(crystal_values, crystal_values.T)\n    attention_scores = softmax(similarity_scores / np.sqrt(dimension))\n    weighted_patterns = np.dot(attention_scores, crystal_values)\n    return [round(x, 4) for x in weighted_patterns.tolist()]\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'lower'", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates BM25 scores for documents in a corpus given a query.\n\n    Args:\n        corpus (list of str): A list of documents.\n        query (str): The search query.\n        k1 (float): BM25 parameter for term frequency saturation.\n        b (float): BM25 parameter for document length normalization.\n\n    Returns:\n        list of float: A list of BM25 scores for each document in the corpus,\n                       rounded to three decimal places.\n    \"\"\"\n    tokenized_corpus = [doc.lower().split() for doc in corpus]\n    document_lengths = [len(doc) for doc in tokenized_corpus]\n    avg_document_length = np.mean(document_lengths)\n    query_tokens = query.lower().split()\n    term_frequencies = []\n    for doc in tokenized_corpus:\n        term_frequencies.append(Counter(doc))\n    scores = []\n    for (i, doc) in enumerate(tokenized_corpus):\n        score = 0.0\n        for term in query_tokens:\n            if term in term_frequencies[i]:\n                tf = term_frequencies[i][term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (document_lengths[i] / avg_document_length))\n                score += np.log((document_lengths[i] + 1) / document_lengths[i]) * (numerator / denominator)\n        scores.append(score)\n    rounded_scores = [round(score, 3) for score in scores]\n    return rounded_scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    from sklearn.metrics import f1_score\n    return round(f1_score(y_true, y_pred), 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nPI = 3.14159\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Forecasts power consumption for day 15, accounting for a linear trend and daily fluctuation.\n\n    Args:\n        consumption_data (list): A list of 10 daily power consumption measurements.\n\n    Returns:\n        int: The forecasted power consumption for day 15, including a 5% safety margin, rounded up.\n    \"\"\"\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data):\n        fluctuation = 10 * math.sin(2 * PI * (i + 1) / 10)\n        detrended_consumption = consumption - fluctuation\n        detrended_data.append(detrended_consumption)\n    n = len(detrended_data)\n    sum_x = sum(range(1, n + 1))\n    sum_y = sum(detrended_data)\n    sum_xy = sum([(i + 1) * detrended_data[i] for i in range(n)])\n    sum_x2 = sum([(i + 1) ** 2 for i in range(n)])\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    b = (sum_y - m * sum_x) / n\n    day_15_base_consumption = m * 15 + b\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    rounded_consumption = round(day_15_consumption)\n    safety_margin = rounded_consumption * 0.05\n    final_consumption = rounded_consumption + math.ceil(safety_margin)\n    return int(final_consumption)\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    return round(np.mean(np.abs(y_true - y_pred)), 3)\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n\n    Args:\n        X (np.ndarray): Input tensor of shape (batch_size, seq_len, d_model).\n        W_q (np.ndarray): Weight matrix for query projections of shape (d_model, d_k).\n        W_k (np.ndarray): Weight matrix for key projections of shape (d_model, d_k).\n        W_v (np.ndarray): Weight matrix for value projections of shape (d_model, d_v).\n        n_heads (int): Number of attention heads.\n\n    Returns:\n        list: Output tensor after multi-head attention and reshaping, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n        \"\"\"\n        Computes the query, key, and value tensors.\n\n        Args:\n            X (np.ndarray): Input tensor of shape (batch_size, seq_len, d_model).\n            W_q (np.ndarray): Weight matrix for query projections of shape (d_model, d_k).\n            W_k (np.ndarray): Weight matrix for key projections of shape (d_model, d_k).\n            W_v (np.ndarray): Weight matrix for value projections of shape (d_model, d_v).\n\n        Returns:\n            tuple: A tuple containing the query, key, and value tensors.\n        \"\"\"\n        Q = np.dot(X, W_q)\n        K = np.dot(X, W_k)\n        V = np.dot(X, W_v)\n        return (Q, K, V)\n\n    def self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the self-attention output.\n\n        Args:\n            Q (np.ndarray): Query tensor of shape (batch_size, seq_len, d_k).\n            K (np.ndarray): Key tensor of shape (batch_size, seq_len, d_k).\n            V (np.ndarray): Value tensor of shape (batch_size, seq_len, d_v).\n\n        Returns:\n            np.ndarray: The self-attention output tensor.\n        \"\"\"\n        d_k = Q.shape[-1]\n        scores = np.dot(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n        output = np.dot(attention_weights, V)\n        return output\n    (batch_size, seq_len, d_model) = X.shape\n    d_k = W_q.shape[1]\n    d_v = W_v.shape[1]\n    X_qkv = compute_qkv(X, W_q, W_k, W_v)\n    (Q, K, V) = X_qkv\n    Q = Q.reshape(batch_size, seq_len, n_heads, d_k // n_heads).transpose(0, 2, 1, 3)\n    K = K.reshape(batch_size, seq_len, n_heads, d_k // n_heads).transpose(0, 2, 1, 3)\n    V = V.reshape(batch_size, seq_len, n_heads, d_v // n_heads).transpose(0, 2, 1, 3)\n    attention_outputs = []\n    for i in range(n_heads):\n        attention_outputs.append(self_attention(Q[:, i, :, :], K[:, i, :, :], V[:, i, :, :]))\n    attention_outputs = np.concatenate(attention_outputs, axis=-1)\n    output = attention_outputs.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return np.round(output.tolist(), 4)\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    if n != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n00 = 0\n    n01 = 0\n    n10 = 0\n    n11 = 0\n    for i in range(n):\n        if x[i] == 0 and y[i] == 0:\n            n00 += 1\n        elif x[i] == 0 and y[i] == 1:\n            n01 += 1\n        elif x[i] == 1 and y[i] == 0:\n            n10 += 1\n        elif x[i] == 1 and y[i] == 1:\n            n11 += 1\n    try:\n        phi = (n11 * n00 - n10 * n01) / ((n11 + n10) * (n01 + n00) * (n11 + n01) * (n00 + n10)) ** 0.5\n    except ZeroDivisionError:\n        return 0.0\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x >= 0:\n        return round(math.log(1 + math.exp(x)), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_surrogate = np.minimum(rhos * A, np.clip(rhos, 1 - epsilon, 1 + epsilon) * A)\n    kl_divergence = np.mean(np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref), axis=-1))\n    objective = np.mean(clipped_surrogate) - beta * kl_divergence\n    return round(objective, 6)\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": false, "result": "failed: test case failed: selu(1.0)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    sigmoid = 1 / (1 + np.exp(-z))\n    predictions = (sigmoid >= 0.5).astype(int)\n    return predictions.reshape(-1).tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (10,5) (5,10) (10,5) ", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    C = y.shape[0]\n    M = X.shape[1]\n    B = np.zeros((C, M))\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, B.T)\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n        loss = -np.sum(np.log(probs[np.arange(C), y])) / C\n        losses.append(round(loss, 4))\n        dz = probs.copy()\n        dz[np.arange(C), y] -= 1\n        dz /= C\n        dB = np.dot(X.T, dz)\n        B -= learning_rate * dB\n    return ([round(val, 4) for row in B.tolist() for val in row], losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    loss_history = []\n    for i in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear_model))\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss_history.append(round(loss, 4))\n        dw = 1 / n_samples * np.dot(X.T, predictions - y)\n        db = 1 / n_samples * np.sum(predictions - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return ([round(w, 4) for w in weights.tolist()], round(bias, 4), loss_history)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: shapes (6,8) and (6,8) not aligned: 8 (dim 1) != 6 (dim 0)", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    attention_scores = np.dot(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n    attention_scores = np.where(mask == 0, -1000000000.0, attention_scores)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([0,0,0,0])", "solution": "\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    unique_colors = len(set(apples))\n    total_apples = len(apples)\n    if unique_colors == 0:\n        return 0.0\n    disorder_value = unique_colors / total_apples\n    return round(disorder_value, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Layer Normalization on an input tensor.\n\n    Args:\n        X (np.ndarray): A 3D array representing batch size, sequence length, and feature dimensions.\n        gamma (np.ndarray): Scaling parameter.\n        beta (np.ndarray): Shifting parameter.\n        epsilon (float): A small value to avoid division by zero.\n\n    Returns:\n        list: The normalized X as a list, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled = gamma * X_normalized + beta\n    return np.round(X_scaled.tolist(), 5)\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Computes the METEOR score for evaluating machine translation quality.\n\n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float): Weight for unigram precision.\n        beta (float): Weight for unigram recall.\n        gamma (float): Penalty weight for fragmentation.\n\n    Returns:\n        float: The METEOR score, rounded to 3 decimal places.\n    \"\"\"\n    ref_words = reference.lower().split()\n    cand_words = candidate.lower().split()\n    matched_words = 0\n    for word in cand_words:\n        if word in ref_words:\n            matched_words += 1\n    if len(cand_words) == 0:\n        precision = 0.0\n    else:\n        precision = matched_words / len(cand_words)\n    if len(ref_words) == 0:\n        recall = 0.0\n    else:\n        recall = matched_words / len(ref_words)\n    if precision == 0.0 and recall == 0.0:\n        f_mean = 0.0\n    else:\n        f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    chunk_length = 0\n    if len(cand_words) > 0:\n        chunks = 0\n        for i in range(len(cand_words)):\n            if i == 0 or cand_words[i] != cand_words[i - 1]:\n                chunks += 1\n        chunk_length = matched_words / chunks if chunks > 0 else 0.0\n    score = f_mean * (1 - gamma * (1 - chunk_length))\n    return round(score, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) of two events.\n\n    Args:\n        joint_counts (int): The number of times both events occur together.\n        total_counts_x (int): The total number of times event x occurs.\n        total_counts_y (int): The total number of times event y occurs.\n        total_samples (int): The total number of samples.\n\n    Returns:\n        float: The PMI value, rounded to 3 decimal places.\n    \"\"\"\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    if p_x == 0 or p_y == 0 or p_xy == 0:\n        return 0.0\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    Min-Max normalization helps ensure that all features contribute equally to a model by scaling them to a common range.\n    For example:\n    min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\n    min_max([0, 10, 20, 30, 40]) == [0.0, 0.25, 0.5, 0.75, 1.0]\n    min_max([10, 5, 20, 15, 25]) == [0.0, 0.25, 1.0, 0.5, 1.0]\n    \"\"\"\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized_x = [(float(i) - min_val) / (max_val - min_val) for i in x]\n    return [round(val, 4) for val in normalized_x]\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with a shortcut connection.\n\n    Args:\n        x (np.ndarray): The input array.\n        w1 (np.ndarray): The weights for the first layer.\n        w2 (np.ndarray): The weights for the second layer.\n\n    Returns:\n        list: The output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    x1 = np.dot(x, w1)\n    x1 = np.maximum(0, x1)\n    x2 = np.dot(x1, w2)\n    x2 = np.maximum(0, x2)\n    x_residual = x + x2\n    x_final = np.maximum(0, x_residual)\n    return np.round(x_final.tolist(), 4)\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n\n    Args:\n        x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n        np.ndarray: A 1D array of shape (channels,) containing the average of each feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array (BCHW format).\n\n    Args:\n        X (np.ndarray): Input array of shape (batch, channels, height, width).\n        gamma (np.ndarray): Scale parameter (channels).\n        beta (np.ndarray): Shift parameter (channels).\n        epsilon (float): Small value for numerical stability.\n\n    Returns:\n        list: Batch Normalized output as a list, rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled = gamma * X_normalized + beta\n    return X_scaled.round(4).tolist()\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Args:\n        c (float): The coefficient of the polynomial term.\n        x (float): The point at which to evaluate the derivative.\n        n (float): The exponent of the polynomial term.\n\n    Returns:\n        float: The value of the derivative, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Args:\n        vectors: A list of 2D vectors (lists of floats).\n        tol: Tolerance value to determine linear independence.\n\n    Returns:\n        A list of orthonormal vectors (unit length and orthogonal to each other)\n        that span the same subspace.\n    \"\"\"\n    basis = []\n    for v in vectors:\n        v = np.array(v, dtype=float)\n        for u in basis:\n            v = v - np.dot(v, u) * u\n        if np.linalg.norm(v) > tol:\n            basis.append(v / np.linalg.norm(v))\n    return [list(np.round(u.tolist(), 4)) for u in basis]\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n\n    Args:\n        a (list or numpy.ndarray): The first 3D vector.\n        b (list or numpy.ndarray): The second 3D vector.\n\n    Returns:\n        list: The cross product of a and b, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    cross_product_result = np.cross(a, b)\n    return cross_product_result.round(4).tolist()\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n        A (numpy.ndarray): The square coefficient matrix.\n        b (numpy.ndarray): The constant vector.\n\n    Returns:\n        list: The solution vector x, rounded to the nearest 4th decimal.\n              Returns -1 if the system has no unique solution (det(A) == 0).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    n = A.shape[0]\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    return np.round(x.tolist(), 4)\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n\n    Args:\n        p (list[float]): A list representing the first discrete probability distribution.\n        q (list[float]): A list representing the second discrete probability distribution.\n\n    Returns:\n        float: The Bhattacharyya distance rounded to 4 decimal places.\n               Returns 0.0 if the inputs have different lengths or are empty.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    distance = -np.log(bc)\n    return round(distance, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"Computes the element-wise sum of two vectors.\n\n    Args:\n        a: The first vector (list of numbers).\n        b: The second vector (list of numbers).\n\n    Returns:\n        A new vector representing the element-wise sum if the operation is valid.\n        Returns -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    return result\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient estimator using the REINFORCE algorithm.\n\n    Args:\n        theta: A 2D NumPy array of shape (num_states, num_actions) representing the policy parameters.\n        episodes: A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n        A NumPy array representing the average gradient of the log-policy multiplied by the return at each time step,\n        reshaped into a 1D array using tolist().\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        returns = []\n        R = 0\n        for (s, a, r) in reversed(episode):\n            R = r + 0.99 * R\n            returns.insert(0, R)\n        for (t, (s, a, r)) in enumerate(episode):\n            policy = np.exp(theta[s, :]) / np.sum(np.exp(theta[s, :]))\n            gradient[s, a] += np.log(policy[a]) * returns[t]\n    avg_gradient = gradient / len(episodes)\n    return avg_gradient.reshape(-1).tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n        n_experts (int): Number of experts in the MoE layer.\n        k_active (int): Number of active experts (sparsity).\n        d_in (int): Input dimension.\n        d_out (int): Output dimension.\n\n    Returns:\n        float: Percentage of FLOPs saved by using the MoE layer.\n    \"\"\"\n    dense_flops = d_in * d_out\n    moe_flops = k_active / n_experts * d_in * d_out\n    savings = (1 - moe_flops / dense_flops) * 100\n    return round(savings, 1)\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Args:\n        X: Input matrix (batch_size, dim).\n        W_g: Gating network weight matrix (dim, num_experts).\n        W_noise: Noise weight matrix (dim, num_experts).\n        N: Pre-sampled noise matrix (batch_size, num_experts).\n        k: Sparsity constraint (number of experts to select).\n\n    Returns:\n        Gating probabilities matrix (batch_size, num_experts).\n    \"\"\"\n    logits = np.matmul(X, W_g) + N * W_noise\n    (topk_logits, topk_indices) = (np.partition(logits, k, axis=1)[:, :k], np.argsort(logits, axis=1)[:, :k])\n    mask = np.zeros_like(logits)\n    mask[np.arange(logits.shape[0])[:, None], topk_indices] = 1\n    topk_logits = np.exp(topk_logits - np.max(topk_logits, axis=1, keepdims=True))\n    topk_probs = topk_logits / np.sum(topk_logits, axis=1, keepdims=True)\n    gating_probs = np.zeros_like(logits)\n    gating_probs[np.arange(logits.shape[0])[:, None], topk_indices] = topk_probs\n    return np.round(gating_probs.tolist(), 4)\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (4,4) (3,4,2) ", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n\n    Args:\n        x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n        We (np.ndarray): Expert weight matrices. A list of n_experts matrices, each of shape (input_dim, expert_dim).\n        Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n        n_experts (int): The number of experts.\n        top_k (int): The number of top experts to use for each token.\n\n    Returns:\n        np.ndarray: The MoE output tensor of shape (batch_size, expert_dim).\n    \"\"\"\n    batch_size = x.shape[0]\n    expert_dim = We[0].shape[1]\n    gating_logits = np.dot(x, Wg)\n    gating_probs = np.exp(gating_logits) / np.sum(np.exp(gating_logits), axis=1, keepdims=True)\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    moe_output = np.zeros((batch_size, expert_dim))\n    for i in range(batch_size):\n        for j in top_k_indices[i]:\n            moe_output[i] += gating_probs[i, j] * np.dot(x[i], We[j])\n    return np.round(moe_output, 4).tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n        X (np.ndarray): Input tensor with shape (B, C, H, W).\n        gamma (np.ndarray): Scale parameter with shape (1, C, 1, 1).\n        beta (np.ndarray): Shift parameter with shape (1, C, 1, 1).\n        num_groups (int): Number of groups to normalize over.\n        epsilon (float): Small value to avoid division by zero.\n\n    Returns:\n        np.ndarray: Normalized tensor with shape (B, C, H, W).\n    \"\"\"\n    (B, C, H, W) = X.shape\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X, axis=(0, 2, 3, 4), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3, 4), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(variance + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    X_out = gamma * X_norm + beta\n    return np.round(X_out.tolist(), 4)\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum using gradient descent.\n\n    Args:\n        start_x (float): The initial value of x.\n        learning_rate (float): The learning rate for gradient descent.\n        tolerance (float): The tolerance for convergence.\n        max_iters (int): The maximum number of iterations.\n\n    Returns:\n        float: The value of x where f(x) reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for i in range(max_iters):\n        derivative = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * derivative\n        if abs(x_new - x) < tolerance:\n            return round(x_new, 4)\n        x = x_new\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function.\n\n    Args:\n        x (np.ndarray): Input tensor.\n        alpha (float): Scaling factor for the input.\n        gamma (float): Parameter controlling the slope of the DyT function.\n        beta (float): Parameter controlling the shift of the DyT function.\n\n    Returns:\n        list[float]: The DyT-transformed tensor, reshaped to a 1D list and rounded to 4 decimal places.\n    \"\"\"\n    dy_tanh = alpha * x / (gamma + np.exp(-alpha * x)) + beta\n    return np.round(dy_tanh.reshape(-1).tolist(), 4)\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
