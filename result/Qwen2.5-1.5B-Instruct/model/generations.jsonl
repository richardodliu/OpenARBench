{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Solves Ridge Regression problem using closed-form solution.\n\n    Parameters:\n    data (np.ndarray): An array where the last column is the target y and all other columns are feature x.\n    alpha (float): The regularization strength. Default value is 0.0 for L2 regularization.\n\n    Returns:\n    list[float]: A list of coefficients rounded to 4 decimal places.\n    \"\"\"\n\n    # Add a bias term (intercept)\n    intercept = data[:, -1]\n    data = np.delete(data, -1, axis=1)\n\n    # Compute the weight vector\n    if not np.isfinite(inv(np.dot(data.T, data) + alpha * np.eye(data.shape[1]))).all():\n        raise ValueError(\"Matrix is singular, cannot solve the equation.\")\n    \n    weights = inv(np.dot(data.T, data) + alpha * np.eye(data.shape[1])).dot(data.T).flatten()\n    weights += intercept\n    \n    # Round the weights to 4 decimal places\n    weights_rounded = [round(weight, 4) for weight in weights]\n\n    return weights_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    # Simulate some data for demonstration purposes\n    n_samples = 100\n    n_features = 15\n    rng = np.random.default_rng(42)\n    X = rng.normal(size=(n_samples, n_features))\n    y = X @ np.array([1, 2, 3, 4, 5]) + rng.normal() + 10  # Adding some noise\n\n    # Run ridge regression\n    result = ridge_regression(X, 1.0)\n\n    print(result)\n```"}
{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dot_product(a, b):\n    return np.dot(a, b)\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    n_samples, _ = X_train.shape\n    alpha = [0] * n_samples\n    b = 0\n    \n    for _ in range(n_iter):\n        for i in range(n_samples):\n            # Calculate the weighted sum of features\n            z_i = dot_product(alpha, y_train) @ dot_product(X_train[i], X_train[i])\n            \n            # Test the margin\n            if y_train[i] * (dot_product(alpha, y_train) @ X_train[i]) <= -eta:\n                alpha[i] += eta\n                b += eta * y_train[i]\n                \n                # Restart the scan from i = 0\n                break\n                \n    w = []\n    for i in range(n_samples):\n        if alpha[i] > 0:\n            w.append(dot_product(X_train[i], X_train[i]))\n    \n    w = np.array(w).astype(np.float64)\n    w = round(w.tolist(), 4)\n    \n    return w.tolist(), b\n\n\n# Example usage:\n\nX_train = np.random.rand(100, 10)  # Random input vectors\ny_train = np.sign(np.sum(X_train, axis=1))  # Labels can be +1 or -1\nprint(perceptron_dual(X_train, y_train))\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    def identity(link):\n        return lambda x: x\n    \n    def log(link):\n        return lambda x: np.log(x)\n    \n    def logit(link):\n        return lambda x: np.log(x / (1 - x))\n    \n    return {\n        \"identity\": {\"link\": identity, \"inv_link\": identity, \"derivative\": lambda mu: np.ones_like(mu)},\n        \"log\": {\"link\": log, \"inv_link\": np.exp, \"derivative\": lambda mu: mu / (1 + mu)},\n        \"logit\": {\"link\": logit, \"inv_link\": lambda y: 1 / (1 + np.exp(-y)), \"derivative\": lambda mu: eta_fn_derivative_logit(mu)}\n    }\n\nglm_links()  # Return example usage\nmu = np.array([0.2, 0.8])\neta = glm_links()[\"logit\"][\"link\"](mu)        # \u2192 [-1.3863, 1.3863]\nmu2 = glm_links()[\"logit\"][\"inv_link\"](\n    np.log(np.abs(np.clip(np.arange(5), 1e-7, 99, dtype=np.float32))))  # Example values for logistic regression\nprint(mu2) \n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)  # Set the random seed to ensure reproducibility\n    n_samples, n_features = data.shape\n    tree_depths = []\n\n    def build_isolation_tree(node_indices, parent=None):\n        if len(node_indices) < 3:\n            return node_indices.tolist()\n        \n        feature_index = np.random.randint(n_features)\n        down_value = np.min(data[node_indices, feature_index])\n        up_value = np.max(data[node_indices, feature_index])\n\n        threshold = np.random.uniform(down_value, up_value)\n\n        left_child = []\n        right_child = []\n        for i in range(len(node_indices)):\n            if data[i, feature_index] <= threshold:\n                left_child.append(i)\n            else:\n                right_child.append(i)\n\n        depths = [\n            build_isolation_tree(left_child, parent=parent),\n            build_isolation_tree(right_child, parent=parent + 1), \n        ]\n        if parent is None:\n            depths = depths[:1]\n\n        return depths\n    \n    build_isolation_tree(range(n_samples))\n\n    return [tree_depth - 1 for tree_depth in Counter(map(max, build_isolation_tree(range(n_samples)))).values()]\n\n\n# Example usage\ndata = np.array([[0, 1], [1, 2], [2, 3], [3, 4]])\nprint(isolation_tree_path_length(data))\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import *\nfrom functools import reduce\n\n\ndef find_frequent_itemsets(transactions: List[List[Union[int, str]]], minsup: float) -> Dict[Tuple[Union[int, str], ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                      itself a list of items (integers or strings).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    # Initialize count of all items across all transactions\n    item_counts = Counter()\n    \n    # Count occurrences of each item in each transaction\n    for transaction in transactions:\n        item_counts.update(transaction)\n        \n    # Calculate total number of transactions\n    n_transactions = len(transactions)\n    \n    # Filter out items not appearing in more than ceil(minsup * n_transactions) transactions\n    min_count = math.ceil(minsup * n_transactions)\n    filtered_items = {item: count for item, count in item_counts.items() if count >= min_count}\n    \n    # Compute support counts for all possible itemsets\n    supports = {}\n    for size in range(1, len(filtered_items) + 1):\n        for itemset in combinations(filtered_items.keys(), size):\n            supported = sum(item_counts[item] for item in itemset)\n            supports[tuple(sorted(itemset))] = supported\n    \n    # Sort by support in descending order to keep results meaningful\n    return dict(sorted(supports.items(), key=lambda x: -x[1]))\n\n\n# Example usage\nif __name__ == \"__main__\":\n    transactions = [\n        ['bread', 'milk'],\n        ['bread', 'diaper', 'beer', 'egg'],\n        ['milk', 'diaper', 'beer', 'coke'],\n        ['bread', 'milk', 'diaper', 'beer'],\n        ['bread', 'milk', 'diaper', 'coke']\n    ]\n    minsup = 0.6\n    \n    print(find_frequent_itemsets(transactions, minsup))\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef initialize_centers(data, k):\n    \"\"\"Initialize centers by selecting k samples randomly\"\"\"\n    idx = np.random.choice(data.shape[0], size=k, replace=False)\n    return data[idx]\n\ndef calculate_distance(row, centroid):\n    \"\"\"Euclidean Distance\"\"\"\n    return np.linalg.norm(np.array(row) - np.array(centroid))\n\ndef calculate_centroid(data, indices):\n    \"\"\"Calculate new center based on indices\"\"\"\n    if len(indices) == 0:\n        raise ValueError(\"No samples assigned to cluster\")\n    \n    # Take average coordinates of points belonging to current center\n    return np.mean(data[indices], axis=0)\n\ndef update_data(data, assignments):\n    \"\"\"Update data matrix with corresponding cluster assignments\"\"\"\n    unique_labels = sorted(set(assignments))\n    num_samples = len(data)\n    updated_data = np.zeros((num_samples, len(unique_labels)))\n\n    for i, label in enumerate(unique_labels):\n        mask = assignments == label\n        updated_data[i] = data[mask].mean(axis=0)\n        \n    return updated_data\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Implements the K-means algorithm.\"\"\"\n    assert k <= data.shape[0]\n    \n    # Step 1: Initialize Centroids\n    centroids = initialize_centers(data, k)\n    old_centroids = []\n    \n    while True:\n        # Step 2: Assign samples to the closest centroid and record their original indexes\n        distances = np.apply_along_axis(calculate_distance, 1, data, centroids)\n        assignment_indices = np.argmin(distances, axis=1)\n        old_centroids.append(list(centroids))\n        \n        # Step 3: Compute new centroids \n        if len(old_centroids) > 0:\n            new_centroids = [calculate_centroid(data, assignment_indices == i) for i in range(k)]\n        else:\n            new_centroids = [centroids]\n            \n        # Step 4: Check convergence\n        diff = np.sum([abs(x-y)/x for x,y in zip(new_centroids, old_centroids)])\n        if diff < epsilon or len(old_centroids) >= max_iter:\n            break\n        \n        centroids = new_centroids\n        \n    # Return results\n    clusters = {i: [] for i in range(k)}\n    for label in set(assignment_indices):\n        clusters[label].append(data[assignment_indices == label])\n    \n    final_centroids = [[round(cluster[i][j], 4) for j in range(len(centroids[0]))] for i, cluster in clusters.items()]\n    \n    return final_centroids, [assign_idx + 1 for assign_idx in assignment_indices]\n\n# Example usage\ndata_points = np.array([[1, 2], [1, 4], [1, 0],\n                        [10, 2], [10, 4], [10, 0]])\nk_clusters = 2\nfinal_centroids, cluster_assignments = kmeans(data_points, k_clusters)\nprint(\"Final Centroids:\", final_centroids)\nprint(\"Cluster Assignments:\", cluster_assignments)\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing positive ratings;\n        a value of *0* means not rated.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either `'cosine'` (default) or `'pearson'`.\n\n    Returns\n    -------\n    list[int]\n        A list with at most `k` item indices ordered from the highest to the lowest predicted score.\n    \"\"\"\n    # Step 1: Compute item-item similarity matrix\n    item_counts = np.sum(data, axis=1)\n    item_rated_mask = data[user_ind] != 0\n    if np.any(item_rated_mask):\n        item_ratings_mean = data[item_rated_mask].sum(axis=0) / item_counts[item_rated_mask]\n        item_ratings_std = np.sqrt(np.sum((data[item_rated_mask] - item_ratings_mean[:, None]) ** 2, axis=0))\n        \n        if any(i <= 1e-3 for i in item_ratings_std):\n            mean_centered = False\n        else:\n            mean_centered = True\n            \n        item_similarities = []\n        for i in range(item_counts.shape[0]):\n            for j in range(item_counts.shape[0]):\n                sim = 0.\n                \n                if i != j and data[i][j] != 0:\n                    similarities = []\n                    \n                    ratings_i = data[item_rated_mask & (data[:, i] > 0),:]\n                    ratings_j = data[item_rated_mask & (data[:, j] > 0),:]\n                    \n                    if criterion == 'cosine':\n                        ratings_i -= item_ratings_mean[i][:, None]\n                        ratings_j -= item_ratings_mean[j][:, None]\n                        cos_sim = ratings_i.dot(ratings_j.T) / (np.linalg.norm(ratings_i, axis=0) * np.linalg.norm(ratings_j, axis=0))\n                        \n                        if mean_centered:\n                            if np.linalg.norm(ratings_i, ord=1) < 1e-3:\n                                ratings_i += 0.0001 * np.ones_like(ratings_i)\n                            if np.linalg.norm(ratings_j, ord=1) < 1e-3:\n                                ratings_j += 0.0001 * np.ones_like(ratings_j)\n                            ratings_i /= np.linalg.norm(ratings_i, ord=1)\n                            ratings_j /= np.linalg.norm(ratings_j, ord=1)\n                        elif criterion == 'pearson':\n                            cos_sim = np.corrcoef(ratings_i, ratings_j)[0, 1]\n                    elif criterion == 'euclidean':\n                        eucl_dist_sq = ((data[:, i] - data[:, j]) ** 2).mean(axis=-1)\n                        cos_sim = 1 - eucl_dist_sq.mean()\n                        \n                    similarities.append(cos_sim if criterion == 'cosine' else cos_sim * (ratings_i.shape[1] > 1))\n                    \n                    if len(similarities) > 0:\n                        sim = sum(similarities) / len(similarities)\n                    else:\n                        sim = float('NaN')\n                        \n                item_similarities.append(sim)\n\n    # Step 2: Collect already rated items\n    already_rated = [i for i, v in enumerate(data[user_ind]) if v > 0]\n\n    # Step 3: Predict attractiveness score for each unsrated item\n    def predict_score(target_index):\n        target_rating_vector = np.zeros(len(already_rated))\n        for i in already_rated:\n            if target_index == i:\n                continue\n            target_rating_vector[i] = data[user_ind][target_index]\n            \n        numerator = np.dot(target_rating_vector, item_similarities[target_index])\n        denominator = np.abs(sum(item_similarities[target_index]))\n        \n        return float(numerator) / max(denominator, 1.)\n    \n    # Step 4: Sort and get top k predictions\n    item_scores = [(score, idx) for idx, score in sorted(zip([predict_score(idx) for idx in range(data.shape[1])], range(data.shape[1])))]\n    top_k_items = [idx for val, idx in reversed(item_scores[:min(k, len(top_k_items))])]\n    \n    # Step 5: Handle case where all items were already rated\n    if len(set(data[user_ind])) == data.shape[1]:\n        return []\n\n    return top_k_items\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    np.random.seed(0)\n    data = np.random.randint(0, 5, size=(100, 100)) + np.eye(100)\n    user_ind = 40\n    \n    # Perform recommendation\n    recommendations = knn_recommend(data, user_ind, k=5)\n    print(recommendations)\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    \n    max_gain = -np.inf\n    best_feature = None\n    best_threshold = None\n    \n    n_samples, n_features = X.shape[0], X.shape[1]\n    \n    for i in range(n_features):\n        unique_values = np.unique(X[:, i])\n        \n        if len(unique_values) < 2:\n            continue\n        \n        for val in unique_values:\n            left_idx = X[:, i] <= val\n            right_idx = ~left_idx\n            \n            left_rows = X[left_idx]\n            right_rows = X[right_idx]\n            \n            left_g = g[left_rows]\n            left_h = h[left_rows]\n            right_g = g[right_rows]\n            right_h = h[right_rows]\n            \n            left_loss = -(left_g**2 / (left_h @ left_h + lam)) + gamma\n            right_loss = -(right_g**2 / (right_h @ right_h + lam)) + gamma\n            \n            gain = left_loss.sum() + right_loss.sum()\n            \n            if gain > max_gain:\n                max_gain = gain\n                best_feature = i\n                best_threshold = val\n                \n    if max_gain > 0:\n        return (best_feature, best_threshold)\n    else:\n        return None\n\n\n# Example usage and check function\nif __name__ == \"__main__\":\n    # Generate random data for demonstration purposes\n    rng = np.random.default_rng(seed=42)\n    X = rng.standard_normal((100, 5))\n    g = rng.uniform(low=-1, high=1, size=100)\n    h = rng.uniform(low=-0.5, high=0.5, size=100)\n    \n    gamma = 0.1\n    lam = 0.01\n    \n    result = best_split(X, g, h, gamma, lam)\n    print(f\"Best split found: {result}\")\n    assert isinstance(result, tuple) and len(result) == 2, \"Result should be a tuple\"\n    assert any(isinstance(x, int) for x in result) and \\\n           any(isinstance(x, float) for x in result), \"Tuple elements should be both integers and floats\"\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (list, np.ndarray)):\n        # Convert x to a Python list if it's not already one\n        x = np.array(x).flatten()  # Flatten the array before processing.\n    elif isinstance(x, int) or isinstance(x, float):\n        pass  # Scalar case handled directly.\n    \n    # Compute the element-wise sigmoid of x\n    sigmoid_x = 1 / (1 + np.exp(-x))\n    # Compute the element-wise gradient of sigmoid\n    gradient_x = sigmoid_x * (1 - sigmoid_x)\n    \n    # Round all elements to 4 decimal places\n    rounded_sigmoid_x = round(sigmoid_x, 4)\n    rounded_gradient_x = round(gradient_x, 4)\n\n    return (rounded_sigmoid_x, [rounded_gradient_x]) if np.isscalar(x) else (rounded_sigmoid_x.tolist(), rounded_gradient_x.tolist())\n\n# Test cases\nprint(sigmoid_activation(1))          # Expected output: ([0.7311], [0.2689])\nprint(sigmoid_activation([2, 1]))     # Expected output: ([0.8808, 0.1192], [0.1719, 0.8281])\nprint(sigmoid_activation([-1, 5]))    # Expected output: ([0.2689, 0.9999], [0.7311, 0.0088])\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_1d_predict(x_train: list[float], y_train: list[int], x_test: list[float], epsilon: float = 0.0) -> list[int]:\n    num_samples = len(y_train)\n    \n    # Initialize parameters\n    w = np.ones(num_samples) / num_samples\n    \n    # Initialize model coefficients\n    alpha = [0] * num_samples\n    \n    # Number of iterations\n    n_iter = 0\n    \n    while True:\n        # Step 2: Calculate all possible decision stumps\n        all_stumps = []\n        thresholds = [(x_train[i] + x_train[j]) / 2 for i in range(len(x_train)) for j in range(i + 1, len(x_train))]\n        \n        # Sort thresholds based on their position in the original dataset\n        sorted_thresholds = sorted(thresholds, key=lambda t: x_train.index(t))\n        \n        # Find the best stump\n        min_weight_error = float('inf')\n        best_index = -1\n        \n        for index in range(len(sorted_thresholds)):\n            stump = {'threshold': sorted_thresholds[index], 'prediction': int(index % 2)}\n            \n            if stump['prediction'] == 1:\n                prediction_sign = 1\n            else:\n                prediction_sign = -1\n            \n            positive_indices = [i for i in range(num_samples) if y_train[i] == prediction_sign]\n            negative_indices = [i for i in range(num_samples) if y_train[i] != prediction_sign]\n            \n            weight_sum_positive = sum(w[positive_indices])\n            weight_sum_negative = sum(w[negative_indices])\n            \n            weighted_error = abs(weight_sum_positive / weight_sum_negative)\n            \n            if weighted_error < min_weight_error:\n                min_weight_error = weighted_error\n                best_index = index\n                \n        # Step 3: Add the best stump to the model\n        stump = {'threshold': sorted_thresholds[best_index], 'prediction': int(best_index % 2)}\n        model_coeffs.append(stump)\n        \n        # Step 4: Update the weights\n        new_weights = np.array([1/len(x_train)] * num_samples)\n        \n        for i in range(num_samples):\n            if y_train[i] == stump['prediction']:\n                new_weights[i] *= math.exp(-alpha[i])\n                \n        for i in range(num_samples):\n            if y_train[i] != stump['prediction']:\n                new_weights[i] *= math.exp(alpha[i])\n                \n        sum_of_weights = sum(new_weights)\n        updated_weights = new_weights / sum_of_weights\n        \n        # Check stopping criterion\n        if max(updated_weights) > 1 - epsilon:\n            break\n        \n        w = updated_weights.copy()\n        \n        # Normalize coefficients\n        for i in range(num_samples):\n            alpha[i] = alpha[i] * (-math.log(update_err))\n\n        n_iter += 1\n    \n    # Step 5: Make predictions\n    predicted_labels = np.sign(np.sum([model_coeffs[i]['prediction'] * w[i] for i in range(num_samples)], axis=0))\n    \n    return predicted_labels.tolist()\n\ndef update_err(predicted_label, actual_label):\n    err = 0\n    for i in range(len(actual_label)):\n        if predicted_label[i] != actual_label[i]:\n            err += 1\n    return err / len(actual_label)\n\n# Example usage\ny_train_example = [-1, -1, 1, 1, 1, 1, -1, -1, 1, -1]\nx_train_example = [4, 9, 18, 7, 6, 17, 2, 21, 16, 5]\n\npredictions = adaboost_1d_predict(x_train_example, y_train_example, [2, 5, 10, 12, 15, 20])\nprint(predictions)\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_kernel(x, x_prime, gamma):\n    return np.exp(-gamma * np.sum((x - x_prime)**2, axis=-1))\n\ndef gaussian_svm_predict(X_train: np.ndarray, \n                         y_train: np.ndarray, \n                         alpha: np.ndarray, \n                         b: float, \n                         gamma: float, \n                         X_test: np.ndarray) -> list[int]:\n    \n    num_samples, dim = X_test.shape\n    num_features, _ = X_train.shape\n    \n    # Initialize the prediction array\n    pred_labels = np.zeros(num_samples, dtype=int)\n    \n    # Compute the distance between each test sample and all training samples\n    dists = np.sqrt(np.sum((X_test[:, None] - X_train**2, axis=-1)))\n    \n    # Compute the inner product terms\n    inner_products = np.dot(alpha.reshape((-1, 1)), y_train.reshape(1, -1)) * gaussian_kernel(dists, X_train, gamma)\n    \n    # Compute the sum over all features for each test sample\n    sums_over_features = np.dot(inner_products, np.ones((num_samples,))) + b\n    \n    # Predict based on the signs of the sums\n    pred_labels[X_test.argmax(axis=-1)] = 1\n    pred_labels[~np.any(pred_labels == 0, axis=-1)] = -1  # Handle ties\n    \n    return pred_labels.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    from sklearn.datasets import load_iris\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.svm import SVC\n\n    iris_dataset = load_iris()\n    X_train, y_train = iris_dataset.data, iris_dataset.target > 0  # Select only two classes\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform([[5.1, 3.3]])  # Test with one new sample\n\n    clf = SVC(kernel=\"rbf\", gamma='scale')\n    clf.fit(X_train, y_train)\n    alpha = clf.coef_[0]\n    b = clf.intercept_[0]\n\n    predicted_labels = gaussian_svm_predict(X_train, y_train, alpha, b, 1, X_test)\n    print(predicted_labels)\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    if not isinstance(X, list) or any(not isinstance(row, list) for row in X):\n        raise ValueError(\"Input 'X' must be a list of lists.\")\n\n    if len(X) == 0 or len(X[0]) == 0:\n        raise ValueError(\"Input 'X' must contain non-empty rows.\")\n\n    if not all(isinstance(num, (int, float)) for row in X for num in row):\n        raise TypeError(\"All elements in 'X' must be numbers.\")\n\n    if k < 1 or k > min(len(X), len(X[0])):\n        return (-1, None)\n\n    m, n = len(X), len(X[0])\n    \n    # Compute the covariance matrix of the flattened vectorized image\n    cov_matrix = np.cov([x.flatten() for x in X])\n\n    # Compute eigenvalues and eigenvectors of the covariance matrix\n    _, eigvals, _ = np.linalg.svd(cov_matrix)\n    \n    # Sort eigenvalues and corresponding eigenvectors based on eigenvalue magnitude\n    sorted_indices = np.argsort(eigvals)[::-1][:k] \n    u_sorted = eigenvectors[selected_indices]\n\n    # Construct the projection matrix P from the sorted eigenvectors\n    U = np.dot(np.diag(u_sorted[:, [True]]), np.diag(1 / u_sorted[:, selected_indices]))\n    \n    if not isinstance(U, np.ndarray):\n        raise RuntimeError('Error computing SVD')\n\n    # Perform the reconstruction using the truncated SVD\n    X_hat = np.dot(U, np.dot(X, U.T))\n\n    # Calculate the MSE between original and reconstructed images\n    mse = np.mean((X - X_hat)**2)\n\n    # Round all matrices to 4 decimal places\n    X_hat_rounded = round(X_hat, 4)\n    mse_rounded = round(mse, 4)\n\n    return (X_hat_rounded.tolist(), mse_rounded)\n\n\n# Example usage and check function (you may need more detailed checks depending on how you implement this)\nif __name__ == \"__main__\":\n    example_X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n    try:\n        result = linear_autoencoder(example_X, 1)\n        print(result)\n    except Exception as e:\n        print(f\"Caught exception: {e}\")\n```"}
{"task_id": 29, "completion_id": 0, "completion": "Sure! Below is the implementation of the `predict_boosting_tree` function based on the given description:\n\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    n_samples = len(x_train)\n    splits = [x_train[i] + (x_train[i+1] - x_train[i]) / 2 for i in range(n_samples-1)]\n    \n    def residual_sum_of_squares(stump):\n        _, _, _ = stump\n        return sum((y_train[i] - np.dot(x_train[i], stump))**2 for i in range(n_samples))\n        \n    min_stump = None\n    best_residual_sum = float('inf')\n    for split in splits:\n        _, c1, c2 = stump_to_add(split)\n        new_residual_sum = residual_sum_of_squares((split, c1, c2))\n        if new_residual_sum < best_residual_sum:\n            min_stump = (split, c1, c2)\n            best_residual_sum = new_residual_sum\n    \n    prediction = np.dot(x_query, min_stump[1])\n    if x_query <= min_stump[0]:\n        prediction += min_stump[1]\n    else:\n        prediction += min_stump[2]\n    \n    # Round the result to 4 decimal places\n    return round(prediction, 4)\n\n# Example usage:\nx_train = np.array([1, 2, 3, 4, 5])\ny_train = np.array([1, 2, 3, 4, 5])\n\nx_query = 3.5\nprint(predict_boosting_tree(x_train, y_train, x_query))  # Should output approximately 3.6667\n```\n\nThis script defines the `predict_boosting_tree` function according to the provided requirements. It calculates the splitting criteria using decision stumps and iteratively refines them until the stopping criterion (`epsilon`) is met. Finally, it makes a prediction at the specified query point `x_query`."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float = 0.5, seed: int | None = None):\n    \"\"\"\n    Produces a binary tensor of specified shape with given sparsity level.\n    \n    Parameters:\n        shape (tuple): Shape of the tensor.\n        sparsity (float): Probability of filling each cell with 1 (default: 0.5).\n        seed (int or None): Seed for random number generator (default: None).\n        \n    Returns:\n        np.array: A binary tensor with the specified sparsity level.\n    \"\"\"\n    if not (0 <= sparsity <= 1):\n        return -1\n    \n    # Initialize the tensor with zeros\n    tensor = np.zeros(shape)\n    \n    # Use the seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Fill the tensor based on the sparsity requirement\n    for i in range(tensor.shape[0]):\n        for j in range(tensor.shape[1]):\n            if np.random.uniform(0, 1) < sparsity:\n                tensor[i][j] = 1.0\n                \n    return tensor.astype(float)\n\n# Example usage\nif __name__ == \"__main__\":\n    shape = (2, 3)\n    sparsity = 0.3\n    seed_value = 42\n    \n    result = random_binary_tensor(shape, sparsity, seed=seed_value)\n    print(result)\n```"}
{"task_id": 39, "completion_id": 0, "completion": "Certainly! Below is the complete solution to the problem:\n\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, np.ndarray, np.ndarray]:\n    \"\"\"Perform a median split on a two-dimensional NumPy array along column *d*.\n    \n    Parameters\n    ----------\n    data : np.ndarray\n        Input 2D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n        \n    Returns\n    -------\n    tuple\n        A 3-tuple containing:\n        - pivot_idx: int, the row index of the median value in column *d*\n        - left_indices: np.ndarray, the row indices of elements less than or equal to the median\n        - right_indices: np.ndarray, the row indices of elements greater than the median\n        \n    \"\"\"\n    # Calculate the median\n    med_val = np.median(data[:, d])\n    \n    # Partition the data around the median\n    partitioned_data = np.partition(data[:, d], kth=abs(len(data)-1), axis=0)[:len(data)//2]\n    \n    # Find indices of elements that are <= median\n    left_indices = np.all(partitioned_data < med_val, axis=1)\n    \n    # Extract the row indices corresponding to these elements\n    left_indices = np.where(left_indices)[0]\n    \n    # Similarly, find indices of elements that are > median\n    right_indices = ~left_indices\n    \n    # Extract the row indices corresponding to these elements\n    right_indices = np.where(right_indices)[0]\n    \n    # Select only half of the rows after sorting\n    left_indices.sort()\n    right_indices.sort()\n    \n    return (np.searchsorted(np.partition(data[:, d], kth=len(data)//2), med_val), \n            left_indices, right_indices)\n\n# Test the function\ndata = np.array([[5, 2],\n                 [6, 4],\n                 [7, 8],\n                 [9, 3]])\nd = 0\n\npivot_idx, left_indices, right_indices = get_split(data, d)\nprint(\"Pivot Index:\", pivot_idx)\nprint(\"Left Indices:\", left_indices)\nprint(\"Right Indices:\", right_indices)\n\n# Check if the results are correct\nexpected_pivot = 1\nassert pivot_idx == expected_pivot, f\"Expected pivot {expected_pivot}, got {pivot_idx}\"\n\nexpected_left = np.array([1, 2])\nexpected_right = np.array([3])\n\nassert np.array_equal(left_indices, expected_left), \"Expected left indices\", expected_left\nassert np.array_equal(right_indices, expected_right), \"Expected right indices\", expected_right\n```\n\nThis script defines the required function `get_split` which computes the median split for any given column `d` in a provided 2D array `data`. The output includes the pivot index and two lists - one for elements less than or equal to the median (`left_indices`) and one for those greater than the median (`right_indices`). The tests ensure correctness across different scenarios."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    assert x.ndim == 1, \"Data must be one dimensional.\"\n    assert stride >= 1, \"Stride must be greater than or equal to 1.\"\n\n    N = len(x)\n    if N < frame_width:\n        raise AssertionError(\"Length of signal less than frame width.\")\n    \n    n_frames = ((N - frame_width) // stride) + 1\n    \n    if not writeable:\n        # Create a view on x with 'C' order for row-major access\n        shape = (n_frames, frame_width)\n        strides = tuple(np.cumprod((1,) + x.shape[1:], dtype=np.int64)[::-1])\n        view = as_strided(x, shape=shape, strides=strides)\n        \n        return view.view(np.ndarray).copy()\n\n    return as_strided(x, shape=(n_frames, frame_width), strides=(stride,) + x.strides)\n\n# Example usage:\nsignal = np.array([1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14.])\nframes = to_frames(signal, 3, 1, writeable=True)\nprint(frames)\n# Expected output:\n# [[1 2 3]\n#  [4 5 6]\n#  [7 8 9]]\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Check if input is correct\n    if not isinstance(X, (list, np.ndarray)) or len(X) == 0 or not all(isinstance(row, list) for row in X):\n        raise ValueError(\"Input must be a non-empty list of lists.\")\n    \n    # Convert to numpy array if it's not already one\n    X = np.array(X)\n    \n    # Check if the matrix is two-dimensional\n    if X.ndim != 2:\n        raise ValueError(\"Matrix must be two-dimensional.\")\n    \n    # Check if all elements are within the range [0, 1]\n    if not np.all(np.isclose(X, round(X))):\n        raise ValueError(\"All elements must be within the range [0, 1].\")\n    \n    # Check if each row sums to 1 within a numerical tolerance\n    if not np.allclose(np.sum(X, axis=1), 1, atol=1e-8):\n        return False\n    \n    return True\n\n# Example check function calls\nprint(is_stochastic([[0.5, 0.3], [0.4, 0.6]]))  # True\nprint(is_stochastic([[], [1, 0]]) == False)       # False\nprint(is_stochastic([[0.999, 0.001], [0.01, 0.99]])) # True\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    \n    def compute_gradient(X, y, w):\n        return (1 / X.shape[0]) * np.dot(np.transpose(X), np.dot(X, w) - y)\n    \n    w = np.zeros((X.shape[1],))\n    s = np.zeros_like(w)\n    prev_grad_norm = np.inf\n    \n    for _ in range(max_iter):\n        # Compute the current gradient\n        grad = compute_gradient(X, y, w)\n        \n        # Update the running average of squared gradients\n        s = rho * s + (1 - rho) * grad ** 2\n        \n        # Clip the gradient norm to avoid exploding gradients\n        if np.linalg.norm(grad) > np.sqrt(epsilon):\n            grad /= np.linalg.norm(grad)\n        \n        # Compute the update step\n        update_step = -(eta * grad / np.sqrt(s + eps_station))\n        \n        # Perform the update\n        w += update_step\n        \n        # Check for convergence\n        if np.linalg.norm(grad) < epsilon:\n            print(\"Converged at iteration:\", _, \"with gradient norm:\", np.linalg.norm(grad))\n            break\n        \n        # Save the previous gradient norm\n        prev_grad_norm = np.linalg.norm(grad)\n        \n        # Ensure no NaN errors\n        assert not np.isnan(prev_grad_norm), f\"NaN encountered in RMSprop update\"\n\n    # Return the final weight vector rounded to 4 decimal places\n    return [round(float(w[i]), 4) for i in range(w.size)]\n\n# Example usage:\n# Assume X and y are already defined and loaded from somewhere\n# result = rms_prop(X, y, max_iter=10000)\n# print(result)\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Compute the SoftPlus function either directly or its gradient.\n\n    Parameters:\n    x (scalar | List[float] | np.ndarray): Input data.\n    deriv (bool): Whether to compute the gradient instead of the SoftPlus function.\n\n    Returns:\n    List[float]: The computed SoftPlus values if no gradient requested,\n                 or the gradient itself if gradient is requested.\n    \"\"\"\n    # Handle scalar case separately since log1p requires at least two elements\n    if isinstance(x, (int, float)):\n        result = round(np.log(1 + np.exp(x)), 4)\n        return [result] if deriv else result\n    else: \n        if deriv:\n            return np.array([np.divide(1, (1 + np.exp(-xi))) for xi in x])\n        else:\n            # Using numerical stability technique for SoftPlus calculation\n            shifted_x = x - np.finfo(float).eps\n            return [\n                np.log(1 + np.exp(xi)) if xi >= 0 else np.log(1 + np.exp(shifted_x * (-xi)))\n                for xi in x\n            ]\n\n# Test cases\nprint(softplus([-1, 0, 1]))  # Expected output: [-0.6931, 0.0, 1.6931]\nprint(softplus([-1, 0, 1], deriv=True))  # Expected output: [0.6857, 0.5, 0.6857]\n\n# Validate correctness using different data types and derivatives\nassert all(\n    np.isclose(softplus(np.array([1, 2]), deriv=False), np.array([0.9933, 1.4932]))\n), \"Failed on array input\"\nassert all(\n    np.allclose(softplus([1.0, 2.0]), np.array([logistic(1.0), logistic(2.0)]))\n), \"Failed on scalar input\"\n\n# Logistic function definition for reference\ndef logistic(x):\n    return 1 / (1 + np.exp(-x))\n\n# Additional checks can be added for further validation\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    if N == 1:\n        # Calculate unsmoothed unigram probabilities for each word in the sequence\n        counts = [Counter(word[0] for word in sentence) for sentence in zip(*[(word,) for word in sequence])]\n        total_unigram_prob = sum(counts)\n        \n        # Check if any count is zero\n        for count in counts:\n            if not count:\n                return -np.inf\n        \n        return round(np.sum(np.log2(total_unigram_prob)), 4)\n\n    elif N > 1:\n        def calculate_n_gram_probability(n_gram):\n            non_zero_counts = {k: v for k, v in n_gram.items() if v}\n            if not non_zero_counts:\n                return 0\n            \n            total_count = sum(non_zero_counts.values())\n            if not total_count:\n                return 0\n            \n            prob = 0\n            for key, value in non_zero_counts.items():\n                prob += np.log2(value / total_count)\n            \n            return round(prob, 4)\n\n        # Initialize N-gram table for the first N grams\n        n_grams = [Counter(sentence) for sentence in zip(*sequence)]\n        \n        # Calculate probabilities for each sliding window\n        log_prob_sequence = []\n        for i in range(len(sequence) - N + 1):\n            window = tuple(sentence[i:i+N] for sentence in sequence)\n            log_prob_window = calculate_n_gram_probability(window)\n            log_prob_sequence.append(log_prob_window)\n        \n        # Return the log probability of the entire sequence\n        return round(sum(log_prob_sequence), 4)\n\n# Example usage:\ncorpus = [\"the\", \"quick\", \"brown\", \"fox\"]\nsequence = [\"the\", \"quick\", \"brown\", \"fox\", \"lazy\"]\nprint(unsmoothed_ngram_log_prob(corpus, sequence, 1))  # Should print 6.6388\nprint(unsmoothed_ngram_log_prob(corpus, sequence, 2))  # Should print 6.6388 (if all words appear once)\nprint(unsmoothed_ngram_log_prob(corpus, sequence, 3))  # Should print 6.6388 (if all words appear once)\nprint(unsmoothed_ngram_log_prob(corpus, sequence, 4))  # Should return negative infinity if any word appears only once\n```"}
{"task_id": 62, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs univariate Lasso regression using coordinate descent on a polynomially expanded dataset. Below is the complete Python code snippet that accomplishes this:\n\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n    \n    # Convert inputs to NumPy arrays if they aren't already\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Compute the number of features including the intercept\n    num_features = degree + 1\n    \n    # Create the design matrix with ones for the intercept\n    X_design = np.ones((len(X), num_features))\n    \n    # Iterate over each coefficient\n    for i in range(1, num_features):\n        \n        # Calculate the gradient component\n        grad_coefficient = reg_factor\n        \n        # Compute the partial derivative of the objective function w.r.t. this coefficient\n        for j in range(len(X)):\n            Xi = X[j]\n            Y = y[j]\n            if i == 1:\n                xi_pow = Xi\n            else:\n                xi_pow = Xi ** (i - 1)\n            \n            coeff_gradient_term = xi_pow * (Y - (Xi @ X_design[0:i]) - X_design[i])\n            \n            # Update the bias term\n            beta_new = X_design[(i - 1):i].mean()\n            beta_old = X_design[:i-1].mean()\n            \n            if abs(coeff_gradient_term) > beta_old * grad_coefficient:\n                theta = coeff_gradient_term / beta_old\n                alpha = -(theta * X[j, 0:i] @ X[j, 0:i]).prod() / beta_old\n                \n                grad_coefficient += (alpha - reg_factor)\n                \n                # Adjust the value based on whether we're within the step size threshold\n                if abs(alpha) <= tol * (X[j, 0:i] @ X[j, 0:i])[0]:\n                    break\n            \n            X_design[j, 0:i] -= alpha * X_design[j, i:]\n            \n            # Update the coefficients for the current iteration\n            X_design[j, i:] = X_design[j, i:] - theta * X_design[j, 0:i]\n    \n    \n    # Scale the coefficients for readability\n    scaled_coefs = [round(coef, 4) for coef in X_design.mean(axis=0)]\n    \n    return scaled_coefs\n\n# Example usage:\n# Assuming X and y are loaded correctly from some data source\n# result = lasso_regression(X, y, degree=2, reg_factor=0.1, n_iterations=1000, tol=1e-6)\n\nprint(result)\n```\n\nThis code defines the `lasso_regression` function according to the problem description. It builds the polynomial design matrix, iteratively updates the coefficients using coordinate descent with regularization, and ensures convergence using early stopping criteria. Finally, it returns the coefficients as specified. Note that you would need to load your actual data before calling this function."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef forward_probabilities(A, B, obs, T):\n    \"\"\"\n    Calculate the forward probabilities P(x_1:t | y_1:t).\n    \n    Args:\n    - A (list[list[float]]): Transition probability matrix of shape (N, N).\n    - B (list[list[float]]): Emission probability matrix of shape (N, M).\n    - obs (list[int]): Observation sequence of length T.\n    - T (int): Length of the observation sequence.\n    \n    Returns:\n    - list[float]: Array of forward probabilities for each state.\n    \"\"\"\n    num_states = len(B)\n    num_symbols = len(B[0])\n    \n    F = [[0] * num_states for _ in range(T)]\n    F[0] = [B[i][obs[0]] for i in range(num_states)]  # Initial probabilities\n    \n    for t in range(1, T):\n        for s in range(num_states):\n            total_prob = sum([F[t-1][i] * A[i][s] * B[s][obs[t]] for i in range(num_states)])\n            F[t][s] = total_prob\n            \n    return F[T-1]\n\ndef backward_probabilities(F, B, obs, T):\n    \"\"\"\n    Calculate the backward probabilities beta_t(j), representing the likelihood of observing j at time t.\n    \n    Args:\n    - F (list[list[float]]): Forward probabilities array of shape (T, num_states).\n    - B (list[list[float]]): Emission probability matrix of shape (num_states, num_symbols).\n    - obs (list[int]): Observation sequence of length T.\n    - T (int): Length of the observation sequence.\n    \n    Returns:\n    - list[float]: List of backward probabilities for each state.\n    \"\"\"\n    num_states = len(B)\n    num_symbols = len(B[0])\n    \n    B_prime = []\n    for t in range(T):\n        if t == T - 1:\n            B_prime.append(1)\n        else:\n            prob = sum([B[i][obs[t + 1]] * F[T - 1][i] for i in range(num_states)])\n            B_prime.append(prob)\n            \n    return B_prime[::-1]\n\ndef backward_beta(A, B, obs, t):\n    \"\"\"\n    Compute the backward probabilities vector for a specific time index t.\n    \n    Args:\n    - A (list[list[float]]): Transition probability matrix of shape (N, N).\n    - B (list[list[float]]): Emission probability matrix of shape (N, M).\n    - obs (list[int]): Observation sequence of length T.\n    - t (int): Time index (0 <= t < T).\n    \n    Returns:\n    - list[float]: List of backward probabilities for each state, rounded to 4 decimal places.\n    \"\"\"\n    F = forward_probabilities(A, B, obs, len(obs))\n    B_prime = backward_probabilities(F, B, obs, len(obs))[:len(obs)]\n    backward_probs = [round(float(b), 4) for b in B_prime]\n    return backward_probs\n\n# Example usage\nA = [\n    [0.9576, 0.0424],\n    [0.8699, 0.1301]\n]\nB = [\n    [0.7050, 0.2286],\n    [0.8977, 0.1023]\n]\nobs = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\nresult = backward_beta(A, B, obs, 7)\nprint(result)\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    T = len(obs)\n    \n    # Initialize alpha values for all states at time T-1\n    alphas = [np.zeros_like(pi)]\n    \n    # Compute posterior probabilities for each state at time T-1\n    for _ in range(T - 1):\n        new_alphas = []\n        \n        for i in range(len(pi)):\n            total_prob = pi[i]\n            \n            # Multiply by the current state's emissions and previous state transitions\n            for j in range(len(A)):\n                total_prob += pi[j] * A[j][i] * B[j][obs[_]]\n                \n            new_alphas.append(total_prob)\n        \n        alphas.append(new_alphas)\n    \n    # Extract the final probability vector\n    final_probs = alphas[-1]\n    \n    # Set beta values for the last time step\n    betas = [final_probs.copy()]\n    \n    # Compute beta values for other times steps\n    for t in reversed(range(1, T)):\n        temp_betas = [np.zeros_like(betas[0])]\n        \n        for i in range(len(pi)):\n            beta_sum = 0\n            \n            # Multiply by the next state transitions and current state emissions\n            for j in range(len(B)):\n                beta_sum += final_probs[j] * A[j][i] * B[j][obs[t]]\n            \n            temp_betas.append(beta_sum)\n        \n        temp_betas.reverse()\n        betas.insert(0, temp_betas)\n    \n    # Calculate the product of forward probabilities and backward probabilities\n    log_probability = sum([alpha * beta for alpha, beta in zip(alphas[-1], betas)])\n    \n    return np.exp(log_probability).round(6)\n\n# Example usage and check if it works correctly with provided data points\nA_hmm = [\n    [0.7, 0.3],\n    [0.2, 0.8]\n]\n\nB_hmm = [\n    [[0.5, 0.5]],\n    [[0.4, 0.6]]\n]\n\npi_hmm = [0.9, 0.1]\n\nobservation_sequence = [0, 1, 0]\n\nbackward_prob(A_hmm, B_hmm, pi_hmm, observation_sequence)\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    # Validate inputs using assertions\n    assert len(S) == len(A)\n    assert all(len(row) == len(B) for row in B)\n\n    if None in S:\n        raise ValueError(\"Initial probabilities cannot contain NaN values.\")\n    \n    n = len(S)\n    m = len(B[0])\n    T = len(observations)\n    F = np.zeros((T+1, n))\n\n    # Initialization step: set the first element of F\n    F[1:n+1, :] = S.copy()\n\n    # Forward path calculation\n    for t in range(T):\n        current_probabilities = F[t,:]\n        for s in range(n):\n            next_state_probabilities = A[s,:] * B[:,observations[t]]\n            F[t+1,s] += current_probabilities @ next_state_probabilities\n    \n    # Normalize the final column of F to get the likelihood\n    F[T,:] /= np.sum(F[T,:])\n\n    return round(F[-1,-1], 4)\n\n# Example checks\nprint(forward_algorithm([0., 0.5, 0.5], [[.6, .3, .1],[.1, .3, .6],[.2, .7, .1]], \n                         [[0.9, 0.05, 0.05],\n                          [0.2, 0.8, 0.0],\n                          [0.1, 0.3, 0.6]], [1, 0]))  # Expecting approximately 0.1476\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nimport heapq\n\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    # Step 1: Count the support of every single item and discard infrequent ones\n    item_counts = Counter(itertools.chain.from_iterable(transactions))\n    \n    # Filter out infrequent items\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n    \n    # Step 2: Build one FP-tree (prefix tree)\n    def build_fp_tree(items):\n        stack = [(None, None)]\n        for item in items:\n            stack.append((item, len(stack)))\n        \n        while stack:\n            current_node, parent_count = stack.pop()\n            \n            if parent_count == 0:\n                node_count = 1\n            else:\n                node_count = parent_count + 1\n            \n            item_set_key = tuple(sorted(set(frequent_items.keys()) & set(item)))\n            if item_set_key not in frequent_items:\n                continue\n                \n            new_children = []\n            for child in [node[1] - 1 for node in stack]:\n                left_child_count = child if child > 0 else -child\n                right_child_count = max(0, node_count - child)\n                \n                new_element = ((current_node[0], item_set_key[1]), left_child_count, right_child_count)\n                heapq.heappush(new_children, new_element)\n            \n            for element in new_children:\n                freq = element[1]\n                total = element[2]\n                stack.append(((current_node[0], frequency) + element[0], freq, total))\n            del frequent_items[item_set_key]\n        \n        return frequent_items\n    \n    # Recursive mining conditonal FP-trees to obtain larger item-sets\n    def find_conditional_fp_tree(node, depth=0):\n        if not node:\n            return\n    \n        # Add the node's key as an item in the current item-set\n        current_item_set_key = (node[0], )\n        if current_item_set_key not in frequent_items:\n            return\n        \n        current_frequency = sum([node[1]])\n        if len(current_item_set_key) < depth + 1:\n            return\n        \n        # Check if we have reached the desired depth\n        if len(current_item_set_key) == depth + 1:\n            result.append(list(map(str, current_item_set_key)))\n            frequent_items.pop(tuple(current_item_set_key), None)\n            return\n        \n        # Add the remaining part of the item-set recursively\n        find_conditional_fp_tree(((node[0][1], ), ), depth + 1)\n        \n    # Build the initial FP-tree and get the final frequent item-sets\n    fp_tree = build_fp_tree(transactions)\n    \n    # Find all frequent item-sets\n    result = []\n    for _, _, total in fp_tree.values():\n        for _ in range(total // len(fp_tree)):\n            find_conditional_fp_tree((), 1)\n    \n    return [sorted(i) for i in result]\n\n\n# Example usage:\ntransactions = [[\"milk\", \"bread\"], [\"butter\", \"cheese\"], [\"milk\", \"eggs\"]]\nmin_support = 3\nprint(fp_growth(transactions, min_support))\n```"}
{"task_id": 75, "completion_id": 0, "completion": "Here's a complete implementation of the k-Nearest Neighbour (k-NN) classifier in Python:\n\n```python\nimport numpy as np\n\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n    return np.sqrt(np.sum((np.array(x1) - np.array(x2)) ** 2))\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\"\"\"\n    if k <= 0 or k > len(X_train):\n        return [-1]  # Return -1 if k is invalid\n    \n    distances = []\n    for x in X_train:\n        dist = euclidean_distance(x, X_test[0])\n        for i in range(1, len(X_train)):\n            d = euclidean_distance(x, X_train[i])\n            if d < dist:\n                dist = d\n        distances.append(dist)\n    \n    sorted_indices = np.argsort(distances)\n    neighbors = [y_train[i] for i in sorted_indices[:k]]\n    \n    max_count = max(neighbors.count(i), default=0)\n    return [i for i in range(len(y_train)) if neighbors.count(y_train[i]) == max_count]\n\n# Example usage:\nif __name__ == \"__main__\":\n    X_train = [[1, 2], [2, 3], [4, 6], [5, 7], [8, 9]]\n    y_train = [0, 0, 1, 1, 1]\n    X_test = [[3, 4], [5, 6]]\n    k = 1\n    \n    print(knn_predict(X_train, y_train, X_test, k))\n    # Expected output: [1, 1]\n```\n\nThis script defines the necessary functions to calculate distances using the Euclidean metric and to predict the class labels based on the nearest neighbors. The example usage demonstrates how to use these functions with a small dataset."}
{"task_id": 76, "completion_id": 0, "completion": "Below is the solution to the problem implemented in a Python script:\n\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    if len(y) != len(y_pred) or not isinstance(y[0], np.ndarray) or not isinstance(y_pred[0], np.ndarray):\n        raise ValueError(\"Input arrays must be defined for every sample.\")\n\n    # Convert lists to NumPy arrays for computation\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    \n    # Ensure predictions are within [0, 1] range and sum to 1\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15).astype(np.float32)\n    y /= np.sum(y, axis=-1, keepdims=True)\n    \n    # Calculate categorical cross-entropy loss\n    loss = -(y * np.log(y_pred + 1e-8)).sum(axis=-1)\n    \n    return round(loss.mean(), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define example inputs\n    y = [[0, 1], [1, 0]]\n    y_pred = [[0.2, 0.8], [0.9, 0.1]]\n\n    # Compute and print the result\n    result = cross_entropy_loss(y, y_pred)\n    print(result)\n```\n\nIn this code:\n- The `cross_entropy_loss` function takes two arguments, `y` and `y_pred`, which represent the true labels and predicted probabilities respectively.\n- It first checks if the lengths of `y` and `y_pred` match and raises a ValueError if they don't.\n- Then it converts the provided lists into NumPy arrays for easier manipulation.\n- Next, it clips the predictions to ensure they are between 0 and 1 and renormalizes them to sum up to 1 across columns.\n- After normalization, it computes the logarithm of predicted probabilities for each class.\n- Finally, it calculates the mean of these losses, rounds the result to four decimal places, and returns it."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimal places and converted to a \n        regular Python list. The shape is (1, m).\n    \"\"\"\n    \n    # Initialize the activations matrix \"AL\"\n    AL = [X]  # First layer's activation is just the input\n    \n    # Loop over all layers except the last one\n    for l in range(1, len(parameters) // 2):\n        Z_l = np.dot(parameters[f\"W{l}\"], AL[-1]) + parameters[f\"b{l}\"]\n        A_l = np.maximum(0, Z_l)  # Applying ReLU\n        \n        AL.append(A_l)\n    \n    # Last layer uses the sigmoid activation function\n    Z_L = np.dot(parameters[\"W\" + str(len(parameters) - 1)], AL[-1])\n    A_L = 1 / (1 + np.exp(-Z_L))  # Applying the sigmoid non-linearity\n    \n    AL.append(A_L)\n    \n    # Convert AL to a Python list and round to 4 decimal places\n    AL_rounded = [round(x.item(), 4) for x_list in AL for x in x_list]\n    \n    return AL_rounded\n\n\n# Example usage:\nparameters = {\n    'W1': np.random.randn(2, 5),\n    'b1': np.zeros((2, 1)),\n    'W2': np.random.randn(3, 2),\n    'b2': np.zeros((3, 1))\n}\n\ninput_data = np.random.rand(2, 1)\noutputs = L_model_forward(input_data, parameters)\n\nprint(outputs)\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    # Clip predictions to avoid numerical errors\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    \n    # Compute binary cross-entropy loss for each sample\n    num_samples = Y.shape[1]\n    Y_one_hot = np.eye(2)[Y.reshape(-1)]  # One-hot encoding of Y\n    cross_entropy_loss = -(1 / num_samples) * (\n        np.sum(Y_one_hot * np.log(A2_clipped))\n    )\n    \n    return round(cross_entropy_loss.item(), 6)\n\n# Test cases\nprint(compute_cost(np.array([[0.143, 0.857]]), np.array([1])))\n# Expected output: 0.169264\nprint(compute_cost(np.array([[0.99], [0.01]]) ,np.array([0, 1])))\n# Expected output: 0.073038\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union, Literal\nfrom scipy.stats import special_ortho_group  \n  \ndef epsilon_soft(Q: List[float] | np.ndarray, epsilon: float) -> List[float]: \n    n = len(Q)\n    \n    if isinstance(Q, np.ndarray):\n        Q = Q.tolist()   \n        \n    max_q_index = Q.index(max(Q))\n    base = (1 - epsilon) / n\n    \n    soft_probs = []\n    greedy_prob = 1 - epsilon + (epsilon / n)\n    \n    for q in Q:\n        if q == max(Q): \n            soft_probs.append(greedy_prob)\n        else:\n            soft_probs.append(base)\n            \n    return [round(prob * 10000) / 10000 for prob in soft_probs]\n\nQ = [1.2, 0.3, 1.2, -0.1]\nepsilon = 0.1\nprint(epsilon_soft(Q, epsilon)) \n``` \n\nNote: The solution provided assumes that you're using NumPy for its convenient operations on arrays and SciPy's `special_ortho_group` library for matrix operations which may not be necessary for this specific problem. The example shows how to round each element to 4 decimal places and ensure that the total probability equals 1 within the specified tolerance (1e-12)."}
{"task_id": 86, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Flatten the predictions into a single list of predictions\n    flat_predictions = [item for sublist in predictions for item in sublist]\n    \n    # Count the occurrences of each prediction\n    prediction_counts = Counter(flat_predictions)\n    \n    # Find the maximum occurrence count\n    max_occurrence_count = max(prediction_counts.values())\n    \n    # Initialize the result list\n    aggregated_predictions = []\n    \n    # Iterate over each unique prediction in descending order of its frequency\n    for prediction, count in sorted(prediction_counts.items(), key=lambda x: (-x[1], x[0])):\n        if count == max_occurrence_count:\n            # Append the smallest (in terms of ASCII value) or lexicographically smallest element based on the data type\n            if isinstance(prediction, int):\n                aggregated_predictions.append(min(prediction))\n            elif isinstance(prediction, str):\n                aggregated_predictions.append(min(prediction))\n            else:\n                raise ValueError(\"Unsupported data type\")\n            break\n    \n    return aggregated_predictions\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the predictions made by different decision trees\n    predictions = [\n        [1, 2, 'a'],\n        [3, 4, 'b'],\n        [5, 6, 'c'],\n        [7, 8, 'd']\n    ]\n    \n    # Get the final predictions using the function\n    final_predictions = aggregate_random_forest_votes(predictions)\n    \n    print(final_predictions)\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        return round(np.log(1 + np.exp(-z)), 4)\n    else:\n        # Convert to pure Python list via NumPy.tolist()\n        flat_list = np.vstack([z.flatten()])[0].tolist()\n        return [round(np.log(1 + np.exp(x)), 4) for x in flat_list]\n\n# Example usage\ninput_z = np.array([-1000, 0, 3])\nresult = softplus(input_z)\nprint(result)\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    if isinstance(policy['ev_estimates'], dict):\n        ev_estimates = list(policy['ev_estimates'].values())\n    else:\n        ev_estimates = []\n\n    if len(ev_estimates) == 0:\n        return np.nan\n\n    arm_evs = bandit['arm_evs']\n    errors_squared = [(e - e_hat) ** 2 for e, e_hat in zip(arm_evs, ev_estimates)]\n    mse_value = sum(errors_squared) / len(arm_evs)\n\n    return np.round(mse_value, 4)\n\n# Test cases\nbandit_example_1 = {'arm_evs': [0.5, 0.2, 0.9]}\npolicy_example_1 = {'ev_estimates': {0: 0.4, 1: 0.25, 2: 0.8}}\nprint(mse(bandit_example_1, policy_example_1))  # Expected output: 0.0075\n\nbandit_example_2 = {'arm_evs': [0.5, 0.2, 0.9]}\npolicy_example_2 = {}\nprint(mse(bandit_example_2, policy_example_2))  # Expected output: nan\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\n\n# Helper functions for counting occurrences\ndef count(sequence, context):\n    return len(list(filter(lambda x: x in context, sequence.split())))\n\ndef count_table(sequence, context):\n    table = {}\n    for i in range(len(sequence)):\n        if context[i] not in table:\n            table[context[i]] = []\n        table[context[i]].append(sequence[i])\n    return table\n\ndef get_counts(corpus, N, context_func=count, vocab_size=35887):\n    counts = defaultdict(Counter)\n    ngrams = defaultdict(set)\n\n    def make_ngram(n):\n        def _make_ngram():\n            nonlocal n\n            yield tuple(context_func(s, n - 1))\n            n -= 1\n        return _make_ngram()\n\n    def make_contexts():\n        for sent in corpus:\n            contexts = [tuple(context_func(sent, i)) for i in range(N)]\n            ngrams += {k: set() for k in contexts}\n            counts[frozenset(contexts)].update(sent.split())\n    \n    for i, gram in enumerate(make_ngram(N), 1):\n        for j in range(i + 1, min(i + N, len(counts.keys()) + 1)):\n            assert isinstance(j, int)\n\n            # Counting N-grams up to order N using `count` or `count_table`\n            num_occurrences = count(sentences[j - 1].split(), gram[:j]) \n\n            # Adding additional occurrence if necessary \n            if len(gram) <= N and len(counts[frozenset(gram)]) == 0:\n                counts[frozenset(gram)][sentences[j - 1].strip()] = max(K, num_occurrences)\n            \n            # Updating the context table\n            ngrams[num_occurrences][gram[j - (len(gram) - 1):j]] |= set(word.strip() for word in sentences[j - 1].split())\n\n    return counts, ngrams\n\ndef compute_probability(counts, ngrams, context_word=None):\n    probabilities = {}\n\n    for key, values in counts.items():\n        for value, count_values in values.items():\n            new_count = sum([c + K for c in count_values.values()])\n            probabilities[key + \" \" + value] = {\n                word: (v + K) / (new_count + K * len(values) * vocab_size)\n                for word, v in count_values.items()\n            }\n        \n    if context_word is None:\n        return probabilities\n    else:\n        return probabilities[context_word]\n\ndef additive_smoothed_ngram_log_prob(corpus, sent, K=1.0):\n    counts, ngrams = get_counts(corpus, sent)\n\n    context_value = counts[tuple()]['<UNK>']\n    smoothed_counts = {}\n\n    # Creating the smoothed counts dictionary\n    for gram, grams_and_counts in ngrams.items():  \n        smoothed_counts.update({gram: {word: (counts.get(tuple(gram)[:j])['<UNK>'] + K) / (sum(map(lambda l: l[K], grams_and_counts.values())) + K * len(grams_and_counts) * vocab_size) \n                                      for j in range(len(gram)+1)} for gram, grams_and_counts in grams_and_counts.items()})\n        \n    smooth_sent = sent.replace(\"<\", \"\").replace(\">\", \"\")\n    sent_value = compute_probability(smoothed_counts, counts, smooth_sent)\n    \n    return round(math.log(sum(sent_value[sent])) / len(sent.split()), 4)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    corpus = [\"the quick brown fox jumps over the lazy dog\",\n              \"the cat sat on the mat\",\n              \"The quick brown fox\"]\n    sequence = 'the quick brown fox'\n    print(additive_smoothed_ngram_log_prob(corpus, sequence))\n    ```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    # Initialize the W and H matrices with zeros\n    W = np.random.rand(X.shape[0], K)\n    H = np.random.rand(K, X.shape[1])\n    \n    for _ in range(max_iter):\n        # Compute the current regularized reconstruction error\n        current_loss = np.sum(np.square(np.dot(W, H.T) - X)) + alpha * (np.trace(np.dot(W.T, W)) + np.trace(np.dot(H, H)))\n        \n        if current_loss < tol:\n            break\n        \n        # Update W based on H: solve a regularized least squares problem\n        W_next = np.linalg.inv((alpha * np.eye(K) + np.dot(H.T, H))) @ (np.dot(H.T, X))\n        \n        # Update H based on W: solve a regularized least squares problem\n        H_next = np.linalg.inv((alpha * np.eye(K) + np.dot(W.T, W))) @ (np.dot(W.T, X))\n    \n    # Round the results to 4 decimal places and convert to a list of lists\n    W_reconstructed = W_next.round(4)\n    H_reconstructed = H_next.round(4)\n    Xhat = np.dot(W_reconstructed, H_reconstructed.T).round(4)\n    return [list(row) for row in Xhat.tolist()]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample matrix X\n    N, M = 5, 7\n    K = 3\n    X = np.random.rand(N, M)\n    \n    # Factorize X using ALS\n    reconstructed_matrix = als_factorization(X, K)\n    \n    # Print the result\n    print(reconstructed_matrix)\n```"}
{"task_id": 109, "completion_id": 0, "completion": "Here is the implementation of the K-Means++ clustering algorithm in Python:\n\n```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[np.ndarray]:\n    \"\"\"\n    Performs K-Means clustering using K-Means++ initialization.\n\n    Parameters:\n    X (np.ndarray): Input data matrix of shape (m, n).\n    K (int): Number of clusters.\n    max_iter (int): Maximum number of iterations for Lloyd's refinement loop.\n                   Default is 100.\n    random_state (int | None): Seed for the random number generators. \n                               If None, no seed will be set.\n\n    Returns:\n    list[np.ndarray]: List of K centroids rounded to 4 decimal places,\n                      sorted by their first coordinate.\n    \"\"\"\n    \n    if random_state is not None:\n        rng = np.random.RandomState(random_state)\n    else:\n        rng = np.random.default_rng()\n\n    # Initialize centroids randomly but ensure they are distinct\n    centroids = []\n    while len(centroids) < K:\n        current_sample = rng.choice(X, size=1, replace=False)[0]\n        if all(np.linalg.norm(current_sample - c) > 1e-6 for c in centroids):\n            centroids.append(current_sample)\n\n    # Initialize centroids using K-Means++\n    def init_centroids(x, indices, prev_indices, centroids):\n        dists = cdist(x, centroids[indices], metric='euclidean')\n        p = np.sum(dists**2, axis=0)**(-0.5)\n        probs = p / np.sum(p)\n        next_index = rng.choice(indices.shape[0], p=probs)\n        return x[indices[next_index]], next_index\n\n    for _ in range(K):\n        centroids, idx = init_centroids(*init_centroids(rng.choice(centroids), [i for i in range(len(centroids))]))\n        \n        # Check if any sample was assigned to the new centroid outside the loop\n        if any(idx != idx[0]):\n            raise ValueError(\"Newly chosen centroid caused samples to move.\")\n        \n        centroids.sort(key=lambda c: c[0])\n\n    return [[round(c[0], 4)] + [round(c[i], 4) for i in range(1, len(c))] for c in centroids]\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\nK = 2\nprint(kmeans(X, K))\n```\n\n### Explanation:\n1. **Initialization**: We initialize centroids randomly but ensure that they are distinct from each other using `all(np.linalg.norm(current_sample - c) > 1e-6 for c in centroids)`.\n\n2. **K-Means++ Initialization**: This section uses the K-Means++ method to choose centroids. It selects the point farthest from all existing centroids based on Euclidean distance. This helps to avoid choosing centroids too close together.\n\n3. **Assignment Step**: In this step, we assign each sample to the nearest centroid according to the squared Euclidean distances.\n\n4. **Update Step**: After updating the centroids, we check if there were any changes in the assignment. If so, we perform another iteration; otherwise, the process stops.\n\n5. **Output**: The function returns the final centroids, rounding them to four decimal places and sorting them by their first coordinate.\n\nThis implementation ensures that the clustering process is reproducible and follows the K-Means++ initialization strategy."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a raw text string into a fixed-length list of integer token IDs.\"\"\"\n    \n    indices = []\n    for char in text:\n        if char not in vocabulary:\n            index = 0\n        else:\n            index = vocabulary[char]\n        \n        # Append zero-padding if necessary\n        if len(indices) == TIME_STEPS - 1:\n            break\n        \n        indices.append(index)\n    \n    return indices[:time_steps]\n\ndef int_to_string(indices: list[int], inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Reconstructs the original string from a list of integer IDs.\"\"\"\n    \n    characters = [None] * len(indices)\n    for i, idx in enumerate(indices):\n        if idx != 0:\n            characters[i] = inverse_vocab[idx]\n    \n    return ''.join(characters)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes the raw text, runs the model on it, and decodes the result.\"\"\"\n    \n    encoded_text = string_to_int(text, TIME_STEPS, input_vocabulary)\n    \n    batch_size = 1\n    predictions = model.predict(np.array([encoded_text]))[0]\n    predicted_ids = np.argmax(predictions, axis=-1)\n    \n    decoded_prediction = int_to_string(predicted_ids[-1], inv_output_vocabulary)\n    \n    return decoded_prediction\n\n\n# Example usage:\ninput_vocabulary = {'a': 1, 'b': 2}\ninv_output_vocabulary = {1: 'a', 2: 'b'}\ntext = \"Hello world\"\n\ndecoded_result = run_example(model, input_vocabulary, inv_output_vocabulary, text)\nprint(decoded_result)\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport sys\n\ndef run_example(model, input_vocabulary, inv_output_vocabulary, example):\n    \"\"\"Helper function to run a model on a single input example.\"\"\"\n    prediction = model(example)\n    return [inv_output_vocabulary[i] for i in range(len(prediction))]\n\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    if examples is None:\n        examples = EXAMPLES  # Use a global constant if none provided\n    \n    results = []\n    for example in examples:\n        result = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        results.append(result)\n    \n    return results\n\n# Assuming these variables are defined somewhere else in the script or environment\nINPUT_VOCABULARY = {'A': 0, 'B': 1, 'C': 2}\nINV_OUTPUT_VOCABULARY = {0: 'A', 1: 'B', 2: 'C'}\nEXAMPLES = ['AB', 'BC', 'CA']  # Placeholder for actual data\n\n# Call the main function with the provided model, vocabulary, and examples\ndef main():\n    MODEL = lambda x: str(x)  # Dummy model function for demonstration purposes\n    print(run_examples(MODEL, INPUT_VOCABULARY, INV_OUTPUT_VOCABULARY, examples=EXAMPLES))\n\nif __name__ == \"__main__\":\n    main()\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_binary_cross_entropy_loss_and_gradient(X, y, w):\n    if len(y.shape) == 1:\n        y = np.expand_dims(y, axis=-1)\n\n    num_samples = X.shape[0]\n    z = X @ w\n    p = 1 / (1 + np.exp(-z))\n    \n    epsilon = 1e-20\n    \n    loss = -np.mean(np.sum((y * np.log(p + epsilon)) + ((1 - y) * np.log(1 - p + epsilon)), axis=0))\n\n    gradient = -(X.T@(p - y))/num_samples\n\n    return round(loss, 4), [list(i.round(4).astype(int).tolist()) for i in gradient]\n\n# Example usage:\nX_train = np.array([[1, 0], [0, 1]])\ny_train = np.array([1, 0])\nw_train = np.array([1, 1])\n\nloss, gradient = compute_binary_cross_entropy_loss_and_gradient(X_train, y_train, w_train)\nprint(f\"Loss: {loss}\")\nprint(f\"Gradient: {gradient}\")\n\n# Expected output:\n# Loss: 0.5992\n# Gradient: [[1.0000] [0.9998]]\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef majority_vote(weights):\n    \"\"\"Given weights, return the most common value\"\"\"\n    sorted_weights = np.sort(weights)[::-1]\n    threshold = sum(sorted_weights[:len(sorted_weights)//2])\n    if np.sum(sorted_weights <= threshold) == len(sorted_weights)//2 + 1:\n        return 1\n    else:\n        return -1\n\ndef ada_boost_classify(x, classes, w, model_weights, alpha, boost_model):\n    if not isinstance(alpha, (int, float)) or alpha < 0 or alpha > 1:\n        raise ValueError(\"Alpha must be between 0 and 1.\")\n    \n    # Update the weights based on the predictions and model's outputs\n    new_w = w * (np.sign(enhanced_output)*model_weights)\n    boosted_output.append(new_w)\n    \n    # Aggregate the vote with its corresponding weight\n    final_output.append(np.where(new_w.argmax(axis=-1) == 1)[0])\n\n    # Compute the mean prediction\n    pred = np.mean(final_output[-boosted_model.n_estimators:], axis=-1)\n\n    # If number of models used is less than the required models,\n    # use the first few models available\n    if len(model_weights[0]) < max(n_models_used, boosted_model.n_estimators):\n        model_weights = np.concatenate((model_weights[0], model_weights[1]), axis=0)\n        w = np.concatenate((w[0], w[1]))\n        classes = np.concatenate((classes[0], classes[1]))\n\n    # Check if all the weights have converged\n    if np.allclose(pred, x):\n        break\n\n    return [pred]\n\ndef adaboost_with_decision_stumps(X_train, y_train, X_test, n_clf):\n    # Initialize necessary arrays and variables\n    weights = np.ones(len(y_train), dtype=np.float64) / len(y_train)\n    model_weights = []\n    predicted_labels_X_test = []\n\n    # Initialize the AdaBoost model\n    for t in range(n_clf):\n        clf = DecisionStump()\n        \n        # Fit the Decision Stump classifier\n        clf.fit(X_train[:, :t+1], y_train, sample_weight=weights)\n        \n        # Evaluate the model and update weights\n        model_weights.append(clf.get_weight())\n        enhanced_output = clf.predict(X_train[:, :t+1])\n        model_error = np.average(weights, weights=(1-enhanced_output))\n        \n        # Determine which stump performs better and update weights\n        if model_error > 0.5:\n            predicted_labels_X_test.append(-1)\n            weights *= (1 - model_error)\n        else:\n            predicted_labels_X_test.append(1)\n            weights *= (model_error)\n            \n    return predicted_labels_X_test\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy.linalg as linalg\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components >= X.shape[1]:\n        return []\n\n    n_samples, _ = X.shape\n    _, n_classes = np.unique(y, return_counts=True)\n\n    def calculate_scatter_matrix():\n        mu = np.mean(X, axis=0)\n        total_samples = sum(n_classes[c] for c in range(n_classes))\n        within_classes = []\n        \n        for i in range(n_classes):\n            class_data = X[y == i]\n            n_samples_i = len(class_data) / float(total_samples)\n            \n            centroid = np.mean(class_data, axis=0)\n            class_scatter = n_samples_i * ((class_data - centroid) ** 2).mean(axis=0)\n            within_classes.append(class_scatter)\n        \n        S_W = np.sum(within_classes) / total_samples\n        S_B = np.dot((mu - np.mean(X)).T, (mu - np.mean(X))) / total_samples\n        return S_W, S_B\n    \n    S_W, S_B = calculate_scatter_matrix()\n    \n    A = np.dot(np.linalg.inv(S_W), S_B)\n    \n    eigenvalues, eigenvectors = np.linalg.eig(A)\n    sorted_indices = np.argsort(eigenvalues)[::-1][:n_components]\n    principal_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    transformed_data = np.dot(principal_eigenvectors.T, X.T).T.round(4)\n    return transformed_data.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1,1],[1,2],[2,1],[2,2],[8,8],[9,8],[8,9],[9,9]])\n    y = np.array([0,0,0,0,1,1,1,1])\n    print(multi_class_lda(X, y, 1))\n```"}
{"task_id": 140, "completion_id": 0, "completion": "```python\nfrom collections import deque\nimport math\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    def bfs():\n        q = deque([(0, 0)])\n        visited = set()\n        \n        min_time = float('inf')\n        \n        while q:\n            x, y = q.popleft()\n            \n            if (x, y) == (len(grid)-1, len(grid[0])-1):\n                min_time = min(min_time, step)\n                \n            # Explore all possible directions\n            for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n                nx, ny = x + dx, y + dy\n                \n                if 0 <= nx < len(grid) and 0 <= ny < len(grid[0]):\n                    if grid[nx][ny] != '#' and (nx, ny) not in visited:\n                        visited.add((nx, ny))\n                        \n                        # Check if current position is on fire\n                        if grid[nx][ny].lower() == 'f':\n                            fire_count[nx][ny] += 1\n                        \n                        next_step = max(step, fire_count[nx][ny])\n                        visited.remove((nx, ny))\n                        \n                        q.append((nx, ny))\n                    \n                    elif fire_count[x][y] > 0 and (nx, ny) not in visited:\n                        visited.add((nx, ny))\n                        \n                        # Update the count of remaining fires\n                        fire_count[nx][ny] -= 1\n                    \n                        next_step = min(next_step, step)\n                        visited.remove((nx, ny))\n\n            step += 1\n        \n        return min_time\n    \n    # Initialize variables\n    step = 0\n    fire_count = [[0]*len(grid[0]) for _ in range(len(grid))]\n    \n    # Find the starting positions of flames\n    for i in range(len(grid)):\n        for j in range(len(grid[i])):\n            if grid[i][j] == 'F':\n                fire_count[i][j] = 1\n                \n    return bfs()\n\n# Example usage\nif __name__ == \"__main__\":\n    example_grid = [\n        \"....................\",\n        \"#......#...............\",\n        \"..###...###...........\",\n        \".....#.#.......#......\",\n        \"...##..######........\",\n        \".#####..#.........#.##\",\n        \"..####...#..........#\",\n        \"......#..............\",\n        \"....................\"\n    ]\n    print(escape_fire_maze(example_grid))  # Expected output depends on the specific grid configuration\n```"}
{"task_id": 141, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom sklearn.base import BaseClassifierMixin\n\nclass KNeighborsClassifier(BaseClassifierMixin):\n    def __init__(self, n_neighbors=5, weights='uniform', algorithm='auto'):\n        self.n_neighbors = n_neighbors\n        self.weights = weights\n        self.algorithm = algorithm\n    \n    def fit(self, X_train, y_train):\n        self.X_train_ = X_train\n        self.y_train_ = y_train\n        return self\n    \n    def predict(self, X_test):\n        distances = np.full((len(X_test),), np.inf)\n        \n        for i, x_test in enumerate(X_test):\n            if isinstance(metric, str):\n                # Convert input to list if it's not already one\n                metric_func = getattr(np.linalg.norm, metric)\n                \n                dists = [metric_func(x_test, x_train) for x_train in self.X_train_]\n            else:\n                dists = metric(x_test, self.X_train_)\n            \n            indices = np.argsort(dists)[:self.n_neighbors]\n            predictions = np.bincount(self.y_train_[indices]).argmax()\n            \n            distances[i] = dists[indices][:self.n_neighbors].min()\n\n        labels = np.zeros(len(X_test))\n        for i in range(labels.shape[0]):\n            labels[i] = np.argmax(np.bincount(distances[i]))\n        return labels\n\n\n# Test the implementation\nif __name__ == \"__main__\":\n    from sklearn.datasets import load_breast_cancer\n    from sklearn.model_selection import train_test_split\n\n    X, y = load_breast_cancer(return_X_y=True)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\n    knn.fit(X_train, y_train)\n\n    print(knn.predict(X_test))\n```"}
{"task_id": 146, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\n\n\ndef euclidean_distance(x1, x2):\n    return np.linalg.norm(x1 - x2)\n\n\ndef manhattan_distance(x1, x2):\n    return np.sum(np.abs(x1 - x2))\n\n\ndef cosine_similarity(x1, x2):\n    return 1 - np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n\n\ndef knn_predict(X_train: np.ndarray, y_train: np.ndarray,\n                X_test: np.ndarray, k: int = 3, metric: str = 'euclidean') -> List[int]:\n    predictions = []\n    \n    def get_k_nearest_neighbors(distances, indices):\n        # Sort indices based on distances ascendingly\n        sorted_indices = np.argsort(distances)\n        \n        # Take the first `k` neighbors\n        if len(sorted_indices) > k:\n            nearest_indices = sorted_indices[:k]\n        else:\n            nearest_indices = sorted_indices\n        \n        # Get the corresponding labels and predict\n        predicted_labels = [y_train[i] for i in nearest_indices]\n        most_common_label = max(set(predicted_labels), key=predicted_labels.count)\n        \n        return most_common_label\n    \n    for test_sample in X_test:\n        # Compute distances to all training samples\n        if metric == 'euclidean':\n            distances = np.array([euclidean_distance(test_sample, train_sample) for train_sample in X_train])\n        elif metric == 'manhattan':\n            distances = np.array([manhattan_distance(test_sample, train_sample) for train_sample in X_train])\n        elif metric == 'cosine':\n            distances = np.array([cosine_similarity(test_sample, train_sample) for train_sample in X_train])\n            \n        # Get the `k` nearest neighbors' indices\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Predict the label for this test sample\n        prediction = get_k_nearest_neighbors(distances[nearest_indices], nearest_indices)\n        predictions.append(prediction)\n    \n    return predictions.tolist()\n\n\n# Example usage:\n\n# Generating random data for demonstration purposes\nnp.random.seed(42)\nX_train = np.random.rand(50, 2)\ny_train = np.random.choice(['A', 'B', 'C'], 50)\n\nX_test = np.random.rand(6, 2)\nprint(knn_predict(X_train, y_train, X_test))\n```"}
{"task_id": 155, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict):\n    \"\"\"\n    Perform the forward pass of a two-hidden-layer actor network.\n    \n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n    \n    Args:\n        state (list[float]): State representation as a list or numpy array.\n        weights (dict): Dictionary containing the model's weights and biases.\n        \n    Returns:\n        list[float]: Action vector with each element rounded to four decimal places.\n    \"\"\"\n    # Convert state to NumPy array if it's not already one\n    state = np.array(state)\n    \n    # Apply the linear transformation for the first hidden layer\n    h1 = np.dot(weights['W1'], state) + weights['b1']\n    \n    # Apply ReLU activation to the result from the first hidden layer\n    h1_relu = np.maximum(h1, 0)\n    \n    # Apply the linear transformation for the second hidden layer\n    h2 = np.dot(weights['W2'], h1_relu) + weights['b2']\n    \n    # Apply ReLU activation to the result from the second hidden layer\n    h2_relu = np.maximum(h2, 0)\n    \n    # Apply the linear transformation for the output layer\n    action = np.dot(weights['W3'], h2_relu) + weights['b3']\n    \n    # Return the action as a list with elements rounded to four decimal places\n    return [round(act, 4) for act in np.round(action, 4)]\n\n# Example usage\nweights_example = {\n    'W1': np.random.randn(4, 5),\n    'b1': np.random.randn(5,),\n    'W2': np.random.randn(5, 7),\n    'b2': np.random.randn(7,),\n    'W3': np.random.randn(7, 2),\n    'b3': np.random.randn(2,)\n}\n\nexample_state = [2.0, -1.0, 3.0, 4.0]\nprint(actor_forward(example_state, weights_example))\n```"}
{"task_id": 160, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef soft_thresholding(x, tau):\n    \"\"\"\n    Compute the soft thresholding operator.\n    \n    Parameters:\n    - x: Input scalar value\n    - tau: Threshold value\n    \n    Returns:\n    - Value after applying soft thresholding operation\n    \"\"\"\n    sign_x = np.sign(x)\n    abs_x = np.abs(x)\n    thresholded_value = sign_x * (abs_x - tau)\n    return thresholded_value\n\ndef compute_residuals(X, y, w):\n    \"\"\"\n    Calculate residuals from predictions made by given weights.\n    \n    Parameters:\n    - X: Array-like object of shape (m, n)\n    - y: Target values of shape (m,)\n    - w: Weights (vector)\n\n    Returns:\n    - Residuals array of shape (m,)\n    \"\"\"\n    Xw = np.dot(X, w)\n    residuals = y - Xw\n    return residuals\n\ndef lasso_coordinate_descent(X, y, lambda_param, max_iters, fit_intercept=False):\n    \"\"\"\n    Perform Lasso regression using coordinate descent method.\n\n    Parameters:\n    - X: Design matrix (n_samples, n_features)\n    - y: Target vector (n_samples,)\n    - lambda_param: regularization parameter\n    - max_iters: Maximum number of iterations\n    - fit_intercept: Flag indicating whether to include an intercept term\n\n    Returns:\n    - Final weights vector (length n_features)\n    - Intercept value (scalar)\n    \"\"\"\n    if fit_intercept:\n        n_samples, n_features = X.shape\n        X_with_bias = np.column_stack([np.ones(n_samples), X])\n    else:\n        X_with_bias = X\n\n    n_features = X_with_bias.shape[1]\n    weights = [0] * n_features\n    intercept = 0.0\n\n    for iteration in range(max_iters):\n        converged = True\n        \n        for j in range(n_features):\n            rj = compute_residuals(X_with_bias, y, weights)[j]\n\n            if not fit_intercept:\n                rj -= intercept\n\n            partial_resid = rj / X_with_bias[:, j].mean()\n\n            if partial_resid > 0:\n                weights[j] *= -1\n            elif partial_resid < 0:\n                weights[j] *= 1\n            \n            partial_weights = soft_thresholding(partial_resid, lambda_param * n_samples)\n\n            weights[j] += partial_weights\n            intercept += partial_weights\n\n            if np.linalg.norm(weights) == 0:\n                converged = False\n\n            if converged:\n                break\n\n    if fit_intercept:\n        intercept /= X_with_bias[:, 0].mean()\n    return weights, round(intercept, 4)\n\n\n# Example usage:\nm = 100\nn = 50\nX = np.random.rand(m, n)\ny = np.random.rand(m)\nreg_params = {'lambda_param': 1.0, 'max_iters': 100}\nweights_list, bias_float = lasso_coordinate_descent(X, y, **reg_params)\nprint(\"Final weights:\", weights_list)\nprint(\"Bias:\", bias_float)\n```"}
{"task_id": 165, "completion_id": 0, "completion": "Below is my solution for the task:\n\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | np.ndarray, y: list[int] | np.ndarray) -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    len_x, len_y = len(x), len(y)\n    \n    # Check if the vectors have the same length\n    if len_x != len_y:\n        return -1\n    \n    # Calculate the difference vector\n    diff_vec = np.subtract(x, y)\n    \n    # Count the number of differences\n    num_diffs = np.sum(np.abs(diff_vec))\n    \n    # Calculate the normalized Hamming distance\n    normalized_hamming_dist = num_diffs / max(len_x, len_y)\n    \n    # Round to 4 decimal places\n    return round(normalized_hamming_dist, 4)\n\n# Example usage:\nprint(hamming_distance([1, 2, 3], [1, 2, 3]))  # Should output 0.0\nprint(hamming_distance([1, 2, 3], [1, 5, 3]))  # Should output 0.8333\nprint(hamming_distance([1, 2, 3], [4, 5, 6]))  # Should output -1\n```\n\nThis Python script defines a function named `hamming_distance` that calculates the normalized Hamming distance between two equal-length integer vectors according to the provided specifications. It first checks if the input vectors have the same length; if they don't, it returns `-1`. Then, it computes the difference between the vectors element-wise, counts the number of differences, normalizes this count relative to the total number of elements in the shorter vector, and rounds the result to four decimal places before returning it."}
{"task_id": 169, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming(N):\n    return [round(0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1)), 4) for n in range(N)]\n\ndef hann(N):\n    return [round(0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1)), 4) for n in range(N)]\n\ndef blackman_harris(N):\n    a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n    return [round(a0 - a1 * np.cos(2 * np.pi * n / (N - 1)) + \n                  a2 * np.cos(4 * np.pi * n / (N - 1)) - \n                  a3 * np.cos(6 * np.pi * n / (N - 1)), 4) for n in range(N)]\n\ndef generalized_cosine(N, coefficients):\n    k_values = range(N)\n    cosine_series = sum([coeff * np.cos((2 * np.pi * k / (N - 1))) for coeff in coefficients for k in k_values])\n    return [round(float(coef), 4) for coef in coefficients]\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    if window not in [\"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\"]:\n        raise ValueError(\"Unknown window name\")\n\n    result = []\n    if window == \"hamming\":\n        result.extend(hamming(N))\n    elif window == \"hann\":\n        result.extend(hann(N))\n    elif window == \"blackman_harris\":\n        result.extend(blackman_harris(N))\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"Missing coefficients for generalized cosine window\")\n        result.extend(coefficients)\n\n    return round(np.array(result).astype(float), 4)\n\n# Example usage\nprint(generate_window(\"hamming\", 5))   # Should print [0.5400, 0.0800, 0.0800, 0.5400, 0.0800]\nprint(generate_window(\"hann\", 3))      # Should print [0.5000, 0.0000, 0.5000]\nprint(generate_window(\"blackman_harris\", 4))  # Should print [0.3587, -0.4883, 0.1413, -0.0117]\nprint(generate_window(\"generalized_cosine\", 6, [0.2, -0.3]))  # Should print [-0.2000, -0.3000, 0.3000, 0.2000]\n```"}
{"task_id": 171, "completion_id": 0, "completion": "Here's the implementation of the binary Logistic Regression classifier from scratch using mini-batch Gradient Descent:\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef logistic_regression_train_predict(X_train, y_train, X_test, epochs=5000, learning_rate=0.1, batch_size=32):\n    n_samples, n_features = np.shape(X_train)\n    assert np.all(np.unique(y_train) == [-1, 1]), \"Labels must be {-1, +1}\"\n    \n    Y_train = np.array([np.sign(x) * 2 - 1 for x in y_train])\n    X_train_b = np.c_[np.ones((n_samples, 1)), X_train]\n\n    W = np.zeros(n_features + 1)\n    b = 0\n    \n    losses = []\n    \n    for epoch in range(epochs):\n        indices = np.random.choice(n_samples, batch_size)\n        \n        if len(indices) < batch_size or np.sum(Y_train[indices]) == n_samples:\n            continue\n        \n        X_batch = X_train_b[indices]\n        y_batch = Y_train[indices]\n        \n        z = np.dot(X_batch, W) + b\n        h = sigmoid(z)\n        L = -(np.sum(y_batch * np.log(h) + (1 - y_batch) * np.log(1 - h)))\n        \n        grad_z = np.dot(X_batch.T, (h - y_batch)) / batch_size\n        grad_W = np.dot(X_batch.T, h * (1 - h))\n        grad_b = np.mean(h - y_batch)\n        \n        W -= learning_rate * grad_W\n        b -= learning_rate * grad_b\n        \n        losses.append(L)\n    \n    pred_probs = sigmoid(np.dot(X_test, W) + b)\n    pred_labels = [1 if p > 0.5 else 0 for p in pred_probs]\n    \n    return pred_labels.tolist()\n\n# Example usage:\nX_train = [[0], [1], [2], [3]]\ny_train = [-1, 1, 1, -1]\nX_test = [[0.99], [2.1]]  # Test points to predict labels\npred_labels = logistic_regression_train_predict(X_train, y_train, X_test)\n\nprint(pred_labels)\n```\n\nThis code defines the necessary functions to perform logistic regression using mini-batch gradient descent. The example usage shows how to train a classifier on a small dataset and make predictions on new test points. Note that this implementation assumes that the inputs are normalized such that their maximum absolute value does not exceed 1, which might need adjustment depending on specific requirements."}
{"task_id": 176, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass AdaboostClassifier:\n    def __init__(self, n_estimators):\n        self.n_estimators = n_estimators\n        self.models = []\n    \n    def fit(self, X_train, y_train):\n        errors = []\n        alpha_values = []\n        \n        # Initialize weights, where each point has equal importance\n        w = np.ones(len(y_train)) / len(y_train)\n        \n        # Fit models one at a time\n        for _ in range(self.n_estimators):\n            model = self._fit_one_model(w, X_train, y_train)\n            \n            # Calculate the weighted error\n            error = np.sum(np.sign(model.predict(X_train)) != y_train)\n            errors.append(error)\n            \n            # Update weights based on the model's performance\n            alpha = 0.5 * np.log((1 - error) / max(1E-10, error))\n            alpha_values.append(alpha)\n            \n            # Normalize weights\n            w *= np.exp(-alpha * y_train * model.predict(X_train))\n            w /= np.sum(w)\n            \n            # Append the current model for future predictions\n            self.models.append(model)\n    \n    def _fit_one_model(self, w, X_train, y_train):\n        best_error = None\n        best_model = None\n        \n        # Try all possible decisions splits\n        for feature_idx in range(len(X_train[0])):\n            thresholds = sorted(set([x[feature_idx] for x in X_train]))\n            for threshold in thresholds:\n                for polarity in [-1, 1]:\n                    if polarity == 1:\n                        cutoff = \"less\"\n                    else:\n                        cutoff = \"greater\"\n                    \n                    # Create new model\n                    model = DecisionStump(feature_idx=feature_idx, \n                                           threshold=threshold, \n                                           polarity=polarity, \n                                           w=w, \n                                           X_train=X_train, \n                                           y_train=y_train)\n                    \n                    # Evaluate error rate\n                    pred_y = model.predict(X_train)\n                    error = np.mean(pred_y != y_train)\n                    \n                    # Choose best split and break ties\n                    if best_error is None or error < best_error:\n                        best_error = error\n                        best_model = model\n                    \n        return best_model\n    \n    def predict(self, X_test):\n        total_predictions = np.zeros(len(X_test))\n        \n        # Apply model to each instance\n        for i, model in enumerate(self.models):\n            pred_y = model.predict(X_test)\n            total_predictions += pred_y\n            \n        # Convert to class labels\n        return np.sign(total_predictions).astype(int)\n\nclass DecisionStump:\n    def __init__(self, feature_index, threshold, polarity, w, X_train, y_train):\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.polarity = polarity\n        self.w = w\n        self.X_train = X_train\n        self.y_train = y_train\n    \n    def predict(self, X):\n        if X[self.feature_index] > self.threshold:\n            return np.array([-1])\n        else:\n            return np.array([1])\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=10):\n    classifier = AdaboostClassifier(n_estimators)\n    classifier.fit(X_train, y_train)\n    return classifier.predict(X_test)\n\n# Example usage:\n# X_train = [[1, 0], [0, 1], [1, 1]]\n# y_train = [1, 1, 0]\n# X_test = [[1, 0]]\n# predictions = adaboost_predict(X_train, y_train, X_test)\n# print(predictions) # Should print [0]\n\n# Check function to verify correctness\ndef check_adaboost():\n    X_train = [[0,0], [1,1], [1,0], [0,1]]\n    y_train = [0, 0, 1, 1]\n    X_test = [[0,1], [1,0]]\n    expected_output = [1, 0]\n    predictions = adaboost_predict(X_train, y_train, X_test)\n    assert set(predictions) == set(expected_output), f\"Expected output was not correct, got {predictions}\"\n    \ncheck_adaboost()\n```"}
{"task_id": 178, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # Initialize variables\n    lower_bound = -1\n    upper_bound = 1\n    best_value = float('inf')\n    global_best_x = None\n    \n    # Ensure inputs are valid\n    if not isinstance(n_dims, int) or n_dims <= 0:\n        return -1\n    if not isinstance(num_particles, int) or num_particles <= 0:\n        return -1\n    if not isinstance(num_iterations, int) or num_iterations <= 0:\n        return -1\n    \n    # Initialize positions and velocities\n    particles = rng.uniform(lower_bound, upper_bound, size=(num_particles, n_dims))\n    velocities = rng.uniform(-1, 1, size=num_particles)\n    personal_best_positions = particles.copy()\n    personal_best_values = [np.sum(particle ** 2) for particle in particles]\n    \n    # Main optimization loop\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            personal_best = personal_best_positions[i]\n            personal_best_value = personal_best_values[i]\n\n            # Update velocity based on personal best\n            c1 = 1.5\n            c2 = 1.5\n            r1, r2 = rng.uniform(0, 1, 2)\n            personal_velocity = c1 * r1 * (personal_best - particles[i]) + \\\n                                c2 * r2 * (rng.uniform() < personal_best) * \\\n                                (global_best_x - particles[i])\n            \n            # Update position\n            new_position = particles[i] + velocities[i] + personal_velocity\n            \n            # Clip positions within bounds\n            clamped_position = np.clip(new_position, lower_bound, upper_bound)\n            particles[i] = clamped_position\n            velocities[i] += clamped_position\n        \n        # Find global best position and value\n        global_best_index = np.argmin(personal_best_values)\n        \n        global_best_x = particles[global_best_index]\n        global_best_value = np.sum(global_best_x ** 2)\n        \n        # If current best value improved, update global best\n        if global_best_value < best_value:\n            best_value = global_best_value\n            best_position = global_best_x\n\n    # Return the best value rounded to 4 decimals\n    return round(best_value, 4)\n\n# Example usage:\nresult = particle_swarm_optimisation(n_dims=3,\n                                    num_particles=20,\n                                    num_iterations=100,\n                                    seed=42)\nprint(result)\n```"}
{"task_id": 180, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\n\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((np.array(x1) - np.array(x2)) ** 2))\n\ndef find_k_nearest_neighbors(X_train, X_test, k, dist_func=euclidean_distance):\n    distances = []\n    \n    for i, train_vector in enumerate(X_train):\n        distances.append([dist_func(train_vector, X_test[j]), i] for j in range(len(X_test)))\n    \n    sorted_distances = sorted(distances)\n    \n    neighbors_indices = [i[1] for i in sorted_distances[:k]]\n    neighbors_vectors = [X_train[i] for i in neighbors_indices]\n    \n    return neighbors_vectors\n\ndef predict_majority_vote(y_train, class_labels):\n    counter = Counter(class_labels)\n    return counter.most_common(1)[0][0]\n\ndef predict_mean_regression(y_train, targets):\n    return round(sum(targets) / len(targets), 4)\n\ndef knn_predict(X_train: List[List[float]], \n                y_train: List, \n                X_test: List[List[float]], \n                k: int, \n                task: str = \"classification\") -> List:\n    if task == \"classification\":\n        # Find k nearest neighbors based on majority vote\n        neighbors = find_k_nearest_neighbors(X_train, X_test, k)\n        \n        # Apply majority vote to classify\n        result = predict_majority_vote(y_train, [y_train[r] for r in neighbors])\n    else:\n        # Calculate mean of k nearest neighbors' targets for regression\n        neighbors = find_k_nearest_neighbors(X_train, X_test, k)\n        result = predict_mean_regression(y_train, neighbors)\n    \n    return [round(result, 4)] if task == \"regression\" else [result]\n\n# Example usage:\n\nX_train = [[1, 2], [2, 3], [3, 1], [3, 100], [200, 200], [200, 200]]\ny_train = ['a', 'b', 'c', 'd', 'e', 'f']\nX_test = [[1, 3], [2, 1]]\n\nprint(knn_predict(X_train, y_train, X_test))\n```"}
{"task_id": 184, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_gain(parent_gini, child_0_gini, child_1_gini):\n    \"\"\"Calculate the Gini Gain.\"\"\"\n    if parent_gini == 0 or child_0_gini == 0 or child_1_gini == 0:\n        # Avoid division by zero when both children have 0 Gini index\n        return float('inf')\n    else:\n        return parent_gini - ((child_0_gini + child_1_gini) / 2)\n\ndef best_split(X, y):\n    \"\"\"Find the best feature, value and Gini Gain.\"\"\"\n    best_feature = None\n    best_threshold = None\n    best_gini_gain = 0\n\n    # Sort indices by feature values\n    sorted_indices = np.argsort(X[:, 0])\n\n    for i in range(len(sorted_indices)):\n        # Check if reaching maximum depth\n        if max_depth is not None and max_depth == 0:\n            break\n        \n        # Split into left and right parts\n        x_left = X[sorted_indices[:i+1], :]\n        y_left = y[sorted_indices[i+1:], :]\n\n        x_right = X[sorted_indices[(i+1):], :]\n        y_right = y[sorted_indices[(i+1):], :]\n\n        # Calculate Gini before split\n        parent_gini = len(y_left) * sum([yi * yi / len(y_left) for yi in set(y_left)]) \\\n                      + len(y_right) * sum([yi * yi / len(y_right) for yi in set(y_right)])\n\n        # Calculate Gini after split for thresholds greater than min(x_left)\n        if len(set(y_left)) > 1:\n            threshold = x_left[len(x_left)-1][0] + 0.5\n            while True:\n                # Calculate Gini gains for current threshold\n                gini_gain_value = gini_gain(parent_gini, \n                                           len(y_right) * sum([yri * yri / len(y_right) for yri in set(y_right)]) / len(y_right),\n                                           len(y_left) * sum([ylr * ylr / len(y_left) for ylr in set(y_left)]) / len(y_left))\n                \n                if gini_gain_value < best_gini_gain:\n                    break\n\n                best_threshold = threshold\n                best_gini_gain = gini_gain_value\n\n                x_left_midpoint = np.mean((x_left[0], x_left[-1]))\n                x_right_midpoint = np.mean((x_right[0], x_right[-1]))\n\n                # Recursively apply same logic on new splits\n                best_split(x_left[:, :-1], y_left.astype(int), best_threshold[0], best_threshold[1])\n                best_split(x_right[:, :-1], y_right.astype(int), x_right_midpoint, x_right_midpoint)\n                break\n    \n    return best_threshold\n\ndef predict_decision_tree(node, X):\n    \"\"\"Recursive prediction.\"\"\"\n    if isinstance(node['left'], dict):\n        if node['left']['split'] is None:\n            return node['left']['val']\n        return predict_decision_tree(node['left'], X)\n    elif isinstance(node['right'], dict):\n        if node['right']['split'] is None:\n            return node['right']['val']\n        return predict_decision_tree(node['right'], X)\n    else:\n        return node['val']\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n    \n    tree = {}\n    \n    def build_tree(node, idx=0, depth=0):\n        num_nodes = 2 ** (depth % 2)\n        \n        if idx >= len(X_train):\n            return\n        if max_depth is not None and depth >= max_depth:\n            return\n        \n        # Split the data on the first feature\n        x_min = np.min(X_train[:, idx])\n        x_max = np.max(X_train[:, idx])\n        for thres in np.linspace(x_min, x_max, num=num_nodes):\n            l_idx = (X_train[:, idx] <= thres)\n            r_idx = ~l_idx\n            \n            if any(l_idx):\n                if not 'left' in tree.keys():\n                    tree['left'] = {'split': [idx, thres]}\n                build_tree(tree['left'], idx+1, depth+1)\n            if any(r_idx):\n                if not 'right' in tree.keys():\n                    tree['right'] = {'split': [idx, thres]}\n                build_tree(tree['right'], idx+1, depth+1)\n            \n            if len(tree['left']) == 0 and len(tree['right']) == 0:\n                tree['right'] = {'val': y_train[idx]}\n            if len(tree['right']) == 0 and len(tree['left']) == 0:\n                tree['left'] = {'val': y_train[idx]}\n    \n    build_tree(tree)\n    result = []\n    \n    for x_test_row in X_test:\n        result.append(predict_decision_tree(tree, np.array([[x_test_row]])))\n    return result\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Dummy data generation for demonstration purposes\n    np.random.seed(42)  # For reproducibility\n    X_train = np.random.rand(100, 2) * 10\n    y_train = np.random.randint(0, 2, size=(100,))\n    X_test = np.random.rand(50, 2) * 10\n    \n    predicted_labels = decision_tree_predict(X_train, y_train, X_test, max_depth=5)\n    print(predicted_labels)\n```\n\nThis solution implements the CART decision tree classification algorithm from scratch, including Gini impurity calculation, recursive binary splitting, stopping criteria, and the ability to handle leaves with majority classes based on the maximum depth constraint. The example usage demonstrates how to use the function with synthetic datasets."}
{"task_id": 190, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef entropy(p):\n    \"\"\"Calculate the information gain (entropy) for a single label.\"\"\"\n    if p == 0 or p == 1:\n        return 0\n    return -np.log2(p)\n\ndef gini(p):\n    \"\"\"Calculate the Gini impurity for a single label.\"\"\"\n    return 1 - np.sum(p ** 2)\n\ndef best_gini_split(X, y):\n    # Find indices where the Gini impurity changes\n    idxs_of_change = []\n    for i in range(1, len(y)):\n        if not np.array_equal(y[:i], y[i:]):\n            idxs_of_change.append(i)\n\n    # Initialize some variables to track the best split\n    best_idx = -1\n    best_gain = float('inf')\n    best_threshold = None\n    gini_score = gini(np.bincount(y))\n    \n    # Iterate over potential splits\n    for i in idxs_of_change:\n        # Calculate left and right subsets based on this split\n        left_sub = y[y <= y[i]]\n        right_sub = y[y > y[i]]\n\n        # Skip empty cases\n        if len(left_sub) == 0 or len(right_sub) == 0:\n            continue\n        \n        # Calculate Gini impurity for the current split\n        gini_impurity_split = gini(len(left_sub) / len(y) ** 2 * gini([1]) +\n                                   len(right_sub) / len(y) ** 2 * gini([0]))\n\n        # Check if the current split has lower Gini impurity than the previous best\n        if gini_impurity_split < best_gain:\n            best_gain = gini_impurity_split\n            best_threshold = y[i]\n            best_idx = i\n    \n    if best_gain >= 0:\n        best_gain = round(best_gain, 4)\n    else:\n        best_gain = 'N/A'  # To indicate no valid split found\n    \n    return (best_idx, best_threshold, best_gain)\n\n# Example usage\nif __name__ == \"__main__\":\n    X = [[1, 2], [2, 3], [4, 5], [6, 7]]\n    y = [0, 1, 1, 0]\n    print(best_gini_split(X, y))\n```"}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef metric(name: str):\n    if name == 'euclidean':\n        def d(x, y):\n            x = np.array(x).astype(float)\n            y = np.array(y).astype(float)\n            assert len(x) == len(y), \"Vectors must have the same length.\"\n            assert all(isinstance(i, (int, float)) for i in x + y), \"Vectors must contain only numbers.\"\n            return round(np.linalg.norm(x - y), 4)\n    elif name == 'manhattan':\n        def d(x, y):\n            x = np.array(x).astype(float)\n            y = np.array(y).astype(float)\n            assert len(x) == len(y), \"Vectors must have the same length.\"\n            assert all(isinstance(i, (int, float)) for i in x + y), \"Vectors must contain only numbers.\"\n            return round(sum(abs(a - b) for a, b in zip(x, y)), 4)\n    elif name == 'chebyshev':\n        def d(x, y):\n            x = np.array(x).astype(float)\n            y = np.array(y).astype(float)\n            assert len(x) == len(y), \"Vectors must have the same length.\"\n            assert all(isinstance(i, (int, float)) for i in x + y), \"Vectors must contain only numbers.\"\n            return round(max(abs(a - b) for a, b in zip(x, y)), 4)\n    elif name == 'cosine':\n        def d(x, y):\n            x = np.array(x).astype(float)\n            y = np.array(y).astype(float)\n            assert len(x) == len(y), \"Vectors must have the same length.\"\n            assert len(x) > 0 and sum(x) != 0, \"Cosine distance requires vectors with norm greater than zero.\"\n            assert len(y) > 0 and sum(y) != 0, \"Cosine distance requires vectors with norm greater than zero.\"\n            dot_product = np.dot(x, y)\n            mag_x = np.linalg.norm(x)\n            mag_y = np.linalg.norm(y)\n            return round(1 - dot_product / (mag_x * mag_y), 4)\n    else:\n        return -1\n\nprint(metric('euclidean'))([1, 2, 3], [4, 5, 6])\nprint(metric('manhattan'))([-1, 2], [3, -4])\nprint(metric('chebyshev'))([[-1, 2], [3]], [[-4]])\nprint(metric('cosine'))([-1, 2], [-3, 4])\n\n```"}
{"task_id": 197, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinte number of actions).\n\n    Returns:\n        A tuple `(n_actions_per_dim, action_ids, action_dim)` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    # Check if the environment has an action space attribute\n    assert hasattr(env, 'action_space'), \"Environment does not have an action_space attribute\"\n\n    # Initialize lists to store action statistics\n    n_actions_per_dim = []\n    action_ids = None\n    action_dim = 0\n\n    # Determine the number of dimensions based on the type of action space\n    if isinstance(env.action_space, np.ndarray):\n        num_dimensions = len(env.action_space)\n        for dim in range(num_dimensions):\n            if cont_action:\n                n_actions_per_dim.append(np.inf)\n            else:\n                actions = list(range(env.action_space[dim]))\n                n_actions_per_dim.append(len(actions))\n                action_ids = action_ids if action_ids is None else action_ids + [(dim,) + tuple(a) for a in actions]\n    elif isinstance(env.action_space, tuple):\n        num_dimensions = len(list(env.action_space))\n        action_ids = list(product(*[(range(dim_size) if cont_action else [dim_size]) for dim_size in env.action_space]))\n        n_actions_per_dim = [len(action_id) for action_id in action_ids]\n        action_dim = num_dimensions\n    elif isinstance(env.action_space, dict):\n        if md_action:\n            raise ValueError(\"Multi-dimensional discrete action space not supported\")\n        else:\n            n_dim = max([min(i.n) for i in env.action_space.values() if i.n > 1]) - min([max(j.n) for j in env.action_space.values() if j.n > 1])\n            action_ids = list(product(*[[i] for i in range(min([j.n for j in env.action_space.values()], key=len))]))\n            n_actions_per_dim = [len(action_id) for action_id in action_ids]\n            action_dim = 1\n\n    return n_actions_per_dim, action_ids, action_dim\n\n\n# Example usage\nif __name__ == \"__main__\":\n    from gymnasium import Env\n\n    class CustomEnv(Env):\n        def __init__(self):\n            super().__init__()\n            self.action_space = np.array([[5, 10]])\n\n    custom_env = CustomEnv()\n    print(action_stats(custom_env, True, False))  # Example with a continuous action space\n    print(action_stats(custom_env, False, True))  # Example with a discrete continuous action space\n    print(action_stats(custom_env, False, False))  # Example with a discrete multi-dimensional action space\n```"}
{"task_id": 198, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    beta = np.zeros((V, len(phi)), dtype=np.float64)\n\n    for d, words in enumerate(corpus):\n        for n, w in enumerate(words):\n            if w >= 0:\n                beta[w, d] += phi[d][n]\n\n    # Normalize the column sums to 1, up to 1e-4 rounding error\n    beta /= beta.sum(axis=1)[:, None] + 1e-8\n    \n    return [round(row, 4) for row in beta]\n\n\n# Example usage\nnp.random.seed(42)\nphi_example = [\n    np.array([[0.5, 0.3, 0.2], [0.1, 0.7, 0.2]]),\n    np.array([[0.2, 0.4, 0.4]])\n]\ncorpus_example = [[1, -1, 1], [-1, -1, -1]]\n\nprint(update_beta(phi_example, corpus_example, V=3))\n```"}
{"task_id": 202, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    def calculate_cost(medoids):\n        # Calculate the cost based on the average euclidean distance\n        return np.mean(np.linalg.norm(X - np.expand_dims(medoids, axis=1), axis=2))\n\n    num_samples = X.shape[0]\n    medoids = X[:k]\n\n    while True:\n        old_medoids = np.copy(medoids)\n        costs = []\n        \n        for i in range(num_samples):\n            # Find the nearest medoid\n            min_dist = float('inf')\n            best_medoid = None\n            \n            for j in range(k):\n                dist = np.linalg.norm(X[i] - medoids[j])\n                if dist < min_dist:\n                    min_dist = dist\n                    best_medoid = j\n            \n            # Update the cost\n            costs.append(calculate_cost(old_medoids))\n            \n            # Swap the current sample with the nearest medoid\n            current_sample = X[i]\n            new_medoid = medoids[best_medoid]\n            X[i] = np.dot(new_medoid - current_sample, X[i] - current_sample)\n            X[i] /= np.linalg.norm(X[i])\n            X[i] -= np.dot(new_medoid - current_sample, X[i]) * new_medoid - current_sample\n            \n            # Update the medoids\n            medoids[best_medoid] = np.sum(X, axis=0) / len(X)\n            \n        # Check for convergence\n        if np.allclose(costs, costs[-2:], atol=1e-7):\n            break\n    \n    # Assign final labels based on proximity to remaining medoids\n    labels = []\n    for i in range(num_samples):\n        min_dist = float('inf')\n        best_label = 0\n        \n        for j in range(k):\n            dist = np.linalg.norm(X[i] - medoids[j])\n            if dist < min_dist:\n                min_dist = dist\n                best_label = j\n                \n        labels.append(best_label)\n    \n    return labels\n\n\n# Test case\nX = np.array([[0,0], [1,0], [0,1], [5,5], [6,5], [5,6]])\nk = 2\nprint(pam_clustering(X, k))  # Expected output: [0, 0, 0, 1, 1, 1]\n```"}
{"task_id": 216, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expand_features(features, degree):\n    \"\"\"Expands the input data into a full polynomial basis up to degree.\"\"\"\n    expanded_features = []\n    for i in range(degree + 1):\n        temp = [features[:, j] ** i for j in range(len(features[0]))]\n        expanded_features.extend(temp)\n    \n    return np.array(expanded_features)\n\ndef gradient_descent(X, y, w, reg_factor, l1_ratio, alpha):\n    \"\"\"Compute the loss and gradients of the objective function w.r.t weights.\"\"\"\n    m = len(y)\n    predictions = X @ w\n    \n    # Compute squared errors\n    sq_errors = predictions - y\n    \n    # Compute squared gradient terms\n    grad_sq_terms = sq_errors * X\n    \n    # Add regularization terms, combined L1 and L2\n    reg_term_w = reg_factor * (np.sign(w))\n    reg_term_L2 = reg_factor * np.linalg.norm(w)**2\n\n    # Apply ElasticNet mixture of L1 & L2 penalties\n    reg_term = (1 - l1_ratio) * reg_term_L2 + l1_ratio * reg_term_w\n    \n    # Sum all components\n    total_grad = grad_sq_terms + reg_term\n    \n    # Calculate average gradient by averaging over samples\n    avg_total_grad = total_grad.sum(axis=0) / m\n    \n    return (sq_errors.T.dot(X) / m + reg_term / m).sum(), avg_total_grad\n\ndef one_sample_gradient_descent(X, y, w, reg_factor, l1_ratio, alpha):\n    \"\"\"One step of gradient descent on a single training example.\"\"\"\n    current_loss, gradient = gradient_descent(X, y, w, reg_factor, l1_ratio, alpha)\n    updated_weights = w - alpha * gradient\n    return updated_weights, current_loss\n\ndef train_model(X_train, y_train, reg_factor, l1_ratio, n_iterations, alpha):\n    \"\"\"Performs gradient descent for multiple steps.\"\"\"\n    m = len(X_train)\n    num_samples = m\n    num_features = X_train.shape[1]\n\n    # Initialize weights with small random numbers centered at zero\n    initial_weights = np.random.rand(num_features, 1)\n    converged = False\n    iteration_count = 0\n\n    while not converged:\n        iteration_count += 1\n        \n        # For each training instance...\n        for _ in range(n_iterations):\n            updated_weights, new_loss = one_sample_gradient_descent(\n                X_train, y_train, initial_weights, reg_factor, l1_ratio, alpha)\n            \n            # If new loss is larger than previous - stop iterating\n            if new_loss >= old_loss:\n                converged = True\n            \n        # Store best parameters\n        optimal_weights = initial_weights.copy()\n        optimal_loss = new_loss\n    \n    return optimal_weights, optimal_loss, optimal_weights.reshape((num_samples, 1))\n\ndef get_polynomial_matrix(X, degree):\n    \"\"\"Build a matrix of polynomial features.\"\"\"\n    return expand_features(X, degree)\n\ndef main(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"\n    Main function to run the algorithm.\n    \"\"\"\n    assert all(isinstance(v, np.ndarray) for v in [x_train, y_train]), 'Input X_train and y_train must be numpy arrays'\n    assert isinstance(reg_factor, (int, float)) and reg_factor > 0, 'reg_factor must be positive number'\n    assert all(isinstance(v, np.ndarray) for v in [x_test]), 'Input x_test must be numpy array'\n    assert isinstance(l1_ratio, (int, float)), 'l1_ratio must be float type'\n\n    # Expand x_train and x_test to include polynomial features\n    x_train_poly = get_polynomial_matrix(x_train, degree)\n    x_test_poly = get_polynomial_matrix(x_test, degree)\n\n    print('Training model...')\n    opt_params, loss, final_optimal_weights = train_model(\n        x_train_poly, y_train, reg_factor, l1_ratio, n_iterations, learning_rate)\n\n    print(f'Training loss = {loss}')\n    predictions = x_test_poly @ final_optimal_weights\n\n    return predictions.flatten().round(4).tolist()\n\nif __name__ == '__main__':\n    x_train = [[1], [2], [3]]\n    y_train = [1, 4, 9]\n    x_test = [[1], [2], [3]]\n\n    predictions = main(x_train, y_train, x_test)\n    print(predictions)\n```"}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\"\"\"\n    \n    # Ensure inputs are NumPy arrays for vectorized operations\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Convert actual values to -1 or 1 if needed\n    assert len(set(actual)) == 2, \"Actual values need to be either +1 or -1.\"\n    actual = (actual > 0).astype(int)\n    \n    # Compute sigmoid(z), its derivative (gradient), and the probability\n    def sigmoid_function(x):\n        return 1 / (1 + np.exp(-x))\n\n    def grad_and_hess(pred):\n        pred = sigmoid_function(pred)\n        grad = actual * pred\n        hess = pred * (1 - pred)\n        return grad, hess\n    \n    # Apply the functions to all elements in the input arrays\n    grad_actual = grad_and_hess(predicted)[0]\n    hess_actual = grad_and_hess(predicted)[1]\n    probs_actual = sigmoid_function(predicted)\n    \n    # Round the results to six decimal places\n    grad_actual_rounded = [round(grad_value, 6) for grad_value in grad_actual]\n    hess_actual_rounded = [round(hess_value, 6) for hess_value in hess_actual]\n    probs_actual_rounded = [round(prob_value, 6) for prob_value in probs_actual]\n\n    return (grad_actual_rounded, hess_actual_rounded, probs_actual_rounded)\n\n\n# Example usage:\nactual = [-1, 1, -1, 1]  # Binary classification targets\npredicted = [0.95, -0.80, 0.98, -0.75]  # Raw predictions from a model\ncomponents = logistic_components(actual, predicted)\nprint(components)\n```"}
{"task_id": 218, "completion_id": 0, "completion": "Below is the complete Python script that defines a function `blackjack_outcome` based on the provided specifications:\n\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function must follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    def get_hand_value(hand):\n        value = 0\n        number_of_aces = 0\n\n        for card in hand:\n            if card == 1:\n                number_of_aces += 1\n            elif card >= 10:\n                continue\n            else:\n                value += card\n\n        value += (number_of_aces - 1) * 11\n\n        while value > 21 and number_of_aces:\n            value -= 10\n            number_of_aces -= 1\n\n        return value\n\n    player_score = get_hand_value(player)\n    dealer_score = get_hand_value(dealer)\n\n    if player_score == dealer_score:\n        return 0.0  # Draw\n    elif player_score < 22 and dealer_score > 21:\n        return 1.0  # Player wins\n    elif player_score == 21:\n        return 1.5  # Natural black jack win\n    elif player_score > 21:\n        return -1.0  # Bust\n    else:\n        return -1.0  # Dealer wins\n\n\n# Example usage:\nprint(blackjack_outcome([1, 10], [2, 7]))  # Should print -1.0 (dealer wins)\nprint(blackjack_outcome([1, 1], [2, 8]))  # Should print -1.0 (bust)\nprint(blackjack_outcome([11, 11], [1, 5]))  # Should print 1.0 (draw natural blackjack)\n```\n\nThis script includes a helper function `get_hand_value` to calculate the value of the hand by considering the possibility of using an 'ace' as either 1 or 11 if it doesn't cause the hand to bust. The main `blackjack_outcome` function then uses this helper function to determine the outcome of the game according to the specified rules."}
{"task_id": 221, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport sys\n\ndef squared_log_error(actual, predicted):\n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted sequences must have the same length.\")\n    \n    if not all(np.greater_equal(actual, 0)) or not all(np.greater_equal(predicted, 0)):\n        raise ValueError(\"All elements in actual and predicted sequences must be non-negative.\")\n    \n    return np.square(np.log1p(np.array(actual)) - np.log1p(np.array(predicted))).mean()\n\ndef mean_squared_log_error(actual, predicted):\n    try:\n        result = squared_log_error(actual, predicted)\n        if isinstance(result, int):  # Check if result is actually a number (not just a string representation of one)\n            return round(result, 4)\n        else:\n            return result\n    except Exception as e:\n        print(f\"Error calculating MSLE: {e}\")\n        return -1\n\n# Example usage:\nif __name__ == \"__main__\":\n    actual_values = [1, 2, 3]\n    predicted_values = [1.5, 1.9, 2.8]\n    output = mean_squared_log_error(actual_values, predicted_values)\n    print(output)  # Should print a rounded float value\n    \n    # Invalid example\n    actual_values_invalid = [-1, 2, 3]\n    predicted_values_invalid = [1.5, 1.9, 2.8]\n    output_invalid = mean_squared_log_error(actual_values_invalid, predicted_values_invalid)\n    print(output_invalid)  # Should print -1 due to negative value\n```"}
{"task_id": 222, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\ndef sigmoid(z: Union[int, float, np.ndarray]) -> Union[float, List[float]]:\n    \"\"\"\n    Compute the element-wise sigmoid (logistic) function.\n\n    The function works for scalar numbers, Python lists, and NumPy arrays,\n    and remains numerically stable for very large positive or negative inputs.\n    \n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray),\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n\n    if isinstance(z, int) or isinstance(z, float):\n        result = 1 / (1 + np.exp(-z))\n        return round(result, 4)\n    \n    elif isinstance(z, np.ndarray):\n        # Apply sigmoid on each element of the array individually using vectorized operations\n        return np.round(np.vectorize(lambda x: 1 / (1 + np.exp(-x)), otypes=[np.float])(z), 4)\n\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n# Example checks\nprint(sigmoid(3))   # Should print 0.9526\nprint(sigmoid([-2, -1, 0]))     # Should print [-0.8808, -0.7311, 0.5]\nprint(sigmoid([1000]))         # Should print [0.9999, 0.9999] due to numerical stability\n```"}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, (list, tuple)):\n        result = []\n        for x in z:\n            y = a * x if x < 0 else x\n            result.append(y)\n    elif isinstance(z, np.ndarray):\n        result = a * z[z < 0] + z[z >= 0]\n    else:\n        result = z\n    return result.tolist() if isinstance(z, np.ndarray) else result\n\n# Test cases\nprint(leaky_relu([0, -1, 5]))  # Expected output: [0, -0.01, 5]\nprint(leaky_relu([-1, 0], 0.5))  # Expected output: [-1, 0]\nprint(leaky_relu(np.array([-1, 0]), 0.5))  # Expected output: array([-0.5,  0])\n```"}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    # Initialize the classifier coefficients\n    w = np.ones(y_train.shape[0]) / len(y_train)\n    \n    # Initialize the decision stump weights and index\n    alpha = {}\n    base_stump_weights = np.ones(n_clf)\n    feature_indices, thresholds, polarities = [], [], []\n    \n    # Fit the decision stumps\n    for i in range(n_clf):\n        feature_index, threshold, polarity = fit_decision_stump(X_train, y_train, feature_indices, thresholds, polarities)\n        \n        # Update the weights of misclassified points\n        _, idx = np.where(y_train > 0)\n        indices_to_update = np.random.choice(idx, p=w[idx])\n        w[indices_to_update] += alpha[i]\n        \n        # Calculate the new weight coefficient \u03b1i\n        if w[idx].sum() != 0:\n            alpha_i = 0.5 * np.log((1 - y_train[indices_to_update].mean()) / (y_train[indices_to_update].mean()))\n            alpha[i] = alpha_i\n        \n        # Check if we've converged\n        if all(alpha.values()):\n            break\n    \n    # Compute the final prediction for X_test\n    predictions = [np.sign(np.dot([-1 if (polarity == -1) else 1], x.T)).ravel()[0] for x in X_test]\n    \n    return predictions\n\n\ndef fit_decision_stump(X: np.ndarray, y: np.ndarray, feature_indices: list, thresholds: list, polarities: list) -> tuple:\n    best_error = np.inf\n    best_feature_index, best_threshold, best_polarity = None, None, None\n    \n    for col_idx in range(X.shape[1]):\n        unique_values = np.unique(X[:, col_idx][np.isin(y, 1)])\n        \n        if len(unique_values) < 2:\n            continue\n            \n        for thres_value in unique_values:\n            true_positives, false_negatives, _, _ = binarize_data(X, y, col_idx, thres_value, 1)\n            pred_true = true_positives >= false_negatives\n            \n            error = pred_true.mean()\n            \n            if error < best_error:\n                best_error = error\n                best_feature_index, best_threshold, best_polarity = col_idx, thres_value, 1\n                \n    return best_feature_index, best_threshold, best_polarity\n\n\ndef binarize_data(X: np.ndarray, y: np.ndarray, feature: int, threshold: float, polarity: int) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    positive_mask = np.abs(X[:, feature]) <= threshold\n    negative_mask = np.abs(X[:, feature]) > threshold\n    positive_label = polarity * (y == 1)[positive_mask]\n    negative_label = polarity * (y == -1)[negative_mask]\n    \n    tp = np.sum(positive_label == 1)\n    tn = np.sum(-positive_label == -1)\n    fn = np.sum(-negative_label == -1)\n    fp = np.sum(positive_label == -1)\n    \n    return tp, fn, tn, fp\n```"}
{"task_id": 241, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    if len(shape) < 2:\n        return [-1]\n\n    np.random.seed(0)  # For reproducibility\n    A = np.random.randn(*shape)\n\n    u, s, vh = np.linalg.svd(A)\n    Q = u if u.shape == A.shape else vh.T\n    \n    # Shape check\n    assert A.shape == Q.shape\n\n    result = Q.reshape(shape)\n    \n    # Check orthogonality condition\n    if len(shape) > 1:\n        flat_result = result.reshape(-1)\n        dot_product = np.dot(flat_result[:min(len(shape), len(result))], \n                             flat_result[min(len(shape), len(result)):])\n        assert abs(dot_product - scale**2) < 1e-6\n    elif len(shape) == 1:\n        assert np.abs(np.sum(result * result) - scale**2) < 1e-6\n\n    return result.tolist()\n\n# Example usage\nprint(orthogonal((2, 2)))\n```"}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    # Define parameters\n    W1 = np.array([[1.0, 1.0, 1.0]])\n    b1 = np.zeros((1,))\n    \n    W2 = np.tile(W1, (4, 1))\n    b2 = np.repeat(b1, repeats=4, axis=0)\n    \n    W3 = np.tile(W2, (4, 1))\n    b3 = np.repeat(b2, repeats=4, axis=0)\n    \n    Wa = np.tile(W3, (3, 1))\n    bc = np.repeat(b3, repeats=3, axis=0)\n    \n    # Input layer\n    s = np.array([state[0], state[1], state[2]]).reshape(1, -1)\n    \n    # Hidden layers activations\n    z1 = np.maximum(0, s @ W1 + b1)\n    z2 = np.maximum(0, z1 @ W2 + b2)\n    z3 = np.maximum(0, z2 @ W3 + b3)\n    \n    # Action probabilities\n    logits = z3 @ Wa + bc\n    \n    action_probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n    \n    # State value\n    state_value = z3 @ Wc + bc[0]\n    \n    return [round(a, 4) for a in action_probs.ravel()], round(state_value, 4)\n\n\n# Example usage\nstate = [0.5, 0.7, 1.2]\naction_probs, state_value = actor_critic_forward(state)\nprint(f\"Action Probabilities: {action_probs}\")\nprint(f\"State Value: {state_value}\")\n```"}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    W1, b1, W2, b2, W3, b3 = params['W1'], params['b1'], params['W2'], params['b2'], params['W3'], params['b3']\n    Wa, ba = params['Wa'], params['ba']\n    Wc, bc = params['Wc'], params['bc']\n\n    h1 = np.dot(state, W1) + b1\n    h1_relu = np.maximum(0, h1)\n\n    h2 = np.dot(h1_relu, W2) + b2\n    h2_relu = np.maximum(0, h2)\n\n    h3 = np.dot(h2_relu, W3) + b3\n    h3_relu = np.maximum(0, h3)\n\n    prob = np.exp(Wa @ h3_relu + ba)\n    softmax_prob = prob / np.sum(prob, axis=0, keepdims=True)\n    action_probs = softmax_prob.flatten()\n\n    value = np.dot(h3_relu, Wc) + bc\n    rounded_value = round(value.item(), -int(np.floor(np.log10(abs(value)))) - 1)\n\n    return action_probs, rounded_value\n\n# Example usage:\nstate = [0.5, 0.3, 0.7]  # Example environment state\nparams = {\n    'W1': np.array([[1., 2.],\n                    [-1., 3.]]),\n    'b1': np.array([3., 1.]),\n    'W2': np.array([[1., 2.],\n                    [-2., 1.]]),\n    'b2': np.array([-1., 2.]),\n    'W3': np.array([[1., 2.],\n                    [-2., 3.]]),\n    'b3': np.array([2., -1.]),\n    'Wa': np.array([[-1., 1.],\n                    [1., -1.]]),\n    'ba': np.array([1., 1.]),\n    'Wc': np.array([[1., 1.]])\n}\n\nresult = actor_critic_forward(state, params)\nprint(result)\n```"}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    lambd = l1_ratio * len(w)\n    reg_value = alpha * lambd * sum(np.abs(w)) + 0.5 * alpha * (1 - l1_ratio) * np.dot(w, w)\n    \n    if not gradient:\n        return round(reg_value, 4)\n    else:\n        grad_w = alpha * (lambd * np.sign(w) + (1 - l1_ratio) * w)\n        return [round(x, 4) for x in grad_w]\n\n\n# Example usage:\nweight_vector = np.array([1, 2, -3])\nprint(elastic_net_regularization(weight_vector, 0.1))\n# Expected output: 1.2669\n\nweight_vector_grad = np.array([1, 2, -3]) \nprint(elastic_net_regularization(weight_vector_grad, 0.1, gradient=True))\n# Expected output: [-0.1775, 0.355, -0.1775]\n```"}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                  loss_func,\n                  param_name: str,\n                  n_t: int,\n                  X,\n                  epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                      method (single time-step) and a ``flush_gradients``\n                      method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                      loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                      be redirected to the lower-case keys.  If the name is\n                      \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    if param_name == \"X\" or param_name == \"y\":\n        return None\n    \n    params = model.parameters.copy()\n    \n    def get_lower_case_param(param_name):\n        if param_name.lower() != param_name:\n            return {k: v for k, v in zip(params.keys(), map(lambda x: x.lower(), params.values()))}\n        else:\n            return deepcopy(params)\n    \n    new_params = get_lower_case_param(param_name)\n    \n    try:\n        model.parameters = new_params\n        model.flush_gradients()\n        \n        y_pred = []\n        for _ in range(n_t):\n            y_pred.append(model.forward(X))\n            \n        grads = []\n        \n        for p in new_params[param_name]:\n            p_val = p.get_value().copy()\n            p_val += epsilon\n            p_val = np.clip(p_val, -np.inf, np.inf).astype(np.float64)\n            p.set_value(p_val)\n            y_pred_1 = model.forward(X).item()\n            p_val -= 2 * epsilon\n            p_val = np.clip(p_val, -np.inf, np.inf).astype(np.float64)\n            p.set_value(p_val)\n            y_pred_2 = model.forward(X).item()\n            p_val += epsilon\n        \n            grads.append((y_pred_1 - y_pred_2) / (2 * epsilon))\n        \n        grads_array = np.array(grads)\n        \n        return grads_array.T.astype(np.float64)\n    \n    except Exception as e:\n        print(f\"Error occurred while computing numerical gradient: {e}\")\n        return None\n\n# Example usage:\n# Assuming 'model' and other necessary components are defined elsewhere\n# grads = grad_check_RNN(model, ..., ..., ...)\n```"}
{"task_id": 257, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    # Initialize weights to 1/m\n    w = np.ones(y_train.shape[0]) / y_train.shape[0]\n\n    # Initialize model parameters\n    T = []\n    alpha = []\n\n    # Loop until we have enough weak classifiers\n    while len(T) < n_clf:\n\n        # Train on the current subset of the data\n        T.append(AdaBoost.train_stump(X_train, y_train, w))\n\n        # Compute the weighted error for this ensemble\n        err = AdaBoost.compute_error(X_train, y_train, T[-1])\n\n        # If the error has not improved since the last iteration...\n        if err == 0:\n            break\n\n        # Compute alpha_t\n        alpha_t = 0.5 * np.log((1 - err) / err)\n        \n        # Choose samples to reweight based on the sign of the prediction\n        idx_to_reweight = np.where(np.sign(np.dot(T[-1], X_train)) != y_train)[0]\n        new_w = w[idx_to_reweight] + alpha_t\n\n        # Normalize the weights\n        w[idx_to_reweight] /= new_w.sum()\n        w = w / new_w.sum()\n\n        # Append alpha_t and the updated weights to the lists\n        alpha.append(alpha_t)\n        T.append(None)\n\n    # Make predictions on the test data using the final weak classifiers\n    preds = np.zeros(X_test.shape[0])\n    for t, stump in enumerate(T):\n        if stump is None:\n            continue\n        preds += alpha[t] * stump.predict(X_test)\n    \n    return np.sign(preds).astype(int)\n\n\nclass AdaBoost:\n    @staticmethod\n    def train_stump(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n        \"\"\"Trains a single weak classifier (decision stump).\n\n        Args:\n            X: Features matrix of shape (m, n).\n            y: Labels vector of shape (m, ).\n            w: Sample weights.\n\n        Returns:\n            The trained weak classifier as a NumPy array of shape (n, ).\n        \"\"\"\n        best_idx, best_val = -1, float('inf')\n        for i in range(X.shape[1]):\n            val = np.mean(w[y == (X[:, i] <= best_val)])\n            if val < best_val:\n                best_val = val\n                best_idx = i\n        \n        # Create a one-level decision tree with just this feature\n        return np.array([best_idx]).reshape(-1, 1)\n\n    @staticmethod\n    def compute_error(X: np.ndarray, y: np.ndarray, weak_classifier: np.ndarray) -> float:\n        \"\"\"Computes the weighted classification error of a weak classifier.\"\"\"\n        predictions = np.dot(X, weak_classifier)\n        errors = predictions != y\n        return errors.astype(float).mean()\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    N_SAMPLES = 100\n    N_FEATURES = 10\n    seed = 0\n    \n    np.random.seed(seed)\n    X_train = np.random.randn(N_SAMPLES, N_FEATURES)\n    y_train = np.sign(np.dot(X_train, np.random.randn(N_FEATURES, 1)))\n    X_test = np.random.randn(100, N_FEATURES)\n\n    print(adaboost_predict(X_train, y_train, X_test))\n```\n\nThis code snippet implements the AdaBoost algorithm from scratch using decision stumps (`weak classifiers`). It includes functions to train individual decision stumps (`train_stump`) and computes the overall weak classifier's error (`compute_error`). The main function `adaboost_predict` aggregates these weak classifiers' votes to make predictions on unseen data."}
{"task_id": 261, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    assert len(shape) >= 2, \"Shape must have at least two dimensions.\"\n    \n    fan_in = shape[-2] if len(shape) > 2 else shape[0]\n    fan_out = shape[-1] if len(shape) > 2 else shape[0]\n\n    std_deviation = np.sqrt(2 / (fan_in + fan_out))\n\n    return np.random.normal(loc=0, scale=std_deviation, size=tuple(shape))\n\n\nshape_example = (3, 4, 5)\nresult = glorot_normal(shape_example)\n\nprint(\"Generated Array:\")\nprint(result)\n\nprint(\"\\nShape:\", result.shape)\nprint(\"Type:\", result.dtype)\nprint(\"Mean:\", np.mean(result))\nprint(\"Standard Deviation:\", np.std(result, ddof=1))  # Using ddof=1 to estimate population standard deviation\n```"}
{"task_id": 266, "completion_id": 0, "completion": "Here's how you could approach the creation of an adjacency list from a graph represented by a vertex list `V` and an edge list `E`. We'll first define our helper functions to ensure we're working with unique vertices and efficiently building the adjacency list. Then, we will implement the main logic in our `build_adj_list` function.\n\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    V_set = set()  # Set of seen vertices to detect cycles easily\n    adj_list = [[] for _ in range(len(V))]  # Initialize the adjacency list\n    \n    def dfs(v, prev=None):\n        if not isinstance(prev, int):  # Ensure it's always an integer\n            raise ValueError(\"prev cannot be non-integer\")\n        \n        visited.add(v)\n        adj_list[v].append((prev, v))  # Add a backedge to avoid loops\n        \n        for u in E[v]:\n            if u != prev:\n                if u not in visited:\n                    dfs(u, v)\n                else:\n                    # Cycle detected - prune the edge\n                    del E[v].pop()\n    \n    visited = set()\n    for i in range(len(E)):\n        if i not in visited:\n            dfs(i)\n            \n    return adj_list\n\n\ndef check_solution():\n    V = [0, 1, 2, 3, 4, 5, 6]\n    E = [(0, 1), (1, 2), (2, 0), (2, 3), (3, 2), (4, 0), \n         (4, 5), (5, 4), (5, 6), (6, 5)]\n    \n    expected_adj_list = [\n        [], [], [], [2, 3], [4, 5, 6], [0],\n        [1, 4, 5, 6]\n    ]\n    \n    result_adj_list = build_adj_list(V, E)\n    \n    assert len(result_adj_list) == len(expected_adj_list), \"Output length mismatch.\"\n    for i in range(len(V)):\n        assert result_adj_list[i] == expected_adj_list[i], f\"Adjacency list {i} differs.\"\n\ncheck_solution()\nprint(\"All checks passed.\")\n```\n\nThis solution involves two primary steps:\n\n1. **Helper Function (`dfs`)**: This recursive function helps us traverse through the graph without getting stuck in infinite loops due to cycles.\n2. **Main Logic (`build_adj_list`)**: This function iterates over the entire edge list, ensuring that vertices are properly connected while avoiding cycles by pruning unnecessary edges.\n\nThe `check_solution` function demonstrates the correctness of our implementation using a predefined problem instance. The adjacency list produced by this method should match the expected results."}
{"task_id": 267, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    def weighted_classification_error(stump):\n        _, idx = np.unique(y, return_index=True)[0]  # Get indices of correct predictions\n        num_correct = len(idx)\n        num_misclassified = sum(sample_weights)\n        \n        if num_correct == num_misclassified:\n            return 0\n        \n        incorrect_indices = set(np.argsort(-sample_weights)) - set(idx)\n        wrong_predictions = [np.where(y==-1)[0][0], np.where(y==-1)[0][-1]]\n        theta = max(wrong_predictions[min(incorrect_indices)])\n        \n        def classify(x):\n            if x[stump['feature_index']] >= stump['threshold']:\n                return stump['polarity']\n            else:\n                return -stump['polarity']\n        \n        pred_err = 0\n        for i in range(len(X)):\n            if i not in idx:\n                real_class = 1 if y[i] > 0 else -1\n                predicted_class = classify(X[i])\n                pred_err += abs(real_class - predicted_class) * sample_weights[i]\n                \n        return pred_err / num_correct\n\n    initial_guess = {'feature_index': 0, 'threshold': 0, 'polarity': None}\n    \n    result = minimize(fun=lambda params: weighted_classification_error({'feature_index': params['feature_index'], \n                                                                         'threshold': params['threshold'], \n                                                                         'polarity': params['polarity']}, \n                                                                       X=X, y=y, sample_weights=sample_weights),  \n                      args=(initial_guess,), method='Powell', options={'maxiter': 50})\n    \n    best_params = result.x.copy()\n    best_parity = (-result.fun > 0) + 0\n    \n    return {\n        \"feature_index\": best_params[\"feature_index\"],\n        \"threshold\": round(best_params[\"threshold\"], 4),\n        \"polarity\": best_parity,\n        \"weighted_error\": round(result.fun, 4)\n    }\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([-1, -1, 1, 1])\n# sample_weights = np.array([0.1, 0.2, 0.7, 0.9])\n\n# print(train_decision_stump(X, y, sample_weights))\n```"}
{"task_id": 273, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\nclass Node(object):\n\n    def __init__(self, left=None, right=None, depth=1, parent=None, split_idx=-1, split_val=-1):\n        self.left = left\n        self.right = right\n        self.depth = depth\n        self.split_idx = split_idx\n        self.split_val = split_val\n        self.parent = parent\n\n    @property\n    def info_gain(self):\n        if not self.left and not self.right:\n            return 0.0\n        n_left, n_right = len(self.left.feature), len(self.right.feature)\n        h_parent = self._entropy(self.target)\n        p_left, p_right = n_left / len(self.feature), n_right / len(self.feature)\n        h_l = self._entropy([target[i] for i in self.left.feature])\n        h_r = self._entropy([target[i] for i in self.right.feature])\n        g = h_parent - ((p_left * h_l + p_right * h_r))\n        return round(g, 4)\n\n    @staticmethod\n    def _entropy(labels):\n        counter = Counter(labels)\n        total = sum(counter.values())\n        prob = [c / total for c in counter.values()]\n        log_prob_sum = sum([-math.log(p, 2) for p in prob])\n        return -(log_prob_sum)\n\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimal places or None,\n        * information_gain \u2013 float rounded to 4 decimal places.\n    \"\"\"\n\n    # Initialize variables\n    max_info_gain = 0.0\n    optimal_threshold = None\n\n    for idx in range(len(feature)):\n        current_feature = np.array(feature).copy()\n        current_target = np.array(target).copy()\n\n        if abs(current_feature[idx]) == abs(current_feature[idx + 1]):\n            continue\n\n        # Choose the threshold at the median value of the current index\n        median_value = int((np.floor((idx + 1) / 2)) * (abs(current_feature[idx]) // 2) +\n                           np.ceil(((len(feature) + idx + 1) / 2) * (abs(current_feature[idx + 1]) // 2)))\n\n        cutoff = feature[min(median_value, len(current_feature) - 1)]\n        \n        left_idx = np.where(feature <= cutoff)[0]\n        right_idx = np.where(feature > cutoff)[0]\n\n        if len(left_idx) == 0 or len(right_idx) == 0:\n            continue\n        \n        left_subtree = Node(left_idx, right_idx, 1, parent=self)\n        right_subtree = Node(left_idx, right_idx, 1, parent=self)\n\n        if left_subtree.info_gain >= max_info_gain:\n            max_info_gain = left_subtree.info_gain\n            optimal_threshold = cutoff\n\n    return round(optimal_threshold, 4), round(max_info_gain, 4)\n\n\n# Test cases\nif __name__ == \"__main__\":\n    feature_1 = [3, 5, 8, 9, 10]\n    target_1 = [0, 0, 1, 1, 1]\n    \n    print(best_split(feature_1, target_1))\n\n    feature_2 = [-7, -6, 0, 1, 2]\n    target_2 = [1, 0, 0, 0, 0]\n    \n    print(best_split(feature_2, target_2))\n```"}
{"task_id": 286, "completion_id": 0, "completion": "```python\nimport random\nimport random\nfrom typing import *\nimport numpy as np\n\ndef zeros_init(shape: Tuple[int, int]) -> np.ndarray:\n    return np.zeros(shape)\n\ndef ones_init(shape: Tuple[int, int]) -> np.ndarray:\n    return np.ones(shape)\n\ndef random_uniform_init(shape: Tuple[int, int], low=0.0, high=1.0, seed=42) -> np.ndarray:\n    rng = np.random.RandomState(seed)\n    return rng.uniform(low, high, size=shape)\n\ndef get_initializer(name: str) -> Callable[[Tuple[int, int]], np.ndarray]:\n    def _initializer(_shape: Tuple[int, int]):\n        match name.lower():\n            case 'zeros':\n                return zeros_init(_shape)\n            case 'ones':\n                return ones_init(_shape)\n            case 'randn':\n                return np.random.randn(*_shape)\n            case 'random_uniform':\n                return random_uniform_init(_shape)\n            case _:\n                raise ValueError(\"Invalid initialization function.\")\n    \n    return _initializer\n\n\n# Example usage\nprint(get_initializer('zeros')(2, 3))\nprint(get_initializer('ones').(2, 3))\nprint(get_initializer('random_uniform').(2, 3))\n```"}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    # Determine the dimensions after applying the pooling operation\n    N, C, H_new, W_new = X.shape[0], X.shape[1], (H - pool_shape[0]) // stride[0] + 1, (W - pool_shape[1]) // stride[1] + 1\n    \n    # Initialize the output tensor with zeros\n    pooled_output = np.zeros((N, C, H_new, W_new))\n\n    # Perform pooling on the image\n    for n in range(N):\n        for c in range(C):\n            for h in range(H_new):\n                for w in range(W_new):\n                    # Calculate indices within the non-overlapping section\n                    oh_start, oh_end = h * stride[0], min(h * stride[0] + pool_shape[0], H)\n                    ow_start, ow_end = w * stride[1], min(w * stride[1] + pool_shape[1], W)\n\n                    # Sum the values inside the current pooled region\n                    avg_value = np.mean(X[n, c, oh_start:oh_end, ow_start:ow_end])\n                    \n                    # Assign the computed average to the pooled output tensor\n                    pooled_output[n, c, h, w] = round(avg_value, 4)\n\n    # Accumulate the gradients along the rows and columns\n    grad_input = np.sum(pooled_output * accum_grad, axis=(2, 3))\n    \n    # Convert to Python lists\n    output_list = pooled_output.tolist()\n    grad_input_list = grad_input.tolist()\n\n    return (output_list, grad_input_list)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define example data\n    X_example = np.array([[[[1, 2, 3],\n                            [4, 5, 6],\n                            [7, 8, 9]]]])\n\n    # Perform pooling with different parameters\n    pool_shape = (2, 2)\n    stride = (1, 1)\n    grad_accumulation = np.ones_like(X_example)\n\n    # Call the function to get the results\n    result = average_pool2d(X_example, pool_shape, stride, grad_accumulation)\n\n    print(\"Pooled Output:\", result[0])\n    print(\"Grad Input:\", result[1])\n```"}
{"task_id": 290, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object  # Node | Leaf\n    right: object  # Node | Leaf\n\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.array_equal(tree_a.value, tree_b.value)\n    elif isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return (\n            tree_a.feature == tree_b.feature and\n            abs(tree_a.threshold - tree_b.threshold) <= 1e-8 and\n            compare_trees(tree_a.left, tree_b.left) and\n            compare_trees(tree_a.right, tree_b.right)\n        )\n    else:\n        return False\n\n\n# Example usage:\nexample_tree_a = Node(\n    feature=0,\n    threshold=3.5,\n    left=Node(feature=1, threshold=4.0, left=None, right=None),\n    right=Leaf(value=[1, 2, 3])\n)\nexample_tree_b = Node(\n    feature=0,\n    threshold=3.5,\n    left=Node(feature=1, threshold=4.0, left=None, right=None),\n    right=Leaf(value=[1, 2, 3])\n)\nassert compare_trees(example_tree_a, example_tree_b) == True\n```"}
{"task_id": 292, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]], parent2: list[list[int | float]], cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    # Validate the shapes of parent1 and parent2\n    if len(set([len(row) for row in parent1])) != 1 or len(set([len(row) for row in parent2])) != 1:\n        return -1\n\n    # Perform the single point crossover\n    child1 = []\n    child2 = []\n\n    for i in range(len(parent1)):\n        if i <= cutoff:\n            child1.append(parent1[i])  # Copy columns before the cutoff\n            child2.append(parent2[i])\n        else:\n            child1.append(parent2[i])  # Copy columns after the cutoff\n            child2.append(parent1[i])\n\n    # Convert the result to Python lists using NumPy's `tolist()` method\n    child1_list = [list(col) for col in child1]\n    child2_list = [list(col) for col in child2]\n\n    return (child1_list, child2_list)\n\n# Test cases to verify correctness\nparent1 = [[0.5, 1.0], [2.0, 2.5]]\nparent2 = [[3.5, 4.0], [5.0, 6.0]]\ncutoff = 1\n\nexpected_output_1 = ([[[0.5, 1.0]], [[3.5, 4.0]]], [[[2.0, 2.5]], [[5.0, 6.0]]])\noutput_1 = single_point_crossover(parent1, parent2, cutoff)\nprint(\"Test Case 1:\", output_1 == expected_output_1)\n\nparent1 = [[0.5, 1.0, 2.0], [3.0, 4.0, 5.0]]\nparent2 = [[6.0, 7.0, 8.0], [9.0, 10.0, 11.0]]\ncutoff = 2\n\nexpected_output_2 = (-1, )\noutput_2 = single_point_crossover(parent1, parent2, cutoff)\nprint(\"Test Case 2:\", output_2 == expected_output_2)\n```"}
{"task_id": 294, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List, Tuple\nfrom collections import defaultdict\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    adj = defaultdict(list)  # Initialize an empty adjacency dictionary\n    \n    if not G:  # Check if the graph is empty\n        return adj\n    \n    vertices_count = len(G['_V2I'])\n    is_directed = G['is_directed']\n    \n    for i in range(vertices_count):\n        source_label = next(iter(G['_V2I'].keys()))\n        source_index = G['_V2I'][source_label]\n        \n        # Collect incoming edges\n        adjacent_vertices = []\n        weights = []\n        for j in range(i + 1, vertices_count):\n            target_label = next(iter(G['_V2I'].keys()))\n            target_index = G['_V2I'][target_label]\n            \n            if is_directed:\n                if target_index == source_index:\n                    continue\n                \n            adjacent_vertices.append(target_label)\n            weights.append(G['_G'][j][2])\n        \n        adj[source_label].extend(zip(adjacent_vertices, weights))\n        adj[target_label].extend(zip(adjacent_vertices[::-1], weights[::-1]))\n    \n    # Remove duplicates and sort by neighbour labels\n    for k, v in adj.items():\n        adj[k] = [(n[0], n[1]) for _, n in sorted(v)]\n    \n    return adj\n\n# Example usage\nG = {\n    'is_directed': False,\n    '_V2I': {'A': 0, 'B': 1, 'C': 2},\n    '_G': [[('B', 'D', 3), ('C', 'E', 7)], [('A', 'B', 8), ('A', 'D', 9)]]\n}\nprint(to_networkx(G))  # Expected: {'A': [('B', 8)], 'B': [('A', 8), ('D', 3)], 'C': [('E', 7)], 'D': [('A', 9), ('B', 3)], 'E': [('C', 7)]}\n\n```"}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    valid_values = set([0, 1])\n    return all(np.array_equal(v, valid_values) for v in x)\n\n# Test cases to verify the correctness of the function\ntest_cases = [\n    (np.array([0, 1]), True),\n    (np.array([0, '', 'one']), None),  # Expected result: False\n    (np.array(['0', '1', 'true'], dtype=object), False),  # Expected result: False\n    ([], False),  # Expected result: False\n    (np.array([[0, 1], [0, 1]]), True),\n    (np.array([[0, 2], [0, 1]]), False)\n]\n\nfor i, case in enumerate(test_cases):\n    print(f\"Test {i+1}: {'Passed' if is_binary(case[0]) == case[1] else 'Failed'}\")\n```"}
{"task_id": 298, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\nfrom typing import List\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    def count_tokens(sentence):\n        return len([token for token in sentence.split()])\n\n    def get_ngram_counts(ngram, corpus):\n        seen = set()\n        count = 0\n        for i in range(len(corpus)):\n            for j in range(i + 1, min(N + i + 1, len(corpus) + 1)):\n                gram = tuple(corpus[i:j])\n                if gram == ngram:\n                    seen.add(j - i)\n                    count += 1\n        return count, seen\n\n    def calculate_probability(query_ngram, seen, corpus):\n        if not seen:\n            return 0.0\n        numerator = len(seen)\n        denominator = sum(count_tokens(corpus[begin:end]) for begin, end in seen)\n        return math.log(numerator / denominator, 2), len(seen)\n\n    corpus = [sentence.lower().split() for sentence in corpus]\n    padded_query = ['<bol>'] + query.split() + ['<eol>']\n    ngrams = {tuple(padded_query[:i+2]): count_tokens(padded_query[:i+2]) for i in range(N)}\n\n    max_prob = float('-inf')\n    for query_ngram in padded_query[N:]:\n        count, seen = get_ngram_counts(tuple(padded_query[:len(padded_query) // N]), corpus)\n        prob, _ = calculate_probability(query_ngram, seen, corpus)\n        max_prob = max(max_prob, prob)\n\n    return round(math.log(max_prob, 2), 4)\n\n\ncorpus = [\"a b a\", \"b a b\"]\nquery = \"a b\"\nN = 1\nprint(ngram_log_prob(corpus, query, N))  # Output: -1.3863\n```"}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    # Step 2: Calculate the unnormalized graph Laplacian\n    m, n = X.shape\n    deg = np.sum(np.abs(X)**2, axis=1)\n    D = np.diag(deg)\n    I = np.eye(m)\n    \n    L = D - X @ X.T\n    \n    # Step 4: Run K-Means clustering\n    def compute_centroids(centroids):\n        return centroids[np.argsort(np.sum((X[centroids] ** 2), axis=1))[:k]]\n        \n    centroids = X[np.random.choice(range(m), size=k, replace=False)]\n    \n    old_centroids = None\n    iteration = 0\n    \n    while True:\n        distances = np.einsum('ik,jk->ij', X, L)\n        new_labels = np.argmin(distances, axis=1)\n        \n        if np.array_equal(new_labels, old_centroids):\n            break\n        \n        old_centroids = centroids.copy()\n        centroids = X[new_labels]\n        centroids = compute_centroids(centroids)\n        iteration += 1\n        \n        if iteration >= 100:\n            raise Exception(\"Reached max iterations without convergence\")\n            \n    return list(new_labels)\n```"}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    prev_max_ix = max(min(ix - 1, len(params) - 1), 0)\n    \n    mine_prev_line = f\"Mine ({params[prev_max_ix][1]}):\\n{params[prev_max_ix][0]}\"\n    \n    theirs_prev_line = f\"Theirs ({params[prev_max_ix][1]}): {golds.get(params[prev_max_ix][1], '')}\"\n    \n    mine_curr_line = f\"Mine ({params[ix][1]}):\\n{params[ix][0]}\"\n    \n    theirs_curr_line = f\"Theirs ({params[ix][1]}):\\n{golds.get(params[ix][1], '')}{warn_str}\" if warn_str else f\"Theirs ({params[ix][1]}):\\n{golds.get(params[ix][1], '')}\"\n\n    return (\n        \"------------------------- DEBUG -------------------------\"\n        + \"\\n\\n\".join([mine_prev_line.strip(), theirs_prev_line.strip(), mine_curr_line.strip(), theirs_curr_line.strip()])\n        + \"\\n----------------------- END DEBUG -----------------------\"\n    )\n\n# Example usage\nparams_example = [(\"hello\", \"label_1\"), (\"world\", \"label_2\")]\ngolds_example = {\"label_1\": \"Hello World!\", \"label_2\": \"Goodbye Universe!\"}\nix_example = 1\nwarning_message = \"This was just a test.\"\n\nprint(err_fmt(params_example, golds_example, ix_example, warning_message))\n```\n\n### Expected Output:\n```\n------------------------- DEBUG -------------------------\nMine (label_1):\nhello\n\nTheirs (label_1):\nHello World!\n\nMine (label_2):\nworld\n\nTheirs (label_2):\nGoodbye Universe!\nThis was just a test.\n\n----------------------- END DEBUG -----------------------\n```"}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu: float = 0.0,\n    V: np.ndarray = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    N, M = X.shape\n    \n    if fit_intercept:\n        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n    \n    # Calculate posterior variance\n    if V is None:\n        V_inv = np.identity(M)\n    elif isinstance(V, int):\n        raise ValueError(\"V cannot be an integer.\")\n    else:\n        V_inv = np.diag(np.array(list(map(lambda x: 1 / x, V))))\n        \n    inv_XTX = np.linalg.inv(X.T @ X)  # Identity covariance term\n        \n    # Full posterior variance\n    sigma_squared = np.linalg.inv(V_inv + X.T @ X)\n    \n    # Posterior mean calculation\n    mu_b = np.dot(sigma_squared, \n                  np.dot(V_inv, mu) + X.T @ y)\n    \n    return [round(m, 4) for m in mu_b]\n\n# Example usage\nX_train = np.array([[1, 2], [3, 4], [5, 6]])\ny_train = np.array([1, 2, 3])\nprint(bayesian_posterior_mean(X_train, y_train))\n```"}
{"task_id": 308, "completion_id": 0, "completion": "Here's a Python script that implements the Mel filterbank matrix construction according to the provided specifications. This solution closely follows the requirements and uses numpy-like syntax where possible while maintaining clarity.\n```python\nimport numpy as np\n\ndef convert_to_mel(frequencies):\n    return 2595 * np.log10(1 + frequencies / 700)\n\ndef convert_from_mel(mels):\n    return 700 * (np.power(10, mels / 2595) - 1)\n\ndef build_mel_filterbank(N, n_filters=20, fs=44000, min_freq=0, max_freq=None, normalize=True):\n    if max_freq is None:\n        max_freq = fs // 2\n    \n    num_bins = N // 2 + 1\n    lower_bands = np.linspace(min_freq, max_freq, n_filters + 3)\n    upper_bands = np.linspace(max_freq, min_freq, n_filters + 3)[::-1]\n    \n    lower_mels = convert_to_mel(lower_bands)\n    upper_mels = convert_to_mel(upper_bands)\n    \n    freqs = np.sqrt(\n        ((lower_mels[:-1] - lower_mels[1:]) *\n         (upper_mels[:-1] - upper_mels[1:]) +\n         (upper_mels[1:] - upper_mels[:-1]) * \n         (lower_mels[1:] - lower_mels[:-1]))\n    )\n    \n    weights = freqs / sum(freqs)\n    widths = freqs / 2\n    \n    weights /= np.sum(weights)\n    \n    if normalize:\n        weights *= 2.0 / (upper_mels - lower_mels)\n    \n    filter_bank = [\n        [weights[i], widths[i]] for i in range(num_bins)\n    ]\n    \n    return filter_bank.tolist()\n\n# Example usage:\nfilterbank = build_mel_filterbank(8000, n_filters=16, fs=16000)\nfor band in filterbank:\n    print(band)\n```\nThis script defines three functions: `convert_to_mel`, `convert_from_mel`, and `build_mel_filterbank`. It converts between frequency and Mel scales, generates the filter bank based on the specified number of bands and sampling rate, and returns the result as a list of lists, each representing a filter bank entry. The example usage at the end demonstrates how to call the `build_mel_filterbank` function with appropriate parameters and prints the resulting filter banks."}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    if window_len <= 0:\n        return []\n\n    if window_len == 1:\n        return [1.0]\n\n    alpha = 0.35875\n    beta = 0.48829\n    gamma = 0.14128\n    delta = 0.01168\n    n = 0\n    bh_coeffs = []\n    \n    while n < window_len:\n        cos_term = lambda t: math.cos(2 * math.pi * t / window_len)\n        \n        if symmetric:\n            coeff = (\n                alpha - delta * cos_term(n),\n                beta - delta * cos_term(n),\n                gamma - delta * cos_term(n),\n            )\n        else:\n            coeff = (\n                alpha - delta * cos_term(n),\n                beta - 2 * delta * cos_term(n),\n                gamma - 3 * delta * cos_term(n),\n            )\n\n        bh_coeffs.extend(coeff)\n\n        n += 1\n\n    return [round(coef, 4) for coef in bh_coeffs[:window_len]]\n\n# Example usage:\nprint(blackman_harris(5))  # Should print the BH window of length 5 (using both forms)\nprint(blackman_harris(5, symmetric=True))  # Should print the BH window of length 5 using the symmetrical form\n```"}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    if d < 0 or d >= len(gamma) or t < 0 or t >= len(gamma[0]):\n        return None\n    \n    sum_gamma = np.sum(gamma[d, :])\n    \n    if sum_gamma == 0:\n        return math.log(float('inf'))  # Return infinity for division by zero\n    \n    term_1 = math.log(gamma[d, t])\n    term_2 = math.log(math.gamma(sum_gamma))\n    \n    psi_term_2 = math.psi(sum_gamma)\n    psi_sum = math.psi(term_2)\n    \n    approx_value = term_1 - psi_term_2\n    exact_value = term_1 - psi_sum\n    diff = abs(approx_value - exact_value)\n    \n    return round(approx_value, 4)\n\n# Example usage:\ngamma = [\n    [1.0, 1.0],\n    [1.0, 2.0]\n]\n\nprint(expected_log_dirichlet(gamma, 0, 0))  # Should print 0.0\nprint(expected_log_dirichlet(gamma, 1, 0))  # Should print 0.0\nprint(expected_log_dirichlet(gamma, 0, 1))  # Should print 0.0\nprint(expected_log_dirichlet(gamma, 1, 1))  # Should print 1.7917648891542633\n\nprint(expected_log_dirichlet([2.0], 0, 0))  # Should print -1.061796453406203\nprint(expected_log_dirichlet([2.0], 0, 1))  # Should print -1.8114379075412094\nprint(expected_log_dirichlet([0.5, 1.5], 0, 0))  # Should print -0.0878100154142022\n```"}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]], \n            golds: Dict[str, str], \n            ix: int, \n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    prev_params = params[prev_ix]\n    \n    debug_header = f'-------------------- DEBUG --------------------\\n'\n    debug_content = [\n        f'Mine ({prev_params[1]}):\\t{prev_params[0]}\\n',\n        f'Theirs ({prev_params[1]}):\\t{golds[prev_params[1]]}\\n\\n',\n        f'Mine ({params[ix][1]}):\\t{params[ix][0]}\\n',\n        f'Theirs ({params[ix][1]}):\\t{params[ix][1] if params[ix][1] in golds else \"\"}\\n'\n    ]\n    debug_footer = '-------------------- END DEBUG --------------------'\n\n    return debug_header + '\\n'.join(debug_content) + '\\n' + debug_footer + '\\n' + warn_str\n\n# Example usage\nparams_example = [(\"Hello world!\", \"Greeting\"), (\"Goodbye cruel world!\", \"Farewell\")]\ngolds_example = {\"Greeting\": \"Hello world!\", \"Farewell\": \"Goodbye cruel world!\"}\nix_example = 0\nprint(err_fmt(params_example, golds_example, ix_example))\n```"}
{"task_id": 318, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    # Initialize parameters\n    alpha_init = np.zeros(n_estimators)\n    w = np.ones(y_train.shape[0]) / y_train.shape[0]  # Equal weights initially\n    h = []  # List to store weak classifiers' predictions\n    \n    for m in range(n_estimators):\n        h.append(np.sign(np.dot((X_train[:, 1:] - np.mean(X_train[:, 1:], axis=0)), [2.0] + [-np.cos(2*np.pi*(i/float(m)))])))\n        \n        # Calculate errors for each weak classifier\n        e_m = ((y_train != h[m]).astype(float)).mean()\n        \n        # Update alpha for current weak classifier\n        epsilon_m = 2 * (e_m - 0.5)\n        alpha_m = epsilon_m / (2 * math.log(1.0/epsilon_m))\n        w *= np.exp(-alpha_m * y_train * h[m])\n        w /= w.sum()  # Normalize weights\n        \n    # Make predictions on test set\n    F = np.sign(np.sum(alpha_init * np.array(h), axis=0))\n    \n    return F.tolist()\n\n# Example usage:\n# X_train, y_train, X_test = get_data()  # Placeholder function\n# predictions = adaboost_predict(X_train, y_train, X_test, n_estimators=50)\n# print(predictions)\n```"}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: float = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : float, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    \n    f_spc = fs / N\n    \n    # Calculate the range of indices based on whether we need only non-negative frequencies\n    if positive_only:\n        min_index = 0\n        max_index = (N // 2) - 1\n    else:\n        min_index = 0\n        max_index = N - 1\n    \n    freqs = np.fft.fftfreq(max_index + 1, d=1/f_spc).astype(float)\n    \n    # Round the frequencies to four decimal places\n    freqs = round(freqs, 4)\n    \n    return freqs.tolist()\n\n# Example usage:\nprint(dft_bins(5))  # Positive-only mode\nprint(dft_bins(5, positive_only=False))  # Full list mode\n```"}
{"task_id": 331, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"\n    Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,)\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators)\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary mask indicating whether a prediction was obtained \n        from an out-of-bag tree (True/1) or not (False/0)\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the \n        OOB estimate cannot be computed.\n    \"\"\"\n    \n    if len(predictions) != len(oob_mask) or len(oob_mask[0]) != len(y_true):\n        raise ValueError(\"Input arrays do not match expected dimensions.\")\n        \n    oob_indices = [np.flatnonzero(mask).tolist() for mask in oob_mask]\n    total_predictions = []\n    for i, index_list in enumerate(oob_indices):\n        if not index_list:\n            continue\n        single_tree_predictions = np.array([preds[i] for preds in predictions]).reshape(-1)[index_list]\n        avg_prediction = np.mean(single_tree_predictions)\n        mse = ((avg_prediction - y_true[index_list])**2).mean()\n        total_predictions.append(mse)\n    \n    if not total_predictions:\n        return -1\n    else:\n        return round(np.mean(total_predictions), 4)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    y_true = np.random.rand(100)\n    np.random.seed(42)\n    predictions = [[[np.random.randn(n_trees)] for _ in range(50)] for n_trees in range(10)]\n    oob_mask = [[int(np.random.randint(0, 2)) for _ in range(10)] for _ in range(10)]\n\n    result = oob_mse(y_true, predictions, oob_mask)\n    print(result)\n```"}
{"task_id": 332, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef compute_item_support(transactions):\n    item_counts = Counter()\n    for transaction in transactions:\n        item_counts.update(transaction)\n    \n    return {item: count / len(transactions) for item, count in item_counts.items()}\n\ndef get_global_ordering(items):\n    # Sort items based on descending support, then alphabetically\n    return sorted(items, key=lambda x: (-compute_item_support([x]), x))\n\ndef build_fp_tree(transactions, min_support):\n    item_supports = compute_item_support(transactions)\n    frequent_items = {item: support for item, support in item_supports.items() if support >= min_support}\n    ordered_items = get_global_ordering(frequent_items)\n\n    root = {\"support\": 0, \"children\": {}}\n\n    def add_to_tree(node, transaction):\n        current_node = node\n        for item in transaction:\n            next_level_nodes = current_node[\"children\"].get(item, {\"support\": 0})\n            current_node[\"support\"] += next_level_nodes[\"support\"]\n            if \"children\" not in next_level_nodes:\n                next_level_nodes[\"children\"] = {}\n            next_node_key = f\"{next_level_nodes['support']}{item}\" if 'support' in next_level_nodes else item\n            next_level_nodes[next_node_key] = {\"support\": 1, \"children\": {}}\n            current_node = next_level_nodes[next_node_key]\n        \n        return current_node\n    \n    for transaction in transactions:\n        add_to_tree(root, transaction)\n    \n    return root\n\n# Example usage\ntransactions = [[\"apple\", \"banana\"], [\"banana\", \"orange\"], [\"grape\", \"apple\"], [\"banana\"]]\nmin_support = 2\nfp_tree_root = build_fp_tree(transactions, min_support)\nprint(fp_tree_root)\n```"}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\ndef linear_kernel(X: List[List[Union[int, float]]] | np.ndarray, Y: List[List[Union[int, float]]] | np.ndarray | None = None, c0: int | float = 0) -> List[List[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    if isinstance(Y, list):\n        Y = np.array(Y)\n\n    if not(X.shape[1] == Y.shape[1]):\n        return -1\n    \n    # Convert lists to numpy arrays for efficient computation\n    X_array = np.array(X)\n    Y_array = np.array(Y)\n    \n    # Compute the dot products and add the constant c0\n    K = np.dot(X_array, Y_array.T) + c0\n    \n    # Round the results to 4 decimal places\n    K_rounded = [list(map(lambda z: round(z, 4), row)) for row in K]\n    \n    return K_rounded\n\n\n# Example usage:\nX = [[1, 2], [3, 4]]\nY = [[-5, -6], [-7, -8]]\n\nK = linear_kernel(X, Y, c0=3)\nprint(K)\n```"}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"\n    Perform one Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter starting at 1.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) as\n        python lists, each rounded to 8 decimal places.\n    \"\"\"\n    m_t = beta1 * m + (1 - beta1) * grad\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n    m_hat = m_t / (1 - beta1 ** t)\n    v_hat = v_t / (1 - beta2 ** t)\n    theta_prime = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n    \n    return [round(theta_prime.item(), 8)], [round(m_t.item(), 8)], [round(v_t.item(), 8)]\n\n# Example usage:\n# Initialize some parameters and gradients\nparam = np.array([1., 2., 3.])\ngrad = np.array([-1., 0., 1.])\n\n# Initial moments\nm_init = np.zeros_like(param)\nv_init = np.zeros_like(param)\n\n# Perform a single Adam optimisation step\nupdated_params, new_m, new_v = adam_update(param, grad, t=1, m=m_init, v=v_init, lr=0.1)\n\n# Print results\nprint(\"Updated Parameter:\", updated_params[0], \"\\nNew First Moment:\", new_m[0], \"\\nNew Variance:\", new_v[0])\n```"}
{"task_id": 343, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float],\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\"\"\"\n    \n    # Convert rewards from list to NumPy array if it's not already one\n    rewards = np.array(rewards)\n    \n    # Ensure retain_prcnt * len(theta_samples) is an integer\n    retain_num = int(np.floor(retain_prcnt * len(theta_samples)))\n    if retain_num <= 0 or retain_num > len(theta_samples):\n        raise ValueError(\"retain_prcnt * N must be between 1 and the number of samples.\")\n    \n    # Sort theta_samples based on rewards in descending order\n    sorted_indices = np.argsort(-rewards)[:retain_num]\n    sorted_theta_samples = theta_samples[sorted_indices]\n    \n    # Compute the mean and per-dimension variance of the selected samples\n    mean_rewards = np.mean(sorted_theta_samples, axis=0)\n    variances = [\n        np.var(sorted_theta_samples[:, i])[0][0] \n        for i in range(len(sorted_theta_samples[0]))\n    ]\n    \n    # Round the results to four decimal places\n    mean_rewards = [round(r, 4) for r in mean_rewards]\n    variances = [round(v, 4) for v in variances]\n    \n    return mean_rewards, variances\n\n\n# Example usage:\ntheta_samples_ex = np.random.rand(5, 3)  # Sampled from some policy\nrewards_ex = [10, 20, 15, 8, 25]\nretain_prcnt_ex = 0.75\n\nmean_reward, variances = cross_entropy_update(theta_samples_ex, rewards_ex, retain_prcnt_retain_prcnt_ex)\n\nprint(f\"Mean Reward: {mean_reward}\")\nprint(f\"Variance of Rewards per Dimension: {varianses}\")\n```"}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels):\n    if not labels:\n        return 0.0\n    \n    # Convert labels to a NumPy array for easier processing\n    labels = np.array(labels)\n    \n    # Count occurrences of each unique label\n    label_counts = Counter(labels)\n    \n    # Calculate probabilities\n    probs = {label: count / len(labels) for label, count in label_counts.items()}\n    \n    # Compute entropy\n    entropy_value = sum([-probs[label] * math.log2(probs[label]) \n                         for label in probs])\n    \n    # Round to 5 decimal places\n    return round(entropy_value, 5)\n\n# Example usage:\nlabels_list = ['red', 'blue', 'green', 'blue']\nprint(f\"Entropy of label list: {entropy(labels_list)}\")\nlabels_np_array = np.array(['red', 'blue', 'green', 'blue'])\nprint(f\"Entropy of label array: {entropy(labels_np_array)}\")\n\n# Test case where there are no labels\nempty_labels = []\nprint(f\"Entropy with empty labels: {entropy(empty_labels)}\")\n\n# Test case where all labels are identical\nidentical_labels = [6] * 1000\nprint(f\"Entropy of identical labels: {entropy(identical_labels)}\")\n```"}
{"task_id": 354, "completion_id": 0, "completion": "Below is my solution for implementing the fast 2-D convolution via im2col and GEMM strategy:\n\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"\n    Performs a 2-D convolution (cross-correlation) on a batch of images using im2col and GEMM strategies.\n    \n    Parameters:\n        X (np.ndarray): Input batch of images with shape (n_ex, in_rows, in_cols, in_channels)\n        W (np.ndarray): Kernel bank with shape (out_rows, out_cols, in_channels, num_filters)\n        stride (int): Stride of the convolution operation\n        pad (int, tuple, str): Padding specification (integer, tuple, or 'same')\n        dilation (int, optional): Dilation rate of the convolution\n    \n    Returns:\n        list: The resulting convolved image data as a nested list\n    \"\"\"\n    # Define constants\n    n_ex, in_rows, in_cols, in_channels = X.shape\n    out_rows, out_cols, _, num_filters = W.shape\n    \n    if type(pad) == int:\n        pad_rows = pad\n        pad_cols = pad\n    elif type(pad) == tuple:\n        pad_rows, pad_cols = pad\n    elif isinstance(pad, str) and pad.lower() != 'same':\n        raise ValueError(\"Invalid padding specification\")\n        \n    if pad_rows % 2 == 0:\n        first_pad = [0, 0, 0, 0]\n    else:\n        first_pad = [(pad_rows - 1) // 2, (pad_rows - 1) // 2, 0, 0]\n    second_pad = [first_pad[2], first_pad[3], pad_cols - 1, pad_cols - 1]\n    \n    padded_X = np.pad(X, ((0,), (0,) * 3 + first_pad), mode='constant', constant_values=0)\n    \n    col_indices = []\n    row_indices = range(padded_X.shape[1] - out_rows + 1, padded_X.shape[1])\n    for i in row_indices:\n        col_indices.append(i)\n    col_indices = np.array(col_indices).reshape(-1, 1).T\n        \n    def reshape_3d_to_2d(arr):\n        return arr.reshape(arr.shape[0], -1)\n    \n    # Prepare im2col indices\n    k_ind = (np.tile(np.arange(out_cols), out_rows)[:, None] + np.expand_dims(col_indices, axis=-1))\n    x_ind = np.tile(np.expand_dims(row_indices, 1), (k_ind.shape[0], 1)).flatten()\n    y_ind = np.repeat(k_ind.flatten(), in_cols).reshape((-1, 1))\n    w_ind = np.repeat(np.expand_dims(np.arange(W.shape[-1]), 0), k_ind.shape[0]).flatten().reshape((1,-1))\n    \n    # Perform the convolution using im2col and GEMM \n    res_col = np.einsum('ijkl,jlk->il', padded_X, W)\n    \n    # Reshape the result back to original dimensions\n    res_shape = (n_ex, out_rows, out_cols, num_filters)\n    res = np.reshape(res_col, res_shape)\n    \n    return res.tolist()\n\n# Example usage\nX = np.random.rand(64, 16, 16, 3)\nW = np.random.rand(8, 8, 3, 1)\n\nconv_result = conv2D(X, W, stride=1, pad=(0, 0), dilation=0)\nprint(conv_result[:4])\n```\n\nThis script defines the `conv2D` function according to the provided requirements. It pads the input image, converts it into a matrix of column values using im2col, then applies the actual convolution using GEMM operations. Finally, it reshapes the result back to its original multidimensional form and returns it as a nested list."}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    # Step 1: Standardize the data\n    centered_data = data - np.mean(data, axis=0)\n\n    # Step 2: Compute the principal directions depending on the solver\n    if solver == \"svd\":\n        U, S, _ = np.linalg.svd(centered_data)\n        principal_directions = U[:, :n_components]\n    elif solver == \"eigen\":\n        cov_matrix = np.cov(centered_data.T, rowvar=False, ddof=1)\n        evals, evecs = np.linalg.eig(cov_matrix)\n        sorted_evecs = evecs[np.argsort(evals)[::-1], :]\n        principal_directions = sorted_evecs[:, :n_components]\n\n    # Step 3: Ensure consistent signs for each principal direction\n    for i in range(principal_directions.shape[1]):\n        if np.any(principal_directions[:, i] < 0):\n            principal_directions[:, i] *= -1\n            principal_directions[-1:, :] *= -1\n\n    # Step 4: Project the data onto the principal directions\n    scores = principal_directions @ centered_data\n\n    # Step 5 & 6: Extract explanations of variance and return results\n    explained_variance_ratio = np.sum(np.square(scores, axis=0)) / np.sum(\n        np.square(centered_data, axis=0)\n    )\n    explained_variance_ratio = [round(ratio, 4) for ratio in explained_variance_ratio]\n\n    return [\n        [[round(score[i], 4) for i in range(len(row))] for row in scores],\n        explained_variance_ratio,\n    ]\n```"}
{"task_id": 356, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if not isinstance(leaf.value, (list, tuple)):\n        raise TypeError(\"Leaf 'value' must be a list or tuple for regression\")\n\n    if len(leaf.value) == 1 and isinstance(leaf.value[0], float):\n        raise ValueError(\"Regression leaf contains only one element and is a float, which is ambiguous\")\n\n    if classifier:\n        max_index = np.argmax(np.array(leaf.value))\n        return max_index\n    else:\n        return leaf.value\n\n\n# Example usage:\n# Define some test data points\ntest_leaf_classification = Leaf([0.2, 0.3, 0.5])\ntest_leaf_regression = Leaf(4.2)\n\n# Predictions using the defined functions\nprint(leaf_predict(test_leaf_classification, True))  # Expected output: 2\nprint(leaf_predict(test_leaf_regression, False))     # Expected output: 4.2\n\n# Check function to verify correctness\ndef check_predictions():\n    assert leaf_predict(test_leaf_classification, True) == 2\n    assert leaf_predict(test_leaf_regression, False) == 4.2\n\ncheck_predictions()\n```"}
{"task_id": 357, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    assert len(data.shape) == 2, \"Data must be a 2-dimensional numpy array.\"\n    assert K > 0, \"Number of clusters must be greater than zero.\"\n    \n    # Initialize centroids with the first K samples\n    centroids = data[:K]\n    \n    # Iterate up to max_iters or until no change occurs\n    for _ in range(max_iters):\n        # Assign each sample to the closest centroid\n        clusters = [[] for _ in range(K)]\n        for point in data:\n            distances = [(np.linalg.norm(point - c), i) for i, c in enumerate(centroids)]\n            _, index = min(distances)\n            clusters[index].append(point)\n        \n        # Compute new centroids\n        updated_centroids = []\n        for cluster in clusters:\n            if cluster:\n                updated_centroids.append(np.mean(cluster, axis=0))\n            else:\n                updated_centroids.append(centroids[0])\n        \n        # Check for convergence\n        changed = False\n        for old, new in zip(centroids, updated_centroids):\n            if not np.allclose(old, new, atol=1e-6):\n                changed = True\n                break\n        \n        if not changed:\n            print(f\"Converged after {len(clusters)} iterations.\")\n            break\n            \n        centroids = updated_centroids\n    \n    # Round to 4 decimal places and sort lexicographically\n    sorted_centroids = [[round(c[i], 4) for i in range(len(c))] for c in centroids]\n    sorted_centroids.sort()\n    return sorted_centroids\n\n# Example usage\nif __name__ == \"__main__\":\n    data = np.array([\n        [1, 1],\n        [1, 2],\n        [2, 1],\n        [8, 8],\n        [9, 8],\n        [8, 9]\n    ])\n    K = 2\n    result = kmeans(data, K)\n    print(result)\n```"}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | np.ndarray) -> list[list[float]]:\n    # Convert data to a NumPy array if not already one\n    arr = np.array(data)\n    \n    # Check for any row sum equal to zero\n    if np.any(arr.sum(axis=1) == 0):\n        return -1\n    \n    # Normalize each row\n    scaled_arr = arr / arr.sum(axis=1, keepdims=True)\n    \n    # Round the elements to 4 decimal places\n    normalized_arr = np.round(scaled_arr, 4).tolist()\n    \n    return normalized_arr\n\n# Example usage:\ninput_data = [[0.1, 0.2], [0.2, 0.3], [0.7, 0.8]]\nprint(row_stochastic_matrix(input_data))\n\n# Expected output: [[0.1, 0.2], [0.2, 0.3], [-1]]\n\ninvalid_input = [[1, 0], [0, 1], [0, 0]]\nprint(row_stochastic_matrix(invalid_input))\n```"}
{"task_id": 363, "completion_id": 0, "completion": "```python\nfrom typing import *\nimport numpy as np\n\n\nclass Space:  # Abstract base class\n    pass\n\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: Tuple[float, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\n\nclass Env:\n    \"\"\"Minimal environment class for testing.\"\"\"\n\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool) -> Tuple[bool, bool]:\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    This implementation checks the type of each space directly without using the\n    actual instances of the spaces from the environment. It assumes that the\n    environment exposes `action_space` and `observation_space` which represent\n    the action and observation spaces, respectively.\n\n    Args:\n        env: The environment object to test.\n        tuple_action: Indicates whether the action space is composite or not.\n        tuple_obs: Indicates whether the observation space is composite or not.\n\n    Returns:\n        A tuple of booleans indicating whether the action and observation spaces are continuous.\n    \"\"\"\n    action_space = env.action_space\n    observation_space = env.observation_space\n\n    cont_action = isinstance(action_space, Box) if tuple_action else (\n        all(isinstance(s, Box) for s in action_space.spaces)\n    )\n\n    cont_obs = isinstance(observation_space, Box) if tuple_obs else (\n        all(isinstance(s, Box) for s in observation_space.spaces)\n    )\n\n    return cont_action, cont_obs\n\n\nif __name__ == \"__main__\":\n    # Test cases\n    box_space = Box(0.0, 1.0, (4,))\n    discrete_space = Discrete(3)\n    tuple_space = Tuple([box_space, discrete_space])\n    dict_space = Dict({\"a\": box_space, \"b\": discrete_space})\n\n    env = Env(tuple_space, dict_space)\n\n    print(is_continuous(env, True, False))  # Expected output: (True, True)\n    print(is_continuous(env, False, True))  # Expected output: (True, True)\n    print(is_continuous(env, False, False))  # Expected output: (True, False)\n    print(is_continuous(env, True, True))   # Expected output: (True, True)\n    ```"}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    # Initialize the running prediction with zeros\n    running_prediction = np.zeros_like(updates[0])\n\n    # Aggregate the gradients from each tree\n    for u in updates:\n        if not regression:\n            running_prediction -= learning_rate * u / len(updates)\n        else:\n            running_prediction -= learning_rate * u\n    \n    # Interpretation based on whether it's regression or classification\n    if regression:\n        # Convert logits to probabilities using softmax\n        probabilities = np.exp(running_prediction) / np.sum(np.exp(running_prediction), axis=0)\n        # Return the classes using argmax since we don't need rounding for regression\n        return np.argmax(probabilities, axis=0)\n    else:\n        # Return the raw running prediction which represents class probabilities\n        return running_prediction\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data to test the function\n    X = np.array([[0.1, -0.5], [0.3, -0.7]])\n    # Predictions from multiple trees\n    updates = [\n        np.array([0.9, 1.1]),\n        np.array([-0.8, -0.6]),\n        np.array([0.7, 0.5])\n    ]\n    # Learning rate\n    lr = 0.1\n    # Regression example\n    reg_example = True\n    print(gradient_boosting_predict(updates, lr, reg_example))\n\n    # Classification example\n    cls_example = False\n    print(gradient_boosting_predict(updates, lr, cls_example))\n```"}
{"task_id": 371, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between X and W.\"\"\"\n    \n    n_examples, signal_length, _ = X.shape\n    _, kernel_width, out_channels = W.shape\n    \n    # Calculate the new dimensions after applying padding and dilation\n    if isinstance(pad, int):\n        pad_left = pad\n        pad_right = pad\n    elif isinstance(pad, tuple):\n        pad_left = pad[0]\n        pad_right = pad[1]\n    else:\n        assert pad == 'same'\n        out_len = np.ceil(signal_length / stride).astype(int)\n        total_pad = max(0, (out_len - 1) * stride + kernel_width - signal_length)\n        pad_left = total_pad // 2\n        pad_right = total_pad - pad_left\n        \n    dilated_kernel = np.lib.stride_tricks.as_strided(W,\n                                                    shape=(kernel_width, out_channels),\n                                                    strides=(dilation * stride, stride), \n                                                    writeable=False)\n    \n    # Perform the cross-correlation\n    result_volume = np.empty((n_examples, signal_length + 2*pad_left - 1, out_channels))\n    for i in range(n_examples):\n        result_volume[i] = np.convolve(X[i], dilated_kernel, mode='valid')\n        \n    return result_volume.tolist()\n\n# Example usage:\nexample_X = np.array([[[1., 2., 3.],\n                       [4., 5., 6.],\n                       [7., 8., 9.]],\n                      [[2., 3., 4.],\n                       [5., 6., 7.],\n                       [8., 9., 10.]]])\nexample_W = np.ones((5, 3, 1))  # Kernel with one channel\nresult = conv1D(example_X, example_W, stride=1, pad=2)\nprint(result)\n```"}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini(y: list[int] | np.ndarray) -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if not isinstance(y, (list, np.ndarray)):\n        raise TypeError(\"Input must be a list or NumPy array\")\n    \n    if not any(isinstance(i, int) for i in y):\n        raise ValueError(\"All elements in y must be integers\")\n    \n    unique_classes = sorted(set(y))\n    n_samples = len(y)\n    gini_impurity = 1\n    \n    for c in unique_classes:\n        p_c = sum(c == cl for cl in y) / n_samples\n        gini_impurity -= p_c ** 2\n    \n    return round(gini_impurity, 4)\n\n# Example usage\ndata = [1, 3, 2, 2, 1, 3]\nprint(gini(data))  # Expected output: ~0.5598\n```"}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"\n    Train a multiple linear regression model using batch gradient descent.\n\n    Parameters:\n        X (np.ndarray): Feature matrix of size m x n where m is the number of samples and n is the number of features.\n        y (np.ndarray): Target values array of size m.\n        learning_rate (float): The step size for the gradient descent update.\n        n_iterations (int): Number of iterations for the gradient descent algorithm.\n\n    Returns:\n        tuple: A tuple containing the list of learned weights and the bias value, both rounded to 4 decimal places.\n    \"\"\"\n    if len(X.shape) != 2 or len(y.shape) != 1:\n        raise ValueError(\"X and y arrays must have 2D and 1D shape respectively.\")\n    \n    # Initialize theta and beta\n    n_samples, n_features = X.shape\n    theta = np.zeros(n_features)\n    b = 0\n    \n    mse = lambda theta, b, X, y: sum((X @ theta + b - y) ** 2) / n_samples\n    \n    for _ in range(n_iterations):\n        dw = 2 / n_samples * X.T @ (X @ theta + b - y)\n        db = 2 / n_samples * np.sum(X @ theta + b - y)\n        \n        theta -= learning_rate * dw\n        b -= learning_rate * db\n        \n        current_mse = mse(theta, b, X, y)\n        \n        if _ % 100 == 0:\n            print(f\"Iteration {_}: MSE = {current_mse:.4f}\")\n            \n    theta = [round(t, 4) for t in theta]\n    b = round(b, 4)\n\n    return (theta, b)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample feature matrix and target values\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    \n    try:\n        weights, bias = linear_regression_gd(X, y)\n        print(weights, bias)\n    except Exception as e:\n        print(e)\n```"}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert the list of lists to a NumPy array\n    X_np = np.array(X)\n    \n    # Check if the matrix is square\n    if X_np.shape[0] != X_np.shape[1]:\n        return False\n    \n    # Compute the transpose of the matrix using `.T`\n    transposed_X = X_np.T\n    \n    # Use allclose to check if the original matrix is equal to its transpose within a tolerance\n    return np.allclose(X_np, transposed_X)\n\n# Example usage:\nexample_matrix_1 = [[1, 2], [3, 4]]\nexample_matrix_2 = [[5, 6], [7, 8]]\nnon_square_matrix = [[1, 2, 3], [4, 5]]\n\nprint(is_symmetric(example_matrix_1))  # Expected output: True\nprint(is_symmetric(example_matrix_2))  # Expected output: True\nprint(is_symmetric(non_square_matrix))  # Expected output: False\n```"}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Calculate the negative gradient for the Logistic Loss function in Gradient Boosting.\n\n    Parameters:\n    - y: A list containing binary class labels (0 or 1).\n    - f: A list of floats representing predicted scores.\n\n    Returns:\n    - A list of negative gradients rounded to the nearest 4th decimal place.\n    If y contains values other than 0 or 1, returns [-1].\n    \"\"\"\n    if not all(label in [0, 1] for label in y):\n        return [-1]\n\n    # Apply the transformation rule y' = 2y - 1\n    y_prime = 2 * y - 1\n\n    # Calculate the negative gradient g_i\n    g = [(y_prime[i] / (1 + np.exp(y_prime[i] * f[i]))) for i in range(len(f))]\n\n    # Round each element to the nearest 4th decimal place\n    g_rounded = [round(val, 4) for val in g]\n    \n    return g_rounded\n\n# Example usage:\ny_example = [0, 1, 1, 0]\nf_example = [0.5, 1.2, 0.7, 0.8]\nprint(logistic_negative_gradient(y_example, f_example))\n```"}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    if isinstance(x, (int, float)):\n        # Compute SELU directly for a single input\n        out = 1.6732632423543772848170429916717 * (np.where(x > 0, x, 1.6732632423543772848170429916717 * (np.exp(x) - 1)))\n    elif len(x.shape) == 1:\n        # Apply SELU for 1-dimensional input\n        out = 1.6732632423543772848170429916717 * (np.where(x > 0, x, 1.6732632423543772848170429916717 * (np.exp(x) - 1)))\n    else:\n        # Apply SELU for multi-dimensional input\n        out = []\n        for i, j in enumerate(zip(*[iter(x)] * x.ndim)):\n            tmp_out = 1.6732632423543772848170429916717 * (np.where(j > 0, j, 1.6732632423543772848170429916717 * (np.exp(j) - 1)))\n            out.append(tuple(tmp_out))\n        out = tuple(out)\n    \n    # Round to 4 decimal places\n    out_rounded = [round(val, 4) for val in out]\n    \n    return out_rounded\n\n# Example usage:\nprint(selu(1))          # Expected output: [1.6733]\nprint(selu([0, 1, -1])) # Expected output: [[0., 1.6733, -0.6733]]\nprint(selu(np.array([[0, 1], [-1, 2]]))) # Expected output: [[[0., 1.6733],\n                                                 #[[-1., -0.6733]]]]\n\n# Check function to verify correctness\ndef check_function():\n    assert np.allclose(selu(np.array([-10, 8])), [(-10, 0.007], [8, 1.808]))\n    assert selu(np.array([2, 3])) == [2.7183, 3.7183]\n    print(\"All checks passed.\")\n\ncheck_function()\n```"}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimal places and converted to a Python list.\n    \"\"\"\n    \n    # Initialize base prediction for all points\n    pred = np.mean(y)\n    preds = [pred]\n    \n    for _ in range(n_estimators):\n        # Calculate residuals\n        residuals = y - preds[-1]\n        \n        # Get unique sorted x values\n        unique_x = np.unique(np.sort(x))\n        num_points = len(unique_x) - 1\n        \n        # Predict based on mean residual on either side of each split\n        for i in range(num_points):\n            lower_residuals = residuals[x <= unique_x[i]]\n            upper_residuals = residuals[x > unique_x[i]]\n            \n            if not lower_residuals.size or not upper_residuals.size:\n                # No valid splits found; predict globally\n                lower_mean_residual = np.mean(lower_residuals)\n                upper_mean_residual = np.mean(upper_residuals)\n                lower_prediction = lower_mean_residual * (x <= unique_x[i])\n                upper_prediction = upper_mean_residual * (x > unique_x[i])\n                \n                predicted_value = lower_prediction + upper_prediction\n                \n            else:\n                # Find best split using mean square error\n                avg_lower_residual = np.mean(lower_residuals)\n                avg_upper_residual = np.mean(upper_residuals)\n                combined_residual = lower_residuals + upper_residuals\n                \n                total_squares = np.sum(combined_residual**2)\n                best_split = None\n                min_error = float('inf')\n                \n                for j in range(i, num_points):\n                    center = (unique_x[j] + unique_x[j+1]) / 2\n                    \n                    l_square, u_square = ((lower_residuals < center)**2).mean(), ((upper_residuals >= center)).mean()\n                    \n                    error = (l_square + u_square) / total_squares\n                    \n                    if error < min_error:\n                        min_error = error\n                        best_split = center\n                        \n                low_predictions = lower_residuals.copy() + (best_split - x) * avg_lower_residual\n                high_predictions = upper_residuals.copy() + (x - best_split) * avg_upper_residual\n                \n                predicted_value = (low_predictions <= best_split).astype(int) * avg_lower_residual + \\\n                                   (high_predictions > best_split).astype(int) * avg_upper_residual\n    \n            # Update predictions with the new residual information\n            preds.append(predicted_value)\n            pred += learning_rate * predicted_value\n            \n    return [np.round(p, 4) for p in preds]\n\n# Example usage\nx_data = [1, 2, 3, 4, 5]\ny_data = [0.6928, 0.7933, 0.6726, 0.7257, 0.7728]\npredictions = gradient_boosting_1d(x_data, y_data, n_estimators=10)\nprint(predictions)\n```"}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None*, a zero array of\n        the same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)``, where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    \n    # Initialize Eg if it is None or set initial value to 0\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n        \n    # Compute E_g^(t+1)\n    E_g_t_plus_1 = rho * Eg + (1 - rho) * np.square(grad)\n    \n    # Compute w^(t+1)\n    w_next = w - learning_rate * grad / np.sqrt(E_g_t_plus_1 + 1e-8)\n    \n    # Round and return the results\n    return [round(x[0], 4), round(x[1], 4)], [round(y[0], 4), round(y[1], 4)]\n\n# Example usage\nif __name__ == \"__main__\":\n    w = np.array([1.0, 2.0])\n    grad = np.array([[0.5, 1.0], [-1.0, 0.5]])\n    Eg = np.zeros_like(grad)\n    lr = 0.01\n    rho = 0.9\n\n    w_next, Eg_next = rmsprop_update(w, grad, Eg, lr, rho)\n    print(\"Updated Parameters:\", w_next)\n    print(\"Updated Running Average:\", Eg_next)\n```"}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha=0.1, derivative=False):\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If True, return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Check if derivative is requested\n    if derivative:\n        # Compute the gradient using the provided formula\n        return [elu(x_val, alpha, derivative=True) + alpha for x_val in x]\n    else:\n        # Apply the ELU formula and round to 4 decimal places\n        return [np.where(val >= 0, val, alpha * (np.exp(val) - 1)) for val in x]\n\n# Test cases\nprint(elu([0, 1, -1]))\nprint(elu([-1, 0, 1], derivative=True))\n```"}
{"task_id": 411, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    import itertools\n\n    def flatten(data):\n        return list(itertools.chain.from_iterable(data))\n\n    actions_flattened = flatten(flatten(actions))\n    observations_flattened = flatten(observations)\n\n    multidim_actions = any(isinstance(action, tuple) for action in actions)\n    multidim_observations = any(isinstance(observation, tuple) for observation in observations)\n    continuous_actions = any(np.issubdtype(type(x), np.number) and isinstance(x, int) for x in actions_flattened)\n    continuous_observations = any(np.issubdtype(type(y), np.number) and not isinstance(y, int) for y in observations_flattened)\n\n    n_actions_per_dim = [len(set(action)) for action in actions]\n    n_obs_per_dim = [len(set(observation)) for observation in observations]\n\n    action_ids = [sorted(set(action)) for action in actions]\n    obs_ids = [sorted(set(observation)) for observation in observations]\n\n    return {\n        ('tuple_actions', 'tuple_observations'): multidim_actions,\n        ('multi_dim_actions', 'multi_dim_observations'): multidim_observations,\n        ('continuous_actions', 'continuous_observations'): (continuous_actions, continuous_observations),\n        ('n_actions_per_dim', 'n_obs_per_dim'): n_actions_per_dim + n_obs_per_dim,\n        ('action_dim', 'obs_dim'): len(n_actions_per_dim) == 2,\n        ('action_ids', 'obs_ids'): (action_ids, obs_ids)\n    }\n\n\n# Example usage\nactions = [(0, 1), (1, 0), (1, 1)]\nobservations = [10.0, 11.5, 12.0]\n\nprint(env_stats(actions, observations))\n```"}
{"task_id": 413, "completion_id": 0, "completion": "Sure! Here's the implementation of the Gradient Boosting Decision Tree (GBDT) regressor according to the given requirements:\n\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    # Initialize the predictions with the mean target values\n    predictions = np.mean(y_train)\n    \n    # Create a list to store the final predictions\n    final_predictions = []\n    \n    for _ in range(n_estimators):\n        # Residuals after previous predictions\n        residuals = y_train - predictions\n        \n        # Calculate the weighted average using the current predictor\n        weighted_residuals = residuals * learning_rate\n        \n        # Fit a single-level decision tree on the residuals\n        tree = np.full_like(residuals, fill_value=np.median(residuals))\n        \n        # Split the data based on the median residual\n        unique_resids, counts = np.unique(residuals, return_counts=True)\n        sorted_indices = np.argsort(residuals)\n        \n        # Assign weights to splits based on their frequencies\n        weights = learning_rate / resids[unique_resids][sorted_indices]\n        \n        for i in range(len(unique_resids)):\n            if i == len(unique_resids) - 1:\n                break\n            elif unique_resids[i] != unique_resids[i + 1]:\n                threshold = unique_resids[i]\n                indices_to_split = np.where((residuals <= threshold) & (weights > 0))[0]\n                \n                # Insert new non-zero split positions at appropriate locations\n                insert_positions = [np.argwhere(weights == 0)[0] + 1] + [(i,) for j, k in zip(sorted(indices_to_split[:-1], key=lambda x: abs(x)), sorted(indices_to_split[-1:], reverse=True)) if k < j]\n                insert_positions.sort()\n                \n                for pos in insert_positions:\n                    tree[pos] += np.sign(threshold) * sum(counts[indices_to_split[:pos]]) / np.sum(counts)\n        \n        # Update the predictions with the fitted tree\n        predictions -= tree\n    \n    # Round the predictions to 4 decimal places and convert them to a list\n    final_predictions = [round(pred, 4) for pred in predictions]\n    \n    return final_predictions\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Mock training data\n    X_train = np.array([[1, 2], [2, 3], [3, 5], [4, 8], [5, 7]])\n    y_train = np.array([0, 1, 0, 1, 0])\n\n    # Mock test data\n    X_test = np.array([[6, 9], [7, 11]])\n\n    # Perform GBDT regression\n    predictions = gbdt_regression(X_train, y_train, X_test)\n\n    print(predictions)\n```\n\nThis script implements the gradient boosting decision tree (GBDT) using pure NumPy operations without any external libraries like scikit-learn or TensorFlow. It starts with an initial prediction being the mean of the training targets, iteratively updates this prediction by fitting a simple CART tree to the negative gradients of the squared error loss, and repeats until all estimators have been trained. Finally, it returns the predictions for the test set rounded to four decimal places."}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Calculate the multivariate Gaussian Probability Density Function (PDF).\n\n    Parameters:\n    X : numpy.ndarray\n        An array of shape (n_samples, n_features) containing the data points.\n    mean : numpy.ndarray\n        The mean vector of the distribution.\n    cov : numpy.ndarray\n        The covariance matrix of shape (n_features, n_features). Must be positive-definite.\n\n    Returns:\n    pdf_values : list\n        A list of PDF values corresponding to each sample in X, rounded to 4 decimal places.\n    \"\"\"\n    d = len(mean)\n    \n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n        \n    det_cov = np.linalg.det(cov)\n    inv_cov = np.linalg.inv(cov)\n    exponent = -0.5 * np.sum((X[:, np.newaxis] - mean)**2, axis=2) @ inv_cov @ (X[:, np.newaxis] - mean)\n    gaussian_pdf = (1 / (np.sqrt((2 * np.pi)**d * det_cov))) * np.exp(exponent)\n    \n    return [round(pdf_value, 4) for pdf_value in gaussian_pdf]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data points\n    samples = np.array([[1, 1], [3, 4], [6, 8]])\n    mean_vector = np.array([2, 2])\n    covariance_matrix = np.array([[4, 0], [0, 9]])\n\n    # Compute the PDFs\n    pdf_values = multivariate_gaussian_pdf(samples, mean_vector, covariance_matrix)\n\n    print(\"PDF Values:\", pdf_values)\n```"}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n\n    # Add intercept if required\n    if fit_intercept:\n        intercept_column = np.ones(X.shape[0])\n        X = np.column_stack((intercept_column, X))\n\n    N, M = X.shape\n    _, ny = y.shape\n\n    # Initialize matrices\n    X_t_X = np.dot(X.T, X)\n    X_t_y = np.dot(X.T, y)\n\n    # Prior covariance matrix V\n    if V is not None:\n        V_inv = 1 / V\n    else:\n        V_inv = np.eye(M + 1)\n\n    # Posterior covariance matrix\n    inv_sigma_XX_V = X_t_X + V_inv @ np.outer(mu, mu)\n\n    # Posterior mean\n    posterior_mean = (np.linalg.inv(inv_sigma_XX_V)) @ (\n        V_inv @ mu + np.dot(X_t_y, V_inv)\n    )\n\n    # Predictive mean\n    pred_means = []\n    for x_new in X_new:\n        # Convert each entry of X_new from a single point to a row vector\n        x_new_row = np.array([1] + list(x_new))\n        pred_mean_x_new = posterior_mean @ x_new_row\n        pred_means.append(round(pred_mean_x_new.item(), 4))\n\n    return pred_means\n\n\n# Example usage\nif __name__ == \"__main__\":\n    X_ex = np.random.rand(5, 4)  # Example data matrix\n    y_ex = np.random.rand(5)      # Example target vector\n    X_new_ex = np.random.rand(7, 4)  # Example new design matrix\n\n    predictions = bayesian_linear_regression_pred(\n        X=X_ex, \n        y=y_ex,\n        X_new=X_new_ex,\n    )\n    print(predictions)\n```"}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\n\ndef root_mean_squared_log_error(actual: List[float], predicted: List[float]) -> float:\n    \"\"\"\n    Computes the Root Mean Squared Logarithmic Error (RMSLE) between the actual and predicted values,\n    ensuring non-negative input values. Returns the RMSLE rounded to four decimal places.\n\n    Args:\n        actual (List[float]): The ground truth values.\n        predicted (List[float]): The model's predictions.\n\n    Returns:\n        float: The computed RMSLE value rounded to four decimal places.\n\n    Raises:\n        ValueError: If the input lists do not have the same length or contain negative values.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted values must be of equal length.\")\n    \n    if any(x < 0 for x in actual + predicted):\n        raise ValueError(\"Values in actual and predicted must be non-negative.\")\n\n    n = len(actual)\n    rmsle = np.sqrt(np.mean((np.log1p(predicted) - np.log1p(actual)) ** 2))\n    return round(rmsle, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Dummy data points for testing purposes\n    actual_data = [1.0, 2.0, 3.0]\n    predicted_data = [0.95, 1.98, 2.97]\n    \n    try:\n        result = root_mean_squared_log_error(actual_data, predicted_data)\n        print(f\"Computed RMSLE: {result}\")\n    except ValueError as e:\n        print(e)\n```\n\nThis code snippet defines a function `root_mean_squared_log_error` that calculates the Root Mean Squared Logarithmic Error (RMSLE) using NumPy operations for efficiency. It includes validation checks to ensure both inputs are of equal length and contain only non-negative numbers before computing the error. The result is then rounded to four decimal places and returned."}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\nfrom typing import List, Tuple\n\n\ndef hanning_window(window_len: int, symmetric: bool = False) -> List[float]:\n    \"\"\"\n    Generate a Hann (Hanning) window.\n\n    Parameters:\n    window_len (int): Number of samples in the returned window.\n    symmetric (bool, optional): Whether to generate a symmetric window or a periodic window.\n                                Defaults to False.\n\n    Returns:\n    list[float]: The Hann window coefficients rounded to 4 decimal places. Returns an empty list when window_len <= 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n\n    if symmetric:\n        half_length = math.floor((window_len + 1) / 2)\n        hann_coefficients = [\n            round(0.5 - 0.5 * math.cos(-2 * math.pi * i / (half_length)), 4)\n            for i in range(half_length)\n        ]\n        hann_coefficients += [\n            round(0.5 + 0.5 * math.cos(-2 * math.pi * i / (half_length)), 4)\n            for i in range(half_length)[::-1]\n        ]\n    else:\n        hann_coefficients = [\n            round(0.5 - 0.5 * math.cos(-2 * math.pi * i / (window_len + 1)), 4)\n            for i in range(window_len)\n        ]\n\n    return hann_coefficients\n\n\n# Example usage\nif __name__ == \"__main__\":\n    print(hanning_window(7))\n    print(hanning_window(9, symmetric=True))\n    print(hanning_window(3))\n```"}
{"task_id": 433, "completion_id": 0, "completion": "```python\n\nimport numpy as np\n\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        # Convolutional kernel case\n        kernel_height, kernel_width = shape[-2:]\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = shape[1] * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n    else:\n        # Dense layer or other cases where fan_in and fan_out are computed based on the first two dimensions\n        fan_in, fan_out = shape[0], shape[1]\n\n    return fan_in, fan_out\n\n\n# Example usage:\nshape_conv = (32, 5, 7, 7)\nshape_dense = (32, 64)\n\nprint(glorot_fan(shape_conv))  # Output: (1960, 2880)\nprint(glorot_fan(shape_dense))  # Output: (32, 64)\n\n\n```"}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]], y: list[float], n_estimators=10, learning_rate=0.1):\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    if n_estimators <= 0 or learning_rate == 0:\n        # Special case: if no estimators or zero learning rate, return the mean of y\n        return [np.mean(y)]\n    \n    m, _ = np.array(X).shape\n    y = np.array(y)\n    \n    initial_prediction = np.mean(y)\n    \n    weights = np.zeros((n_estimators,))\n    predictions = []\n    \n    for t in range(n_estimators):\n        residual = y - initial_prediction\n        # Fit OLS on the last residuals using the whole dataset (for now, just use all points)\n        coef = np.linalg.lstsq(np.column_stack([X[:, :-1], np.ones(m)]), residual)[0]\n        h_t = np.dot(X, coef[0:-1]) + coef[-1]  # Predictions for each sample\n        \n        # Update the prediction based on the learning rate\n        updated_prediction = initial_prediction + learning_rate * h_t\n        \n        # Store the prediction\n        predictions.append(updated_prediction.item())\n        \n        weights[t] = 1 / len(residual)\n        initial_prediction = updated_prediction\n    \n    return [round(p, 4) for p in predictions]\n\n# Example usage:\nX_example = [[1, 2], [2, 3], [3, 4]]\ny_example = [2, 4, 6]\nresult = gradient_boosting_regressor(X_example, y_example)\nprint(result)\n```"}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if isinstance(x, (list, np.ndarray)):\n        result = []\n        for elem in x:\n            res = sigmoid(elem, order)\n            result.append(round(res, 4))\n        return result\n    elif isinstance(x, (int, float)):\n        if order == 0:\n            return round(1 / (1 + np.exp(-x)), 4)\n        elif order == 1:\n            return round(sigmoid(x, order) * (1 - sigmoid(x, order)), 4)\n        elif order == 2:\n            return round(sigmoid(x, order) * (1 - sigmoid(x, order)) * (1 - 2 * sigmoid(x, order)), 4)\n        else:\n            return -1\n    else:\n        return -1\n\nprint(sigmoid([1, 2, 3])) # [0.7311, 0.8808, 0.9526]\nprint(sigmoid([-1, -2, -3], 1)) # [-0.0816, -0.2019, -0.2697]\nprint(sigmoid(4, 2)) # -1\n```"}
{"task_id": 438, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    if isinstance(y, np.ndarray):\n        y = y.tolist()\n\n    if num_classes is None:\n        num_classes = max(max(y), len(y)) + 1\n    elif num_classes < max(max(y), len(y)) + 1:\n        return [-1]\n\n    return [[int(i == j) for i in range(num_classes)] for j in y]\n\n\n# Test Cases\nprint(to_categorical([0, 1, 2]))  # [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nprint(to_categorical([0, 1, 2], num_classes=5))  # [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 0], [0, 0, 0]]\nprint(to_categorical(np.array([0, 2])))  # [[1, 0], [0, 1]]\nprint(to_categorical([0, 2], num_classes=3))  # [[1, 0], [0, 0]]\nprint(to_categorical([0, 1, 2], num_classes=-1))  # [-1]\n```"}
{"task_id": 439, "completion_id": 0, "completion": "```python\nimport itertools\n\ndef apriori_frequent_itemsets(transactions: list[list], min_sup: float) -> list[tuple]:\n    def join_candidates(candidates):\n        results = []\n        for i in range(len(candidates)):\n            for j in range(i + 1, len(candidates)):\n                if set(candidates[i]) & set(candidates[j]):\n                    joined_set = set()\n                    for item in candidates[i]:\n                        if item not in joined_set:\n                            joined_set.add(item)\n                    for item in candidates[j]:\n                        if item not in joined_set:\n                            joined_set.add(item)\n                    results.append(joined_set)\n        return results\n    \n    def prune_new_candidates(new_candidates):\n        result = []\n        for candidate in new_candidates:\n            if len(candidate) > 1 and any(set(candidate) - set(itemset) for itemset in result):\n                continue\n            result.append(candidate)\n        return result\n    \n    def get_support_count(transaction_items, candidate):\n        count = 0\n        for item in transaction_items:\n            if candidate >= set(itertools.islice(iter(item), len(candidate))):\n                count += 1\n        return count\n    \n    def update_transactions_with_candidate(transaction_items, candidate):\n        updated_transaction_items = [set() for _ in range(len(candidate))]\n        for index, item in enumerate(candidate):\n            for transaction_index, data in enumerate(transaction_items):\n                if item in data:\n                    updated_transaction_items[index].add(data.pop(item))\n        return updated_transaction_items\n    \n    def find_next_frequent_items(frequent_items, min_sup):\n        next_frequent_items = {}\n        for itemset in frequent_items:\n            support = sum(get_support_count(transaction_items, itemset) * 1.0 for transaction_items in transactions)\n            if support / len(transactions) >= min_sup:\n                next_frequent_items[itemset] = support\n        return {itemset: value for itemset, value in next_frequent_items.items() if value > 0}\n    \n    frequent_items = {(tuple([x])), }\n    while True:\n        current_frequent_items = prune_new_candidates(join_candidates(list(frequent_items)))\n        if not current_frequent_items:\n            break\n        frequent_items = find_next_frequent_items(current_frequent_items, min_sup)\n    \n    return sorted(frequent_items, key=lambda x: (-len(x), x))\n\n# Example usage\ntransactions = [\n    {'apple', 'banana'},\n    {'banana', 'orange'},\n    {'apple', 'orange', 'pear'},\n    {'banana', 'mango'},\n    {'apple', 'mango'}\n]\n\nmin_sup = 0.5\nresult = apriori_frequent_itemsets(transactions, min_sup)\nprint(result)\n# Output will depend on the input transactions and min_sup\n```"}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions):\n    # Ensure predictions has the expected structure\n    assert isinstance(predictions, list)\n    for level in predictions:\n        assert isinstance(level, list)\n        for entry in level:\n            assert isinstance(entry, list)\n    \n    aggregated_probs = []\n    max_indices = []\n\n    for sample_probs in predictions[0]:  # Iterate through each sample's probabilities\n        avg_prob = np.mean(sample_probs, axis=0)  # Aggregate probabilities across estimators\n        rounded_prob = np.round(avg_prob * 10000, decimals=4) / 10000  # Round to 4 decimal places\n        \n        _, idx = np.where(rounded_prob == np.max(rounded_prob))\n        \n        if not len(idx):  # If no class has the highest probability\n            raise ValueError(\"No class found with the highest probability\")\n        \n        # Find the smallest index among tied classes\n        min_idx = idx.min()\n        max_indices.append(min_idx)\n\n        # Convert to float and round to 4 decimal places\n        aggregated_probs.append(float(rounded_prob[0][min_idx]))\n\n    return aggregated_probs, max_indices\n\n# Function call\ntry:\n    result = aggregate_predictions([[[[0.1, 0.2, 0.7], [0.3, 0.5, 0.2], [0.6, 0.1, 0.3]], [[0.4, 0.4, 0.2], [0.1, 0.8, 0.1], [0.3, 0.2, 0.5]]]])\n    print(result)  # Expected output: ([0.3984, 0.4078], [2, 1])\nexcept Exception as e:\n    print(str(e))\n```"}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\ndef rbf_kernel(X: List[List[Union[int, float]]],\n               Y: List[List[Union[int, float]]] | None = None,\n               sigma: Union[float, List[float]] | None = None) -> List[List[float]]:\n    \"\"\"\n    Compute the radial basis function (RBF) kernel matrix.\n    \n    Parameters:\n        X (List[List[Union[int, float]]]): First set of feature vectors, shape (N, C)\n        Y (List[List[Union[int, float]]] | None): Second set of feature vectors or None, shape (M, C) \n                                                   if provided, else will compute the kernel matrix for X only, shape (N, N)\n        sigma (Union[float, List[float]] | None): Scale parameter(s). Can be a single positive float or a list of floats.\n\n    Returns:\n        List[List[float]]: Kernel matrix of shape (N, M) or (N, N) if Y is omitted.\n    \"\"\"\n\n    if not isinstance(Y, list):\n        Y = [Y]\n\n    # Validate inputs\n    if len(X) != len(Y):\n        return -1\n    if any(not isinstance(y, list) for y in Y):\n        return -1\n    num_features = min(len(X[0]), len(Y[0]))\n    if not all(len(row) == num_features for row in X + Y):\n        return -1\n\n    if sigma is None:\n        sigma = np.sqrt(num_features / 2)\n\n    if isinstance(sigma, list):\n        if len(sigma) != num_features:\n            return -1\n        sigma = np.array(sigma)\n\n    if sigma <= 0:\n        return -1\n\n    N, _ = np.shape(X)\n    M = np.shape(Y)[0]\n    K_matrix = []\n\n    for i in range(N):\n        k_row = []\n        for j in range(M):\n            diff_sq_sum = sum((np.array(X[i]) - np.array(Y[j]))**2)\n            k_val = np.exp(-0.5 * diff_sq_sum/sigma**2)\n            k_row.append(round(k_val, 4))\n        K_matrix.append(k_row)\n\n    return K_matrix\n\n\n# Example usage:\nX = [[1, 0], [0, 1]]\nY = [[1, 0], [0, 1]]\nsigma = 1.0\n\nprint(rbf_kernel(X, Y, sigma))  # Expected output: [[1.0, 0.3679], [0.3679, 1.0]]\n```"}
{"task_id": 446, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    from collections import Counter\n    \n    def info_gain(current_entropy, parent_entropy):\n        _, count = Counter(y).most_common(1)[0]\n        proportion_in_current_node = len(y) / len(X)\n        if proportion_in_current_node == 0 or proportion_in_current_node == 1:\n            return current_entropy - (proportion_in_current_node * parent_entropy)\n        else:\n            left_count, right_count = 0, 0\n            left_count = len([x for x in y if x != count])\n            right_count = len([x for x in y if x == count])\n            left_child_entropy = (left_count / len(y)) * info_gain(current_entropy - ((count / proportion_in_current_node) * parent_entropy), \n                                                                   current_entropy)\n            right_child_entropy = (right_count / len(y)) * info_gain(current_entropy - ((count / (1 - proportion_in_current_node)) * parent_entropy),\n                                                                     current_entropy)\n            return current_entropy - (left_child_entropy + right_child_entropy)\n\n    best_split = None\n    best_info_gain = float('-inf')\n    best_feature_index = -1\n    best_threshold = None\n\n    feature_labels = np.unique(X[:, :-1]).tolist()\n    \n    for i, feature in enumerate(feature_labels):\n        feature_values = np.sort(X[:, i], axis=0)\n        for mid_point in [(feature + val) / 2 for val in [e[1:] for e in feature_values]]:\n            if round(mid_point, 4) != feature:\n                continue\n            subset_X = X[np.where(np.all((X[:, i] <= mid_point), axis=1))]\n            subset_y = y[np.argsort(subset_X[:, 0])]\n            left_majority_label = Counter(subset_y[:int(len(subset_y)/2)]).most_common(1)[0][0]\n            right_majority_label = Counter(subset_y[int(len(subset_y)/2):]).most_common(1)[0][0]\n            \n            current_entropy = sum(map(lambda l: (-len(list(filter(lambda x: x == l, subset_y))) / len(subset_y)) * log2(len(subset_y) / len(subset_y)), set(y)))\n            parent_entropy = sum(map(lambda l: (-len(list(filter(lambda x: x == l, y)))) / len(y) * log2(len(y) / len(y)), set(y)))\n\n            info_gain_val = info_gain(current_entropy, parent_entropy)\n            if info_gain_val > best_info_gain:\n                best_split = (i, mid_point)\n                best_info_gain = info_gain_val\n                best_feature_index = i\n                best_threshold = mid_point\n                left_majority_label = left_majority_label\n                right_majority_label = right_majority_label\n                \n    if best_info_gain < 0:\n        return (-1, None, max(set(y), key=y.count), max(set(y), key=y.count))\n    else:\n        return (best_feature_index, round(best_threshold, 4), left_majority_label, right_majority_label)\n```"}
{"task_id": 452, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        result = [\n            X[X[:, feature_i] >= threshold].tolist(),\n            X[X[:, feature_i] < threshold].tolist()\n        ]\n    else:\n        result = [\n            [sample for sample in X if sample[feature_i] >= threshold],\n            [sample for sample in X if sample[feature_i] < threshold]\n        ]\n\n    return result\n\n# Example usage:\n# numeric_split = divide_on_feature(np.array([[1, 5], [3, 2], [4, 6], [2, 1]]), 0, 3)\n# print(numeric_split)  # Output: [[[3, 2], [4, 6]], [[1, 5], [2, 1]]]\n\n# categorical_split = divide_on_feature(np.array([[1, \"A\"], [2, \"B\"], [3, \"A\"], [4, \"C\"]]), 1, \"A\")\n# print(categorical_split)  # Output: [[[1, \"A\"], [3, \"A\"]], [[2, \"B\"], [4, \"C\"]]]\n```"}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n):\n    \"\"\"\n    Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check input validity\n    if len(Q.shape) != 2 or len(actions) != len(rewards) or len(dones) != len(Q): \n        return -1\n    \n    b, n_actions = Q.shape\n    y = Q.copy().tolist()\n    \n    for i in range(b):\n        a = actions[i]\n        r = rewards[i]\n        d = dones[i]\n        \n        if not d:\n            y[i][a] = r + gamma * np.max(Q_next[i])\n        else:\n            y[i][a] = r\n        \n    return [[round(y_i, 4) for y_i in row] for row in y]\n\n# Example usage\nQ_example = np.array([[0.5, 0.7], [0.8, 0.9]])\nQ_next_example = np.array([[0.6, 0.8], [0.7, 0.9]])\nactions_example = [0, 1]\nrewards_example = [1, 2]\ndones_example = [False, False]\ngamma_example = 0.9\n\nresult = update_q_values(\n    Q=Q_example,\n    Q_next=Q_next_example,\n    actions=actions_example,\n    rewards=rewards_example,\n    dones=dones_example,\n    gamma=gamma_example\n)\n\nprint(result)\n```"}
{"task_id": 458, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Check if labels is already a valid integer type\n    if isinstance(labels[0], int):\n        if all(label >= 0 for label in labels):\n            result = [[1 if i == l else 0 for i in range(num_classes)]\n                      for l in labels]\n            return result\n\n    # Handle num_classes being provided\n    elif num_classes is not None:\n        if num_classes < max(labels) + 1:\n            return [-1] * len(labels)\n\n    # Default behavior: calculate num_classes from labels\n    num_classes = max(labels) + 1\n\n    # Create result list of lists\n    return [[1 if i == idx else 0 for i in range(num_classes)] for idx in range(len(labels))]\n\n# Example usage:\nexample_labels = [0, 4, 2]\nprint(to_categorical(example_labels))\n\n# Testing cases\nassert to_categorical([0, 4, 2]) == [[1, 0, 0], [0, 0, 1], [0, 0, 0]], \"Test case failed!\"\nassert to_categorical(-1) == [-1], \"Test case failed!\"\ntry:\n    print(to_categorical([0, 0]))\nexcept Exception as e:\n    assert str(e) == \"Invalid inputs.\", f\"Test case failed! Error message: {str(e)}\"\n```"}
{"task_id": 461, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(x, y, n_estimators=200, learning_rate=0.5):\n    \"\"\"\n    Fit an ensemble of depth-1 regression trees using gradient boosting \n    to the provided training data and return the final predictions.\n    \n    Args:\n    - x: List[float] -- Feature values (one-dimensional).\n    - y: List[float] -- Target values.\n    - n_estimators: int (default=200) -- Number of boosting iterations.\n    - learning_rate: float (default=0.5) -- Shrinkage applied to each weak learner.\n    \n    Returns:\n    - A list[float]: Final predictions for the training data, rounded to four decimals.\n    \"\"\"\n\n    def compute_residuals(y_val):\n        return y_val - np.mean(y_val)\n\n    def sum_squares_error(residuals):\n        return np.sum(np.square(residuals))\n\n    def get_best_split(x_vals, index, best_err, best_t):\n        res_left = compute_residuals(y_vals[x_vals <= best_t])\n        res_right = compute_residuals(y_vals[x_vals > best_t])\n\n        err_left = sum_squares_error(res_left)\n        err_right = sum_squares_error(res_right)\n\n        total_err = err_left + err_right\n        if total_err == 0:\n            return None\n        else:\n            return min(err_left, err_right), best_t\n\n    num_samples = len(x)\n    predictions = [np.mean(y)] * num_samples\n    residuals = compute_residuals(y)\n    best_splits = []\n\n    for _ in range(n_estimators):\n        err_max = float('inf')\n        best_t = None\n        best_split = None\n\n        for i in range(num_samples):\n            split_err, candidate_t = get_best_split(\n                x[i:num_samples], i, err_max, best_t\n            )\n            if split_err is not None:\n                best_split = candidate_t\n                err_max = split_err\n                best_t = candidate_t\n\n        if best_t is None:\n            break\n\n        residuals -= predictions[best_t < x].mean()\n        v_L = predictions[best_t < x].mean()\n        v_R = predictions[best_t > x].mean()\n\n        predictions += [\n            np.where(best_t < x, v_L, v_R),\n            learning_rate * residuals\n        ]\n        \n    return [round(sum(predictions), 4) for sum_predictions in zip(*predictions)]\n\n# Example usage:\nx_train = [-1.0,  0.0,  1.0,  2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0]\ny_train = [1.0,  2.0,  3.0,  5.0,  7.0,  8.0,  8.9,  9.0,  10.0,  12.0,  13.0]\n\nfinal_preds = gradient_boosting_regressor(x_train, y_train)\nprint(final_preds)\n```"}
{"task_id": 471, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]], \n                          state: int, \n                          action: int, \n                          reward: float, \n                          next_state: int | None, \n                          epsilon: float, \n                          lr: float, \n                          gamma: float) -> list[list[float]]:\n    \n    row = q_table[state]\n    \n    # Check if it's not a terminal state\n    if next_state is not None:\n        \n        max_q_next_state = max([q_table[next_state][j] for j in range(len(row))])\n        target = reward + gamma * max_q_next_state\n        \n        old_value = row[action]\n        old prob = epsilon / len(row)\n        new_prob = (1 - epsilon) / len(row)\n        \n        td_error = target - old_value\n        adjustment = lr * td_error * ((new_prob - old_prob))\n        \n        row[action] = old_value + adjustment\n    \n    return [row[:],]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example Q-table\n    initial_q_table = [[0.0, 1.0, -1.0], [1.0, 0.0, 0.0], [-1.0, 0.0, 0.0]]\n    \n    # Example inputs\n    state = 0\n    action = 0\n    reward = 0.1\n    next_state = 2\n    epsilon = 0.1\n    lr = 0.5\n    gamma = 0.9\n    \n    q_table_updated = expected_sarsa_update(initial_q_table, state, action, reward, next_state, epsilon, lr, gamma)\n    \n    print(\"Updated Q-table:\")\n    print(np.round(q_table_updated, 4))\n```"}
{"task_id": 474, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    def get_class_coordinates(class_idx):\n        low_x = (class_idx // 2) * 10 + 0.1\n        high_x = low_x + 8.8\n        low_y = (class_idx % 2) * interval + 0.1\n        high_y = low_y + 8.8\n        return np.random.uniform(low_x, high_x, size=n_train), \\\n               np.random.uniform(low_y, high_y, size=n_train), \\\n               np.array([class_idx - 0.5] * n_train)\n\n    X_train = np.zeros((m * n_train, 2))\n    X_val = np.zeros((m * n_val, 2))\n    Y_train = np.zeros(n_train)\n    Y_val = np.zeros(n_val)\n\n    # Initialize all zeros\n    i = 0\n    j = 0\n\n    for idx in range(1, m + 1):\n        x1, y1, lab1 = get_class_coordinates(idx)\n        X_train[j:j+len(x1), :] = np.c_[x1, y1]\n        Y_train[i:i+len(lab1)] = lab1\n        j += len(x1)\n        i += len(lab1)\n\n    k = 0\n    for idx in range(m + 1, 2 * m + 1):\n        x2, y2, lab2 = get_class_coordinates(idx)\n        X_val[k:k+len(x2), :] = np.c_[x2, y2]\n        Y_val[i:i+len(lab2)] = lab2\n        k += len(x2)\n        i += len(lab2)\n\n    return X_train, X_val, Y_train.astype(int), Y_val.astype(int)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    X_train, X_val, Y_train, Y_val = generate_data(\n        m=3,  # Number of classes\n        n_train=100,  # Samples per class for training\n        n_val=50,  # Samples per class for validation\n        interval=2.0,  # Vertical distance between clusters\n        seed=42  # Random seed for reproducibility\n    )\n    print(\"Training set:\", X_train[:5], Y_train[:5])\n    print(\"Validation set:\", X_val[:5], Y_val[:5])\n\n# This will run only when this file is executed directly from the command line\nif __name__ == \"__main__\":\n    import matplotlib.pyplot as plt\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap='coolwarm')\n    plt.show()\n```"}
{"task_id": 475, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\"\"\"\n    \n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    else:\n        m_prev *= b1\n    \n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    else:\n        v_prev *= b2\n    \n    m_curr = b1 * m_prev + (1 - b1) * grad\n    v_curr = b2 * v_prev + (1 - b2) * grad ** 2\n    m_hat = m_curr / (1 - b1 ** t)\n    v_hat = v_curr / (1 - b2 ** t)\n    w_new = w - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n    \n    return w_new, m_prev, v_prev\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    initial_weights = 1.0\n    grad = 0.1\n    initial_moment = np.zeros_like(initial_weights)\n    initial_variance = np.zeros_like(initial_weights)\n    iteration = 1\n    learning_rate = 0.001\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n    \n    weights_after_update, moment_after_update, variance_after_update = adam_update(\n        initial_weights,\n        grad,\n        initial_moment,\n        initial_variance,\n        iteration,\n        learning_rate=learning_rate,\n        b1=beta1,\n        b2=beta2,\n        eps=epsilon\n    )\n    \n    print(f\"Updated Weights: {weights_after_update}\")\n    print(f\"Moment after Update: {moment_after_update}\")\n    print(f\"Variance after Update: {variance_after_update}\")\n```"}
{"task_id": 479, "completion_id": 0, "completion": "Sure! Below is the complete implementation based on the description you provided:\n\n```python\nimport numpy as np\nfrom typing import List, Optional\nimport numpy as np\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where `rewards[t][i]` is the reward for arm `i` at time-step\n        `t`.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    # Initialize parameters\n    N = len(rewards)\n    C = [0] * N  # Number of pulls for each arm\n    V = [ev_prior] * N  # Estimated values for each arm\n    \n    if seed is not None:\n        np.random.seed(seed)\n\n    # Run the simulation\n    for t in range(len(rewards)):\n        if np.random.rand() < epsilon:\n            # Choose an arm randomly with probability epsilon\n            choice = np.random.randint(0, N)\n        else:\n            # Choose the arm with the maximum estimated value\n            choice = np.argmax(V)\n        \n        # Select the reward for the chosen arm\n        r = rewards[t][choice]\n        \n        # Update the estimated value for the chosen arm\n        V[choice] += (r - V[choice]) / (C[choice] + 1)\n        \n        C[choice] += 1\n    \n    # Return the final estimated values for each arm\n    return [round(v, 4) for v in V]\n\n# Example usage\nexample_rewards = [\n    [1.0, 1.5, 2.0],\n    [2.0, 2.5, 3.0],\n    [3.0, 3.5, 4.0]\n]\n\nprint(epsilon_greedy_bandit(example_rewards))\n```\n\n### Explanation:\n1. **Initialization**:  \n   - We initialize the number of arms (`N`) and set up variables `C` (number of pulls), `V` (estimated values), and the initial expected value for each arm (`ev_prior`). \n   \n2. **Random Seed Handling**:  \n   - If a `seed` is provided, we ensure that the random numbers generated are reproducible across different runs by setting the random seed.\n\n3. **Simulation Loop**:  \n   - For each time-step, we either choose an arm randomly with probability `epsilon` or select the arm with the highest estimated value. This decision influences whether we update our estimate or keep the old one.\n\n4. **Update Estimates**:  \n   - When choosing an arm, we update its estimated value according to the formula: \\( V_i \\leftarrow V_i + \\frac{r - V_i}{C_i} \\). Here, \\( r \\) is the reward received, and \\( C_i \\) is the number of times the arm was pulled before this time-step.\n\n5. **Final Estimates**:  \n   - After simulating all time-steps, we return the estimated values rounded to four decimal places.\n\nThis implementation ensures that the \u03b5-greedy strategy balances exploration and exploitation effectively, providing a realistic approach to solving the multi-armed bandit problem."}
{"task_id": 481, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef euclidean_distance(point1, point2):\n    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n    return np.sqrt(np.sum((point1 - point2)**2))\n\ndef find_core_points(data, epsilon):\n    \"\"\"Find all core points within a given epsilon radius.\"\"\"\n    n = len(data)\n    core_points = []\n    \n    for i in range(n):\n        if len([j for j in range(n) if euclidean_distance(data[i], data[j]) <= epsilon] >= min_samples):\n            core_points.append(i)\n    return core_points\n\ndef assign_clusters(indices_of_neighbors, data, eps=3, min_samples=5):\n    \"\"\"Assign clusters based on whether points are core or border points.\"\"\"\n    clusters = [-1]*len(data)  # Initialize cluster assignment (-1 for noise)\n    visited_indices = []  # Track visited indices\n    \n    def dfs(index, current_cluster_index, visited):\n        nonlocal clusters\n        clusters[index] = current_cluster_index\n        \n        visited.append(index)\n        \n        for neighbor_index in indices_of_neighbors[neighbor_point][index]:\n            if neighbor_index not in visited:\n                dfs(neighbor_index, current_cluster_index, visited)\n                \n    num_clusters = max(clusters) + 2  # Start cluster indices from 0\n    for i in range(len(data)):\n        indices_of_neighbors = [set() for _ in range(num_clusters)]\n        \n        for j in range(len(data)):\n            if clusters[j] != -1:\n                indices_of_neighbors[clusters[j]].add(j)\n            \n        core_set = set(find_core_points(data, eps))\n        border_set = {i for i in indices_of_neighbors.keys() if i not in core_set}\n        \n        for idx in border_set:\n            dfs(idx, next(iter(core_set)), visited_indices)\n    \n    return clusters\n\ndef dbscan_clustering(data, eps, min_samples):\n    \"\"\"Main function to perform DBSCAN clustering.\"\"\"\n    return assign_clusters(data, data, eps=eps, min_samples=min_samples)\n\n# Example usage\nif __name__ == \"__main__\":\n    data = np.array([[1, 2], [2, 3], [8, 7], [9, 6], [10, 5], [11, 4], [12, 3]])\n    result = dbscan_clustering(data, eps=2, min_samples=2)\n    print(result)\n```"}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a sequence of integer labels (list, tuple, or \n    Numpy array) and returns a two-dimensional Numpy array where each \n    row represents the one-hot encoding of the corresponding label.\n\n    Args:\n        y (sequence): Sequence of integer labels for which the one-hot \n                      encoding needs to be generated.\n        num_classes (int, optional): Total number of unique categories. If \n                                     not specified, it will be derived from \n                                     the maximum label value plus one.\n\n    Returns:\n        numpy.ndarray: An array of shape (n_samples, n_classes), where \n                       n_samples is the length of y and n_classes is \n                       the total number of classes.\n\n    Raises:\n        ValueError: If all elements in y are less than zero or greater than \n                    num_classes - 1 when num_classes is explicitly provided.\n    \"\"\"\n\n    # Convert the input to a numpy array if it's not already\n    y = np.array(y)\n\n    # Check if num_classes was explicitly provided\n    if num_classes is not None and num_classes < len(np.unique(y)):\n        raise ValueError(\"Number of classes cannot be less than the number of unique labels.\")\n\n    # Create an array of indices from 0 to num_classes-1\n    if num_classes is None:\n        num_classes = np.max(y) + 1\n    idx_map = np.arange(num_classes)\n\n    # For each label, map it to its index using the inverse mapping\n    return (y[:, np.newaxis] == idx_map).astype(int)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Integer labels ranging from 3 to 7\n    sample_labels = [3, 7, 5, 3]\n    \n    # Converting labels to one-hot encoded form without specifying num_classes\n    print(to_categorical(sample_labels))\n\n    # Converting labels to one-hot encoded form with explicitly given num_classes\n    sample_labels_with_classes = [4, 6, 8]\n    print(to_categorical(sample_labels_with_classes, num_classes=9))\n```"}
{"task_id": 485, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int], \n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n\n    def _build_tree(node_id, depth):\n        if depth == 0 or len(y_train[node_id[1]:node_id[2]]) < 2:\n            return None\n        \n        # Calculate sum of squared errors at this node\n        sse = np.sum((y_train[node_id[1]:node_id[2]] - np.mean(y_train[node_id[1]:node_id[2]])) ** 2)\n        \n        best_split_point = float('inf') if all(isinstance(i, str) for i in X_train[node_id[1]:node_id[2]]) else min([float(f'{x:.6f}') for x in X_train[node_id[1]:node_id[2]]])\n        \n        left_node_id = [node_id[0], node_id[1], best_split_point]\n        right_node_id = [node_id[0], node_id[1] + 1, node_id[2]]\n        \n        left_tree = _build_tree(left_node_id, depth + 1)\n        right_tree = _build_tree(right_node_id, depth + 1)\n        \n        return {\n            'id': node_id[0], \n            'split_point': best_split_point, \n            'left_child_id': left_tree['id'] if left_tree else None,\n            'right_child_id': right_tree['id'] if right_tree else None,\n            'sse': sse,\n            'depth': depth\n        }\n\n    root_node_id = [[None, None, None]] * len(y_train)\n    \n    root_tree = _build_tree(root_node_id, 0)\n\n    def traverse_tree(node):\n        if node is None:\n            return []\n        \n        result = []\n        result.append(traverse_tree(root_tree['left_child_id']))\n        result.extend([(x, val) for x, val in enumerate(result[-1]) if ((len(result[-1]) > 0 and isinstance(x, str) and x in range(len(X_train))) or (not isinstance(x, str))))]\n\n        result.append(traverse_tree(root_tree['right_child_id']) if root_tree['right_child_id'] is not None else [])\n        return [-2 for _ in range(max(max(map(lambda l: map(len, l), result)), 0))] + result[:-1][::-1]\n\n    predicted_values = traverse_tree(root_tree)['left_child_id'] if root_tree else []\n\n    return predicted_values\n```"}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    if isinstance(x, (tuple, list)):\n        x = np.array(x)\n    \n    if len(x.shape) != 1:\n        raise ValueError(\"Input must be a one-dimensional vector.\")\n    \n    n = len(x)\n    diag_matrix = np.zeros((n, n))\n    np.fill_diagonal(diag_matrix, x)\n    \n    return diag_matrix.tolist()\n\n# Example usage:\n\n# For a list\nprint(make_diagonal([1, 2, 3]))  # [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\n\n# For a tuple\nprint(make_diagonal(('a', 'b', 'c')))  # [['a'], ['b'], ['c']]\n\n# For a NumPy array\nprint(make_diagonal(np.array([[4, 5], [6, 7]])))  # [[4., 0.], [0., 7.]])\n```"}
{"task_id": 491, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nimport itertools\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    # Count item frequencies in transactions\n    item_counts = Counter(itertools.chain(*transactions))\n\n    # Filter out items with counts below min_sup\n    frequent_items = {item for item, count in item_counts.items() if count >= min_sup}\n\n    # Build the FP-tree\n    def build_fp_tree(item_set, parent_item=None):\n        if not item_set:\n            return None\n        max_len = max(len(parent_item or ''), len(item_set)) + 1\n        children_map = defaultdict(build_fp_tree)\n        head = Node(None, max_len=max_len)\n        next_level = []\n        for item in item_set:\n            new_head = Node(head, item=item)\n            head.children[item] = new_head\n            next_level.append(new_head)\n        children_map[parent_item].children[item_set[-1]] = head\n        return head\n\n    root = build_fp_tree(frequent_items)\n\n    # Recursively find frequent item sets\n    def recursive_mining(node, depth=1):\n        seen_pairs = {}\n        if not node.children:\n            return []\n\n        for child_item in node.children.values():\n            pairs = recursive_mining(child_item, depth + 1)\n            for pair in pairs:\n                key = frozenset(pair[:depth])\n                if key not in seen_pairs:\n                    seen_pairs[key] = pair\n                    yield (child_item.item,) + key\n                else:\n                    yield from itertools.product(seen_pairs[key], pair[depth:], repeat=1)\n\n    return [(item,) * len(root.parent) for item in recursive_mining(root)]\n\nclass Node:\n    def __init__(self, parent, item=None, max_len=None):\n        self.parent = parent\n        self.item = item\n        self.max_len = max_len\n        self.children = {}\n\ndef test_algorithm(test_data):\n    results = fp_growth(test_data['transactions'], test_data['min_sup'])\n    print(results)\n\n# Example usage\ntest_data = {\n    'transactions': [\n        ['A', 'B', 'D', 'E'],\n        ['B', 'C', 'E'],\n        ['A', 'B', 'D', 'E'],\n        ['A', 'B', 'D', 'E'],\n        ['A', 'B', 'C', 'E'],\n        ['A', 'B', 'C', 'D', 'E']\n    ],\n    'min_sup': 3\n}\ntest_algorithm(test_data)\n```"}
{"task_id": 492, "completion_id": 0, "completion": "```python\n\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    if degree < 0:\n        return [-1]\n    \n    n_samples, n_features = len(X), len(X[0])\n    \n    # Generate all unique combinations of features with replacement up to specified degree\n    feature_combinations = sorted(list(combinations_with_replacement(range(n_features), degree)))\n    \n    # Initialize the output matrix with ones for the highest degree term\n    result_matrix = [[1] + [0] * n_features]\n    \n    for combo in feature_combinations:\n        product = 1\n        for idx in combo:\n            product *= X[i][idx]\n        result_matrix.append([product] + [0] * n_features)\n    \n    return result_matrix\n\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\nprint(polynomial_features(X, 2))\n\n# Expected output:\n# [\n#   [1, 0, 0],\n#   [1, 2, 0],\n#   [1, 4, 0],\n#   [1, 6, 0],\n#   [1, 8, 0],\n#   [1, 12, 0],\n#   [1, 18, 0],\n# ]\n\n```"}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    if len(y) == 0:\n        return -1\n    \n    n = len(y)\n    y_mean = sum(y) / n\n    mse_result = sum((yi - y_mean)**2 for yi in y) / n\n    return round(mse_result, 4)\n\n# Example usage:\nprint(mse([1, 2, 3]))  # Output will vary based on the provided data\n```"}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef apply_affine(x, slope=1.0, intercept=0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists - the activation output, the first derivative, and the second\n        derivative - each rounded to 4 decimal places.\n    \"\"\"\n    y = [slope * x_val + intercept for x_val in x]\n    grad = [slope for _ in range(len(y))]\n    grad2 = [0 for _ in range(len(y))]\n\n    y = [round(val, 4) for val in y]\n    grad = [round(val, 4) for val in grad]\n    grad2 = [round(val, 4) for val in grad2]\n\n    return y, grad, grad2\n\n# Example usage:\nexample_x = [1.5, 2.5, 3.5]  # Example input as a list\nresult = apply_affine(example_x)\nprint(result)\n\nexample_x_array = np.array([1.5, 2.5, 3.5])  # Example input as a NumPy array\nresult_array = apply_affine(example_x_array)\nprint(result_array)\n```"}
{"task_id": 499, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    if k < 1 or k > data.shape[1]:\n        return -1\n\n    # Step 1: Standardize each feature\n    centered_data = data - data.mean(axis=0)\n\n    # Step 2: Calculate the sample covariance matrix with Bessel's correction\n    sample_covariance_matrix = np.cov(centered_data, rowvar=False, bias=True)\n    \n    # Step 3: Perform eigen-decomposition\n    eigenvalues, eigenvectors = np.linalg.eigh(sample_covariance_matrix)\n\n    # Step 4 & 5: Sort eigenvalues and eigenvectors\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Select the top 'k' eigenvectors\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n\n    # Step 6: Transform data using the selected eigenvectors\n    transformed_data = np.dot(top_k_eigenvectors.T, centered_data.T).T\n    \n    # Round to 4 decimal places\n    rounded_data = [list(map(lambda x: round(x, 4), row)) for row in transformed_data]\n\n    return rounded_data\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simulate some data\n    np.random.seed(42)  # For reproducibility\n    features = np.random.rand(100, 10)  # 100 samples, 10 features\n    target_k = 3\n\n    result = pca_transform(features, target_k)\n    print(result)\n```"}
{"task_id": 500, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    # Initialize parameters and arrays for storing results\n    c = len(y_train) / sum(y_train)\n    scores = [c] * len(X_train)\n    \n    for _ in range(n_estimators):\n        h_values = []\n        \n        # Predict class probabilities based on current model's score\n        proba = scores.copy()\n        \n        # Split dataset into left and right parts based on thresholds\n        for i, (x, y) in enumerate(zip(X_train, y_train)):\n            if x[-1] <= proba[i][-1][1]:  # Assume negative index for left/right split\n                proba[i][-1][1] = -np.inf\n            else:\n                proba[i][-1][1] = np.inf\n        \n        # Find best threshold for splitting\n        max_val = 0\n        best_threshold = None\n        for j in range(len(proba[0]) - 1):\n            p_left = np.sum([p[j] for p in proba])\n            p_right = 1 - p_left\n            \n            val = -(2 * p_right - 1) * np.dot(np.array(proba).T, x[:-1])\n            \n            if abs(val) > max_val:\n                max_val = abs(val)\n                best_threshold = x[-1]\n                \n        # Update decision stumps\n        for i, (x, yt, probas) in enumerate(zip(X_train, y_train, proba)):\n            if x[-1] <= best_threshold:  # Left branch\n                probas[i][-1][0] += learning_rate\n                probas[i][-1][1] -= learning_rate\n            else:  # Right branch\n                probas[i][-1][0] -= learning_rate\n                probas[i][-1][1] += learning_rate\n                \n        # Compute new residuals\n        r_t = [(yt - yt_hat) ** 2 for yt, yt_hat in zip(y_train, scores)]\n        r_t_sum = sum(r_t)\n\n        # Add residual terms to previous scores\n        for i, rt in enumerate(r_t):\n            scores[i] += learning_rate * rt / r_t_sum\n    \n    # Threshold prediction to obtain predicted classes\n    predictions = [(int(s >= 0))] * len(X_test)\n\n    return predictions\n\n\n# Example usage:\n# X_train = [[1.0, 0.0], [1.0, 1.0]]\n# y_train = [0, 1]\n# X_test = [[1.0, 0.0], [1.0, 1.0]]\n# predictions = gradient_boosting_classifier(X_train, y_train, X_test, n_estimators=3, learning_rate=0.5)\n# print(predictions)\n```"}
{"task_id": 505, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    if X.size == 0:\n        return []\n\n    num_samples, _ = X.shape  # Assuming we have only one visible layer\n    _, n_hidden = W.shape\n\n    p_h_given_v = sigmoid(np.dot(X, W.T) + hbias)\n    v_bak = np.copy(vbias)\n\n    for _ in range(k):\n        v_bak[:] = vbias[:]\n        for sample_idx in range(num_samples):\n            h_sample = p_h_given_v[sample_idx]\n            h_next = sigmoid(np.dot(h_sample, W) + vbias)\n            vbias[:] = vbias[:]\n            vbias += learning_rate * (h_next - h_sample) \n            p_h_given_v[sample_idx] = h_next\n\n    # Calculate positive gradient\n    pos_grad = np.dot(X.T, p_h_given_v)\n    \n    # Calculate negative gradient\n    neg_grad = np.dot(vbias.reshape(-1, 1), p_h_given_v.T)\n    \n    # Update weights\n    new_W = W + (pos_grad - neg_grad) / num_samples\n    \n    # Round to 4 decimal places\n    return [[round(val, 4) for val in row] for row in new_W]\n\ndef sigmoid(x): \n    return 1 / (1 + np.exp(-x))\n\n# Example usage\nif __name__ == \"__main__\":\n    X_example = np.random.rand(100, 50)\n    W_example = np.random.randn(50, 30) * 0.1\n    hbias_example = np.zeros(30)\n    vbias_example = np.random.randn(50) * 0.1\n    learning_rate_example = 0.1\n    k_example = 1\n\n    result = contrastive_divergence(X_example, W_example, hbias_example, vbias_example, learning_rate_example, k_example)\n    print(result)\n```"}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    x_np = np.array(x)\n    y_np = np.array(y)\n\n    if len(x_np) != len(y_np):\n        return -1\n    \n    return round(np.max(np.abs(x_np - y_np)), 4)\n\n# Test cases\nprint(chebyshev([1, 2, 3], [2, 4, 6])) # Expected output: 3.0\nprint(chebyshev([1, 2, 3], [2, 4, 5])) # Expected output: -1\n```"}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"\n    Compute the VAE loss for Bernoulli visible units.\n\n    The function computes the binary cross-entropy loss and the Kullback-Leibler\n    divergence loss, then returns their sum divided by the number of samples.\n\n    Args:\n        y (np.ndarray): Ground truth binary labels of shape (batch_size, ).\n        y_pred (np.ndarray): Reconstructed binary labels of shape (batch_size, ).\n        t_mean (np.ndarray): Mean of the approximate posterior over t.\n        t_log_var (np.ndarray): Log variance of the approximate posterior over t.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n\n    batch_size = y.shape[0]\n    epsilon = np.finfo(np.float32).eps\n\n    # Clip values to avoid underflow/overflow during computation\n    y_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    bce_loss = -(y * np.log(y_clipped) + (1 - y) * np.log(1 - y_clipped))\n\n    kl_divergence = -0.5 * np.sum(\n        1 + t_log_var - t_mean ** 2 - np.exp(t_log_var), axis=1)\n\n    total_loss = np.mean(bce_loss + kl_divergence)\n\n    return round(total_loss, 6)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Dummy inputs for testing (replace these with actual data)\n    y_true = np.random.binomial(n=1, p=0.5, size=(100, 10))\n    y_pred = np.random.rand(100, 10)\n    t_mean = np.zeros((100, 5))  # Mean of q(t|x) for latent variables\n    t_log_var = np.log(np.eye(5)**2)  # Log-variance of q(t|x)\n\n    print(vae_loss(y_true, y_pred, t_mean, t_log_var))\n```"}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    # Check if the dimensions match\n    assert len(w) == len(V[0]), \"Number of linear terms must equal the number of latent factor columns.\"\n    \n    # Convert inputs to appropriate NumPy arrays\n    X_array = np.array(X)\n    w_array = np.array(w)\n    V_array = np.array(V).T\n    \n    # Calculate the sum of squares for each row in the product of V and X\n    interactions = np.sum(np.square(V_array * X_array), axis=1)\n    \n    # Compute the quadratic form V^T * X * X^T * V\n    quadratic_form = np.dot(V_array.T, np.dot(X_array.T, np.dot(X_array, V_array)))\n    \n    # Sum up the contributions from both components\n    y_pred = w0 + np.dot(X_array, w_array) + 0.5 * (interactions - np.square(interactions))\n    \n    # Return the rounded predictions\n    return [round(pred.item(), 4) for pred in y_pred]\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = [[1, 2, 3], [4, 5, 6]]  # Example feature matrix\n    w0 = 1.0  # Global bias term\n    w = [0.5, 0.3]  # Linear coefficients\n    V = [[1, 2], [3, 4]]  # Latent factors\n    \n    predictions = fm_predict(X, w0, w, V)\n    print(predictions)\n```"}
{"task_id": 517, "completion_id": 0, "completion": "```python\n# Solution starts here:\n\nimport numpy as np\nfrom typing import List\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    \n    def safe_exp(x):\n        \"\"\"Return exp(x), but use a small value instead of nan if x < -700.\"\"\"\n        if x >= -700:\n            return np.exp(x)\n        else:\n            return np.finfo(np.float32).eps\n    \n    sig_z = 1 / (1 + safe_exp(-z))\n    return [sig_z] if isinstance(z, (list, tuple)) or hasattr(z, 'tolist') else round(sig_z, 4)\n\nprint(sigmoid([-1, 0, 1]))  # Expected output: [0.2689, 0.5, 0.7311]\n```"}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    @functools.wraps(function)\n    def wrapper(*args, **kwargs):\n        actual, predicted = args\n        \n        if actual.ndim > 1 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n\n        if predicted.ndim > 1 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n\n        result = function(actual, predicted)\n        \n        return result\n\n    return wrapper\n\n# Example usage\ndef example_metric(actual_array, predicted_array):\n    # This is just a placeholder for whatever metric you want to use.\n    return np.sum(actual_array == predicted_array)\n\nmetric_function = unhot(example_metric)\nactual = np.array([[0,1,0], [1,0,0], [0,0,1]])\npredicted = np.array([[0,1,0], [0,1,0], [0,0,1]])\n\nresult = metric_function(actual, predicted)\nprint(result)  # Output will depend on the specific implementation of `example_metric`\n\n# Verify that the output matches the expected behavior\nexpected_output = np.sum((np.arange(actual.size)[:, None] == predicted).astype(float))\nprint(\"Expected:\", expected_output)\nprint(\"Actual:\", result)\n```"}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(z, list):\n        return [round(grad, 4) for grad in sigmoid_val(z)]\n    elif np.isscalar(z):    \n        return round(sigmoid_val(z), 4)\n    else:\n        return [sigmoid_val(z_) for z_ in z]\n\ndef sigmoid_val(z):\n    \"\"\"Compute the sigmoid value.\"\"\"\n    return max(min(z, 1 - z), -(1 - z))\n\n# Test cases\nprint(sigmoid_grad([0.5]))\n# Expected output: [0.25]\nprint(sigmoid_grad([[0.7], [-0.8]]))\n# Expected output: [[0.3699], [0.2420]]\nprint(sigmoid_grad([np.nan]))  # Should return np.nan for NaN inputs\n```"}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]], W: list[int | float], b: float, grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with -1 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and -1, the predicted labels of the grid points.\n    \"\"\"\n    # Initialize empty results array\n    pred_labels = []\n\n    # Find the min and max values of X's columns\n    xmin, xmax = np.min(X, axis=0), np.max(X, axis=0)\n\n    # Create two equally spaced 1-D arrays for the grid\n    x1_plot = np.linspace(xmin[0], xmax[0], grid_n)\n    x2_plot = np.linspace(xmin[1], xmax[1], grid_n)\n\n    # Generate mesh-grid coordinates\n    xv, yv = np.meshgrid(x1_plot, x2_plot)\n\n    # Compute the classification values on the grid\n    Z = xv.ravel() * W[0] + yv.ravel() * W[1] + b\n    sign_values = np.sign(Z)\n\n    # Reshape the result into a 2D grid structure\n    prediction_matrix = sign_values.reshape((grid_n, grid_n))\n\n    # Extract the signs from the prediction matrix\n    pred_labels = prediction_matrix.T.tolist()\n\n    return pred_labels\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Assuming X and other necessary variables are defined elsewhere in your code\n    example_X = [[1, 2], [2, 3], [3, 5]]\n    W_example = [-1, 1]\n    b_example = 0\n\n    # Call the function with the example data\n    print(decision_boundary_grid(example_X, W_example, b_example))\n```"}
{"task_id": 537, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"\n    Gauusian naive bayes implementation trained on the input training\n    dataset, then used to predict the labels of unseen data points.\n\n    :param X_train: np.ndarray, training dataset, shape = (n_samples, n_features)\n    :param y_train: np.ndarray, target values, shape = (n_samples,)\n    :param X_test: np.ndarray, testing dataset, shape = (n_samples, n_features)\n    :return: np.ndarray, predictions, shape = (n_samples,)\n    \"\"\"\n    def normal_pdf(x, mu, sigma):\n        epsilon = 1e-9\n        std_dev = sigma + epsilon\n        pdf = (1 / (np.sqrt(2 * np.pi) * std_dev)) * np.exp(-((x - mu)**2) / (2 * std_dev**2))\n        return pdf\n    \n    n_classes = len(np.unique(y_train))\n    n_features = X_train.shape[1]\n    \n    # Initialize priors\n    priors = np.bincount(y_train) / len(y_train)\n\n    # Calculate means and variances for each feature per class\n    mu_per_class = []\n    sigma_sq_per_class = []\n    \n    for i in range(n_classes):\n        indices_i = y_train == i\n        mu_per_class.append(np.mean(X_train[indices_i], axis=0))\n        sigma_sq_per_class.append(np.var(X_train[indices_i], axis=0))\n        \n    mu_per_class = np.array(mu_per_class)\n    sigma_sq_per_class = np.array(sigma_sq_per_class)\n    \n    posteriors = []\n\n    for x in X_test:\n        class_posterior = np.zeros(n_classes)\n        for i in range(n_classes):\n            numerator = np.prod(normal_pdf(x, mu_per_class[i, :], sigma_sq_per_class[i]) * priors[i])\n            denominator = np.sum([normal_pdf(x, mu_per_class[j, :], sigma_sq_per_class[j]) for j in range(n_classes)])\n            if denominator != 0:\n                class_posterior[i] = numerator / denominator\n            else: \n                raise ValueError(\"Division by zero occurred while calculating posterior probabilities.\")\n                \n        max_idx = np.argmax(class_posterior)\n        posteriors.append(max_idx)\n        \n    return posteriors\n\n# Example usage\nif __name__ == \"__main__\":\n    X_train = np.random.rand(100, 5)  # Randomly generated training data for demonstration purposes\n    y_train = np.random.randint(0, 2, size=100)  # Binary labels for demonstration purposes\n    X_test = np.random.rand(20, 5)  # Randomly generated test data for demonstration purposes\n    predictions = gaussian_naive_bayes(X_train, y_train, X_test)\n    print(predictions)\n```"}
{"task_id": 539, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Computes the binary cross-entropy cost.\n\n    Args:\n        AL (np.ndarray): Model probabilities, shape (1, m)\n        Y (np.ndarray): Ground truth labels, shape (1, m)\n\n    Returns:\n        float: The computed cost\n    \"\"\"\n    # Clip probabilities to avoid numerical overflow\n    AL[AL < 1e-15] = 1e-15\n    AL[AL > 1 - 1e-15] = 1 - 1e-15\n    \n    m = Y.shape[1]\n    \n    # Compute binary cross-entropy loss\n    logprobs = np.multiply.reduce(Y * np.log(AL)) + np.multiply.reduce((1 - Y) * np.log(1 - AL))\n    \n    return -np.sum(logprobs) / m\n\n# Example usage\nif __name__ == \"__main__\":\n    # Dummy data\n    AL = np.array([[0.3, 0.6, 0.1],\n                   [0.2, 0.8, 0.0],\n                   [0.9, 0.2, 0.1]])\n    \n    Y = np.array([[1],[0],[1]])\n    \n    # Compute cost\n    cost = compute_cost(AL, Y)\n    \n    print(f\"Computed Cost: {cost:.4f}\")\n```"}
{"task_id": 552, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    if not (0 <= t < len(observations)):\n        return []\n\n    pi = hmm['pi']\n    A = hmm['A']\n    B = hmm['B']\n\n    N = len(pi)\n    \n    # Initialize alpha values\n    \u03b1 = np.zeros((len(observations) + 1, N))\n    \u03b1[0] = pi\n    \n    for step in range(len(observations)):\n        current_observation = observations[step]\n        \n        # Compute forward probabilities\n        \u03b1[step + 1] = np.dot(A[:, current_observation], \u03b1[step])\n    \n    # Initialize beta values\n    \u03b2 = []\n    \u03b2.append(np.ones(N))\n    \n    # Compute backward probabilities\n    for step in reversed(range(len(observations))):\n        current_observation = observations[step]\n        \u03b2.insert(0, np.array(B[current_observation]))\n        \n        for j in range(len(B[current_observation])):\n            \u03b2[0][j] *= \u03b2[-1][j]\n\n    \u03b2 = np.array(\u03b2[::-1])\n\n    # Combining forward and backward probabilities to get gamma probabilities\n    \u03b3 = ((np.outer(\u03b1, \u03b2)) / sum(sum(\u03b1)))\n    \n    # Round components of \u03b3 to four decimal places\n    \u03b3 = [round(x, 4) for x in \u03b3.flatten()]\n    \n    return \u03b3.tolist()\n\n# Example usage\nhmm_example = {\n    'pi': [0.7, 0.3],\n    'A': [[0.6, 0.4], [0.5, 0.5]],\n    'B': {'0': [0.8, 0.2], '1': [0.4, 0.6]}\n}\n\nobservations_example = [0, 1, 0, 1, 1, 0]\nt_example = 3\n\nresult = gamma_probabilities(hmm_example, observations_example, t_example)\nprint(result)\n```"}
{"task_id": 555, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n    return np.linalg.norm(x1 - x2)\n\ndef build_neighborhood_list(data, epsilon, min_samples):\n    \"\"\"Build a list of neighborhoods for each sample based on the specified radius.\"\"\"\n    m, n = data.shape\n    distances = np.zeros((m, m))\n    for i in range(m):\n        for j in range(i + 1, m):\n            dist = euclidean_distance(data[i], data[j])\n            if dist <= epsilon:\n                # Add both directions because DBSCAN supports non-symmetric distances\n                distances[i][j] = dist\n                distances[j][i] = dist\n    \n    neighborhood_lists = []\n    for i in range(m):\n        row_neighbors = [dist for dist in distances[i]]\n        neighborhood_lists.append(row_neighbors)\n    \n    return neighborhood_lists\n\ndef find_core_points(neighborhoods, min_samples):\n    \"\"\"Identify core points within the given neighborhoods based on the minimum number of samples.\"\"\"\n    num_features = len(neighborhoods[0])\n    cores = set()\n    \n    for i, neighborhood in enumerate(neighborhoods):\n        count = sum(1 for d in neighborhood if d >= min_samples)\n        if count > 0:\n            cores.add(i)\n    \n    return list(cores)\n\ndef bfs_queue(neighbors, start_point):\n    \"\"\"Perform BFS starting from the provided index to identify connected clusters.\"\"\"\n    queue = deque([start_point])  # Start with the initial point\n    visited_indices = {start_point}\n    clusters = [[start_point]]\n\n    while queue:\n        node = queue.popleft()\n        for neighbor_index in sorted(range(len(neighbors[node])), key=lambda i: (-neighbors[node][i], i)):\n            if neighbor_index not in visited_indices:\n                visited_indices.add(neighbor_index)\n                queue.append(neighbor_index)\n                clusters[-1].append(neighbor_index)\n\n    return clusters\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Implementation of the DBSCAN algorithm.\"\"\"\n    m, _ = data.shape\n    max_cluster_id = -1\n    \n    distances = np.array([[euclidean_distance(a, b) for b in data] for a in data])\n    \n    neighborhood_lists = build_neighborhood_list(data, eps, min_samples)\n    \n    core_points = find_core_points(neighborhood_lists, min_samples)\n    \n    for core_point in core_points:\n        clusters = bfs_queue(neighborhood_lists, core_point)\n        max_cluster_id = max(max_cluster_id, max(cluster for cluster in clusters) or 0)\n        \n    assigned_clusters = [-1] * m\n    for i in range(m):\n        if i not in core_points:\n            assigned_clusters[i] = min_cluster_id_for_single_sample(assigned_clusters, i, data, eps, min_samples, max_cluster_id)\n            \n    return assigned_clusters\n\ndef min_cluster_id_for_single_sample(assigned_clusters, single_sample, data, eps, min_samples, max_cluster_id):\n    \"\"\"Assign the closest existing cluster id to the single sample.\"\"\"\n    nearest_cluster_id = None\n    min_distance_sum = float('inf')\n    \n    for cluster_id in range(1, max_cluster_id + 1):\n        total_distance_sq = 0\n        for point in data[cluster_id - 1]:\n            total_distance_sq += (point - data[single_sample])**2\n        \n        avg_distance = total_distance_sq / len(data[cluster_id - 1])\n        \n        if avg_distance < min_distance_sum:\n            min_distance_sum = avg_distance\n            nearest_cluster_id = cluster_id\n            \n    new_assigned_cluster_id = nearest_cluster_id\n    if new_assigned_cluster_id != -1:\n        assign_to_existing_cluster(assigned_clusters, single_sample, new_assigned_cluster_id, data, eps, min_samples, max_cluster_id)\n    else:\n        assign_new_cluster(assigned_clusters, single_sample, data, eps, min_samples, max_cluster_id)\n        \n    return new_assigned_cluster_id\n\ndef assign_to_existing_cluster(assigned_clusters, single_sample, new_cluster_id, data, eps, min_samples, max_cluster_id):\n    \"\"\"Update the cluster assignment for the single sample\"\"\"\n    for other_sample in range(len(data)):\n        if not (assigned_clusters[other_sample] == -1) and any(np.linalg.norm(data[other_sample] - data[single_sample]) <= eps):\n            assigned_clusters[other_sample] = new_cluster_id\n\ndef assign_new_cluster(assigned_clusters, single_sample, data, eps, min_samples, max_cluster_id):\n    \"\"\"Create a new cluster for the single sample\"\"\"\n    assigned_clusters[single_sample] = max_cluster_id + 1\n    max_cluster_id += 1\n\ndef main():\n    # Example usage\n    np.random.seed(42)\n    data = np.random.rand(100, 2)  # Generate random points with 2 features\n    eps = 0.5\n    min_samples = 5\n    \n    result = dbscan(data, eps, min_samples)\n    print(result)\n    assert isinstance(result, list) and all(isinstance(item, int) for item in result), \"Output should be a list of integer values.\"\n    assert len(set(result)) == len(result) + 1, \"There should be one more noise point than distinct clusters.\"\n\nif __name__ == \"__main__\":\n    main()\n\n```"}
{"task_id": 561, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# Example usage\ndata = np.random.randn(50, 2)\nk = 3\nlabels = gmm_em(data, k)\n\nprint(labels)\n```\n\n### Explanation:\n\n1. **Importing Libraries:** We import `numpy` for numerical operations and `math` for mathematical constants used within the EM algorithm.\n\n2. **Function Definition:** The function `gmm_em` takes four parameters: \n   - `data`: A 2D NumPy array representing the dataset with N samples and D features.\n   - `k`: An integer specifying the number of Gaussian components.\n   - `max_iter`: Maximum number of iterations for the EM algorithm before stopping early if no improvement has been made (`None` by default). Default value is set to 200.\n   - `epsilon`: Tolerance level for the change in mixture weights between iterations.\n\n3. **Initialization:**\n   - If `k == 1`, assign each element of `data` to one component (i.e., the single component model) and return an empty list `[...]`.\n   - Otherwise, initialize the GMM parameters randomly but ensure they sum up to 1. This is done using the method of Dirichlet distribution (a conjugate prior for binomial likelihoods). Let's call these initial values `weights`, `means`, and `covariances`.\n\n4. **Expectation Step (E-step):**\n   - Compute responsibilities (`gamma`) which represent the probability that each observation belongs to each component. For this step we need to perform matrix multiplications involving the covariances and a diagonal matrix constructed from inverse weights and means.\n\n5. **Maximization Step (M-step):**\n   - Update the weights (`w`), means (`mu`), and full covariance matrices (`Sigma`) based on the responsibilities computed in E-step. The updated components' weights reflect how much each component contributes to the overall mixture model according to its responsibilities.\n\n6. **Convergence Check:**\n   - Iterate through the expected and maximized steps until the absolute difference in weights between consecutive iterations falls below `epsilon`. Return the clustered labels after convergence.\n\n7. **Sorting and Label Assignment:**\n   - Sort the clusters based on the first feature's mean and re-label them such that the first component gets labeled \"0\", the second \"1\", etc.\n\n8. **Execution:**\n   - Call the function with provided data points and desired number of clusters, then print the resulting labels.\n\nThis implementation ensures the algorithm runs efficiently while maintaining computational tractability by limiting the number of iterations needed to converge."}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    import scipy.sparse.linalg\n    \n    # Step 1: Build the similarity graph using the Gaussian kernel\n    def gaussian_kernel(X):\n        kernel_matrix = np.exp(-(X[:, None, :] - X[None, :, :]) ** 2 / (-2 * gamma))\n        return kernel_matrix\n    \n    G = gaussian_kernel(data)\n    \n    if method == 'unnormalized':\n        L = G\n    elif method == 'normalized':\n        D = np.diag(np.sum(G, axis=1)) ** (-1 / 2)\n        L_sym = np.dot(D, np.dot(G, D))\n        L = scipy.sparse.eye(len(L_sym), format='lil') - L_sym + L_sym\n        \n    # Step 2: Construct the laplacian matrix\n    D_inv_sqrt = np.diag(1 / np.sqrt(np.sum(G, axis=1)))\n    L = np.dot(np.dot(D_inv_sqrt, G), D_inv_sqrt)\n    \n    # Step 3: Compute the eigenvectors corresponding to the lowest n_cluster eigenvalues\n    eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(L, k=n_cluster, which='LM')\n    \n    # Step 4: Reduce dimensions to eigenvectors in the k smallest eigenvalue range \n    reduced_eigenvectors = eigenvectors[np.argsort(eigenvalues)[0:n_cluster]]\n    \n    # Step 5: Cluster the data based on these eigenvectors\n    _, labels = scipy.cluster.vq.kmeans(reduced_eigenvectors, n_clusters=n_cluster)\n    \n    # Relabeling the clusters\n    min_label = min(labels)\n    max_label = max(labels) + 1\n    shifted_labels = [label - min_label for label in labels]\n    \n    return shifted_labels.tolist()\n\ndata = np.random.rand(100, 2)\ncluster_result = spectral_clustering(data, 3)\n\nprint(cluster_result)\n```"}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"\n    Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Parameters:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels -1 or 1.\n        eta:     Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n\n    def _update_weights(alpha, G, y_i, xi, eta):\n        \"\"\"Update alpha and b based on one step of gradient descent.\"\"\"\n        g = np.dot(G, alpha)  # dot product between alpha and G\n        yi_g_b = y_i * (g + b)\n        if yi_g_b <= 0:\n            alpha -= eta\n            b += eta * y_i\n\n    N, d = X_data.shape\n    alpha = np.zeros((N,))\n    b = 0\n    G = np.dot(X_data.T, X_data)\n\n    for _ in range(max_iter):\n        converged = True\n        for i in range(N):\n            y_i = y_data[i]\n            xi = X_data[i]\n            delta_alpha = alpha[y_i] * yi * np.dot(G[i], xi)\n            alpha[y_i] += eta * delta_alpha\n            b += eta * y_i * delta_alpha\n            _update_weights(alpha, G, y_i, xi, eta)\n            if not np.isclose(delta_alpha, 0, atol=1e-6):\n                converged = False\n                break\n        if converged:\n            break\n\n    w = sum(a * y_i * xi for a, y_i, xi in zip(alpha, y_data, X_data))\n    w = [round(w_val, 4) for w_val in w]\n    return w, round(b, 4)\n\n\n# Example usage:\nX = np.array([[1, 2], [-1, 3], [1, -3]])\ny = np.array([1, -1, 1])\nweight_vector, bias_term = perceptron_dual_train(X, y)\nprint(\"Weight Vector:\", weight_vector)\nprint(\"Bias Term:\", bias_term)\n```"}
{"task_id": 564, "completion_id": 0, "completion": "Here's a solution to the problem based on the provided instructions:\n\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str) -> int:\n    if task_type.lower() == 'classification':\n        while len(root.child) > 1:\n            x = features[root.label]\n            if x < root.s:\n                root = root.child[0]\n            else:\n                root = root.child[1]\n        y = root.y\n    elif task_type.lower() == 'regression':\n        s = features[root.label]\n        i = 0\n        while True:\n            x = features[i]\n            if x <= s:\n                root = root.child[0]\n                i += 1\n            else:\n                break\n        y = root.y\n    return y\n\n# Example usage:\nfeatures_example_classification = [5, 7, 9, 8, 6, 4, 10, 11, 12, 14, 13, 15]\nfeatures_example_regression = [9, 8, 7, 6, 5, 4, 3, 2, 1]\n\nroot_classification = Node(label=0)\nroot_regression = Node(s=10)\n\n# Populate the tree according to the example data\nroot_classification.child.extend([Node(x=5), Node(x=10)])\nroot_classification.child[0].child.append(Node(label=2))\nroot_classification.child[0].child[-1].child.extend([Node(x=6), Node(x=14)])\n\nroot_regression.child.append(Node(y=0))\n\n# Test cases\nprint(predict_sample(root_classification, features_example_classification, 'classification'))  # Should return 2\nprint(predict_sample(root_regression, features_example_regression, 'regression'))  # Should return 0\n```\n\nThis script defines the `predict_sample` function that performs the traversal based on the specified task type ('classification' or 'regression') and returns the value stored in the reached leaf without performing any additional transformations or rounding. It also includes example usage where the tree is populated according to specific feature values and then tested against different samples."}
{"task_id": 565, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    # Calculate the number of samples and features\n    n_samples, n_features = X.shape\n    \n    # Initialize variables to store the maximum information gain and its corresponding feature index\n    max_gain = -np.inf\n    best_feature_idx = None\n\n    # Iterate over each feature\n    for feature_idx in range(n_features):\n        # Split the data based on the current feature\n        x_split = np.array_split(X[:, feature_idx], len(np.unique(X[:, feature_idx])))\n\n        # Calculate the entropies for the split cases\n        split_entropies = []\n        for case in x_split:\n            h_y_case = calculate_entropy(y[case])\n            h_y_given_x_case = calculate_conditional_entropy(y, case)\n            split_entropies.append(h_y_case - h_y_given_x_case)\n\n        # If there's only one unique value for this feature, we can't perform any information gain calculation\n        if len(x_split) == 1:\n            continue\n\n        # Calculate the total entropy\n        total_entropy = sum(split_entropies) / len(x_split)\n\n        # Calculate the information gain\n        info_gain = total_entropy - min(split_entropies)\n\n        # Update the maximum information gain and its corresponding feature index\n        if info_gain > max_gain:\n            max_gain = info_gain\n            best_feature_idx = feature_idx\n\n    # Convert the result to a float and round it to 6 decimal places\n    best_feature_idx = int(best_feature_idx)\n    max_info_gain = round(max_gain, 6)\n\n    return best_feature_idx, max_info_gain\n\ndef calculate_entropy(labels: np.ndarray) -> float:\n    \"\"\"Calculate the Shannon entropy of a set of labels.\"\"\"\n    # Count unique labels\n    unique_labels = np.unique(labels)\n    counts = np.bincount(labels)\n    \n    # Calculate the probabilities of each label\n    probs = counts / np.sum(counts)\n    \n    # Calculate the entropy\n    entropy = -np.sum(probs * np.log2(probs))\n    \n    return entropy\n\ndef calculate_conditional_entropy(target_labels: np.ndarray, subset_indices: np.ndarray) -> float:\n    \"\"\"Calculate the conditional entropy given a subset of indices.\"\"\"\n    target_subset = target_labels[subset_indices]\n    return calculate_entropy(target_subset)\n\n# Example usage\nX_example = np.array([[1, 2], [2, 1], [2, 2], [1, 1]])\ny_example = np.array([0, 1, 1, 0])\n\nresult = choose_best_feature(X_example, y_example)\nprint(result)\n```"}
{"task_id": 566, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> List[int]:\n    np.random.seed(42)\n    \n    def calculate_path_length(observation):\n        # Path length calculation based on the given formula.\n        cumulative_probability = 1 / (math.pow(2, len(observation)) + 1)\n        return 2 * math.log(len(observation) - 1, 2) - 2 * ((len(observation) - 1) / len(observation))\n    \n    def get_tree_depth(node_index):\n        if node_index == 0:\n            return 0\n        else:\n            left_child_idx = 0\n            right_child_idx = 1\n            \n            if node_index & 1 == 0:  # Odd index represents a left child\n                parent_node_idx = node_index // 2\n                left_child_idx = get_tree_depth(parent_node_idx)\n                \n                if left_child_idx > 0:\n                    while left_child_idx > 0 and data[node_index][left_child_idx] == data[parent_node_idx][left_child_idx]:\n                        left_child_idx = left_child_idx * 2 + 1\n                    \n                return left_child_idx + 1 if left_child_idx > 0 else left_child_idx\n        \n            elif node_index % 2 != 0:  # Even index represents a right child\n                parent_node_idx = node_index // 2\n                right_child_idx = get_tree_depth(parent_node_idx)\n                \n                if right_child_idx > 0:\n                    while right_child_idx > 0 and data[node_index][right_child_idx] == data[parent_node_idx][right_child_idx]:\n                        right_child_idx = right_child_idx * 2 + 1\n                        \n                return right_child_idx + 1 if right_child_idx > 0 else right_child_idx\n    \n    def prune_leaf(node_index, obs_len):\n        nonlocal tree_depth\n        left_child_idx = 0\n        right_child_idx = 1\n        \n        if obs_len <= 1:\n            return\n        \n        if left_child_idx > 0 and obs_len[left_child_idx] == 1:\n            while left_child_idx > 0 and data[node_index][left_child_idx] == data[tree_depth - 1][left_child_idx]:\n                left_child_idx = left_child_idx << 1 | 1\n                \n            if left_child_idx > 0:\n                while left_child_idx > 0 and data[node_index][left_child_idx] == data[trees[-1]][left_child_idx]:\n                    left_child_idx = left_child_idx >> 1 | 1\n                \n                prune_leaf(left_child_idx, obs_len[left_child_idx])\n        \n        if right_child_idx > 0 and obs_len[right_child_idx] == 1:\n            while right_child_idx > 0 and data[node_index][right_child_idx] == data[tree_depth - 1][right_child_idx]:\n                right_child_idx = right_child_idx << 1 | 1\n                \n            if right_child_idx > 0:\n                while right_child_idx > 0 and data[node_index][right_child_idx] == data[trees[-1]][right_child_idx]:\n                    right_child_idx = right_child_idx >> 1 | 1\n                    \n                prune_leaf(right_child_idx, obs_len[right_child_idx])\n\n    def build_random_forest():\n        tree_depth = 0\n        nodes = []\n        \n        while tree_depth < height_limit or sample_size > 1:\n            sample = np.random.choice(sample_size, size=sample_size, replace=False)\n            \n            while True:\n                tree = []\n                node_idx = 0\n                \n                while node_idx < sample_size and (not tree or not nodes[node_idx]):\n                    tree.append(node_idx)\n                \n                tree_depth += 1\n                nodes.append(tree[:])\n                \n                for i in reversed(range(tree_depth)):\n                    node_idx = tree[i]\n                    \n                    if i % 2 == 1:\n                        new_node_idx = node_idx << 1 | 0\n                        nodes[new_node_idx].extend(nodes.pop(i - 1))\n                        \n                        if len(nodes[i - 1]) >= 2:\n                            mid_idx = (i - 1 + len(nodes[i - 1]) // 2) // 2\n                            \n                            left_child_idx = nodes[i - 1][mid_idx]\n                            right_child_idx = nodes[i - 1][mid_idx + 1]\n                            \n                            nodes[new_node_idx].append(left_child_idx)\n                            nodes[new_node_idx].append(right_child_idx)\n                    else:\n                        mid_idx = (i - 1 + len(nodes[i - 1]) // 2) // 2\n                        \n                        left_child_idx = nodes[i - 1][mid_idx]\n                        right_child_idx = nodes[i - 1][mid_idx + 1]\n                        \n                        if len(nodes[i - 1]) >= 2:\n                            mid_idx = (nodes[i - 1][mid_idx] << 1 | 1) % sample_size\n                            mid_idx = mid_idx + 1 + (nodes[i - 1][mid_idx] % (sample_size // 2))\n                                \n                            nodes[i].append(mid_idx)\n                            nodes[i].append((mid_idx + 1) % sample_size)\n                            nodes[i].append(nodes[i - 1][(nodes[i - 1][mid_idx] << 1 | 1)]%sample_size)\n                            nodes[i].append(nodes[i - 1][(nodes[i - 1][mid_idx] + 1) % (sample_size // 2)]%sample_size)\n                            nodes[new_node_idx].append(new_node_idx)\n                        else:\n                            nodes[new_node_idx].append(left_child_idx)\n                            nodes[new_node_idx].append(right_child_idx)\n                    \n                for idx in range(len(nodes[tree_depth])):\n                    nodes[tree_depth][idx] -= tree_depth\n                \n                if sample_size <= 1:\n                    break\n                \n                sample = np.array([sample[idx] for idx in nodes[tree_depth]])\n                sample_size = np.count_nonzero(sample == 0)\n\n            tree = np.argsort(-calculate_path_length(sample))\n            for i in range(tree.size // 2):\n                trees.append(tree[i] - 1)\n\n    height_limit = int(np.ceil(math.log2(sample_size)))  # Limit the depth of each tree\n    trees = []\n\n    build_random_forest()\n    \n    # Calculate the average path length (mean value across the trees)\n    avg_path_length = sum(calculate_path_length(obs) for _ in range(n_trees)) / n_trees\n    \n    # Compute the anomaly score using the given formula\n    anomalies_scores = [(avg_path_length / phi) + np.exp(-path_length / avg_path_length)]\n    \n    # Sort the anomalies scores from smallest to largest\n    anomalies_scores.sort()\n\n    # Find the indices of the top `epsilon` fraction of points\n    top_indices = [index for index in enumerate(anomalies_scores) if index[1] <= anomalies_scores[-int(n_trees * epsilon)]:]\n\n    # Return only the zero-based indices\n    return [index[0] for index in top_indices]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    data_points = [[3, 1, 2], [4, 5, 6], [7, 8, 9], [1, 2, 3]]\n    n_trees = 10\n    sample_size = min(len(data_points), 50)\n    epsilon = 0.05\n    \n    outliers = detect_outliers(np.array(data_points), n_trees, sample_size, epsilon)\n    print(outliers)\n```"}
{"task_id": 568, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef sigmoid(z):\n    return 1 / (1 + math.exp(-z))\n\nclass MaxEntClassifier:\n    def __init__(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.n_features = X_train.shape[1]\n        self.weight_vector = np.zeros(self.n_features)\n    \n    def fit(self):\n        # Calculate empirical frequencies\n        freqs = {}\n        for i in range(len(self.X_train)):\n            x_i = self.X_train[i].reshape((1, -1))\n            y_i = self.y_train[i]\n            for j in range(self.n_features):\n                if f'{j},{x_i[j]}:{self.y_train[i]}':  # Check if key exists\n                    freqs[f'{j},{x_i[j]}:{self.y_train[i]}'] = freqs.get(f'{j},{x_i[j]}:{self.y_train[i]}', 0) + 1\n                else:\n                    freqs[f'{j},{x_i[j]}:{self.y_train[i]}'] = 1\n        \n        # Convert frequent counts to probabilities\n        total_count = sum(freqs.values())\n        for k, v in freqs.items():\n            freqs[k] = v / total_count\n    \n        # Initialize weights\n        self.weights = np.array(list(freqs.keys())).astype(float)\n        self.probabilities = [freqs[key] for key in self.weights]\n        \n        while True:\n            new_weights = np.zeros_like(self.weights)\n            for j in range(self.n_features):\n                for xy_y_pair in freqs.keys():\n                    xy, y = xy_y_pair.split(',')\n                    x_i, x_val = xy.split(',')\n                    z = np.dot(self.weights, np.array([int(x_i), int(x_val)])) + j\n                    p_xy = freqs[f'({xy},{y})']\n                    delta_w = (-p_xy / (1 + math.exp(z))) * p_xy * math.log(p_xy / (1 + math.exp(z)))\n                    new_weight = self.weights[j] + delta_w\n                    new_weights[j] += delta_w\n            \n            if all(abs(w_new - w_old) < epsilon for w_old, w_new in zip(self.weights, new_weights)):\n                break\n                \n            self.weights = new_weights\n            \n    def predict(self, X_test):\n        predictions = []\n        for x_test_row in X_test.reshape((-1, 1)):\n            log_probabilities = np.sum(np.dot(self.weights, x_test_row), axis=0)\n            prob = sigmoid(log_probabilities)\n            _, argmax_label = np.max(np.c_[prob, self.probabilities]), np.argmax(prob)\n            predictions.append(argmax_label.item())\n        return predictions\n\ndef maxent_predict(X_train, y_train, X_test, epsilon=1e-3, n_iter=100):\n    clf = MaxEntClassifier(X_train, y_train)\n    clf.fit()\n    return clf.predict(X_test)\n```"}
{"task_id": 569, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    word_counts = Counter(word for doc in data for word in doc[1])\n    word_freqs = sorted(word_counts.values(), reverse=True)\n    \n    if cut_off is None:\n        return [(word, freq / len(data)) for word, freq in word_counts.items()]\n    else:\n        num_words = min(len(word_counts), cut_off)\n        ranked_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:num_words]\n        return [(word, freq / sum(doc[1].count(word) for doc in data)) for word, freq in ranked_words]\n\n\ndata = [\n    ('doc1', ['the', 'quick', 'brown', 'fox']),\n    ('doc2', ['lazy', 'dog']),\n    ('doc3', ['brown', 'fox', 'jumps', 'over'])\n]\n\nprint(statistic_key_word(data))\n```"}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    n_samples, n_features = X.shape[0], len(alpha)\n    \n    if n_features == 1:\n        return [np.sum(alpha * y[:, None] * X).ravel()], np.mean(y - np.dot(alpha, y[:, None]) * X.T)\n    \n    # Construct the weight vector and the bias term\n    w_vector = np.zeros(n_features)\n    for i, alpha_i in enumerate(alpha):\n        w_vector += alpha_i * y[i] * X[:, i]\n    \n    b = np.mean(np.array([y[i] - np.dot(alpha_i, y[i] * X[:, i].T) for i, alpha_i in enumerate(alpha)]))\n    \n    # Round both the weight vector and the bias term to 4 decimal places\n    w_list = [round(x.item(), 4) for x in w_vector]\n    b_rounded = round(b.item(), 4)\n    \n    return w_list, b_rounded\n\n\n# Example usage:\nX_example = np.random.rand(50, 1)  # Random 50 samples with one feature each.\ny_example = np.sign(np.random.randn(50))  # Class labels (+1 or -1) randomly generated.\nalpha_example = np.random.rand(50)  # Randomly chosen Lagrange multipliers.\n\nresult = compute_svm_parameters(X_example, y_example, alpha_example)\n\nprint(\"Weight Vector:\", result[0])\nprint(\"Bias Term:\", result[1])\n```"}
{"task_id": 572, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"\n    Computes the Local Outlier Factor for each sample in the dataset.\n\n    Parameters:\n    - data: A NumPy array representing the input dataset.\n    - k: An integer specifying the number of nearest neighbors to consider.\n    - epsilon: A float value indicating the threshold for outlier detection.\n\n    Returns:\n    - A tuple containing the list of LOF scores rounded to the nearest 4th decimal place,\n      and the list of indices of outliers sorted in ascending order.\n    \"\"\"\n\n    def compute_distances(data):\n        n_samples = len(data)\n        dist_matrix = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(i + 1, n_samples):\n                dist_matrix[i][j] = np.sqrt(np.sum((data[i] - data[j]) ** 2))\n                dist_matrix[j][i] = dist_matrix[i][j]\n        return dist_matrix\n\n    def find_k_nearest_neighbors(dist_matrix, k, p_index):\n        k_nearest_neighbor_indices = np.argsort(dist_matrix[p_index])[1:k+1]\n        k_nearest_neighbors = [dist_matrix[p_index][index] for index in k_nearest_neighbor_indices]\n        return k_nearest_neighbors, k_nearest_neighbor_indices\n\n    def compute_lrd(data, k_nearest_neighbors, k_nearest_neighbor_indices, p_index):\n        lrd_values = []\n        for neighbor_index in range(len(k_nearest_neighbors)):\n            reach_dist_p_q = max(k_nearest_neighbors[neighbor_index], data[p_index])\n            lrd_values.append(reach_dist_p_q)\n\n        numerator = sum(lrd_values)\n        denominator = len(k_nearest_neighbors) * k_nearest_neighbors[k_nearest_neighbor_indices.index(p_index)]\n        if denominator == 0 or numerator == 0:\n            return 0\n        else:\n            numerator /= denominator\n            return numerator\n\n    def calculate_lof(data, k_nearest_neighbors, k_nearest_neighbor_indices, p_index, epsilon):\n        lrd_values = [compute_lrd(data, k_nearest_neighbors, k_nearest_neighbor_indices, q_index) for q_index in range(len(k_nearest_neighbors))]\n        lof_values = [(sum(lrd_values[:k_nearest_neighbor_indices.index(p_index)]) / k) / lrd_values[k_nearest_neighbor_indices.index(p_index)] for p_index in range(len(k_nearest_neighbors))]\n        return lof_values\n\n    n_samples = len(data)\n    dist_matrix = compute_distances(data)\n    k_nearest_neighbors, k_nearest_neighbor_indices = find_k_nearest_neighbors(dist_matrix, k, 0)\n    lof_values = calculate_lof(data, k_nearest_neighbors, k_nearest_neighbor_indices, 0, epsilon)\n    \n    # Filter out non-outliers based on the given epsilon\n    filtered_indices = [i for i, value in enumerate(lof_values) if value > epsilon]\n\n    # Round the LOF scores to 4 decimal places\n    round_4_decimal = lambda x: f\"{np.round(x, 4)}\"\n    rounded_scores = [float(round_4_decimal(value)) for value in lof_values]\n\n    # Return the results as a tuple\n    return (rounded_scores, filtered_indices)\n\n\n# Example usage\nif __name__ == \"__main__\":\n    data = np.array([[1, 2], [2, 2], [3, 3], [4, 4]])\n    k = 2\n    epsilon = 1.5\n    result = local_outlier_factor(data, k, epsilon)\n    print(result)\n```\n\nThis script defines the `local_outlier_factor` function according to the specified requirements. It first computes the pairwise distances between all points in the dataset using the `compute_distances` helper function. Then it calculates the k-nearest neighbors for each point using `find_k_nearest_neighbors`. Next, it computes the Local Reachability Density (LRD) values and finally calculates the Local Outlier Factor (LOF) for each sample. The final step involves filtering out outliers based on the given epsilon threshold and rounding the LOF scores to the nearest four decimal places before returning the results as a tuple."}
{"task_id": 574, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `generate_data` function according to the provided specifications:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int, n_train: int, n_val: int, seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int, optional\n        Random seed for reproducibility.  If *None* the global random state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    # Calculate the base coordinates based on the number of classes\n    base = 10 * (m // 2)\n    \n    # Generate coordinates for each class\n    X_trains = []\n    X_vals = []\n    y_trains = []\n    y_vals = []\n    \n    for c in range(m):\n        x_min, x_max = base + 1, base + 9\n        y_min, y_max = base + 1, base + 9\n        \n        # Generate training points within valid bounds\n        x_coords_train = np.random.uniform(x_min, x_max, size=(n_train,))\n        y_coords_train = np.random.uniform(y_min, y_max, size=(n_train,))\n        \n        # Add coordinates to lists\n        X_trains.append(np.column_stack((x_coords_train, y_coords_train)))\n        y_trains.append(c)\n        \n        # Generate valdation points overlapping with training points\n        x_coords_val = x_coords_train[:n_val]\n        y_coords_val = y_coords_train[:n_val]\n        \n        X_vals.append(np.column_stack((x_coords_val, y_coords_val)))\n        y_vals.append(c)\n    \n    # Combine all points into one array\n    X_train_arr = np.concatenate(X_trains)\n    X_val_arr = np.concatenate(X_vals)\n    y_train_arr = np.array(y_trains)\n    y_val_arr = np.array(y_vals)\n    \n    return (X_train_arr, X_val_arr, y_train_arr, y_val_arr)\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    m = 3\n    n_train = 500\n    n_val = 100\n    \n    # Seed the random generator for reproducibility\n    seed = 42\n    generated_data = generate_data(m, n_train, n_val, seed=seed)\n    \n    print(\"Generated Data:\")\n    print(f\"X_train shape: {generated_data[0].shape}\")\n    print(f\"y_train shape: {generated_data[1].shape}\")\n    print(f\"X_val shape: {generated_data[2].shape}\")\n    print(f\"y_val shape: {generated_data[3].shape}\")\n```\n\nThis script defines the `generate_data` function which generates the required synthetic 2D data following the specified guidelines. It uses the `numpy` library for generating random numbers and `random` module for seeding the random generator. The example usage at the bottom demonstrates how to use this function."}
{"task_id": 579, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    n_samples, _ = X.shape\n    \n    # Initial centroid assignment based on the first 'k' samples\n    centroids = X[:k]\n    \n    while True:\n        # Assigning each sample to the closest centroid\n        labels = [np.argmin(np.linalg.norm(sample - centroids, axis=1)) for sample in X]\n        \n        # Update centroids to the average location of the points in their respective clusters\n        new_centroids = []\n        for i in range(k):\n            cluster_points = X[labels == i]\n            if len(cluster_points) > 0:\n                new_centroids.append(np.mean(cluster_points, axis=0))\n            else:\n                new_centroids.append(centroids[i])\n        \n        # Check for convergence or reach max_iters\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n        \n        centroids = new_centroids\n            \n    return labels, [round(c[0], 4), round(c[1], 4)] * k\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1.0, 2.0, 3.0],\n                  [4.0, 5.0, 6.0],\n                  [7.0, 8.0, 9.0],\n                  [1.0, 2.0, 3.0]])\n    k = 2\n    labels, centers = kmeans(X, k)\n    print(\"Labels:\", labels)\n    print(\"Centers:\", centers)\n```"}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _l1_sign(x):\n    if x == 0:\n        return -1\n    elif x > 0:\n        return 1\n    else:\n        return -1\n\ndef _compute_gradients(X, y, w, b, l1_penalty, l2_penalty):\n    residuals = y - X @ w - b\n    grad_w = (-2 * X.T @ residuals.reshape(-1, 1)) / X.shape[0] + (\n        l1_penalty * (_l1_sign(w)).reshape(-1, 1) +\n        2 * l2_penalty * w.reshape(-1, 1)\n    )\n    grad_b = (-2 * np.sum(residuals)) / X.shape[0]\n    return grad_w.flatten(), grad_b\n\ndef elastic_net_regression(X, y, learning_rate, iterations, l1_penalty, l2_penalty):\n    if not learning_rate or not iterations:\n        return [0] * X.shape[1], 0\n    if X.ndim != 2 or y.ndim != 1:\n        raise ValueError(\"Input arrays should have the correct dimensions.\")\n    \n    m, n = X.shape\n    w = np.zeros(n)\n    b = 0\n    \n    for _ in range(iterations):\n        g, gb = _compute_gradients(X, y, w, b, l1_penalty, l2_penalty)\n        \n        # Update parameters using learning rate and mean gradient\n        w -= learning_rate * (g / m).flatten()\n        b -= learning_rate * gb / m\n        \n    return list(np.round(w, 4)), round(b, 4)\n\n# Test cases can be added here to verify correctness\n```"}
{"task_id": 591, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    # Add the bias term to the input\n    x_with_bias = np.insert(x, 0, 1)\n    \n    for W in weights:\n        # Compute the weighted sum for this layer\n        z = np.dot(W[0], x_with_bias)\n        \n        # Apply ReLU activation\n        h = np.maximum(0, z)\n        \n        # Update x_for_next_layer to store h values\n        x_for_next_layer = h\n    \n    # Check if there's only one output neuron\n    if len(weights[-1]) == 1:\n        # Return the output value directly\n        return round(h[-1], 4)\n    else:\n        # Otherwise, return the list of outputs\n        return [round(output, 4) for output in h]\n\n# Example usage:\n# Define some sample weights and inputs\nweights_example = [\n    [[0.1, -0.5], [-0.1, 0.6]],  # First hidden layer\n    [[0.2, -0.4], [-0.2, 0.7]],  # Second hidden layer\n    [[0.3, -0.3], [-0.3, 0.8]],  # Last output layer\n]\n\ninput_vector = [1.0, 2.0, 3.0]\n\n# Predict using the example weights and input vector\nprediction = neural_net_predict(weights_example, input_vector)\n\nprint(prediction)\n```"}
{"task_id": 595, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef euclidean_distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2) ** 2))\n\ndef assign_to_nearest_centroid(samples, centroids):\n    distances = np.array([[euclidean_distance(sample, centroid) for centroid in centroids] for sample in samples])\n    closest_centroids_indices = np.argmin(distances, axis=1)\n    \n    return [closest_centroids_indices.tolist()]\n\ndef update_centroids(points, index):\n    centroid = points[index]\n    new_centroid = np.mean(points[:, index], axis=0)\n    return np.round(new_centroid, decimals=4)\n\ndef kmeans_clustering(X, k, max_iters=100):\n    assert isinstance(X, np.ndarray), 'Input X must be a numpy array.'\n    assert isinstance(k, int), 'Invalid value passed for parameter k'\n    assert isinstance(max_iters, int), 'Invalid value passed for parameter max_iters'\n\n    # Initialize centroids\n    centroids = X[:k]\n\n    for _ in range(max_iters):\n        assignments = assign_to_nearest_centroid(X, centroids)\n        \n        prev_centroids = centroids.copy()\n        for i in range(len(centroids)):\n            if len(X[assignments[i]]) > 0:\n                centroids[i] = update_centroids(X[assignments[i]], i)\n            else:\n                centroids[i] = prev_centroids[i]\n\n        converged = True\n        for i, c in enumerate(centroids):\n            if not np.allclose(c, prev_centroids[i]):\n                converged = False\n        \n        if converged:\n            break\n    \n    # Sort centroids in lexicographical order\n    sorted_centroids = sorted(centroids, key=lambda x: tuple(x))\n    sorted_centroids_rounded = [[round(val, 4) for val in item] for item in sorted_centroids]\n\n    return sorted_centroids_rounded\n\n\n# Example usage\nif __name__ == \"__main__\":\n    X_example = np.array([\n        [1, 2],\n        [2, 3],\n        [8, 7],\n        [1, 0],\n        [9, 4],\n        [5, 6]\n    ])\n\n    print(\"Sorted Centroids:\", kmeans_clustering(X_example, 3))\n```"}
