{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n\n    Args:\n        a: A matrix represented as a list of lists.\n        b: A vector represented as a list.\n\n    Returns:\n        A list representing the resulting vector if the operation is valid,\n        or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if not a or not b:\n        return -1\n    num_cols = len(a[0])\n    if len(b) != num_cols:\n        return -1\n    result = []\n    for row in a:\n        dot_product = 0\n        for i in range(num_cols):\n            dot_product += row[i] * b[i]\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n\n    Args:\n        a: A list of lists representing the matrix.\n\n    Returns:\n        A list of lists representing the transpose of the matrix.\n    \"\"\"\n    if not a:\n        return []\n    rows = len(a)\n    cols = len(a[0])\n    transpose = [[0 for _ in range(rows)] for _ in range(cols)]\n    for i in range(rows):\n        for j in range(cols):\n            transpose[j][i] = a[i][j]\n    return transpose"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"Reshapes a given matrix into a specified shape.\n    If it cannot be reshaped, return an empty list \"[]\".\n    \"\"\"\n    try:\n        np_array = np.array(a)\n        reshaped_array = np_array.reshape(new_shape)\n        return reshaped_array.tolist()\n    except ValueError:\n        return []"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix either by row or by column, based on a given mode.\n\n    Args:\n        matrix (list[list[float]]): The input matrix represented as a list of lists of floats.\n        mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n        list[float]: A list of means calculated according to the specified mode.\n                     Returns an empty list if the input matrix is empty or the mode is invalid.\n    \n    Raises:\n        TypeError: if matrix is not a list of lists or mode is not a string.\n        ValueError: if mode is not 'row' or 'column'.\n    \"\"\"\n    if not isinstance(matrix, list):\n        raise TypeError('Matrix must be a list of lists.')\n    if not isinstance(mode, str):\n        raise TypeError('Mode must be a string.')\n    if not matrix:\n        return []\n    for row in matrix:\n        if not isinstance(row, list):\n            raise TypeError('Matrix must be a list of lists.')\n        for element in row:\n            if not isinstance(element, (int, float)):\n                raise TypeError('Matrix elements must be numbers.')\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        if num_cols == 0:\n            return []\n        means = [sum((matrix[i][j] for i in range(num_rows))) / num_rows for j in range(num_cols)]\n    else:\n        raise ValueError(\"Invalid mode. Mode must be 'row' or 'column'.\")\n    return means"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the result.\n\n    Args:\n        matrix: A list of lists representing the matrix.\n        scalar: The scalar value to multiply the matrix by.\n\n    Returns:\n        A new matrix that is the result of multiplying the input matrix by the scalar.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = [scalar * element for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n        matrix (list[list[float|int]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n        list[float]: A list containing the eigenvalues, sorted from highest to lowest.\n    \n    Raises:\n        ValueError: If the input matrix is not a 2x2 matrix.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input matrix must be a 2x2 matrix.')\n    try:\n        eigenvalues = np.linalg.eigvals(matrix)\n        eigenvalues = sorted(eigenvalues, reverse=True)\n        return list(eigenvalues)\n    except np.linalg.LinAlgError:\n        return []"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms a given matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n    The function first validates if the matrices T and S are invertible, and then performs the transformation.\n    In cases where there is no solution return -1.\n    \"\"\"\n    try:\n        A = np.array(A)\n        T = np.array(T)\n        S = np.array(S)\n        if A.shape[0] != A.shape[1]:\n            return -1\n        if T.shape[0] != T.shape[1] or T.shape[0] != A.shape[0]:\n            return -1\n        if S.shape[0] != S.shape[1] or S.shape[0] != A.shape[0]:\n            return -1\n        T_inv = np.linalg.inv(T)\n        S_inv = np.linalg.inv(S)\n        transformed_matrix = np.dot(np.dot(T_inv, A), S_inv)\n        return transformed_matrix.round(4).tolist()\n    except np.linalg.LinAlgError:\n        return -1\n    except Exception as e:\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"Calculates the inverse of a 2x2 matrix.\n\n    Args:\n        matrix: A 2x2 matrix represented as a list of lists.\n\n    Returns:\n        The inverse of the matrix as a list of lists, or None if the matrix is not invertible.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        return None\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse_determinant = 1 / determinant\n    inverse_matrix = [[d * inverse_determinant, -b * inverse_determinant], [-c * inverse_determinant, a * inverse_determinant]]\n    return inverse_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiply two matrices. If the matrices cannot be multiplied, return -1.\n    For example:\n    matrixmul([[1, 2], [3, 4]], [[5, 6], [7, 8]]) == [[19, 22], [43, 50]]\n    matrixmul([[1, 2]], [[3], [4]]) == [[11]]\n    matrixmul([[1, 2], [3, 4]], [[5, 6]]) == -1\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    c = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                c[i][j] += a[i][k] * b[k][j]\n    return c"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n        vectors: A list of lists, where each inner list represents a feature\n                 with its observations.\n\n    Returns:\n        A covariance matrix as a list of lists.\n    \"\"\"\n    num_vectors = len(vectors)\n    num_features = len(vectors[0]) if num_vectors > 0 else 0\n    if num_vectors == 0 or num_features == 0:\n        return []\n    means = [sum(feature) / num_vectors for feature in zip(*vectors)]\n    covariance_matrix = [[0.0] * num_features for _ in range(num_features)]\n    for i in range(num_features):\n        for j in range(num_features):\n            covariance = sum([(vectors[k][i] - means[i]) * (vectors[k][j] - means[j]) for k in range(num_vectors)]) / (num_vectors - 1)\n            covariance_matrix[i][j] = covariance\n    return covariance_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi method.\n\n    Args:\n        A (np.ndarray): The coefficient matrix.\n        b (np.ndarray): The right-hand side vector.\n        n (int): The number of iterations.\n\n    Returns:\n        list: The approximate solution x, rounded to four decimal places.\n    \"\"\"\n    x = np.zeros_like(b, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros_like(x, dtype=float)\n        for i in range(A.shape[0]):\n            summation = 0\n            for j in range(A.shape[1]):\n                if i != j:\n                    summation += A[i, j] * x[j]\n            x_new[i] = (b[i] - summation) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition on a 2x2 matrix by using the jacobian method.\n\n    Args:\n        A (np.ndarray): A 2x2 matrix.\n\n    Returns:\n        tuple: A tuple containing the two singular values (rounded to 4 decimal places).\n    \"\"\"\n    a = A[0, 0]\n    b = A[0, 1]\n    c = A[1, 0]\n    d = A[1, 1]\n    sigma1 = np.sqrt((a + d) ** 2 + 4 * (a * d - b * c)) / 2\n    sigma2 = np.sqrt((a + d) ** 2 + 4 * (a * d - b * c)) / 2\n    sigma1 = np.round(sigma1, 4)\n    sigma2 = np.round(sigma2, 4)\n    return (sigma1, sigma2)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n\n    Args:\n        matrix: A 4x4 matrix represented as a list of lists.\n\n    Returns:\n        The determinant of the matrix.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n\n    def determinant_3x3(submatrix: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n        \"\"\"\n        if len(submatrix) != 3 or any((len(row) != 3 for row in submatrix)):\n            raise ValueError('Input must be a 3x3 matrix.')\n        return submatrix[0][0] * (submatrix[1][1] * submatrix[2][2] - submatrix[1][2] * submatrix[2][1]) - submatrix[0][1] * (submatrix[1][0] * submatrix[2][2] - submatrix[1][2] * submatrix[2][0]) + submatrix[0][2] * (submatrix[1][0] * submatrix[2][1] - submatrix[1][1] * submatrix[2][0])\n    determinant = 0\n    for j in range(4):\n        submatrix = [row[:j] + row[j + 1:] for row in matrix[1:]]\n        determinant += (-1) ** j * matrix[0][j] * determinant_3x3(submatrix)\n    return determinant"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation.\n\n    Args:\n        X (list[list[float]]): A matrix of features.\n        y (list[float]): A vector of target values.\n\n    Returns:\n        list[float]: The coefficients of the linear regression model, rounded to four decimal places.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n    theta = np.round(theta, 4)\n    return theta.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent.\n\n    Args:\n        X (np.ndarray): Features (with a column of ones for the intercept).\n        y (np.ndarray): Target values.\n        alpha (float): Learning rate.\n        iterations (int): Number of iterations.\n\n    Returns:\n        np.ndarray: Coefficients of the linear regression model.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X @ theta\n        error = predictions - y\n        gradient = X.T @ error / m\n        theta = theta - alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Performs feature scaling on a dataset using both standardization and min-max normalization.\n\n    Args:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n        tuple[list[list[float]], list[list[float]]]: A tuple containing two 2D lists:\n            - The first list is the data scaled by standardization.\n            - The second list is the data scaled by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_vals) / (max_vals - min_vals)\n    min_max_normalized_data = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data, min_max_normalized_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Implements the k-Means clustering algorithm.\n\n    Args:\n        points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n        k: An integer representing the number of clusters to form\n        initial_centroids: A list of initial centroid points, each a tuple of coordinates\n        max_iterations: An integer representing the maximum number of iterations to perform\n\n    Returns:\n        A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    centroids = np.array(initial_centroids, dtype=float)\n    points_np = np.array(points, dtype=float)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_np[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            cluster_points = points_np[cluster_assignments == i]\n            if len(cluster_points) > 0:\n                new_centroids[i] = np.mean(cluster_points, axis=0)\n            else:\n                new_centroids[i] = points_np[np.random.choice(points_np.shape[0])]\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = []\n    for centroid in centroids:\n        final_centroids.append(tuple((round(coord, 4) for coord in centroid)))\n    return final_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n\n    Args:\n        X (np.ndarray): The input features.\n        y (np.ndarray): The target variable.\n        k (int): The number of folds.\n        shuffle (bool): Whether to shuffle the data before splitting.\n        random_seed (int): The random seed for shuffling.\n\n    Returns:\n        list: A list of train-test indices for each fold. Each element in the list is a tuple (train_indices, test_indices).\n    \"\"\"\n    n_samples = X.shape[0]\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        indices = np.arange(n_samples)\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = y[indices]\n    fold_size = n_samples // k\n    folds = []\n    for i in range(k):\n        test_start = i * fold_size\n        test_end = (i + 1) * fold_size\n        test_indices = np.arange(test_start, test_end)\n        train_indices = np.concatenate((np.arange(test_start) if i > 0 else np.array([]), np.arange(test_end, n_samples)))\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) from scratch.\n\n    Args:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample\n                           and each column represents a feature.\n        k (int): The number of principal components to return.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list represents a principal\n                           component (eigenvector). The components are sorted by\n                           decreasing eigenvalue.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    eigenvalue_eigenvector_pairs = sorted(zip(eigenvalues, eigenvectors.T), key=lambda x: x[0], reverse=True)\n    principal_components = [list(np.round(pair[1], 4)) for pair in eigenvalue_eigenvector_pairs[:k]]\n    return principal_components"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Implements the decision tree learning algorithm for classification.\n\n    Args:\n        examples: A list of examples (each example is a dict of attribute-value pairs).\n        attributes: A list of attribute names.\n        target_attr: The name of the target attribute.\n\n    Returns:\n        A nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples:\n        return None\n    classes = [example[target_attr] for example in examples]\n    if len(set(classes)) == 1:\n        return classes[0]\n    if not attributes:\n        return max(set(classes), key=classes.count)\n    best_attr = None\n    best_gain = -1\n    for attr in attributes:\n        gain = calculate_information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    if best_gain == 0:\n        return max(set(classes), key=classes.count)\n    tree = {}\n    tree[best_attr] = {}\n    attr_values = set([example[best_attr] for example in examples])\n    for value in attr_values:\n        sub_examples = [example for example in examples if example[best_attr] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(sub_examples, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\ndef calculate_entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculates the entropy of a set of examples with respect to a target attribute.\n\n    Args:\n        examples: A list of examples (each example is a dict of attribute-value pairs).\n        target_attr: The name of the target attribute.\n\n    Returns:\n        The entropy of the examples with respect to the target attribute.\n    \"\"\"\n    classes = [example[target_attr] for example in examples]\n    class_counts = Counter(classes)\n    entropy = 0.0\n    for count in class_counts.values():\n        probability = float(count) / len(examples)\n        entropy -= probability * math.log2(probability)\n    return entropy\ndef calculate_information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculates the information gain of a set of examples when splitting on an attribute.\n\n    Args:\n        examples: A list of examples (each example is a dict of attribute-value pairs).\n        attr: The name of the attribute to split on.\n        target_attr: The name of the target attribute.\n\n    Returns:\n        The information gain of splitting on the attribute.\n    \"\"\"\n    entropy = calculate_entropy(examples, target_attr)\n    attr_values = set([example[attr] for example in examples])\n    weighted_entropy = 0.0\n    for value in attr_values:\n        sub_examples = [example for example in examples if example[attr] == value]\n        weight = float(len(sub_examples)) / len(examples)\n        weighted_entropy += weight * calculate_entropy(sub_examples, target_attr)\n    return entropy - weighted_entropy"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch.\n\n    Args:\n        data (np.ndarray): The dataset as a 2D NumPy array where each row represents a data sample and each column represents a feature.\n        labels (np.ndarray): The label vector as a 1D NumPy array where each entry corresponds to the label of the sample.\n        kernel (str): The choice of kernel ('linear' or 'rbf').\n        lambda_val (float): The regularization parameter.\n        iterations (int): The number of iterations.\n        sigma (float): The bandwidth parameter for the RBF kernel.\n\n    Returns:\n        tuple: A tuple containing the alpha coefficients and bias.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    for _ in range(iterations):\n        for i in range(n_samples):\n            if kernel == 'linear':\n                prediction = np.dot(alpha * labels, data) + bias\n            elif kernel == 'rbf':\n                prediction = np.sum(alpha * labels * np.exp(-sigma * np.linalg.norm(data - data[i], axis=1) ** 2)) + bias\n            else:\n                raise ValueError(\"Invalid kernel choice. Choose 'linear' or 'rbf'.\")\n            if labels[i] * prediction < 1:\n                alpha[i] += labels[i] / lambda_val\n                bias += labels[i] / lambda_val\n            else:\n                alpha[i] -= lambda_val * labels[i] * prediction\n                bias -= lambda_val * labels[i]\n            alpha[alpha < 0] = 0\n    return (alpha.tolist(), round(bias, 4))"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    try:\n        return round(1 / (1 + math.exp(-z)), 4)\n    except OverflowError:\n        if z > 0:\n            return 1.0\n        else:\n            return 0.0"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n        scores (list[float]): A list of scores.\n\n    Returns:\n        list[float]: The softmax values as a list, each rounded to four decimal places.\n    \"\"\"\n    e_x = [math.exp(score) for score in scores]\n    sum_e_x = sum(e_x)\n    softmax_values = [round(e_x[i] / sum_e_x, 4) for i in range(len(scores))]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification,\n    handling multidimensional input features.\n\n    Args:\n        features (list[list[float]]): A list of feature vectors (each vector representing multiple features for an example).\n        labels (list[int]): Associated true binary labels (0 or 1).\n        weights (list[float]): The neuron's weights (one for each feature).\n        bias (float): The neuron's bias.\n\n    Returns:\n        tuple[list[float], float]: A tuple containing:\n            - A list of predicted probabilities after sigmoid activation, rounded to four decimal places.\n            - The mean squared error between the predicted probabilities and the true labels, rounded to four decimal places.\n    \"\"\"\n    predictions = []\n    for feature_vector in features:\n        weighted_sum = sum((w * f for (w, f) in zip(weights, feature_vector))) + bias\n        probability = 1 / (1 + math.exp(-weighted_sum))\n        predictions.append(probability)\n    squared_errors = [(p - l) ** 2 for (p, l) in zip(predictions, labels)]\n    mse = sum(squared_errors) / len(labels)\n    return (np.round(predictions, 4).tolist(), np.round(mse, 4))"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias.\n\n    Args:\n        features (np.ndarray): A list of feature vectors.\n        labels (np.ndarray): Associated true binary labels.\n        initial_weights (np.ndarray): Initial weights for the neuron.\n        initial_bias (float): Initial bias for the neuron.\n        learning_rate (float): The learning rate for gradient descent.\n        epochs (int): The number of epochs to train the neuron.\n\n    Returns:\n        tuple: A tuple containing the updated weights, bias, and a list of MSE values for each epoch.\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-z))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        d_predictions = predictions - labels\n        d_z = d_predictions * predictions * (1 - predictions)\n        d_weights = np.dot(features.T, d_z) / len(features)\n        d_bias = np.sum(d_z) / len(features)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    return (weights, bias, mse_values)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.grad * self.data\n            other.grad += self.grad * other.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += out.grad * (self.data > 0)\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        self.grad = 1\n        nodes = [self]\n        while nodes:\n            node = nodes.pop()\n            for child in node._prev:\n                child._backward()\n                nodes.append(child)"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Given basis vectors in two different bases B and C for R^3,\n    write a Python function to compute the transformation matrix P from basis B to C.\n    For example:\n    transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 0, 0], [0, 1, 0], [0, 0, 1]]) == [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]\n    transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[0, 1, 0], [0, 0, 1], [1, 0, 0]]) == [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]\n    \"\"\"\n    B = np.array(B)\n    C = np.array(C)\n    P = np.linalg.solve(C, B)\n    P = np.round(P, 4)\n    return P.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Args:\n        A (np.ndarray): A 2x2 NumPy array representing the matrix.\n\n    Returns:\n        tuple: A tuple containing the matrices U, S, and V, such that A = U * S * V.\n               U, S, and V are represented as lists of lists.\n    \"\"\"\n    ATA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    singular_values = np.sqrt(eigenvalues).round(4)\n    S = np.diag(singular_values).round(4)\n    V = eigenvectors.round(4).tolist()\n    U = []\n    for i in range(len(singular_values)):\n        if singular_values[i] > 1e-06:\n            v = V[i]\n            Av = A @ v\n            norm = np.linalg.norm(Av)\n            u = Av / norm\n            U.append(u.tolist())\n        else:\n            U.append([0, 0])\n    U = np.array(U).tolist()\n    S = S.tolist()\n    V = V\n    return (U, S, V)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in two numpy arrays, X and y, while maintaining the\n    corresponding order between them.\n\n    Args:\n        X (numpy.ndarray): The input data array.\n        y (numpy.ndarray): The target array.\n        seed (int, optional): Random seed for reproducibility. Defaults to None.\n\n    Returns:\n        tuple: A tuple containing the shuffled X and y arrays as lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    idx = np.arange(X.shape[0])\n    np.random.shuffle(idx)\n    X_shuffled = X[idx]\n    y_shuffled = y[idx]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of data from a numpy array X and an optional numpy array y.\n\n    Args:\n        X (np.ndarray): The input data array.\n        y (np.ndarray, optional): The target data array. Defaults to None.\n        batch_size (int): The size of each batch. Defaults to 64.\n\n    Yields:\n        tuple: A tuple containing a batch of X and y (if y is provided) or just a batch of X.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        X_batch = X[i:i + batch_size]\n        if y is not None:\n            y_batch = y[i:i + batch_size]\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"Divides a dataset into two subsets based on a feature threshold.\n\n    Args:\n        X (np.ndarray): The dataset as a NumPy array.\n        feature_i (int): The index of the feature to use for division.\n        threshold (float): The threshold value.\n\n    Returns:\n        tuple: A tuple containing two lists:\n            - The first list contains samples where X[:, feature_i] >= threshold.\n            - The second list contains samples where X[:, feature_i] < threshold.\n    \"\"\"\n    above_threshold = X[X[:, feature_i] >= threshold]\n    below_threshold = X[X[:, feature_i] < threshold]\n    return (above_threshold.tolist(), below_threshold.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates polynomial features for a given dataset.\n\n    Args:\n        X (numpy.ndarray): A 2D numpy array representing the dataset.\n        degree (int): The maximum degree of the polynomial features.\n\n    Returns:\n        list: A list representing the new 2D numpy array with polynomial features.\n    \"\"\"\n    features = [np.ones(X.shape[0])]\n    for d in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(X.shape[1]), d):\n            features.append(np.prod(X ** d, axis=1))\n    return [f.tolist() for f in features]"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of a given dataset.\n\n    Args:\n        X (np.ndarray): A 2D numpy array representing the dataset features.\n        y (np.ndarray): A 1D numpy array representing the dataset labels.\n        n_subsets (int): The number of random subsets to generate.\n        replacements (bool): Whether to sample with replacements.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list of n_subsets random subsets, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=int(n_samples * 0.5), replace=True)\n        else:\n            indices = np.random.choice(n_samples, size=int(n_samples * 0.5), replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Performs one-hot encoding of nominal values.\n\n    Args:\n        x (np.ndarray): A 1D numpy array of integer values.\n        n_col (int, optional): The number of columns for the one-hot encoded array.\n                                 If None, it is automatically determined from the input array.\n\n    Returns:\n        list: A python list representing the one-hot encoded array.\n    \"\"\"\n    x = np.array(x)\n    if n_col is None:\n        n_col = np.max(x) + 1\n    categorical = np.zeros((len(x), n_col))\n    categorical[np.arange(len(x)), x] = 1\n    return categorical.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n\n    Args:\n        x (numpy.ndarray): A 1D numpy array.\n\n    Returns:\n        numpy.ndarray: A 2D numpy array representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Args:\n        y_true (numpy.ndarray): A 1D numpy array containing the true labels.\n        y_pred (numpy.ndarray): A 1D numpy array containing the predicted labels.\n\n    Returns:\n        float: The accuracy score, rounded to the nearest 4th decimal.\n    \"\"\"\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Args:\n        X (numpy.ndarray): A 2D numpy array representing the dataset.\n        Y (numpy.ndarray, optional): A 2D numpy array representing the second dataset.\n                                      If None, the correlation matrix of X with itself is calculated.\n                                      Defaults to None.\n\n    Returns:\n        list: The correlation matrix as a 2D numpy array, rounded to the nearest 4th decimal,\n              and converted to a python list.\n    \"\"\"\n    if Y is None:\n        Y = X\n    correlation_matrix = np.corrcoef(X, Y)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix.tolist()\n    return correlation_matrix_list"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the fit method for an AdaBoost classifier.\n\n    Args:\n        X (np.ndarray): The dataset of shape (n_samples, n_features).\n        y (np.ndarray): The labels of shape (n_samples,).\n        n_clf (int): The number of classifiers.\n\n    Returns:\n        list: A list of classifiers with their parameters.\n    \"\"\"\n    n_samples = X.shape[0]\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_threshold = 0\n        best_feature = 0\n        best_clf = None\n        min_error = float('inf')\n        for j in range(X.shape[1]):\n            feature_values = sorted(set(X[:, j]))\n            for i in range(len(feature_values) - 1):\n                threshold = (feature_values[i] + feature_values[i + 1]) / 2\n                y_pred = np.array([1 if X[:, j][k] < threshold else -1 for k in range(n_samples)])\n                error = np.sum(weights * (y_pred != y))\n                if error < min_error:\n                    min_error = error\n                    best_threshold = threshold\n                    best_feature = j\n                    best_clf = (j, threshold, y)\n        alpha = 0.5 * math.log((1 - min_error) / min_error)\n        round_alpha = round(alpha, 4)\n        weights *= np.exp(-alpha * y * np.array([1 if X[:, best_feature][k] < best_threshold else -1 for k in range(n_samples)]))\n        weights /= np.sum(weights)\n        round_weights = np.round(weights, 4)\n        classifiers.append((best_feature, best_threshold, round_alpha))\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1D numpy array of scores.\n\n    Args:\n        scores (list): A 1D numpy array of scores.\n\n    Returns:\n        list: A 1D numpy array of log-softmax values.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    shifted_scores = scores - max_score\n    exp_scores = np.exp(shifted_scores)\n    softmax_scores = exp_scores / np.sum(exp_scores)\n    log_softmax_scores = np.log(softmax_scores)\n    return log_softmax_scores.round(4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        \"\"\"\n        Initialize the weights and biases of the layer.\n        \"\"\"\n        if self.input_shape is not None:\n            limit = 1 / math.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n            self.w0 = np.zeros(self.n_units)\n        else:\n            raise ValueError('Input shape must be specified for Dense layer.')\n        self.W_opt = {}\n        self.w0_opt = {}\n\n    def parameters(self):\n        \"\"\"\n        Return the total number of trainable parameters in the layer.\n        \"\"\"\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        \"\"\"\n        Compute the output of the layer.\n        \"\"\"\n        self.layer_input = X\n        self.output = np.dot(X, self.W) + self.w0\n        return self.output\n\n    def backward_pass(self, accum_grad):\n        \"\"\"\n        Calculate the gradient with respect to the input and update the weights and biases.\n        \"\"\"\n        X = self.layer_input\n        dW = np.dot(X.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0)\n        if self.trainable:\n            self.W_opt['dW'] = dW\n            self.w0_opt['dw0'] = dw0\n        d_prev = np.dot(accum_grad, self.W.T)\n        return d_prev\n\n    def output_shape(self):\n        \"\"\"\n        Return the shape of the output produced by the forward pass.\n        \"\"\"\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Performs 2D convolution on an input matrix with a given kernel, padding, and stride.\n\n    Args:\n        input_matrix (np.ndarray): The input matrix to convolve.\n        kernel (np.ndarray): The convolutional kernel.\n        padding (int): The amount of padding to apply.\n        stride (int): The stride of the convolution.\n\n    Returns:\n        list: A list containing the output matrix after reshaping.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            roi = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output_matrix[i, j] = np.round(np.sum(roi * kernel), 4)\n    return output_matrix.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n\n    The ReLU function returns the input if it's greater than 0, otherwise, it returns 0.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The output after applying the ReLU function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculates the Ridge Regression loss.\n\n    Args:\n        X (np.ndarray): The feature matrix.\n        w (np.ndarray): The coefficients.\n        y_true (np.ndarray): The true labels.\n        alpha (float): The regularization parameter.\n\n    Returns:\n        float: The Ridge loss.\n    \"\"\"\n    n_samples = X.shape[0]\n    mse = np.mean((X @ w - y_true) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss = mse + regularization\n    return round(ridge_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n        z (float): The input value.\n        alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        float|int: The value after applying the Leaky ReLU function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n  Computes the linear kernel between two vectors.\n\n  Args:\n    x1: A numpy array representing the first vector.\n    x2: A numpy array representing the second vector.\n\n  Returns:\n    The linear kernel (dot product) of x1 and x2.\n  \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n  Calculates the precision metric.\n\n  Args:\n    y_true (numpy.ndarray): A numpy array containing the true binary labels.\n    y_pred (numpy.ndarray): A numpy array containing the predicted binary labels.\n\n  Returns:\n    float: The precision metric.\n  \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    if tp + fp == 0:\n        return 0.0\n    return tp / (tp + fp)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent (SGD, Batch, or Mini-Batch) with MSE loss.\n\n    Args:\n        X (np.ndarray): Input features.\n        y (np.ndarray): Target values.\n        weights (np.ndarray): Initial weights.\n        learning_rate (float): Learning rate.\n        n_iterations (int): Number of iterations.\n        batch_size (int): Batch size (for Mini-Batch and SGD).\n        method (str): Gradient descent method ('sgd', 'batch', 'mini-batch').\n\n    Returns:\n        list: The optimized weights after n_iterations.\n    \"\"\"\n    m = len(y)\n    for i in range(n_iterations):\n        if method == 'sgd':\n            for j in range(m):\n                y_pred = X[j] @ weights\n                error = y_pred - y[j]\n                gradient = X[j] * error\n                weights = weights - learning_rate * gradient\n        elif method == 'batch':\n            y_pred = X @ weights\n            error = y_pred - y\n            gradient = X.T @ error\n            weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            for k in range(0, m, batch_size):\n                X_batch = X[k:k + batch_size]\n                y_batch = y[k:k + batch_size]\n                y_pred = X_batch @ weights\n                error = y_pred - y_batch\n                gradient = X_batch.T @ error\n                weights = weights - learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose 'sgd', 'batch', or 'mini-batch'.\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Args:\n        matrix (list of lists): The input matrix represented as a list of lists.\n\n    Returns:\n        list of lists: The RREF of the matrix.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    return A.tolist()\n        A[[i, r]] = A[[r, i]]\n        lv = A[r, lead]\n        A[r] = A[r] / lv\n        for i in range(rows):\n            if i != r:\n                lv = A[i, lead]\n                A[i] = A[i] - lv * A[r]\n        lead += 1\n    return A.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm.\n\n    Args:\n        f: The objective function to be optimized.\n        grad: A function that computes the gradient of f.\n        x0: Initial parameter values (NumPy array).\n        learning_rate: The step size (default: 0.001).\n        beta1: Exponential decay rate for the first moment estimates (default: 0.9).\n        beta2: Exponential decay rate for the second moment estimates (default: 0.999).\n        epsilon: A small constant for numerical stability (default: 1e-8).\n        num_iterations: Number of iterations to run the optimizer (default: 1000).\n\n    Returns:\n        The optimized parameters (NumPy array).\n    \"\"\"\n    x = np.array(x0)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent.\n\n    Args:\n        X (np.array): The feature matrix.\n        y (np.array): The target vector.\n        alpha (float): The regularization parameter.\n        learning_rate (float): The learning rate.\n        max_iter (int): The maximum number of iterations.\n        tol (float): The tolerance for convergence.\n\n    Returns:\n        tuple: A tuple containing the weights (w) and bias (b).\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    for _ in range(max_iter):\n        y_pred = X @ w + b\n        dw = 1 / n_samples * X.T @ (y_pred - y) + alpha * np.sign(w)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        w_prev = w.copy()\n        b_prev = b\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        if np.linalg.norm(w - w_prev) < tol and np.linalg.norm(b - b_prev) < tol:\n            break\n    return (w.tolist(), b)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n\n    Args:\n        source (str): The first string.\n        target (str): The second string.\n\n    Returns:\n        int: The OSA distance between the two strings.\n    \"\"\"\n    n = len(source)\n    m = len(target)\n    dp = np.zeros((n + 1, m + 1), dtype=int)\n    for i in range(n + 1):\n        dp[i, 0] = i\n    for j in range(m + 1):\n        dp[0, j] = j\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i, j] = dp[i - 1, j - 1]\n            else:\n                dp[i, j] = min(dp[i - 1, j] + 1, dp[i, j - 1] + 1, dp[i - 1, j - 1] + 1)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n    return dp[n, m]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculates the recall metric in a binary classification setting.\n\n    Args:\n        y_true (list): A list of true binary labels (0 or 1).\n        y_pred (list): A list of predicted binary labels (0 or 1).\n\n    Returns:\n        float: The recall value rounded to three decimal places.\n               Returns 0.0 if the denominator (TP + FN) is zero.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    else:\n        recall_value = tp / denominator\n        return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism.\n\n    Args:\n        X (numpy.ndarray): Input sequence of shape (seq_len, d_model).\n        W_q (numpy.ndarray): Weight matrix for query transformation of shape (d_model, d_k).\n        W_k (numpy.ndarray): Weight matrix for key transformation of shape (d_model, d_k).\n        W_v (numpy.ndarray): Weight matrix for value transformation of shape (d_model, d_v).\n\n    Returns:\n        list: Self-attention output as a list.\n    \"\"\"\n    seq_len = X.shape[0]\n    d_model = X.shape[1]\n    d_k = W_q.shape[1]\n    d_v = W_v.shape[1]\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    attention_scores = np.matmul(Q, K.T)\n    attention_scores = attention_scores / np.sqrt(d_k)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple Recurrent Neural Network (RNN) cell.\n\n    Args:\n        input_sequence (list[list[float]]): A sequence of input vectors.\n        initial_hidden_state (list[float]): The initial hidden state.\n        Wx (list[list[float]]): The weight matrix for input-to-hidden connections.\n        Wh (list[list[float]]): The weight matrix for hidden-to-hidden connections.\n        b (list[float]): The bias vector.\n\n    Returns:\n        list[float]: The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector)\n        combined_input = np.dot(input_vector, Wx) + np.dot(hidden_state, Wh) + b\n        hidden_state = np.tanh(combined_input)\n    return np.round(hidden_state.tolist(), 4)"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n        points (list): A list of [x, y] coordinates.\n        tx (float): The translation distance in the x direction.\n        ty (float): The translation distance in the y direction.\n\n    Returns:\n        list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points_array = np.dot(translation_matrix, points_array.T).T\n    return translated_points_array.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculates the KL divergence between two normal distributions.\n\n    Args:\n        mu_p (float): Mean of the first normal distribution.\n        sigma_p (float): Standard deviation of the first normal distribution.\n        mu_q (float): Mean of the second normal distribution.\n        sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n        float: The KL divergence between the two normal distributions.\n    \"\"\"\n    sigma_p_sq = sigma_p ** 2\n    sigma_q_sq = sigma_q ** 2\n    kl = 0.5 * (np.log(sigma_q_sq / sigma_p_sq) + (sigma_p_sq + (mu_p - mu_q) ** 2) / sigma_q_sq - 1)\n    return kl"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method to solve a system of linear equations (Ax = b).\n\n    Args:\n        A (numpy.ndarray): A square matrix of coefficients.\n        b (numpy.ndarray): The right-hand side vector.\n        n (int): The number of iterations.\n        x_ini (numpy.ndarray, optional): An initial guess for the solution vector (x).\n                                          If not provided, a vector of zeros is assumed.\n\n    Returns:\n        list: The approximated solution vector (x) after performing the specified number of iterations,\n              rounded to the nearest 4th decimal and converted to a Python list.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = int(n)\n    if x_ini is None:\n        x = np.zeros(A.shape[0])\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            sigma = 0\n            for j in range(A.shape[0]):\n                if j != i:\n                    sigma += A[i, j] * x[j]\n            x[i] = (b[i] - sigma) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Performs Gaussian Elimination with partial pivoting to solve the system Ax = b.\n\n    Args:\n        A (numpy.ndarray): The coefficient matrix (n x n).\n        b (numpy.ndarray): The right-hand side vector (n).\n\n    Returns:\n        list: The solution vector x.\n    \"\"\"\n    n = len(b)\n    A = A.astype(float)\n    b = b.astype(float)\n    for k in range(n):\n        max_row = k\n        for i in range(k + 1, n):\n            if abs(A[i, k]) > abs(A[max_row, k]):\n                max_row = i\n        A[[k, max_row]] = A[[max_row, k]]\n        b[[k, max_row]] = b[[max_row, k]]\n        for i in range(k + 1, n):\n            factor = A[i, k] / A[k, k]\n            A[i, k:] = A[i, k:] - factor * A[k, k:]\n            b[i] = b[i] - factor * b[k]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = b[i]\n        for j in range(i + 1, n):\n            x[i] = x[i] - A[i, j] * x[j]\n        x[i] = x[i] / A[i, i]\n    return list(np.round(x, 4))"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = x.shape[1]\n        hidden_states = []\n        cell_states = []\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(T):\n            input_combined = np.concatenate((x[:, t], hidden_state), axis=1)\n            f = np.sigmoid(np.dot(self.Wf, input_combined) + self.bf)\n            i = np.sigmoid(np.dot(self.Wi, input_combined) + self.bi)\n            o = np.sigmoid(np.dot(self.Wo, input_combined) + self.bo)\n            c_tilde = np.tanh(np.dot(self.Wc, input_combined) + self.bc)\n            cell_state = f * cell_state + i * c_tilde\n            hidden_state = o * np.tanh(cell_state)\n            hidden_states.append(hidden_state.tolist())\n            cell_states.append(cell_state.tolist())\n        hidden_states = np.array(hidden_states).round(4)\n        final_hidden_state = hidden_state.round(4)\n        final_cell_state = cell_state.round(4)\n        return (hidden_states.tolist(), final_hidden_state.tolist(), final_cell_state.tolist())"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF scores for a query against a given corpus of documents.\n\n    Args:\n        corpus: A list of documents, where each document is a list of words.\n        query: A list of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n        A list of lists containing the TF-IDF scores for the query words in each document,\n        rounded to five decimal places.\n    \"\"\"\n    if not corpus:\n        return []\n    num_documents = len(corpus)\n    tf_idf_scores = []\n    for document in corpus:\n        document_scores = []\n        for query_term in query:\n            tf = document.count(query_term) / len(document) if document else 0\n            df = sum((1 for doc in corpus if query_term in doc))\n            idf = np.log((num_documents + 1) / (df + 1))\n            tf_idf = tf * idf\n            document_scores.append(round(tf_idf, 5))\n        tf_idf_scores.append(document_scores)\n    return np.array(tf_idf_scores).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    precision = true_positives / (true_positives + false_positives + 1e-10)\n    recall = true_positives / (true_positives + false_negatives + 1e-10)\n    if precision + recall == 0:\n        return 0.0\n    f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall + 1e-10)\n    return round(f_score_value, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hiddens = []\n        outputs = []\n        h_prev = np.zeros((self.hidden_size, 1))\n        for input_val in x:\n            h = np.tanh(np.dot(self.W_xh, input_val) + np.dot(self.W_hh, h_prev) + self.b_h)\n            hiddens.append(h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            h_prev = h\n        return (outputs, hiddens)\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n        \"\"\"\n        hiddens = []\n        outputs = []\n        last_inputs = []\n        h_prev = np.zeros((hidden_size, 1))\n        for input_val in input_sequence:\n            last_inputs.append(input_val)\n            h = np.tanh(np.dot(W_xh, input_val) + np.dot(W_hh, h_prev) + b_h)\n            hiddens.append(h)\n            y = np.dot(W_hy, h) + b_y\n            outputs.append(y)\n            h_prev = h\n        return (outputs, last_inputs, hiddens)\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        total_loss = 0\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        dh_next = np.zeros_like(last_hiddens[-1])\n        for t in reversed(range(len(input_sequence))):\n            output = outputs[t]\n            target = expected_output[t]\n            loss = 0.5 * np.sum((output - target) ** 2)\n            total_loss += loss\n            d_output = output - target\n            dW_hy += np.dot(d_output, last_hiddens[t].T)\n            db_y += d_output\n            d_hidden = np.dot(W_hy.T, d_output)\n            dh_current = d_hidden * (1 - last_hiddens[t] ** 2)\n            dW_xh += np.dot(dh_current, input_sequence[t].T)\n            dW_hh += np.dot(dh_current, last_hiddens[t - 1].T) if t > 0 else np.dot(dh_current, np.zeros_like(last_hiddens[t - 1]).T)\n            db_h += dh_current\n            dh_next = dh_current if t > 0 else np.zeros_like(dh_current)\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n        return total_loss\n\n    def initialize_weights(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with weights set to random values multiplied by 0.01 and biases set to zero.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    x = np.zeros(len(b)) if x0 is None else x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    for i in range(n):\n        Ap = A @ p\n        alpha = r @ r / (p @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        if np.linalg.norm(r) < tol:\n            break\n        p = r + np.dot(r, r) / np.dot(p, p) * p\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    n_samples = len(y)\n    class_counts = np.unique(y, return_counts=True)\n    gini = 1.0\n    for count in class_counts[1]:\n        probability = count / n_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                col_indices.append(j)\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((vi * li for (vi, li) in zip(v, L)))\n    magnitude_squared = sum((li * li for li in L))\n    if magnitude_squared == 0:\n        return [0.0] * len(v)\n    scalar = dot_product / magnitude_squared\n    projection = [scalar * li for li in L]\n    return [round(x, 3) for x in projection]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A using row echelon form.\n\n    Args:\n        A (numpy.ndarray): The input matrix.\n\n    Returns:\n        list: A list of numpy arrays representing the basis vectors of the column space.\n              The basis vectors are rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    rref_A = np.array(A.copy())\n    (rows, cols) = rref_A.shape\n    pivot_row = 0\n    for col in range(cols):\n        pivot = -1\n        for row in range(pivot_row, rows):\n            if abs(rref_A[row, col]) > 1e-10:\n                pivot = row\n                break\n        if pivot != -1:\n            rref_A[[pivot_row, pivot]] = rref_A[[pivot, pivot_row]]\n            rref_A[pivot_row] = rref_A[pivot_row] / rref_A[pivot_row, col]\n            for row in range(rows):\n                if row != pivot_row:\n                    factor = rref_A[row, col]\n                    rref_A[row] = rref_A[row] - factor * rref_A[pivot_row]\n            pivot_row += 1\n    basis = []\n    pivot_cols = []\n    for col in range(cols):\n        is_pivot = False\n        for row in range(rows):\n            if abs(rref_A[row, col] - 1) < 1e-10:\n                is_pivot = True\n                break\n        if is_pivot:\n            pivot_cols.append(col)\n    for col in pivot_cols:\n        basis.append(A[:, col].round(8).tolist())\n    return basis"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate R-squared for Regression Analysis\n\n    R-squared, also known as the coefficient of determination, is a measure that indicates how well the independent variables explain the variability of the dependent variable in a regression model.\n\n    Args:\n        y_true (array-like): Array of true values.\n        y_pred (array-like): Array of predicted values.\n\n    Returns:\n        float: R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r_squared = 1 - rss / tss\n    return round(r_squared, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n\n    Args:\n        img: A 2D matrix representing the grayscale image, where each element\n             is a pixel value between 0 and 255.\n\n    Returns:\n        The average brightness of the image rounded to two decimal places,\n        or -1 if the image matrix is empty, has inconsistent row lengths,\n        or contains invalid pixel values.\n    \"\"\"\n    if not img:\n        return -1\n    row_len = len(img[0])\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        if len(row) != row_len:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_pixels += 1\n            total_brightness += pixel\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n\n    Args:\n        y_true (array-like): Array of actual values.\n        y_pred (array-like): Array of predicted values.\n\n    Returns:\n        float: RMSE value rounded to three decimal places.\n               Returns None if input is invalid or arrays are empty.\n    \"\"\"\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        if y_true.shape != y_pred.shape:\n            print('Error: Input arrays must have the same shape.')\n            return None\n        if y_true.size == 0:\n            print('Error: Input arrays cannot be empty.')\n            return None\n        if not np.issubdtype(y_true.dtype, np.number) or not np.issubdtype(y_pred.dtype, np.number):\n            print('Error: Input arrays must contain numerical values.')\n            return None\n        mse = np.mean((y_true - y_pred) ** 2)\n        rmse_value = np.sqrt(mse)\n        return round(rmse_value, 3)\n    except Exception as e:\n        print(f'An error occurred: {e}')\n        return None"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays.\n\n    Args:\n        y_true (np.ndarray): True labels.\n        y_pred (np.ndarray): Predicted labels.\n\n    Returns:\n        float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    if np.sum(union) == 0:\n        return 1.0\n    jaccard = np.sum(intersection) / np.sum(union)\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score (S\u00f8rensen-Dice coefficient) for binary classification.\n\n    Args:\n        y_true (np.ndarray): A binary array of true labels.\n        y_pred (np.ndarray): A binary array of predicted labels.\n\n    Returns:\n        float: The Dice Score, rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    if intersection == 0:\n        return 0.0\n    else:\n        dice = 2.0 * intersection / (np.sum(y_true) + np.sum(y_pred))\n        return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The hypervectors for the values are created using the same feature seed provided in the `random_seeds` dictionary\n    to ensure reproducibility.\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n\n    Input:\n    - `row`: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - `dim`: The dimensionality of the hypervectors.\n    - `random_seeds`: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    Output:\n    - A composite hypervector representing the entire row.\n    \"\"\"\n\n    def create_hv(name, value, dim, seed):\n        \"\"\"Create a hypervector for a given name and value.\"\"\"\n        name_hv = np.random.randint(0, 2, dim, dtype=int)\n        value_hv = np.random.default_rng(seed).integers(0, 2, dim, dtype=int)\n        return name_hv * value_hv\n    row_hv = np.ones(dim, dtype=int)\n    for (feature_name, feature_value) in row.items():\n        if feature_name in random_seeds:\n            seed = random_seeds[feature_name]\n            feature_hv = create_hv(feature_name, feature_value, dim, seed)\n            row_hv = row_hv * feature_hv\n        else:\n            feature_hv = np.random.randint(0, 2, dim, dtype=int)\n            row_hv = row_hv * feature_hv\n    return row_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n\n    Args:\n        data: A list of lists, where each inner list represents a pair\n              [y_true, y_pred] for one observation. y_true is the actual\n              label, and y_pred is the predicted label.\n\n    Returns:\n        A 2x2 confusion matrix represented as a list of lists.\n        The matrix is structured as follows:\n        [[True_Positives, False_Positives],\n         [False_Negatives, True_Negatives]]\n    \"\"\"\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    true_negatives = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positives += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positives += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negatives += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negatives += 1\n    return [[true_positives, false_positives], [false_negatives, true_negatives]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n        v1 (np.ndarray): The first vector.\n        v2 (np.ndarray): The second vector.\n\n    Returns:\n        float: The cosine similarity between v1 and v2, rounded to three decimal places.\n    \n    Raises:\n        ValueError: If the input vectors have different shapes or are empty.\n        ZeroDivisionError: If either vector has zero magnitude.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ZeroDivisionError('Input vectors cannot have zero magnitude.')\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates performance metrics for a binary classification problem.\n\n    Args:\n        actual: The actual class labels (1 for positive, 0 for negative).\n        predicted: The predicted class labels from the model.\n\n    Returns:\n        A tuple containing:\n        - confusion_matrix: A 2x2 matrix.\n        - accuracy: A float representing the accuracy of the model.\n        - f1_score: A float representing the F1 score of the model.\n        - specificity: A float representing the specificity of the model.\n        - negative_predictive_value: A float representing the negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must have the same length.')\n    for label in actual + predicted:\n        if label not in [0, 1]:\n            raise ValueError('All elements in the actual and predicted lists must be either 0 or 1.')\n    conf_matrix = Counter(zip(actual, predicted))\n    tp = conf_matrix[1, 1]\n    tn = conf_matrix[0, 0]\n    fp = conf_matrix[0, 1]\n    fn = conf_matrix[1, 0]\n    accuracy = round((tp + tn) / len(actual), 3)\n    if tp + fp == 0:\n        f1_score = 0.0\n    else:\n        f1_score = round(2 * (tp / (tp + fp) * tp / (tp + fn)) / (tp / (tp + fp) + tp / (tp + fn)), 3)\n    if tn + fp == 0:\n        specificity = 0.0\n    else:\n        specificity = round(tn / (tn + fp), 3)\n    if tn + fn == 0:\n        negative_predictive_value = 0.0\n    else:\n        negative_predictive_value = round(tn / (tn + fn), 3)\n    return ([[tp, fp], [fn, tn]], accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates descriptive statistics for a given dataset.\n\n    Args:\n        data (list or numpy.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n        dict: A dictionary containing mean, median, mode, variance, standard deviation,\n              percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    try:\n        mode = stats.mode(data)[0][0]\n    except:\n        mode = None\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    q1 = percentiles[0]\n    q3 = percentiles[2]\n    interquartile_range = q3 - q1\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(q1, 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(q3, 4), 'interquartile_range': round(interquartile_range, 4)}\n    return results"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if not 0 <= p <= 1:\n        raise ValueError('Probability p must be between 0 and 1')\n    if not 0 <= k <= n:\n        raise ValueError('Number of successes k must be between 0 and n')\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if k < 0 or lam < 0:\n        return 0\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    contrast = max_val - min_val\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must have the same dimensions')\n    if vec1.ndim != 1:\n        raise ValueError('Vectors must be 1-dimensional')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = []\n        for i in range(degree + 1):\n            features.append(round(x ** i, 8))\n        transformed_data.append(features)\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a sequence length and model dimensionality.\n\n    Args:\n        position (int): The sequence length.\n        d_model (int): The model dimensionality.\n\n    Returns:\n        list: A list containing the positional encodings. Returns -1 if position is 0 or d_model <= 0.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pos = np.arange(position)[np.newaxis, :]\n    i = np.arange(d_model)[np.newaxis, :]\n    angle_rads = (i * (np.pi / 2 ** (d_model / 2 - 1)))[np.newaxis, :]\n    enc = np.zeros((position, d_model), dtype=np.float16)\n    enc[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    enc[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    return enc.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (updated_parameter.tolist(), m_hat.tolist(), v_hat.tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0, 'this': 3, 'is': 4, 'a': 5, 'test': 6}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12, 'd_model': 10, 'd_ff': 64}\n    params = {'wte': np.random.rand(len(DummyBPE().encoder_dict) + 1, hparams['d_model']), 'wpe': np.random.rand(hparams['n_ctx'], hparams['d_model']), 'blocks': [], 'ln_f': {'g': np.ones(hparams['d_model']), 'b': np.zeros(hparams['d_model'])}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Generates text using a simplified GPT-2-like model.\n\n    Args:\n        prompt (str): The initial text to guide the generation process.\n        n_tokens_to_generate (int): Specify how many tokens to output.\n\n    Returns:\n        str: The generated text.\n    \"\"\"\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    encoded_prompt = encoder.encode(prompt)\n    input_ids = encoded_prompt + [0] * (hparams['n_ctx'] - len(encoded_prompt))\n    generated_tokens = encoded_prompt\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][input_ids[-1:], :]\n        positional_embeddings = params['wpe'][len(input_ids) - 1:, :]\n        x = token_embeddings + positional_embeddings\n        attention_output = x\n        ffn_weights = np.random.rand(hparams['d_model'], hparams['d_ff'])\n        ffn_bias = np.zeros(hparams['d_ff'])\n        ffn_output = np.dot(attention_output, ffn_weights) + ffn_bias\n        ffn_output = np.relu(ffn_output)\n        ln_g = params['ln_f']['g']\n        ln_b = params['ln_f']['b']\n        normalized_output = ffn_output * ln_g + ln_b\n        logits = np.random.rand(len(encoder.encoder_dict) + 1)\n        predicted_token_id = np.argmax(logits)\n        generated_tokens.append(predicted_token_id)\n        input_ids.append(predicted_token_id)\n        input_ids = input_ids[-hparams['n_ctx']:]\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    crystal_values = np.array(crystal_values)\n    weights = np.random.rand(n, dimension)\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            attention_scores[i, j] = np.dot(weights[i], weights[j])\n    attention_weights = softmax(attention_scores)\n    weighted_patterns = np.zeros(n)\n    for i in range(n):\n        for j in range(n):\n            weighted_patterns[i] += attention_weights[i, j] * crystal_values[j]\n    weighted_patterns = np.round(weighted_patterns, 4)\n    return weighted_patterns.tolist()"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates BM25 ranking scores for a query against a corpus of documents.\n\n    Args:\n        corpus (list): A list of strings, where each string represents a document.\n        query (str): The query string.\n        k1 (float): BM25 parameter for term frequency saturation.\n        b (float): BM25 parameter for document length normalization.\n\n    Returns:\n        list: A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n    query_terms = query.lower().split()\n    query_term_counts = Counter(query_terms)\n    doc_scores = []\n    for document in corpus:\n        document = document.lower()\n        doc_terms = document.split()\n        doc_term_counts = Counter(doc_terms)\n        score = 0.0\n        for term in query_terms:\n            if term in doc_term_counts:\n                tf = doc_term_counts[term]\n                idf = np.log((len(corpus) - len([d for d in corpus if term in d.lower().split()]) + 0.5) / (len([d for d in corpus if term in d.lower().split()]) + 1))\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (len(doc_terms) / max((len(doc_terms) for doc in corpus))))\n                score += numerator / denominator * idf\n        doc_scores.append(round(score, 3))\n    return doc_scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    from sklearn.metrics import f1_score\n    f1 = f1_score(y_true, y_pred)\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data):\n        fluctuation = 10 * math.sin(2 * PI * (i + 1) / 10)\n        detrended_data.append(consumption - fluctuation)\n    X = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    y = np.array(detrended_data)\n    model = LinearRegression()\n    model.fit(X, y)\n    day_15_base = model.predict(np.array([[15]])[0])\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption = day_15_base + day_15_fluctuation\n    safety_margin = math.ceil(predicted_consumption * 0.05)\n    final_consumption = math.ceil(predicted_consumption) + safety_margin\n    return int(final_consumption)"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    absolute_errors = np.abs(y_true - y_pred)\n    mae = np.mean(absolute_errors)\n    return round(mae, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n\n    Args:\n        X (np.ndarray): Input tensor of shape (batch_size, seq_len, d_model).\n        W_q (np.ndarray): Weight matrix for query transformation of shape (d_model, d_k).\n        W_k (np.ndarray): Weight matrix for key transformation of shape (d_model, d_k).\n        W_v (np.ndarray): Weight matrix for value transformation of shape (d_model, d_v).\n        n_heads (int): Number of attention heads.\n\n    Returns:\n        list: Output tensor after multi-head attention, reshaped to a list.\n    \"\"\"\n    batch_size = X.shape[0]\n    seq_len = X.shape[1]\n    d_model = X.shape[2]\n    d_k = W_q.shape[1]\n    d_v = W_v.shape[1]\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    Q = np.reshape(Q, (batch_size, seq_len, n_heads, d_k))\n    K = np.reshape(K, (batch_size, seq_len, n_heads, d_k))\n    V = np.reshape(V, (batch_size, seq_len, n_heads, d_v))\n    attention_scores = np.matmul(Q, np.transpose(K, (0, 1, 3, 2))) / np.sqrt(d_k)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=3, keepdims=True)\n    attended_values = np.matmul(attention_weights, V)\n    attended_values = np.reshape(attended_values, (batch_size, seq_len, n_heads * d_v))\n    W_o = np.random.rand(n_heads * d_v, d_model)\n    output = np.matmul(attended_values, W_o)\n    return output.round(4).tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    if n != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n_agree = sum((x[i] == y[i] for i in range(n)))\n    n_disagree = n - n_agree\n    expected_agree = n * (sum(x) / n) * (sum(y) / n)\n    expected_disagree = n * (1 - sum(x) / n) * (1 - sum(y) / n)\n    phi = (n_agree - expected_agree) / (n_agree + n_disagree - expected_agree - expected_disagree)\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < 0:\n        return 0.0\n    elif x > 1:\n        return 1.0\n    else:\n        return x"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x >= 0:\n        return round(math.log(1 + math.exp(x)), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "import math\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    if x > 0:\n        return x / (1 + x)\n    else:\n        return -(x / (1 - x))"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    grpo_term = np.sum(np.minimum(rhos * A, clipped_rhos * A))\n    kl_divergence = np.sum(pi_theta_ref * np.log(pi_theta_ref / pi_theta_old))\n    objective = -grpo_term + beta * kl_divergence\n    return round(objective, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * sigmoid(x)\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the sigmoid function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The sigmoid activation value\n    \"\"\"\n    return 1 / (1 + math.exp(-x))"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(scale * (alpha * (math.exp(x) - 1)), 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (C, M) = (X.shape[0], X.shape[1])\n    B = np.zeros((C, M))\n    losses = []\n    for _ in range(iterations):\n        z = X @ B\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        p = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n        loss = -np.sum(np.log(p[np.arange(len(y)), y])) / len(y)\n        losses.append(round(loss, 4))\n        dB = X.T @ (p - np.eye(C)) / len(y)\n        B = B - learning_rate * dB\n    return (round(B.flatten().tolist(), 4), round(np.array(losses).tolist(), 4))"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    loss_history = []\n    for _ in range(iterations):\n        linear_model = X @ weights + bias\n        predictions = 1 / (1 + np.exp(-linear_model))\n        loss = np.mean(-y * np.log(predictions) - (1 - y) * np.log(1 - predictions))\n        loss_history.append(round(loss, 4))\n        dw = 1 / n_samples * X.T @ (predictions - y)\n        db = 1 / n_samples * np.sum(predictions - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (weights.tolist(), loss_history)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.matmul(Q, K.T)\n    masked_attention_scores = attention_scores + mask * -1000000000.0\n    attention_probs = np.exp(masked_attention_scores) / np.sum(np.exp(masked_attention_scores), axis=-1, keepdims=True)\n    output = np.matmul(attention_probs, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    unique_colors = len(set(apples))\n    if unique_colors == 1:\n        return 0.0\n    counts = {}\n    for color in apples:\n        counts[color] = counts.get(color, 0) + 1\n    total_apples = len(apples)\n    disorder_value = 0.0\n    for color in counts:\n        probability = counts[color] / total_apples\n        disorder_value -= probability * probability ** 0\n    disorder_value = disorder_value / (len(counts) * (1 / len(counts)))\n    return round(disorder_value, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Applies layer normalization to an input tensor.\n\n    Args:\n        X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n        gamma (np.ndarray): Scaling parameter of shape (feature_dim,).\n        beta (np.ndarray): Shifting parameter of shape (feature_dim,).\n        epsilon (float): Small value added to the variance for numerical stability.\n\n    Returns:\n        list: Normalized X rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    scaled_X = gamma * normalized_X + beta\n    return scaled_X.round(5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n\n    Args:\n        reference (list): A list of words representing the reference translation.\n        candidate (list): A list of words representing the candidate translation.\n        alpha (float): Weight for unigram precision.\n        beta (float): Penalty for fragmentation.\n        gamma (float): Weight for recall.\n\n    Returns:\n        float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_len = len(reference)\n    cand_len = len(candidate)\n    if ref_len == 0 or cand_len == 0:\n        return 0.0\n    ref_counts = Counter(reference)\n    cand_counts = Counter(candidate)\n    match_count = 0\n    for word in cand_counts:\n        if word in ref_counts:\n            match_count += min(cand_counts[word], ref_counts[word])\n    precision = match_count / cand_len if cand_len > 0 else 0.0\n    recall = match_count / ref_len if ref_len > 0 else 0.0\n    if cand_len > 0:\n        chunks = []\n        current_chunk = [candidate[0]]\n        for i in range(1, cand_len):\n            if candidate[i] in ref_counts and candidate[i - 1] in ref_counts:\n                if ref_counts.index(candidate[i - 1]) + 1 == ref_counts.index(candidate[i]):\n                    current_chunk.append(candidate[i])\n                else:\n                    chunks.append(current_chunk)\n                    current_chunk = [candidate[i]]\n            elif candidate[i] in ref_counts:\n                chunks.append(current_chunk)\n                current_chunk = [candidate[i]]\n            else:\n                chunks.append(current_chunk)\n                current_chunk = [candidate[i]]\n        chunks.append(current_chunk)\n        fragmentation_penalty = 0.0\n        for chunk in chunks:\n            fragmentation_penalty += (1 / len(chunk)) ** beta\n        fragmentation_penalty = 1 - fragmentation_penalty\n    else:\n        fragmentation_penalty = 0.0\n    fmean = precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n    meteor_score = alpha * fmean + (1 - alpha) * fragmentation_penalty\n    return round(meteor_score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    PMI measures how much the actual joint occurrence of events differs from what we would expect by chance.\n\n    Args:\n        joint_counts (float): The joint occurrence count of the two events.\n        total_counts_x (float): The total count of event x.\n        total_counts_y (float): The total count of event y.\n        total_samples (int): The total number of samples.\n\n    Returns:\n        float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x == 0 or p_y == 0:\n        return 0.0\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    Min-Max normalization helps ensure that all features contribute equally to a model by scaling them to a common range.\n\n    Args:\n        x (list[int]): A list of integers to be normalized.\n\n    Returns:\n        list[float]: A list of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized_x = [float(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized_x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with a shortcut connection.\n\n    Args:\n        x (np.ndarray): The input array (1D).\n        w1 (np.ndarray): The weight matrix for the first layer.\n        w2 (np.ndarray): The weight matrix for the second layer.\n\n    Returns:\n        list: The output of the residual block, rounded to 4 decimal places.\n    \"\"\"\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = z2 + x\n    output = np.maximum(0, a2)\n    return output.round(4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array representing feature maps.\n\n    Args:\n        x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n        np.ndarray: 1D array of shape (channels,), where each element is the\n                    average of all values in the corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array in BCHW format.\n\n    Args:\n        X (np.ndarray): Input array in BCHW format (batch, channels, height, width).\n        gamma (np.ndarray): Scale parameter for each channel.\n        beta (np.ndarray): Shift parameter for each channel.\n        epsilon (float): Small value for numerical stability.\n\n    Returns:\n        list: Batch Normalized output rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return [round(x, 4) for x in X_normalized.tolist()]"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form `c * x^n` at a given point `x`.\n\n    Args:\n        c (float): The coefficient of the term.\n        x (float): The point at which to evaluate the derivative.\n        n (float): The exponent of the term.\n\n    Returns:\n        float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Args:\n        vectors (list[list[float]]): A list of 2D vectors.\n        tol (float): Tolerance value to determine linear independence.\n\n    Returns:\n        list[list[float]]: A list of orthonormal vectors (unit length and orthogonal to each other)\n                           that span the same subspace.\n    \"\"\"\n    basis = []\n    for v in vectors:\n        v = np.array(v, dtype=float)\n        for u in basis:\n            proj = np.dot(v, u) * u\n            v -= proj\n        if np.linalg.norm(v) > tol:\n            v = v / np.linalg.norm(v)\n            basis.append(v.tolist())\n    return [list(np.round(v, 4)) for v in basis]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n\n    Args:\n        a (list): The first 3D vector as a list of three numbers.\n        b (list): The second 3D vector as a list of three numbers.\n\n    Returns:\n        list: The cross product of a and b, rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    cross_product_np = np.cross(a_np, b_np)\n    cross_product_list = cross_product_np.tolist()\n    return [round(x, 4) for x in cross_product_list]"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n        A (numpy.ndarray): A square coefficient matrix.\n        b (numpy.ndarray): A constant vector.\n\n    Returns:\n        list: The solution vector x, or -1 if the system has no unique solution.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    n = len(A)\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return x"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n\n    Args:\n        p (list[float]): The first probability distribution.\n        q (list[float]): The second probability distribution.\n\n    Returns:\n        float: The Bhattacharyya distance rounded to 4 decimal places.\n               Returns 0.0 if the inputs have different lengths or are empty.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    if np.sum(p) == 0 or np.sum(q) == 0:\n        return 0.0\n    bc = np.sum(np.sqrt(p * q))\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a: The first vector (list of numbers).\n        b: The second vector (list of numbers).\n\n    Returns:\n        A new vector representing the element-wise sum if the vectors have\n        compatible dimensions. Returns -1 if the vectors have incompatible\n        dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient estimator using the REINFORCE algorithm.\n\n    Args:\n        theta (np.ndarray): Policy parameters of shape (num_states, num_actions).\n        episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode\n            is a list of (state, action, reward) tuples.\n\n    Returns:\n        list: The average policy gradient, rounded to the nearest 4th decimal.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    total_samples = 0\n    for episode in episodes:\n        R = 0\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            total_samples += 1\n            R = reward + 0.99 * R\n            policy = np.exp(theta[state, :] - np.max(theta[state, :])) / np.sum(np.exp(theta[state, :] - np.max(theta[state, :])))\n            log_policy = np.log(policy[action])\n            gradient[state, action] += log_policy * R\n    if total_samples > 0:\n        gradient /= total_samples\n    return gradient.round(4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n        n_experts (int): The number of experts in the MoE layer.\n        k_active (int): The number of active experts for each input.\n        d_in (int): The input dimension.\n        d_out (int): The output dimension.\n\n    Returns:\n        float: The computational cost savings percentage, rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = k_active * 2 * d_in * d_out\n    savings = 1 - flops_moe / flops_dense\n    return round(savings * 100, 1)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (batch_size, num_experts).\n        W_g (np.ndarray): Weight matrix for gating of shape (num_experts, num_experts).\n        W_noise (np.ndarray): Weight matrix for noise of shape (num_experts, num_experts).\n        N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts).\n        k (int): Sparsity constraint (top-k experts).\n\n    Returns:\n        list: Gating probabilities matrix rounded to the nearest 4th decimal.\n    \"\"\"\n    logits = X @ W_g + N\n    (values, indices) = np.topk(logits, k, axis=1)\n    mask = np.zeros_like(logits)\n    for i in range(X.shape[0]):\n        mask[i, indices[i]] = 1\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n    probs = probs * mask\n    probs = probs / np.sum(probs, axis=1, keepdims=True)\n    probs = np.round(probs, 4)\n    return probs.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Sparse Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n\n    Args:\n        x (np.ndarray): Input tensor of shape (batch_size, num_tokens, input_dim).\n        We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n        Wg (np.ndarray): Gating weight matrix of shape (num_tokens, n_experts).\n        n_experts (int): Number of experts.\n        top_k (int): Number of experts to select for each token.\n\n    Returns:\n        list: MoE output of shape (batch_size, num_tokens, output_dim) rounded to the nearest 4th decimal.\n    \"\"\"\n    (batch_size, num_tokens, input_dim) = x.shape\n    g = np.exp(Wg) / np.sum(np.exp(Wg), axis=1, keepdims=True)\n    top_k_indices = np.argsort(g, axis=1)[:, -top_k:]\n    expert_outputs = np.zeros((batch_size, num_tokens, top_k, We.shape[2]))\n    for i in range(batch_size):\n        for j in range(num_tokens):\n            for k in range(top_k):\n                expert_index = top_k_indices[i, k]\n                expert_outputs[i, j, k, :] = np.dot(x[i, j, :], We[expert_index, :, :])\n    output = np.zeros((batch_size, num_tokens, We.shape[2]))\n    for i in range(batch_size):\n        for j in range(num_tokens):\n            gate_probs = g[i, top_k_indices[i]]\n            output[i, j, :] = np.sum(expert_outputs[i, j, :, :] * gate_probs[:, np.newaxis], axis=0)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n        X (np.ndarray): Input tensor of shape (B, C, H, W).\n        gamma (np.ndarray): Scale parameter of shape (C,).\n        beta (np.ndarray): Shift parameter of shape (C,).\n        num_groups (int): Number of groups to normalize over.\n        epsilon (float): Small value to prevent division by zero.\n\n    Returns:\n        list: Normalized tensor as a list.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by the number of groups.'\n    X_reshaped = X.reshape(B, C // num_groups, num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    X_normalized = X_normalized * gamma[:, None, None, None, None] + beta[:, None, None, None, None]\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_normalized = np.round(X_normalized, 4)\n    return X_normalized.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum using gradient descent.\n\n    Args:\n        start_x (float): The starting x value for the gradient descent.\n        learning_rate (float): The learning rate for the gradient descent.\n        tolerance (float): The tolerance for convergence.\n        max_iters (int): The maximum number of iterations.\n\n    Returns:\n        float: The x value where the minimum is found, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        f_prime = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * f_prime\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function.\n\n    DyT replaces layer normalization in Transformer architectures while preserving squashing behavior and enabling stable training.\n\n    Args:\n        x (np.ndarray): Input tensor.\n        alpha (float): Scaling factor.\n        gamma (float): Learnable parameter.\n        beta (float): Learnable parameter.\n\n    Returns:\n        list[float]: Output tensor as a Python list, with values rounded to 4 decimal places.\n    \"\"\"\n    x_norm = x / (alpha + np.abs(x))\n    y = gamma * np.tanh(beta * x_norm)\n    y = np.round(y, 4)\n    return y.tolist()"}
