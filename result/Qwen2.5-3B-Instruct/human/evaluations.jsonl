{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    :param a: List of lists representing the matrix.\n    :param b: List representing the vector.\n    :return: A list representing the resulting vector from the dot product, or -1 if the dimensions are incompatible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function takes a 2D list (matrix) 'a' as input and returns its transpose.\n    \n    Parameters:\n    a (list[list[int|float]]): A 2D list representing the matrix to be transposed.\n    \n    Returns:\n    list[list[int|float]]: The transposed matrix.\n    \"\"\"\n    transposed = []\n    num_rows = len(a)\n    num_cols = len(a[0]) if num_rows > 0 else 0\n    for col_index in range(num_cols):\n        row = []\n        for row_index in range(num_rows):\n            row.append(a[row_index][col_index])\n        transposed.append(row)\n    return transposed\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given 2D list (matrix) into a new shape specified by the user, or returns an empty list if it's not possible.\n    \n    Parameters:\n    a (list[list[int|float]]): The original 2D list to be reshaped.\n    new_shape (tuple[int, int]): A tuple specifying the desired number of rows and columns for the reshaped matrix.\n    \n    Returns:\n    list[list[int|float]]: The reshaped matrix as a list of lists, or an empty list if reshaping isn't possible.\n    \"\"\"\n    arr = np.array(a)\n    try:\n        reshaped_arr = arr.reshape(new_shape)\n    except ValueError:\n        return []\n    return reshaped_arr.tolist()\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean(s) of a matrix by either rows or columns.\n\n    Parameters:\n    matrix (list[list[float]]): The input matrix.\n    mode (str): The mode to calculate the mean, either 'row' or 'column'.\n\n    Returns:\n    list[float]: A list of means corresponding to the specified mode.\n    \"\"\"\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        transposed_matrix = list(map(list, zip(*matrix)))\n        means = [sum(col) / len(col) for col in transposed_matrix]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    return means\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the specified scalar.\n    \n    Parameters:\n    - matrix (list[list[int | float]]): The matrix to be multiplied.\n    - scalar (int | float): The scalar value to multiply by.\n    \n    Returns:\n    - list[list[int | float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n    if not matrix or not all((len(row) == len(matrix[0]) for row in matrix)):\n        raise ValueError('The input must be a non-empty square matrix.')\n    return [[scalar * elem for elem in row] for row in matrix]\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b, c, d) = (matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1])\n    det = a * d - b * c\n    trace = a + d\n    if det >= 0:\n        eigenvalue1 = (trace + math.sqrt(trace ** 2 - 4 * det)) / 2\n        eigenvalue2 = (trace - math.sqrt(trace ** 2 - 4 * det)) / 2\n    else:\n        raise NotImplementedError('Unsupported eigenvalue calculation for non-real determinants.')\n    sorted_eigenvalues = sorted([eigenvalue1, eigenvalue2], reverse=True)\n    return sorted_eigenvalues\nmatrix = [[3, 1], [1, 3]]\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    try:\n        det_T = np.linalg.det(T)\n        det_S = np.linalg.det(S)\n        if det_T == 0 or det_S == 0:\n            return [-1]\n        T_inv = np.linalg.inv(T)\n        S_inv = np.linalg.inv(S)\n        result = np.dot(np.dot(T_inv, A), S)\n        rounded_result = np.round(result, decimals=4)\n        final_result = rounded_result.tolist()\n        return final_result\n    except Exception as e:\n        print(f'An error occurred during the computation: {e}')\n        return [-1]\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    This function takes a 2x2 matrix as input and returns its inverse.\n    If the matrix is not invertible (i.e., determinant is zero), it returns None.\n    \"\"\"\n    (a, b) = (matrix[0][0], matrix[0][1])\n    (c, d) = (matrix[1][0], matrix[1][1])\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_determinant = 1 / determinant\n    inverse_matrix = [[d * inv_determinant, -b * inv_determinant], [-c * inv_determinant, a * inv_determinant]]\n    return inverse_matrix\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices, returning their product. \n    Returns [-1] if the matrices cannot be multiplied due to incompatible dimensions.\n\n    :param a: First 2D list representing the first matrix.\n    :param b: Second 2D list representing the second matrix.\n    :return: The product of matrices a and b as a 2D list, or [-1] if multiplication is not possible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return [-1]\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nfrom typing import List, Union\nimport numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given list of feature vectors.\n    \n    Parameters:\n    vectors (list[list[float]]): A list of lists, where each sublist represents\n                                 a vector of features.\n    \n    Returns:\n    list[list[float]]: The covariance matrix corresponding to the input data.\n    \"\"\"\n    num_vectors = len(vectors)\n    if num_vectors == 0:\n        raise ValueError('The input list cannot be empty.')\n    means = np.mean(vectors, axis=0)\n    centered_vectors = [vector - means for vector in vectors]\n    covariance_matrix = np.cov(centered_vectors, rowvar=False)\n    return covariance_matrix.tolist()\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations A * x = b using the Jacobi iterative method.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix of the linear system.\n        b (np.ndarray): Right-hand side vector of the linear system.\n        n (int): Number of iterations to perform.\n        \n    Returns:\n        list: Approximate solutions after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    x = np.zeros_like(b)\n    x_new = np.copy(x)\n    for _ in range(n):\n        for i in range(len(x)):\n            sum_other_terms = sum((A[i][j] * x[j] for j in range(len(x)) if j != i))\n            x_new[i] = (b[i] - sum_other_terms) / A[i][i]\n        x = np.copy(x_new)\n    rounded_solution = [round(num, 4) for num in x]\n    return rounded_solution\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: invalid index to scalar variable.", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the singular value decomposition of a 2x2 matrix A.\n    \n    Args:\n    A (np.ndarray): A 2x2 matrix.\n    \n    Returns:\n    tuple: A tuple containing U, Sigma, and V^T where U and V are unitary matrices \n           and Sigma is a diagonal matrix of singular values.\n    \"\"\"\n    (a11, a12) = (A[0, 0], A[0, 1])\n    (a21, a22) = (A[1, 0], A[1, 1])\n\n    def eigenvalues(matrix):\n        \"\"\"Calculate eigenvalues of the given matrix.\"\"\"\n        (w, v) = np.linalg.eig(matrix)\n        return sorted(w)\n    B = np.array([[a11, a12 / 2.0], [a12 / 2.0, a22]])\n    lambdas = eigenvalues(B)\n    (sigma1, sigma2) = (lambdas[0] ** 0.5, lambdas[1] ** 0.5)\n    Sigma = np.array([[sigma1, 0], [0, sigma2]])\n    U = np.zeros((2, 2))\n    V = np.zeros((2, 2))\n    U_eigenvectors = eigenvalues(np.outer(np.linalg.inv(np.sqrt(B)), B))[-1]\n    U[:, 0] = U_eigenvectors[0] * np.sqrt(a11)\n    U[:, 1] = U_eigenvectors[1] * np.sqrt(a22)\n    for i in range(2):\n        V[:, i] = np.real_if_close(np.roots([1, -(U[0, i] + U[1, i]), a12 ** 2 / 4 + U[0, i] ** 2 - U[1, i] ** 2]))\n    return (U, Sigma, np.transpose(V))\nA = np.array([[3, 1], [1, 2]])\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": false, "result": "failed: list index out of range", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's expansion.\n\n    :param matrix: A 4x4 list of lists representing the matrix.\n    :return: The determinant of the matrix as a float.\n    \"\"\"\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    det = 0.0\n    sign = 1.0\n    for col in range(4):\n        sub_matrix = [row[:col] + row[col + 1:] for row in matrix[1:]]\n        sub_det = determinant_4x4(sub_matrix)\n        det += sign * matrix[0][col] * sub_det\n        sign *= -1.0\n    return det\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation.\n    \n    Parameters:\n    - X (list[list[float]]): A matrix of features.\n    - y (list[float]): A vector of target values.\n    \n    Returns:\n    - list[float]: Coefficients of the linear regression model, rounded to 4 decimal places.\n    \"\"\"\n    X_array = np.array(X)\n    y_array = np.array(y)\n    weights = np.dot(np.linalg.inv(np.dot(X_array.T, X_array)), np.dot(X_array.T, y_array))\n    return [round(weight, 4) for weight in weights]\nX = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]]\ny = [1, 3, 5, 7]\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent on the given data.\n    \n    Parameters:\n    - X: np.ndarray, feature array with shape (n_samples, n_features).\n    - y: np.ndarray, target values array with shape (n_samples,).\n    - alpha: float, learning rate for gradient descent.\n    - iterations: int, number of iterations for gradient descent.\n    \n    Returns:\n    np.ndarray: A list containing the coefficients of the linear regression model, rounded to four decimal places.\n    \"\"\"\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    weights = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = X @ weights\n        error = predictions - y\n        weights = weights - alpha * (1 / X.shape[0]) * (X.T @ error)\n    fitted_weights = np.round(weights, 4).tolist()\n    return fitted_weights\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the given 2D NumPy array using standardization and min-max normalization.\n    \n    Parameters:\n    - data (np.ndarray): 2D array where each row represents a data sample and each column a feature.\n    \n    Returns:\n    - tuple of two lists of lists:\n        - First list contains rows of standardized values.\n        - Second list contains rows of min-max normalized values.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = [(x - mean) / std for x in data]\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    normalized_data = [(x - min_vals) / (max_vals - min_vals) for x in data]\n    standardized_data_list = [row.tolist() for row in standardized_data]\n    normalized_data_list = [row.tolist() for row in normalized_data]\n    return (standardized_data_list, normalized_data_list)\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (6,2) (2,2) ", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-means clustering on a set of points represented as tuples of (x, y).\n    \n    :param points: List of points, each a tuple of (x, y) coordinates.\n    :param k: Number of clusters to form.\n    :param initial_centroids: Initial centroids of the clusters, each a tuple of (x, y) coordinates.\n    :param max_iterations: Maximum number of iterations to perform clustering.\n    :return: Final centroids of the clusters, each a tuple of (x, y) coordinates.\n    \"\"\"\n    points_array = np.array(points)\n    initial_centroids = np.array(initial_centroids)\n    points_array = (points_array - points_array.mean(axis=0)) / points_array.std(axis=0)\n    distances = lambda p, c: np.linalg.norm(p - c, axis=1)\n\n    def assign_points_to_nearest_centroid(points, centroids):\n        return centroids[np.argmin(distances(points, centroids), axis=0)]\n    assignments = np.zeros(len(points))\n    for _ in range(max_iterations):\n        new_assignments = assign_points_to_nearest_centroid(points_array, initial_centroids)\n        if np.all(assignments == new_assignments):\n            break\n        initial_centroids = np.array([points_array[assignments == i].mean(axis=0) for i in range(k)])\n        assignments = new_assignments\n    final_centroids_rounded = [(round(point[0], 4), round(point[1], 4)) for point in initial_centroids]\n    return final_centroids_rounded\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for +: 'slice' and 'slice'", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None) -> list:\n    \"\"\"\n    Generates train-test splits for k-fold cross-validation.\n    \n    Parameters:\n    X (numpy.ndarray): The feature matrix.\n    y (numpy.ndarray): The target vector.\n    k (int): Number of folds for k-fold cross-validation.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): Seed value for reproducibility.\n    \n    Returns:\n    list: A list of tuples where each tuple contains (train_indices, test_indices).\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    if shuffle:\n        permutation = np.random.permutation(n_samples)\n        (X, y) = (X[permutation], y[permutation])\n    fold_size = n_samples // k\n    splits = []\n    for i in range(k):\n        validation_start = i * fold_size\n        validation_end = min((i + 1) * fold_size, n_samples)\n        validation_indices = slice(validation_start, validation_end)\n        training_indices = []\n        if i == 0:\n            training_indices = slice(fold_size, n_samples)\n        elif i == k - 1:\n            training_indices = slice(0, n_samples - fold_size)\n        else:\n            training_indices = slice(0, validation_start) + slice(validation_end, n_samples)\n        splits.append((training_indices, validation_indices))\n    return splits\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given dataset.\n    \n    Parameters:\n        data (np.ndarray): A 2D NumPy array where each row represents a data sample\n                           and each column represents a feature.\n        k (int): The number of principal components to return.\n    \n    Returns:\n        list[list[float]]: A list of lists, containing the principal components.\n                           Each component is a k-dimensional vector, rounded to 4 decimal places.\n    \"\"\"\n    mean = data.mean(axis=0)\n    std = data.std(axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    idx_sorted = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[idx_sorted]\n    eigenvectors_sorted = eigenvectors[:, idx_sorted]\n    top_k_eigenvectors = eigenvectors_sorted[:, :k]\n    top_k_eigenvectors_rounded = np.round(top_k_eigenvectors, decimals=4)\n    return top_k_eigenvectors_rounded.tolist()\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": false, "result": "failed: unhashable type: 'list'", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef entropy(target_attr):\n    \"\"\"\n    Calculate the entropy of a target attribute's values.\n    \"\"\"\n    total = len(target_attr)\n    if total == 0:\n        return 0\n    counts = Counter(target_attr)\n    entropy = 0\n    for label in counts:\n        probability_of_label = counts[label] / total\n        entropy -= probability_of_label * math.log2(probability_of_label)\n    return entropy\ndef split_examples(examples, attribute, value):\n    \"\"\"\n    Split the examples by the given attribute and value.\n    \"\"\"\n    return [example for example in examples if example[attribute] == value]\ndef choose_best_attribute(examples, attributes, target_attr):\n    \"\"\"\n    Choose the best attribute to split the dataset using information gain.\n    \"\"\"\n    initial_entropy = entropy([example[target_attr] for example in examples])\n    if not attributes or initial_entropy == 0:\n        return None\n    best_attribute = None\n    max_information_gain = -math.inf\n    for attr in attributes:\n        possible_values = set([example[attr] for example in examples])\n        weighted_info_gain = 0\n        for value in possible_values:\n            left_branch = split_examples(examples, attr, value)\n            if not left_branch:\n                continue\n            left_entropy = entropy([example[target_attr] for example in left_branch])\n            weight = len(left_branch) / len(examples)\n            remaining_entropy = initial_entropy - left_entropy\n            weighted_info_gain += weight * remaining_entropy\n        if weighted_info_gain > max_information_gain:\n            best_attribute = attr\n            max_information_gain = weighted_info_gain\n    return best_attribute\ndef learn_decision_tree(examples, attributes, target_attr):\n    \"\"\"\n    Build a decision tree using recursive binary splitting.\n    \"\"\"\n    if len(examples) == 0 or len(attributes) == 0 or target_attr not in attributes:\n        return examples[0][target_attr] if examples else None\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    best_attribute = choose_best_attribute(examples, attributes, target_attr)\n    if best_attribute is None:\n        return examples[0][target_attr]\n    tree = {best_attribute: {}}\n    for value in set([example[best_attribute] for example in examples]):\n        subtree_attributes = attributes.copy()\n        subtree_attributes.remove(best_attribute)\n        subtree_target_attr = [example[target_attr] for example in examples if example[best_attribute] == value]\n        subtree_examples = [example for example in examples if example[best_attribute] == value]\n        subtree_tree = learn_decision_tree(subtree_examples, subtree_attributes, subtree_target_attr)\n        tree[best_attribute][value] = subtree_tree\n    return tree\nexamples = [{'age': 25, 'income': 'high', 'student': 'yes', 'credit_rating': 'fair'}, {'age': 43, 'income': 'medium', 'student': 'yes', 'credit_rating': 'good'}, {'age': 28, 'income': 'high', 'student': 'no', 'credit_rating': 'very_good'}, {'age': 32, 'income': 'low', 'student': 'no', 'credit_rating': 'good'}, {'age': 57, 'income': 'high', 'student': 'yes', 'credit_rating': 'excellent'}]\nattributes = ['age', 'income', 'student', 'credit_rating']\ntarget_attr = 'credit_rating'\ntree = learn_decision_tree(examples, attributes, target_attr)\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Train a Kernel SVM model using Pegasos algorithm on the given dataset with specified kernel type,\n    regularization parameter, and number of iterations.\n    \n    Parameters:\n    data (np.ndarray): The input data matrix where each row is a data point and each column is a feature.\n    labels (np.ndarray): The array of labels for each data point.\n    kernel (str): The type of kernel to be used ('linear' or 'rbf').\n    lambda_val (float): Regularization parameter.\n    iterations (int): Number of iterations to run the algorithm.\n    sigma (float): Parameter used in RBF kernel computation.\n    \n    Returns:\n    list: A list of the optimized alpha coefficients and bias term.\n    \"\"\"\n    (num_samples, num_features) = data.shape\n    alphas = np.zeros(num_samples)\n    b = 0\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'rbf':\n        K = np.exp(-sigma * ((data[:, np.newaxis] - data) ** 2).sum(axis=-1))\n    for _ in range(iterations):\n        for i in range(num_samples):\n            loss = labels[i] * (alphas[i] + b) - 1\n            if alphas[i] > 0 and loss > 0:\n                alphas[i] -= lambda_val / num_samples\n            elif alphas[i] <= 0 and loss <= 0:\n                alphas[i] += lambda_val / num_samples\n            if alphas[i] > 0:\n                alphas[i] -= lambda_val / num_samples\n            elif alphas[i] <= 0:\n                alphas[i] += lambda_val / num_samples\n            alphas[i] *= max(0, 1 - loss * labels[i])\n            b += labels[i] - alphas[i] * K[i]\n    alphas = np.round(alphas, 4)\n    b = np.round(b, 4)\n    return (alphas.tolist(), b.tolist())\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    The sigmoid function is defined as: f(z) = 1 / (1 + e^{-z})\n    \n    Args:\n    z (float): A numeric value representing the input to the sigmoid function.\n    \n    Returns:\n    float: The result of the sigmoid function applied to the input z, rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute softmax values for a list of input numbers (scores).\n    \n    Args:\n    - scores: A list of float numbers representing the input scores.\n\n    Returns:\n    - A list of floats representing the softmax values of the input scores, rounded to four decimal places.\n    \"\"\"\n    if not scores:\n        return []\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [score / sum_exp_scores for score in exp_scores]\n    return [round(val, 4) for val in softmax_values]\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron model with sigmoid activation for binary classification.\n    \n    Parameters:\n        features (list[list[float]]): A list of feature vectors, where each inner list represents a sample's features.\n        labels (list[int]): True binary labels for each sample.\n        weights (list[float]): Weights for each feature in the feature vectors.\n        bias (float): Bias term for the neuron.\n        \n    Returns:\n        tuple: A tuple containing two elements:\n            - list[float]: Predicted probabilities after applying the sigmoid activation.\n            - float: Mean squared error between predicted probabilities and true labels.\n    \"\"\"\n    X = np.array(features)\n    y_true = np.array(labels).reshape(-1, 1)\n    w = np.array(weights).reshape(len(features[0]), 1)\n    b = np.array([bias]).reshape(1, 1)\n    net_input = np.dot(X, w) + b\n    predictions = 1 / (1 + np.exp(-net_input))\n    predicted_probs = predictions.flatten()\n    y_pred = (predicted_probs >= 0.5).astype(int)\n    mse = ((y_true - y_pred) ** 2).mean()\n    return ([round(p, 4) for p in predicted_probs.tolist()], round(mse, 4))\nfeatures = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Trains a single neuron with sigmoid activation function using backpropagation.\n    \n    Parameters:\n        features (np.ndarray): Input feature vectors.\n        labels (np.ndarray): True binary labels.\n        initial_weights (np.ndarray): Initial weights for the neuron.\n        initial_bias (float): Initial bias for the neuron.\n        learning_rate (float): Learning rate for gradient descent.\n        epochs (int): Number of training epochs.\n        \n    Returns:\n        tuple[np.ndarray, float, list[float]]: Updated weights, updated bias, and a list of MSE values for each epoch.\n    \"\"\"\n    num_samples = len(features)\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        linear_combination = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear_combination))\n        loss = np.mean((predictions - labels) ** 2)\n        mse_values.append(np.round(loss, 4))\n        error = predictions - labels\n        dW = 1 / num_samples * np.dot(features.T, error)\n        db = 1 / num_samples * np.sum(error)\n        weights -= learning_rate * dW\n        bias -= learning_rate * db\n    return (weights, bias, mse_values)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": false, "result": "failed: 'str' object has no attribute '_prev'", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (_op := '+'), (self, other))\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (_op := '*'), (self, other))\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        if self.data < 0:\n            self.data = 0\n        self._backward = lambda : None\n        return self\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the transformation matrix P from basis B to C for R^3.\n    \n    Parameters:\n    B (list[list[int]]): The basis vectors of B as a list of lists [b1, b2, b3].\n    C (list[list[int]]): The basis vectors of C as a list of lists [c1, c2, c3].\n    \n    Returns:\n    list[list[float]]: The transformation matrix P from B to C, rounded to 4 decimal places.\n    \"\"\"\n    B_np = np.array(B)\n    C_np = np.array(C)\n    P = np.dot(np.linalg.inv(C_np), B_np)\n    P_rounded = np.round(P, decimals=4).tolist()\n    return P_rounded\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: shape mismatch: value array of shape (2,) could not be broadcast to indexing result of shape (3,)", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Perform Singular Value Decomposition (SVD) on a 2x2 matrix using eigenvalues and eigenvectors.\n    \n    Args:\n        A (np.ndarray): A 2x2 numpy array\n    \n    Returns:\n        tuple: A tuple containing the matrices U, S, and V from SVD, all as lists.\n    \"\"\"\n    ATA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    S = np.zeros((2, 2))\n    idx = eigenvalues > 0\n    sigma = np.sqrt(eigenvalues[idx]).round(4)\n    S[np.triu_indices_from(S, k=0)] = sigma\n    V = eigenvectors.round(4)\n    U = np.zeros((2, 2))\n    U[:, idx] = A @ eigenvectors[:, idx] / sigma[~idx]\n    U = U.round(4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[3, 1], [1, 2]])\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: name 'shuffle_data' is not defined", "solution": "import numpy as np\n\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    assert isinstance(X, np.ndarray), 'Input X must be a Numpy array'\n    if y is not None:\n        assert isinstance(y, np.ndarray) and len(X) == len(y), 'Input arrays X and y must have the same length'\n    num_samples = len(X)\n    num_batches = -(-num_samples // batch_size)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for i in range(num_batches):\n        batch_indices = indices[i * batch_size:(i + 1) * batch_size]\n        if y is None:\n            batch_data = X[batch_indices]\n        else:\n            batch_data = (X[batch_indices], y[batch_indices])\n        yield batch_data\nX = np.array(range(32))\ny = np.arange(32, 48)\nbatch_size = 8\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on a threshold value for a given feature index.\n\n    Parameters:\n    X (numpy.ndarray): The input data matrix where each row represents a sample and each column represents a feature.\n    feature_i (int): Index of the feature to be used for division.\n    threshold (float): The threshold value to divide the feature values.\n\n    Returns:\n    tuple: A tuple containing two numpy arrays - the first array contains samples where the feature value is greater than or equal to the threshold,\n           and the second array contains samples where the feature value is less than the threshold.\n    \"\"\"\n    left = X[X[:, feature_i] >= threshold]\n    right = X[X[:, feature_i] < threshold]\n    return (left, right)\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: list index out of range", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates polynomial features of a given degree from the input features.\n    \n    Parameters:\n    - X (numpy.ndarray): A 2D numpy array where each row is a sample and each column is a feature.\n    - degree (int): The degree of the polynomial features to generate.\n    \n    Returns:\n    - numpy.ndarray: A 2D numpy array containing the generated polynomial features.\n    \"\"\"\n\n    def expand_features(x, current_degree):\n        if current_degree == 0:\n            return [1]\n        else:\n            expanded = []\n            for i in range(1, len(x) + 1):\n                multiset_combinations = set(combinations_with_replacement(range(current_degree + 1), i))\n                for combo in multiset_combinations:\n                    coefficient = 1\n                    term = 1\n                    for (j, power) in enumerate(combo):\n                        coefficient *= x[power]\n                        term += coefficient\n                    expanded.append(term)\n            return expanded\n    expanded_features = []\n    for sample in X:\n        expanded_sample = []\n        for i in range(degree + 1):\n            expanded_sample.extend(expand_features(list(sample), i))\n        expanded_features.append(expanded_sample)\n    return np.array(expanded_features).T.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 5) + inhomogeneous part.", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates n_subsets of randomly selected rows from the 2D numpy array X,\n    with corresponding labels from 1D numpy array y.\n    \n    Parameters:\n    - X: 2D numpy array, features for the dataset\n    - y: 1D numpy array, target values for the dataset\n    - n_subsets: int, number of random subsets to generate\n    - replacements: bool, whether to allow replacement of elements during sampling\n    - seed: int, seed for random number generator\n    \n    Returns:\n    - A list of tuples [(X_subset, y_subset)] for each of the n_subsets generated.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    subsets = []\n    assert len(y) == X.shape[0], 'The lengths of X and y must match.'\n    for _ in range(n_subsets):\n        indices = rng.choice(len(y), size=len(y), replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [np.array(subset).tolist() for subset in subsets]\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array into a one-hot encoded 2D numpy array.\n    \n    Parameters:\n    x (numpy.ndarray): A 1D numpy array of integers.\n    n_col (int, optional): Number of columns for the one-hot encoded output. \n                           If None, n_col will be determined from x.\n                           \n    Returns:\n    numpy.ndarray: A 2D one-hot encoded numpy array.\n    \"\"\"\n    if n_col is None:\n        n_col = int(np.max(x)) + 1\n    y = (np.arange(x.shape[0])[:, None] >= x[:, None]).astype(int)\n    return y.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a 2D diagonal matrix.\n    \n    Parameters:\n    - x : 1D numpy array\n    \n    Returns:\n    - A 2D numpy array which is the diagonal matrix based on input 1D array x.\n    \"\"\"\n    n = len(x)\n    d_matrix = np.zeros((n, n))\n    np.fill_diagonal(d_matrix, x)\n    return d_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score for the given true and predicted labels.\n    \n    :param y_true: numpy array containing true labels.\n    :param y_pred: numpy array containing predicted labels.\n    :return: accuracy score as a float, rounded to 4 decimal places.\n    \"\"\"\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\ny_true = np.array([0, 0, 1, 1, 1])\ny_pred = np.array([0, 0, 1, 1, 0])\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset.\n    \n    Parameters:\n    - X: A 2D numpy array representing the dataset.\n    - Y: An optional 2D numpy array representing another dataset. \n         If not provided, X is used.\n         \n    Returns:\n    - A numpy array representing the correlation matrix, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        corr_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        corr_matrix = np.corrcoef(X, Y, rowvar=False)\n    rounded_corr_matrix = np.round(corr_matrix, 4)\n    return rounded_corr_matrix.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import numpy as np\nimport math\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the fit method for an AdaBoost classifier.\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features) representing the dataset.\n    - y: 1D numpy array of shape (n_samples,) representing the labels.\n    - n_clf: Integer representing the number of classifiers.\n    \n    Returns:\n    - A list of classifiers with their parameters.\n    \"\"\"\n    n_samples = X.shape[0]\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    weak_classifiers = []\n    for _ in range(n_clf):\n        weighted_error = np.sum(sample_weights[y != np.round(np.dot(X, weak_classifiers[-1].coef_.T))])\n        if weighted_error == 0:\n            break\n        theta = 1 / math.log(2)\n        alpha = np.log((1 - weighted_error) / weighted_error) * theta\n        updated_sample_weights = sample_weights * np.exp(-alpha * y * np.round(np.dot(X, weak_classifiers[-1].coef_.T)))\n        sum_updated_weights = np.sum(updated_sample_weights)\n        sample_weights = updated_sample_weights / sum_updated_weights\n        weak_classifier = DecisionTreeClassifier(max_depth=1)\n        weak_classifier.fit(X, y, sample_weight=sample_weights)\n        weak_classifiers.append({'alpha': alpha, 'clf': weak_classifier})\n    return weak_classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1D numpy array of scores.\n    \n    Args:\n    scores (list): A list of scores.\n    \n    Returns:\n    list: A list of log-softmax values rounded to 4 decimal places.\n    \"\"\"\n    score_array = np.array(scores)\n    softmax_scores = np.exp(score_array - np.max(score_array)) / np.sum(np.exp(score_array - np.max(score_array)))\n    log_softmax_scores = np.log(softmax_scores)\n    return log_softmax_scores.tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: Dense.initialize() takes 1 positional argument but 2 were given", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        if input_shape is not None:\n            self.W = np.random.uniform(-1 / np.sqrt(input_shape[0]), 1 / np.sqrt(input_shape[0]), (input_shape[0], n_units))\n            self.w0 = np.zeros((1, n_units))\n        self.optimizer_W = copy.deepcopy(self.W)\n        self.optimizer_w0 = copy.deepcopy(self.w0)\n\n    def initialize(self):\n        if self.input_shape is not None:\n            std_dev = 1 / np.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-std_dev, std_dev, (self.input_shape[0], self.n_units))\n            self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.deepcopy(self.W)\n        self.optimizer_w0 = copy.deepcopy(self.w0)\n\n    def parameters(self):\n        params = []\n        params.append({'params': self.W.flatten(), 'optimizer': self.optimizer_W})\n        params.append({'params': self.w0.flatten(), 'optimizer': self.optimizer_w0})\n        return (len(params[0]['params']), len(params[1]['params']))\n\n    def forward_pass(self, X, training=False):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if training:\n            self.optimizer_W -= self.lr * grad_W\n            self.optimizer_w0 -= self.lr * grad_w0\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a 2D convolution on the input matrix using a specified kernel,\n    with given padding and stride values.\n    \n    Parameters:\n    - input_matrix: np.ndarray, the input 2D array.\n    - kernel: np.ndarray, the convolutional kernel to be applied.\n    - padding: int, the amount of zero padding added to the input matrix edges.\n    - stride: int, the number of units to slide the kernel over the input matrix.\n    \n    Returns:\n    - np.ndarray, the processed output matrix reshaped as a 1D list.\n    \"\"\"\n    assert len(kernel.shape) == 2, 'Kernel must be 2-dimensional.'\n    (rows, cols) = input_matrix.shape\n    new_rows = (rows + 2 * padding) // stride * stride\n    new_cols = (cols + 2 * padding) // stride * stride\n    output_matrix = np.zeros((new_rows, new_cols), dtype=float)\n    for i in range(new_rows):\n        for j in range(new_cols):\n            output_matrix[i, j] = (input_matrix[max(0, i - padding):min(rows, i + padding + 1), max(0, j - padding):min(cols, j + padding + 1)] * kernel).sum()\n    return output_matrix.reshape(-1).tolist()\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the ReLU (Rectified Linear Unit) activation function to the input z.\n\n    Parameters:\n    z (float): Input value\n\n    Returns:\n    float: Output value after applying ReLU activation\n    \"\"\"\n    return max(0.0, z)\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss for given features X, weights w, true labels y_true, and regularization factor alpha.\n    \n    Parameters:\n    X (np.ndarray): The feature matrix with shape (n_samples, n_features).\n    w (np.ndarray): The weights vector of shape (n_features,).\n    y_true (np.ndarray): The true labels array with shape (n_samples,).\n    alpha (float): The regularization strength.\n    \n    Returns:\n    float: The computed Ridge Regression loss.\n    \"\"\"\n    predictions = np.dot(X, w)\n    mse_loss = ((predictions - y_true) ** 2).mean()\n    reg_term = (w ** 2).sum() * (alpha / (2 * X.shape[0]))\n    ridge_loss_value = mse_loss + reg_term\n    return round(ridge_loss_value, 4)\nX = np.array([[1, 2], [3, 4], [5, 6]])\nw = np.array([0, 0])\nalpha = 1.0\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input z.\n\n    Parameters:\n    z (float): Input value to the activation function.\n    alpha (float): Slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: Output value after applying the Leaky ReLU activation function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return z * alpha\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Calculate the linear kernel between two vectors.\n\n    Args:\n    - x1: numpy array representing the first vector.\n    - x2: numpy array representing the second vector.\n\n    Returns:\n    - float: The value of the linear kernel between x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculates the precision metric for binary classification.\n\n    Parameters:\n    - y_true: A numpy array of true binary labels.\n    - y_pred: A numpy array of predicted binary labels.\n\n    Returns:\n    - A float value representing the precision.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not np.issubdtype(y_true.dtype, np.bool_):\n        y_true = y_true == 1\n    if not np.issubdtype(y_pred.dtype, np.bool_):\n        y_pred = y_pred == 1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    precision_value = tp / (tp + fp) if tp + fp > 0 else 0\n    return precision_value\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent for training a linear model with MSE loss.\n    \n    Parameters:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n        y (np.ndarray): Target vector of shape (n_samples,)\n        weights (np.ndarray): Initial weights vector of shape (n_features,)\n        learning_rate (float): Learning rate for gradient descent\n        n_iterations (int): Number of iterations to run gradient descent\n        batch_size (int): Size of mini-batch for mini-batch gradient descent\n        method (str): Type of gradient descent ('sgd', 'batch', or 'mini-batch')\n        \n    Returns:\n        np.ndarray: Final weights after training\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    m = len(y)\n    weights = np.array(weights)\n    for iteration in range(n_iterations):\n        permutation = np.random.permutation(m)\n        X_shuffled = X[permutation]\n        y_shuffled = y[permutation]\n        if method == 'sgd':\n            for i in range(m):\n                xi = X_shuffled[i]\n                yi = y_shuffled[i]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi * error\n                weights -= learning_rate * gradient\n        elif method == 'batch':\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = 2 / m * np.dot(X.T, errors)\n            weights -= learning_rate * gradient\n        else:\n            indices = np.random.choice(m, size=batch_size, replace=False)\n            xi_batch = X_shuffled[indices]\n            yi_batch = y_shuffled[indices]\n            predictions = np.dot(xi_batch, weights)\n            errors = predictions - yi_batch\n            gradient = 2 / batch_size * np.dot(xi_batch.T, errors)\n            weights -= learning_rate * gradient\n    return weights.tolist()\nX = [[1, 3], [2, 5], [3, 7]]\ny = [4, 9, 11]\nweights = [0, 0.5]\nlearning_rate = 0.01\nn_iterations = 1000\nmethod = 'mini-batch'\nbatch_size = 2\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: The matrix must be square to find its RREF.", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts the input matrix into its Reduced Row Echelon Form (RREF).\n    \n    :param matrix: A numpy array representing the matrix to convert.\n    :return: A numpy array representing the RREF of the input matrix.\n    \"\"\"\n    matrix = np.array(matrix)\n    if matrix.shape[0] != matrix.shape[1]:\n        raise ValueError('The matrix must be square to find its RREF.')\n    identity_matrix = np.eye(matrix.shape[0])\n    combined_matrix = np.hstack((matrix, identity_matrix))\n    while np.any(combined_matrix[:, -matrix.shape[0]:].sum(axis=0) != 0):\n        pivot_row_index = np.argmax(combined_matrix[:-matrix.shape[0], :-matrix.shape[0]].T == 1, axis=0)[0]\n        combined_matrix[[pivot_row_index, -matrix.shape[0]]] = combined_matrix[[-matrix.shape[0], pivot_row_index]]\n        for i in range(-matrix.shape[0], matrix.shape[0]):\n            if i != pivot_row_index:\n                factor = combined_matrix[i, :-matrix.shape[0]]\n                combined_matrix[i, :-matrix.shape[0]] = combined_matrix[pivot_row_index, :-matrix.shape[0]] = 0\n                for j in range(-matrix.shape[0], matrix.shape[0]):\n                    combined_matrix[i, j] -= factor * combined_matrix[pivot_row_index, j]\n        combined_matrix = combined_matrix[:, :-matrix.shape[0]]\n    reduced_matrix = combined_matrix[:, :-matrix.shape[0]]\n    return list(reduced_matrix)\nmatrix = [[1, 2, 3, 4], [2, 4, 6, 8], [1, 0, -1, -2]]\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam optimizer implementation\n    \n    Parameters:\n    - f: Objective function\n    - grad: Gradient function\n    - x0: Initial parameter values\n    - learning_rate: Step size\n    - beta1: Exponential decay rate for the first moment estimates\n    - beta2: Exponential decay rate for the second moment estimates\n    - epsilon: Small constant to avoid division by zero\n    - num_iterations: Number of iterations\n    \n    Returns:\n    - Optimized parameters\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    for i in range(num_iterations):\n        grad_x = grad(x0)\n        m = beta1 * m + (1 - beta1) * grad_x\n        v = beta2 * v + (1 - beta2) * grad_x ** 2\n        m_cap = m / (1 - beta1 ** (i + 1))\n        v_cap = v / (1 - beta2 ** (i + 1))\n        dx = learning_rate * m_cap / (np.sqrt(v_cap) + epsilon)\n        x0 -= dx\n    return np.round(x0, decimals=4).tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (3,) (2,) ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Fit Lasso Regression model using gradient descent with L1 regularization.\n    \n    :param X: Feature matrix as a numpy array.\n    :param y: Target vector as a numpy array.\n    :param alpha: Regularization strength.\n    :param learning_rate: Learning rate for gradient descent.\n    :param max_iter: Maximum number of iterations for gradient descent.\n    :param tol: Tolerance for convergence.\n    :return: Tuple containing the final weights and bias, both as numpy arrays.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.random.randn(n_features)\n    b = np.random.randn()\n    for _ in range(max_iter):\n        preds = X @ w + b\n        loss_function = 0.5 * np.mean((preds - y) ** 2) + alpha * np.sum(np.abs(w))\n        dw = 1 / n_samples * (X.T @ (preds - y + alpha * np.sign(w))) + 2 * alpha * np.mean(np.where(w == 0, 0, w), axis=0)\n        db = 1 / n_samples * np.sum(preds - y + alpha * np.sign(w))\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        if np.abs(loss_function - prev_loss) < tol:\n            break\n        prev_loss = loss_function\n    return (w.tolist(), b.tolist())\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n    source (str): The source string.\n    target (str): The target string.\n    \n    Returns:\n    int: The minimum number of edits required to transform the source string into the target string.\n    \"\"\"\n    m = len(source)\n    n = len(target)\n    osa_table = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        for j in range(n + 1):\n            if i == 0:\n                osa_table[i][j] = j\n            elif j == 0:\n                osa_table[i][j] = i\n            elif source[i - 1] == target[j - 1]:\n                osa_table[i][j] = osa_table[i - 1][j - 1]\n            else:\n                osa_table[i][j] = 1 + min(osa_table[i - 1][j], osa_table[i][j - 1], osa_table[i - 1][j - 1])\n    return osa_table[m][n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Parameters:\n    - y_true: List[int] representing the ground truth binary labels.\n    - y_pred: List[int] representing the predicted binary labels.\n\n    Returns:\n    - float: The recall metric rounded to three decimal places.\n    \"\"\"\n    y_true_array = np.array(y_true)\n    y_pred_array = np.array(y_pred)\n    tp = np.sum((y_true_array == 1) & (y_pred_array == 1))\n    fn = np.sum((y_true_array == 1) & (y_pred_array == 0))\n    if tp + fn == 0:\n        return 0.0\n    recall_value = tp / (tp + fn)\n    return round(recall_value, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention mechanism for the given X matrix.\n    \n    Parameters:\n        X (numpy.ndarray): Input tensor with shape (batch_size, seq_length, embed_dim).\n        W_q (numpy.ndarray): Query weights matrix with shape (embed_dim, embed_dim).\n        W_k (numpy.ndarray): Key weights matrix with shape (embed_dim, embed_dim).\n        W_v (numpy.ndarray): Value weights matrix with shape (embed_dim, embed_dim).\n        \n    Returns:\n        attention_output (list): Reshaped self-attention output tensor as a list.\n    \"\"\"\n    (batch_size, seq_length, embed_dim) = X.shape\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scale_factor = embed_dim ** (-0.5)\n    QK_dot_product = np.matmul(Q, K.transpose(0, 2, 1))\n    QK_dot_product_scaled = QK_dot_product * scale_factor\n    attention_scores = QK_dot_product_scaled\n    mask_value = -np.inf\n    mask_matrix = np.triu(np.ones((seq_length, seq_length)), k=1) == 0\n    attention_scores_masked = np.where(mask_matrix, attention_scores, mask_value)\n    softmax_attention_weights = np.exp(attention_scores_masked) / np.sum(np.exp(attention_scores_masked), axis=-1, keepdims=True)\n    context_vectors = np.matmul(softmax_attention_weights, V)\n    attention_output = context_vectors.reshape(batch_size, seq_length, embed_dim)\n    attention_output_list = attention_output.tolist()\n    return attention_output_list\nX = np.array([[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]])\nembed_dim = 2\nW_q = np.random.rand(embed_dim, embed_dim)\nW_k = np.random.rand(embed_dim, embed_dim)\nW_v = np.random.rand(embed_dim, embed_dim)\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements a forward pass for a simple RNN cell.\n    \n    Args:\n    - input_sequence: A list of lists representing the sequence of input vectors at each time step.\n    - initial_hidden_state: A list representing the initial hidden state.\n    - Wx: A list of lists representing the input weights.\n    - Wh: A list of lists representing the recurrent weights.\n    - b: A list representing the biases.\n    \n    Returns:\n    - A list of floats representing the final hidden states after processing the entire sequence.\n    \"\"\"\n    inputs = np.array(input_sequence)\n    hidden_state = np.array([initial_hidden_state])\n    (n_timesteps, n_inputs) = inputs.shape\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    next_hidden_state = hidden_state[0]\n    for t in range(n_timesteps):\n        next_hidden_state = np.tanh(np.dot(next_hidden_state, Wh) + np.dot(inputs[t], Wx) + b)\n        hidden_state = np.vstack((hidden_state, next_hidden_state))\n    hidden_states_rounded = [np.round(hs, decimals=4).tolist() for hs in hidden_state[1:]]\n    return hidden_states_rounded\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": false, "result": "failed: shapes (3,3) and (2,3) not aligned: 3 (dim 1) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    transformed_points = np.dot(translation_matrix, points_array.T).T\n    transformed_points = transformed_points[:, :-1]\n    return transformed_points.tolist()\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    - mu_p (float): Mean of the first normal distribution.\n    - sigma_p (float): Standard deviation of the first normal distribution.\n    - mu_q (float): Mean of the second normal distribution.\n    - sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n    - float: The KL divergence between the two normal distributions.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive')\n    sigma_p = sigma_p ** 2\n    sigma_q = sigma_q ** 2\n    kl_divergence = 0.5 * (sigma_q / sigma_p + (mu_p - mu_q) ** 2 / sigma_p - 1 - np.log(sigma_q / sigma_p))\n    return kl_divergence\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a linear system Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n        A (np.array): Square coefficient matrix.\n        b (np.array): Right-hand side constant array.\n        n (int): Number of iterations.\n        x_ini (np.array, optional): Initial guess for x. If None, uses [0] * len(b).\n        \n    Returns:\n        np.array: Approximated solution vector, rounded to 4 decimals and converted to list.\n    \"\"\"\n    if x_ini is None:\n        x_prev = np.zeros_like(b)\n    else:\n        x_prev = x_ini\n    x = x_prev.copy()\n    A_dominant = A.copy()\n    for i in range(len(A)):\n        max_abs_val = abs(A_dominant[i, i])\n        max_row_idx = i\n        for j in range(len(A)):\n            if abs(A_dominant[j, i]) > max_abs_val:\n                max_abs_val = abs(A_dominant[j, i])\n                max_row_idx = j\n        if max_row_idx != i:\n            A_dominant[[i, max_row_idx]] = A_dominant[[max_row_idx, i]]\n            b[[i, max_row_idx]] = b[[max_row_idx, i]]\n        for j in range(len(A)):\n            if i != j:\n                A_dominant[i, j] /= A_dominant[i, i]\n                b[i] /= A_dominant[i, i]\n    for _ in range(n):\n        x_new = x_prev.copy()\n        for i in range(len(A)):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x_new[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        (x_prev, x) = (x, x_new)\n    return np.round(x, 4).tolist()\nA = np.array([[16.0, -3.0, 5.0, 2.0], [4.0, 17.0, -6.0, 1.0], [1.0, 2.0, 18.0, -5.0], [1.0, 1.0, 1.0, 19.0]])\nb = np.array([3.0, 11.0, 14.0, 15.0])\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b using Gaussian Elimination with Partial Pivoting.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix of the linear system.\n    b (numpy.ndarray): Constant terms matrix of the linear system.\n    \n    Returns:\n    list: The solutions to the linear system.\n    \"\"\"\n    n = len(b)\n    ab = np.column_stack((A, b))\n    for k in range(n - 1):\n        max_index = k\n        for i in range(k + 1, n):\n            if abs(ab[i, k]) > abs(ab[max_index, k]):\n                max_index = i\n        ab[[k, max_index]] = ab[[max_index, k]]\n        (b[k], b[max_index]) = (b[max_index], b[k])\n        (A[k], A[max_index]) = (A[max_index], A[k])\n        for i in range(k + 1, n):\n            factor = A[i, k] / A[k, k]\n            ab[i, :] -= factor * ab[k, :]\n            b[i] -= factor * b[k]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return x.tolist()\nA = np.array([[3, 2, -1], [6, 5, 0], [4, 3, 2]])\nb = np.array([8, 12, 7])\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: name 'hidden_states' is not defined", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def tanh(self, z):\n        return np.tanh(z)\n\n    def softmax(self, z):\n        exp_values = np.exp(z - np.max(z))\n        return exp_values / exp_values.sum(axis=0)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        timesteps = x.shape[0]\n        batch_size = x.shape[1]\n        hidden_states = []\n        h_t = np.copy(initial_hidden_state)\n        c_t = np.copy(initial_cell_state)\n        for t in range(timesteps):\n            xt = x[t]\n            xt = xt.reshape(batch_size, self.input_size)\n            input_all = np.hstack((xt, h_t))\n            f_t = self.sigmoid(np.dot(self.Wf, input_all) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, input_all) + self.bi)\n            c_t_candidate = self.tanh(np.dot(self.Wc, input_all) + self.bc)\n            c_t = f_t * c_t + i_t * c_t_candidate\n            o_t = self.sigmoid(np.dot(self.Wo, input_all) + self.bo)\n            h_t = o_t * self.tanh(c_t)\n            hidden_states.append(h_t)\n        final_hidden_state = h_t\n        final_cell_state = c_t\n        return (hidden_states, final_hidden_state, final_cell_state)\nx = np.array([[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]])\ninitial_hidden_state = np.zeros((1, 1))\ninitial_cell_state = np.zeros((1, 1))\nhidden_states = np.round([np.round(h, 4).tolist() for h in hidden_states], 4)\nfinal_hidden_state = np.round(final_hidden_state, 4).tolist()\nfinal_cell_state = np.round(final_cell_state, 4).tolist()\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    from sklearn.feature_extraction.text import CountVectorizer\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform([' '.join(doc) for doc in corpus])\n    df_matrix = X.T.tocoo()\n    df_values = np.array([len(df_matrix.row[cf]) for cf in range(len(df_matrix.row))])\n    doc_freqs = df_values.astype(float)\n    total_docs = len(corpus)\n    smoothed_doc_freqs = doc_freqs + 1\n    idf_values = np.log(total_docs / (smoothed_doc_freqs + 1))\n    idf_matrix = np.diag(idf_values)\n    query_vector = vectorizer.transform([' '.join(query)])\n    tfidf_scores = np.squeeze(np.asarray(query_vector.T.dot(idf_matrix).dot(X))).T\n    rounded_scores = np.round(tfidf_scores * 10000 // 1) / 10000\n    return np.round(rounded_scores, 4).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox'], ['the', 'dog'], ['the', 'lazy', 'dog'], ['dog', 'ran']]\nquery = ['lazy', 'fox']\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = y_true.flatten()\n    y_pred = y_pred.flatten()\n    num_positives = np.sum(y_true)\n    if num_positives == 0:\n        return 0.0\n    num_negatives = len(y_true) - num_positives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    fscore = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\n    return round(fscore, 3)\ny_true = np.array([0, 0, 0, 1, 1, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 1])\nbeta = 1\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: name 'rnn' is not defined", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, x, h_0=None):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        if h_0 is None:\n            h_0 = np.zeros((self.hidden_size, 1))\n        h_t = h_0\n        y_pred = []\n        hiddens = [h_0]\n        for t in range(x.shape[0]):\n            h_t = np.tanh(np.dot(self.W_hh, h_t) + np.dot(self.W_xh, x[t]) + self.b_h)\n            y_t = np.dot(self.W_hy, h_t) + self.b_y\n            y_pred.append(y_t)\n            hiddens.append(h_t)\n        return (np.array(y_pred), x[-1], hiddens)\n\n    def rnn_backward(self, x, y, h_0, y_pred, hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT to adjust the weights.\n        \"\"\"\n        loss = 0\n        for t in reversed(range(len(y))):\n            d_y = -(np.divide(y[t], y_pred[t]) - np.divide(1 - y[t], 1 - y_pred[t])) * 2 / self.hidden_size\n            d_b_y = np.sum(d_y, axis=1, keepdims=True)\n            d_W_hy = np.dot(d_y, hiddens[t].T)\n            d_tanh = (1 - hiddens[t] ** 2) * d_y\n            d_h_t = np.dot(self.W_hh.T, d_tanh)\n            d_b_h = np.sum(d_tanh, axis=1, keepdims=True)\n            d_W_hh = np.dot(d_tanh, hiddens[t - 1].T)\n            d_W_xh = np.dot(d_tanh, x[t].T)\n            self.b_y -= learning_rate * d_b_y\n            self.W_hy -= learning_rate * d_W_hy\n            self.b_h -= learning_rate * d_b_h\n            self.W_hh -= learning_rate * d_W_hh\n            self.W_xh -= learning_rate * d_W_xh\n            loss += np.mean(np.square(y[t] - y_pred[t]))\n        return loss / len(y)\ndef generate_sequence(length):\n    return np.random.binomial(1, 0.5, size=length)\ninput_size = 3\nhidden_size = 4\noutput_size = 2\nseq_length = 10\nx = generate_sequence(seq_length)\nexpected_output = generate_sequence(seq_length)\nh_0 = None\nloss = rnn.rnn_backward(x, expected_output, h_0, y_pred, _, learning_rate=0.01)\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rsnew = np.dot(r, r)\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n        if np.sqrt(rsnew) < tol:\n            break\n    return x.astype(float).tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (unique_labels, counts_unique) = np.unique(y, return_counts=True)\n    prob = counts_unique / len(y)\n    gini = 1 - np.sum(np.square(prob))\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    (rows, cols) = (len(dense_matrix), len(dense_matrix[0]))\n    values = []\n    cols_indices = []\n    row_pointers = [0]\n    for row_idx in range(rows):\n        for col_idx in range(cols):\n            value = dense_matrix[row_idx][col_idx]\n            if value != 0:\n                values.append(value)\n                cols_indices.append(col_idx)\n        row_pointers.append(row_pointers[-1] + len(values))\n        values.clear()\n        cols_indices.clear()\n    return (values, cols_indices, row_pointers)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    L_normalized = L / np.linalg.norm(L)\n    dot_product = np.dot(v, L_normalized)\n    projection = dot_product * L_normalized\n    projection_rounded = np.round(projection, 3).tolist()\n    return projection_rounded\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        return ([], [], [])\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the basis for the column space of the given matrix A.\n    \n    Parameters:\n    A (np.ndarray): The input matrix whose column space is to be found.\n    \n    Returns:\n    list: A list of lists representing the basis vectors of the column space of A.\n          Each inner list contains the row indices corresponding to the basis vector,\n          which are the columns of A that form a basis for the column space.\n    \"\"\"\n    A_t = A.T\n    (rref, pivot_cols) = np.linalg.qr(A_t)\n    basis_row_indices = np.where(np.abs(rref) >= 1e-09)[0]\n    basis_vectors = [A[:, index].tolist() for index in basis_row_indices]\n    return [np.round(vec, decimals=8).tolist() for vec in basis_vectors]\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) for a regression model.\n    \n    Parameters:\n    - y_true: An array-like object containing the true values of the target variable.\n    - y_pred: An array-like object containing the predicted values from the regression model.\n    \n    Returns:\n    - The R-squared value as a float rounded to 3 decimal places.\n    \"\"\"\n    numerator = np.sum((y_true - y_pred) ** 2)\n    denominator = np.sum((y_true - np.mean(y_true)) ** 2)\n    if denominator == 0:\n        return 0.0\n    r_squared_value = 1 - numerator / denominator\n    return round(r_squared_value, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n\n    Parameters:\n    img (list of list of int): A 2D matrix representing the image. Each element\n                               is a pixel value between 0 and 255.\n\n    Returns:\n    float: The average brightness of the image rounded to two decimal places.\n           Returns -1 for invalid input.\n    \"\"\"\n    if not img or not img[0]:\n        return -1\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n    total_sum = sum((sum(row) for row in img))\n    total_pixels = len(img) * row_length\n    average_brightness = total_sum / total_pixels\n    return round(average_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Square Error (RMSE) between the actual and predicted values.\n    \n    Parameters:\n        y_true (np.ndarray): An array of true values.\n        y_pred (np.ndarray): An array of predicted values.\n        \n    Returns:\n        float: The RMSE value rounded to three decimal places.\n        \n    Edge cases handled:\n        - Mismatched array shapes.\n        - Empty arrays.\n        - Invalid input types.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"The arrays 'y_true' and 'y_pred' must have the same shape.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Input arrays must be of type numpy.ndarray.')\n    errors_squared = np.square(y_true - y_pred)\n    mean_error_squared = np.mean(errors_squared)\n    rmse_value = np.sqrt(mean_error_squared)\n    return round(rmse_value, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between y_true and y_pred.\n    \n    Parameters:\n    y_true (np.ndarray): True binary labels.\n    y_pred (np.ndarray): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_true_bool = y_true.astype(bool)\n    y_pred_bool = y_pred.astype(bool)\n    intersection = np.logical_and(y_true_bool, y_pred_bool).sum()\n    union = np.logical_or(y_true_bool, y_pred_bool).sum()\n    if union == 0:\n        return 0.0\n    jaccard_index_value = intersection / union\n    return round(jaccard_index_value, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score for binary classification.\n    \n    Parameters:\n        y_true (np.array): Binary array representing true labels.\n        y_pred (np.array): Binary array representing predicted labels.\n        \n    Returns:\n        float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Input arrays must be of type np.ndarray')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays must be of the same length')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    total_pixels = y_true.sum() + y_pred.sum()\n    if total_pixels == 0:\n        return 1.0\n    dice_coeff = 2 * intersection / total_pixels\n    return round(dice_coeff, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row: A dictionary representing a dataset row, with keys as feature names and values as their corresponding values.\n    - dim: The dimensionality of the hypervectors.\n    - random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    - A composite hypervector representing the entire row.\n    \"\"\"\n    hvectors = []\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature)\n        if seed is not None:\n            hv = np.random.RandomState(seed).rand(dim)\n            hv /= np.linalg.norm(hv)\n            hvectors.append(hv)\n    composite_hv = np.mean(hvectors, axis=0).tolist()\n    return composite_hv\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n    \n    Parameters:\n    - data: A list of lists where each inner list contains [y_true, y_pred]\n            for one observation. y_true is the true label, and y_pred is the predicted label.\n            \n    Returns:\n    - A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    TP = TN = FP = FN = 0\n    for (true_label, pred_label) in data:\n        if true_label == pred_label:\n            if true_label == 1:\n                TP += 1\n            else:\n                TN += 1\n        elif pred_label == 1:\n            FP += 1\n        else:\n            FN += 1\n    cm = [[TP, FP], [FN, TN]]\n    return cm\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Parameters:\n    - v1 (numpy.ndarray): The first input vector.\n    - v2 (numpy.ndarray): The second input vector.\n\n    Returns:\n    - float: The cosine similarity between v1 and v2, rounded to 3 decimal places.\n    \"\"\"\n    v1_normalized = v1 / np.linalg.norm(v1)\n    v2_normalized = v2 / np.linalg.norm(v2)\n    dot_product = np.dot(v1_normalized, v2_normalized)\n    cosine_sim = dot_product\n    return round(cosine_sim, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates various performance metrics for a binary classification model.\n    \n    Parameters:\n    - actual: List[int] - The actual class labels (1 for positive, 0 for negative).\n    - predicted: List[int] - The predicted class labels from the model.\n    \n    Returns:\n    - A tuple containing:\n      - confusion_matrix: A 2x2 matrix.\n      - accuracy: A float representing the accuracy of the model.\n      - f1_score: A float representing the F1 score of the model.\n      - specificity: A float representing the specificity of the model.\n      - negative_predictive_value: A float representing the negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must have the same length.')\n    actual_counter = Counter(actual)\n    predicted_counter = Counter(predicted)\n    true_positives = sum((a == p == 1 for (a, p) in zip(actual, predicted)))\n    false_negatives = actual_counter.get(0, 0) - true_positives\n    false_positives = predicted_counter.get(0, 0) - true_positives\n    true_negatives = actual_counter.get(1, 0) - false_negatives\n    confusion_matrix = [[true_positives, false_negatives], [false_positives, true_negatives]]\n    total_samples = len(actual)\n    if total_samples == 0:\n        raise ValueError('Cannot calculate metrics with zero samples.')\n    accuracy = (true_positives + true_negatives) / total_samples\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / (true_positives + false_negatives)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    specificity = true_negatives / (true_negatives + false_positives)\n    negative_predictive_value = true_negatives / (true_negatives + false_negatives)\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n        data (list or numpy.ndarray): List or Numpy array of numerical values.\n        \n    Returns:\n        dict: A dictionary containing mean, median, mode, variance, standard deviation,\n              percentiles (25th, 50th, 75th), and interquartile range (IQR) of the data.\n    \"\"\"\n    data = np.array(data)\n    mean_value = round(np.mean(data), 4)\n    median_value = round(np.median(data), 4)\n    variance_value = round(np.var(data), 4)\n    std_deviation_value = round(np.std(data), 4)\n    percentile_25 = round(np.percentile(data, 25), 4)\n    percentile_50 = round(np.percentile(data, 50), 4)\n    percentile_75 = round(np.percentile(data, 75), 4)\n    iqr_value = round(percentile_75 - percentile_25, 4)\n    mode_value = int(round(np.bincount(data).argmax(), 0))\n    result_dict = {'mean': mean_value, 'median': median_value, 'mode': mode_value, 'variance': variance_value, 'standard_deviation': std_deviation_value, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr_value}\n    return result_dict\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    probability = math.comb(n, k) * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(math.pow(x - mean, 2) / (2 * math.pow(std_dev, 2)))\n    pdf_value = 1 / (std_dev * math.sqrt(2 * math.pi)) * math.exp(exponent)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if k < 0:\n        raise ValueError('k must be a non-negative integer')\n    exponent = math.exp(-lam)\n    numerator = lam ** k\n    denominator = math.factorial(k)\n    result = exponent * (numerator / denominator)\n    return round(result, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    The contrast is calculated as the difference between the maximum and minimum pixel values.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: Contrast value.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or len(img.shape) != 2:\n        raise ValueError('Input must be a 2D numpy array.')\n    if np.any(img < 0) or np.any(img > 255):\n        raise ValueError('Pixel values must be between 0 and 255.')\n    return np.ptp(img.flatten())\nimg = np.array([[100, 150, 200], [30, 100, 150]], dtype=np.uint8)\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    result = np.dot(vec1, vec2)\n    return result\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": false, "result": "failed: 'int' object is not iterable", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists representing the polynomial transformed features.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    from itertools import product\n    power_combinations = [list(map(pow, data_array, power)) for power in range(1, degree + 1)]\n    flattened_features = [item for sublist in power_combinations for item in sublist]\n    rounded_features = np.round(flattened_features, 8).tolist()\n    return rounded_features\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int) -> np.ndarray:\n    \"\"\"\n    Calculate the positional encoding for a given position and dimensionality.\n    \n    Args:\n    - position (int): The position at which to calculate the encoding.\n    - d_model (int): The dimensionality of the encoding vector.\n    \n    Returns:\n    - np.ndarray: A 1D array containing the positional encoding, or -1 if position is 0 or d_model <= 0.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1 * np.ones(1, dtype=np.float16)\n    result = np.zeros(d_model, dtype=np.float16)\n    for i in range(0, d_model, 2):\n        freq = np.sin(position / 10000 ** (i / d_model))\n        result[i] = freq\n        if i + 1 < d_model:\n            freq = np.cos(position / 10000 ** ((i + 1) / d_model))\n            result[i + 1] = freq\n    return result.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if abs(training_accuracy - test_accuracy) > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: 'float' object is not iterable", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m_corrected = m / (1 - beta1 ** (t + 1))\n    v_corrected = v / (1 - beta2 ** (t + 1))\n    parameter_updated = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * grad ** 2\n    return ([round(param, 5) for param in parameter_updated.tolist()], [round(param, 5) for param in m_new.tolist()], [round(param, 5) for param in v_new.tolist()])\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: unhashable type: 'numpy.ndarray'", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    tokens = encoder.encode(prompt)\n    prompt_len = len(tokens)\n    wte = params['wte']\n    wpe = params['wpe']\n    ln_f_g = params['ln_f']['g']\n    ln_f_b = params['ln_f']['b']\n    position_ids = np.arange(hparams['n_ctx'])[None, :]\n    position_embeddings = (position_ids + prompt_len) % hparams['n_ctx']\n    pos_embedding = wpe[position_embeddings]\n    token_embeddings = wte[tokens]\n    all_head_size = wte.shape[0]\n    attn_weights = np.random.rand(hparams['n_head'], token_embeddings.shape[1], all_head_size, wpe.shape[1])\n    ffn_output = token_embeddings + pos_embedding\n    ffn_output = ffn_output * ln_f_g + ln_f_b\n    final_output = ffn_output * ln_f_g + ln_f_b\n    generated_tokens = tokens.copy()\n    for _ in range(n_tokens_to_generate):\n        attention_scores = np.random.rand(hparams['n_head'], token_embeddings.shape[1], all_head_size, wpe.shape[1])\n        attn_weights[:, :, :, :] = attention_scores\n        attn_output = token_embeddings\n        ffn_output = attn_output + pos_embedding\n        ffn_output = ffn_output * ln_f_g + ln_f_b\n        final_output = ffn_output * ln_f_g + ln_f_b\n        next_token_indices = np.argmax(attn_weights[:, -1, :], axis=0)\n        generated_tokens.append(next_token_indices)\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: type numpy.ndarray doesn't define __round__ method", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_values = np.exp(values - np.max(values))\n        return e_values / e_values.sum()\n    matrix = np.zeros((n, n, dimension))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                matrix[i, j, :] = [1] * dimension\n            else:\n                matrix[i, j, :] = crystal_values[j] / np.linalg.norm(crystal_values[j])\n    context_vector = np.mean(matrix, axis=0)\n    attention_scores = softmax(np.dot(matrix, context_vector[:, np.newaxis]))\n    final_patterns = np.zeros_like(matrix)\n    for i in range(n):\n        for j in range(dimension):\n            final_patterns[i, :, j] = (matrix[i, :, j] * attention_scores[i]).sum() * context_vector[j]\n    return [round(pattern, 4) for row in final_patterns for pattern in row]\nn = 3\ncrystal_values = [np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])]\ndimension = 3\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: name 'term' is not defined", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\nfrom math import log\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate the BM25 scores for documents given a query.\n    \n    :param corpus: List of documents where each doc is a list of terms (words).\n    :param query: Query string, split into terms.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: A list containing BM25 scores rounded to 3 decimal places for each document.\n    \"\"\"\n    m = len(corpus)\n    n_docs_with_term = Counter((term for document in corpus for term in document))\n    avgdl = sum((len(doc) for doc in corpus)) / m\n\n    def bm25_score(term_freq, doc_length, qtf, N, k1=k1, b=b):\n        return (k1 + 1) * term_freq / (k1 * (1 - b + b * doc_length / avgdl) + term_freq) * (N - n_docs_with_term[term] + 0.5) / (n_docs_with_term[term] + 0.5) * log((N - n_docs_with_term[term] + 0.5) / (n_docs_with_term[term] + 0.5), 10)\n    scores = []\n    query_tfs = Counter(query)\n    for doc in corpus:\n        doc_tf = Counter(doc)\n        score = sum((bm25_score(doc_tf[term], len(doc), query_tfs[term], m) for term in set(query) & set(doc)))\n        scores.append(round(score, 3))\n    return scores\ncorpus = [['the', 'quick', 'brown', 'fox'], ['a', 'lazy', 'dog'], ['the', 'quick', 'dog'], ['a', 'quick', 'fox']]\nquery = 'the quick'\nk1 = 1.5\nb = 0.75\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp == 0:\n        return 0.0\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return round(f1_score, 3)\ny_true = [1, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 1, 1, 0, 0, 1, 1]\nf1_score = calculate_f1_score(y_true, y_pred)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": false, "result": "failed: index 14 is out of bounds for axis 0 with size 10", "solution": "import math\nPI = 3.14159\nimport numpy as np\nimport math\nimport numpy as np\nimport math\ndef power_grid_forecast(consumption_data):\n    daily_fluctuation = 10 * np.sin(2 * math.pi * np.arange(1, 11) / 10)\n    detrended_consumption = consumption_data - daily_fluctuation\n    coefficients = np.polyfit(np.arange(len(detrended_consumption)), detrended_consumption, 1)\n    linear_regression_model = np.poly1d(coefficients)\n    predicted_base_consumption = linear_regression_model(14)\n    fluctuation_15 = daily_fluctuation[14]\n    predicted_consumption_15 = predicted_base_consumption + fluctuation_15\n    prediction_rounded = round(predicted_consumption_15)\n    safety_margin = prediction_rounded * 1.05\n    rounded_final_prediction = math.ceil(safety_margin)\n    return rounded_final_prediction\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    abs_diff = np.abs(y_true - y_pred)\n    mae_value = np.mean(abs_diff)\n    return round(mae_value, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: name 'X' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute query (Q), key (K), value (V) vectors using input matrix X and weight matrices W_q, W_k, and W_v.\n    \n    Parameters:\n    - X (np.ndarray): Input matrix of shape (batch_size, seq_len, embed_dim).\n    - W_q (np.ndarray): Weight matrix for Q of shape (embed_dim, embed_dim_heads * n_heads).\n    - W_k (np.ndarray): Weight matrix for K of shape (embed_dim, embed_dim_heads * n_heads).\n    - W_v (np.ndarray): Weight matrix for V of shape (embed_dim, embed_dim_heads * n_heads).\n    - n_heads (int): Number of heads.\n    \n    Returns:\n    - q (np.ndarray): Query vectors of shape (batch_size, seq_len, embed_dim_heads).\n    - k (np.ndarray): Key vectors of shape (batch_size, seq_len, embed_dim_heads).\n    - v (np.ndarray): Value vectors of shape (batch_size, seq_len, embed_dim_heads).\n    \"\"\"\n    (batch_size, seq_len, embed_dim) = X.shape\n    embed_dim_heads = embed_dim // n_heads\n    scale_factor = np.sqrt(embed_dim_heads)\n    X_permuted = X.reshape(batch_size * seq_len, embed_dim)\n    Q = np.dot(X_permuted, W_q).reshape(batch_size, seq_len, embed_dim_heads)\n    K = np.dot(X_permuted, W_k).reshape(batch_size, seq_len, embed_dim_heads)\n    V = np.dot(X_permuted, W_v).reshape(batch_size, seq_len, embed_dim_heads)\n    scale_Q = Q / scale_factor\n    dot_product = np.matmul(scale_Q, K.T)\n    return (Q, K, V, dot_product)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, dot_product: np.ndarray, mask: np.ndarray=None) -> np.ndarray:\n    \"\"\"\n    Apply softmax to get scaled scores, then multiply with values and transpose the dimensions.\n    \n    Parameters:\n    - Q (np.ndarray): Query vectors.\n    - K (np.ndarray): Key vectors.\n    - V (np.ndarray): Value vectors.\n    - dot_product (np.ndarray): Scaled dot product.\n    - mask (np.ndarray): Padding mask (optional).\n    \n    Returns:\n    - attn_output (np.ndarray): Attention output of shape (batch_size, seq_len, embed_dim_heads).\n    \"\"\"\n    if mask is not None:\n        dot_product *= mask\n    scores = np.exp(dot_product - np.max(dot_product, axis=1, keepdims=True))\n    normalized_scores = scores / np.sum(scores, axis=-1, keepdims=True)\n    attn_output = np.matmul(normalized_scores, V)\n    return attn_output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention on the input matrix X.\n    \n    Parameters:\n    - X (np.ndarray): Input matrix of shape (batch_size, seq_len, embed_dim).\n    - W_q (np.ndarray): Weight matrix for Q of shape (embed_dim, embed_dim_heads * n_heads).\n    - W_k (np.ndarray): Weight matrix for K of shape (embed_dim, embed_dim_heads * n_heads).\n    - W_v (np.ndarray): Weight matrix for V of shape (embed_dim, embed_dim_heads * n_heads).\n    - n_heads (int): Number of heads.\n    \n    Returns:\n    - attn_output_list (list): List of attention outputs, each reshaped back to the original sequence length.\n    \"\"\"\n    (Q, K, V, dot_product) = compute_qkv(X, W_q, W_k, W_v, n_heads)\n    attn_output = self_attention(Q, K, V, dot_product)\n    attn_output_list = [attn_output]\n    for i in range(n_heads):\n        attn_output_list.append(attn_output[:, :, i].reshape(-1, X.shape[1]))\n    return [np.round(x, 4).tolist() for x in attn_output_list]\nX = X.astype(np.float32)\nn_heads = 2\nW_q = np.random.randn(X.shape[-1], X.shape[-1] * n_heads)\nW_k = np.random.randn(X.shape[-1], X.shape[-1] * n_heads)\nW_v = np.random.randn(X.shape[-1], X.shape[-1] * n_heads)\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must be of the same length')\n    count = [[0, 0], [0, 0]]\n    for i in range(len(x)):\n        if x[i] == 0 and y[i] == 0:\n            count[0][0] += 1\n        elif x[i] == 0 and y[i] == 1:\n            count[0][1] += 1\n        elif x[i] == 1 and y[i] == 0:\n            count[1][0] += 1\n        else:\n            count[1][1] += 1\n    n = sum([sum(row) for row in count])\n    p00 = count[0][0] / n\n    p01 = count[0][1] / n\n    p10 = count[1][0] / n\n    p11 = count[1][1] / n\n    phi = (p00 * p11 - p01 * p10) ** 2 / (p00 + p01) / (p10 + p11) / (p00 + p10) / (p01 + p11)\n    return round(phi, 4)\nx = [0, 0, 1, 1, 1]\ny = [0, 1, 0, 1, 1]\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport numpy as np\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    y = np.clip(x + 3, 0, 6) / 6\n    return y\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * x\n    else:\n        return x\nx = -3.0\nalpha = 0.5\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if math.isinf(x):\n        return 0.0\n    elif x > 390.6227:\n        return float('inf')\n    else:\n        result = math.log(1.0 + math.exp(x))\n        return round(result, 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for /: 'list' and 'list'", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, a_min=None, a_max=1 / epsilon)\n    clipped_inv_rhos = 1 / clipped_rhos\n    objective_value = 0.0\n    for (rho, adv, pi_theta_old_val, pi_theta_ref_val, inv_rho) in zip(rhos, A, pi_theta_old, pi_theta_ref, clipped_inv_rhos):\n        term1 = inv_rho * adv * np.log(inv_rho * pi_theta_ref_val / pi_theta_old_val)\n        term2 = adv * inv_rho * (inv_rho * pi_theta_ref_val / pi_theta_old_val - 1)\n        objective_value += term1 + term2\n    kl_penalty = beta * np.mean(np.abs(np.log(pi_theta_ref / pi_theta_old)))\n    return round(objective_value + kl_penalty, 6)\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": false, "result": "failed: test case failed: swish(1)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to 4 decimal places\n    \"\"\"\n    return round(math.pow(np.clip(x, 0, None), 0.5) * x, 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": false, "result": "failed: test case failed: selu(1.0)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    return scale * np.where(x >= 0, x, alpha * np.exp(x) - alpha)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": false, "result": "failed: test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = [1 if prob > 0.5 else 0 for prob in probabilities]\n    return predictions\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 5)", "solution": "import numpy as np\nimport numpy as np\ndef softmax(X):\n    \"\"\"Compute the softmax values for each row of X.\"\"\"\n    e_x = np.exp(X - np.max(X, axis=1, keepdims=True))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef cross_entropy_loss(y_true, y_pred):\n    \"\"\"Cross entropy loss function.\"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n    return -np.sum(np.log(y_pred[range(len(y_true)), y_true])) / len(y_true)\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    (num_samples, num_features) = X.shape\n    num_classes = y.max() + 1\n    W = np.zeros((num_classes, num_features))\n    b = np.zeros(num_classes)\n    losses = []\n    for i in range(iterations):\n        scores = X @ W + b\n        y_pred = softmax(scores)\n        loss = cross_entropy_loss(y, np.argmax(y_pred, axis=1))\n        losses.append(round(loss, 4))\n        grad_W = 1 / num_samples * (X.T @ (y_pred - y))\n        grad_b = 1 / num_samples * np.sum(y_pred - y, axis=0)\n        W -= learning_rate * grad_W\n        b -= learning_rate * grad_b\n        if i % 10 == 0 or i == iterations - 1:\n            print(f'Iteration {i + 1}/{iterations}, Loss: {loss}')\n    return ([W.tolist(), b.tolist()], losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (4,) (5,) ", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[np.ndarray]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    Args:\n    - X : np.ndarray, features matrix of shape (n_samples, n_features)\n    - y : np.ndarray, target vector of shape (n_samples,)\n    - learning_rate : float\n    - iterations : int\n    \n    Returns:\n    - np.ndarray : Optimized coefficients of the logistic regression model, shape (n_features,)\n    - np.ndarray : Loss values over iterations, shape (iterations,)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.random.rand(n_features)\n    bias = 0\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, weights) + bias\n        h = 1 / (1 + np.exp(-z))\n        cost = -y * np.log(h) - (1 - y) * np.log(1 - h)\n        avg_cost = np.mean(cost)\n        dw = 1 / n_samples * np.dot(X.T, h - y)\n        db = 1 / n_samples * np.sum(h - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        losses.append(avg_cost)\n        if i % 10 == 0:\n            print(f'Iteration {i}: Cost {avg_cost:.4f}')\n    losses = np.array(losses).reshape((-1, 1)).round(4).tolist()\n    return (weights, losses)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 1])\nlearning_rate = 0.1\niterations = 100\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: tuple index out of range", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.matmul(Q, K.T) / np.sqrt(K.shape[2])\n    scores *= mask\n    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights /= np.sum(weights, axis=-1, keepdims=True) + 1e-10\n    output = np.matmul(weights, V)\n    return output.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([0,0,0,0])", "solution": "\nimport numpy as np\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    apple_colors = np.array(apples)\n    normalized_colors = (apple_colors - np.min(apple_colors)) / (np.max(apple_colors) - np.min(apple_colors))\n    num_unique_colors = len(np.unique(normalized_colors))\n    total_apples = len(apples)\n    disorder_value = -np.sum((normalized_colors * np.log2(normalized_colors + 1e-10)).sum() / np.log2(total_apples))\n    rounded_disorder_value = round(disorder_value, 4)\n    return rounded_disorder_value\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform Layer Normalization on a 3D array (batch_size x sequence_length x feature_dim).\n    \n    Parameters:\n        X (np.ndarray): The input tensor of shape (batch_size, sequence_length, feature_dim).\n        gamma (np.ndarray): The scale parameter of shape (feature_dim, ).\n        beta (np.ndarray): The shift parameter of shape (feature_dim, ).\n        epsilon (float): A small value to avoid division by zero in normalization step.\n        \n    Returns:\n        list: The normalized output tensor rounded to 5 decimal places.\n    \"\"\"\n    (batch_size, sequence_length, feature_dim) = X.shape\n    mean = np.mean(X, axis=1, keepdims=True)\n    var = np.var(X, axis=1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(var + epsilon)\n    normalized_X = gamma * normalized_X + beta\n    return [round(val, 5) for val in normalized_X.reshape(-1)]\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: division by zero", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the Meteor (Meteor) score between two sentences - a reference sentence and a candidate translation.\n    \n    Parameters:\n    reference (str): The reference (correct) translation.\n    candidate (str): The candidate translation to evaluate.\n    alpha (float): Weighting factor for the penalty term.\n    beta (float): Weighting factor for the similarity ratio.\n    gamma (float): Penalty coefficient for word insertion penalty.\n    \n    Returns:\n    float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n\n    def _edit_distance(s1, s2):\n        (m, n) = (len(s1), len(s2))\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(m + 1):\n            for j in range(n + 1):\n                if i == 0:\n                    dp[i][j] = j\n                elif j == 0:\n                    dp[i][j] = i\n                else:\n                    dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + (1 if s1[i - 1] != s2[j - 1] else 0))\n        return dp[m][n]\n\n    def _unigram_match(s1, s2):\n        return sum((Counter(s1.split()) & Counter(s2.split())).values())\n\n    def _gram_match(s1, s2, gram_size):\n        pass\n\n    def _sentence_boundary_distance(s1, s2):\n        return sum((w not in s2 for w in s1.split()))\n    similarity_ratio = _unigram_match(reference, candidate) / max(_unigram_match(reference, ' '), _unigram_match(candidate, ' '))\n    length_ratio = min(len(reference), len(candidate)) / max(len(reference), len(candidate))\n    edit_dist = _edit_distance(reference, candidate)\n    n_words = max(len(reference), len(candidate))\n    p_unigram = _unigram_match(reference, candidate) / n_words\n    r_unigram = _unigram_match(candidate, reference) / n_words\n    d = edit_dist / n_words\n    P = similarity_ratio * (1 - alpha * d) ** beta\n    R = similarity_ratio * (1 - beta * d) ** alpha\n    F1 = (gamma * P + (1 - gamma) * R) / gamma\n    return round(F1, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Calculate the Pointwise Mutual Information (PMI).\n    \n    Parameters:\n    - joint_counts: The count of occurrences of both events happening together.\n    - total_counts_x: The count of occurrences of event x.\n    - total_counts_y: The count of occurrences of event y.\n    - total_samples: The total number of samples or observations.\n    \n    Returns:\n    A float representing the Pointwise Mutual Information, rounded to 3 decimal places.\n    \"\"\"\n    expected_joint = total_counts_x / total_samples * (total_counts_y / total_samples)\n    pmi = np.log2(joint_counts / total_samples / expected_joint)\n    return round(pmi, 3)\njoint_counts = 100\ntotal_counts_x = 500\ntotal_counts_y = 400\ntotal_samples = 1000\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\ndef min_max(x: List[int]) -> List[float]:\n    \"\"\"\n    Perform Min-Max normalization on a list of integers.\n    \n    Args:\n        x (List[int]): A list of integer values to be normalized.\n        \n    Returns:\n        List[float]: A list of floating-point numbers scaled between 0 and 1.\n    \"\"\"\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized_x = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized_x]\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Creates a simple residual block using NumPy.\n    \n    Args:\n    x (np.ndarray): A 1D input array.\n    w1 (np.ndarray): Weight matrix for the first layer.\n    w2 (np.ndarray): Weight matrix for the second layer.\n    \n    Returns:\n    list: A list containing the final output of the residual block, rounded to 4 decimal places.\n    \"\"\"\n    h = np.dot(x, w1)\n    h_relu = np.maximum(h, 0)\n    y = np.dot(h_relu, w2)\n    shortcut = x\n    out = y + shortcut\n    out_relu = np.maximum(out, 0)\n    return [round(elem, 4) for elem in out_relu.tolist()]\nw1 = np.array([[0.3, 0.7], [0.4, -0.4]]).astype(np.float32)\nw2 = np.array([[-0.1, 0.5], [0.6, 0.2]]).astype(np.float32)\nx = np.array([1.0, 2.0]).astype(np.float32)\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D input array.\n\n    Parameters:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: A 1D NumPy array of shape (channels,) with the average value of each feature map.\n    \"\"\"\n    num_elements = x.shape[0] * x.shape[1]\n    flattened_sum = np.sum(x)\n    avg_values = flattened_sum / num_elements\n    return avg_values\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array X representing a batch of feature maps in the BCHW format.\n    \n    Parameters:\n    - X: np.ndarray, 4D array representing the batch of feature maps (batch, channels, height, width)\n    - gamma: np.ndarray, scale parameter for the normalized data\n    - beta: np.ndarray, shift parameter for the normalized data\n    - epsilon: float, small value added to the variance for numerical stability\n    \n    Returns:\n    - A list of floats representing the normalized feature maps, rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(1, 2, 3), keepdims=True)\n    var = np.var(X, axis=(1, 2, 3), keepdims=True)\n    normalized_data = (X - mean) / np.sqrt(var + epsilon)\n    normalized_data = gamma * normalized_data + beta\n    return [np.round(normalized_data[i], 4).tolist() for i in range(normalized_data.shape[0])]\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which to evaluate the derivative.\n    n (float): Exponent of the polynomial term.\n    \n    Returns:\n    float: Value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a given list of 2D vectors using the Gram-Schmidt process.\n    :param vectors: List of 2D vectors represented as lists [x, y].\n    :param tol: Tolerance value for determining linear independence.\n    :return: A list of orthonormal vectors as lists [x, y] rounded to 4 decimal places.\n    \"\"\"\n    vectors = np.array(vectors)\n    orthonormal_vectors = apply_gram_schmidt(vectors, tol)\n    return [[round(val, 4) for val in vec] for vec in orthonormal_vectors]\ndef apply_gram_schmidt(vectors, tol):\n    v1 = vectors[0]\n    u1 = v1 / np.linalg.norm(v1)\n    orthonormal_vectors = [u1]\n    for i in range(1, len(vectors)):\n        ui = vectors[i]\n        for j in range(i):\n            ui -= np.dot(ui, orthonormal_vectors[j]) * orthonormal_vectors[j]\n        ui = ui / np.linalg.norm(ui)\n        orthonormal_vectors.append(ui)\n    return orthonormal_vectors\nexample_vectors = [[-1, 1], [2, 3], [-3, -1]]\northonormal_vectors = orthonormal_basis(example_vectors)\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n        a (np.array): A numpy array representing the first vector.\n        b (np.array): A numpy array representing the second vector.\n        \n    Returns:\n        list: A list representing the cross product of a and b, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    cross_result = np.cross(a, b)\n    return np.round(cross_result, 4).tolist()\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": false, "result": "failed: list indices must be integers or slices, not tuple", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Cramer's Rule.\n    \n    :param A: Numpy array representing the square coefficient matrix A\n    :param b: Numpy array representing the constant vector b\n    :return: Solution vector x if a unique solution exists, or -1 if not.\n    \"\"\"\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        if det_Ai != 0:\n            x[i] = det_Ai / det_A\n        else:\n            return -1\n    return (round(x[0], 4), round(x[1], 4) if len(x) > 1 else round(x[0], 4))\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    p_sum = np.sum(p)\n    q_sum = np.sum(q)\n    normalized_p = p / p_sum\n    normalized_q = q / q_sum\n    bc = -np.sum(np.sqrt(normalized_p * normalized_q))\n    bdist = -np.log(np.sqrt(bc))\n    return round(bdist, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a: A list of integers or floats.\n        b: A list of integers or floats of the same length as `a`.\n\n    Returns:\n        list[int|float]: A list representing the element-wise sum of `a` and `b`.\n        If the vectors do not have the same length, returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: local variable 'state' referenced before assignment", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list[float]:\n    \"\"\"\n    Computes the average gradient of the log-policy multiplied by the return at each time step \n    using the REINFORCE algorithm.\n    \n    Args:\n    - theta: A 2D numpy array of shape (num_states, num_actions) representing the policy parameters.\n    - episodes: A list of lists, where each sublist represents an episode and contains tuples of (state, action, reward).\n    \n    Returns:\n    - A list of floats representing the computed gradient values.\n    \"\"\"\n    episode_length = len(episodes[0])\n    (num_states, num_actions) = theta.shape\n    advantages = np.zeros(episode_length)\n    returns = np.zeros(episode_length)\n    for i in reversed(range(episode_length)):\n        if i == episode_length - 1:\n            returns[i] = advantages[i]\n        else:\n            next_return = returns[i + 1]\n            returns[i] = advantage = next_return * np.dot(theta[state, :], [1, 0]) if i < episode_length - 1 else advantage\n    for i in range(episode_length):\n        G = returns[i]\n        for j in range(i + 1, episode_length):\n            G = returns[j] = returns[j] * (1 - 0.99) + returns[j] * 0.01\n        advantages[i] = G\n    state_values = np.dot(theta, [1, 0])\n    probabilities = np.exp(state_values) / np.sum(np.exp(state_values), axis=0)\n    log_policies = np.log(probabilities)\n    gradients = np.zeros_like(theta)\n    for (state, _, reward) in episodes:\n        gradients[state] += (log_policies[state] - np.log(probabilities[state])) * advantages[i]\n    norms = np.linalg.norm(gradients, axis=1)\n    norms[norms == 0] = 1\n    normalized_gradients = gradients / norms[:, None]\n    result = [round(float(value), 4) for value in normalized_gradients.flatten().tolist()]\n    return result\nepisodes = [[(0, 0, 1), (0, 1, 2), (1, 0, 3)], [(0, 0, 5), (1, 1, 6), (0, 0, 7)]]\ntheta = np.array([[0.7, 0.3], [0.4, 0.6]])\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Function to calculate the computational efficiency of an MoE (Mixture-of-Experts) layer compared to a dense layer.\n    Computes the floating-point operations (FLOPs) for both layers with given parameters:\n        n_experts: Number of experts in the MoE layer\n        k_active: Number of active experts in the MoE layer\n        d_in: Input dimension size\n        d_out: Output dimension size\n    \n    Returns the FLOP savings percentage rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out + d_in * d_out\n    flops_moe_expert = 3 * d_in * d_out\n    flops_moe_active = k_active * flops_moe_expert\n    flops_moe = flops_moe_active\n    flops_savings = (flops_dense - flops_moe) / flops_dense * 100\n    efficiency_percentage = round(flops_savings, 1)\n    return efficiency_percentage\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Computes the noisy top-k gating probabilities for each element in the input matrix X.\n    \n    Args:\n        X (np.ndarray): Input matrix of shape (n_samples, n_features).\n        W_g (np.ndarray): Weight matrix for calculating gating probabilities, of shape (n_features, n_experts).\n        W_noise (np.ndarray): Noise weights for each row in X, of shape (n_samples, n_features).\n        N (np.ndarray): Pre-sampled noise array, of shape (n_samples, n_features).\n        k (int): The number of top experts to gate each sample.\n        \n    Returns:\n        list: A list of expert indices for each sample in X, rounded to the nearest 4th decimal place.\n    \"\"\"\n    weighted_sum = np.dot(X, W_g) + np.dot(N, W_noise)\n    max_indices = np.argsort(weighted_sum, axis=1)[:, -k:]\n    unique_max_indices = np.unique(max_indices, axis=1)\n    result = []\n    for indices in unique_max_indices:\n        rounded_indices = np.round(indices / k, decimals=4).tolist()\n        result.append(rounded_indices)\n    return result\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,2) (2,3,2) ", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    \"\"\"\n    Compute the Mixture of Experts (MoE) output given the input tensor x, \n    expert weight matrices We, gating weight matrix Wg, number of experts n_experts, and top-k value.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, seq_len, input_dim)\n    - We: Expert weight matrices, a list of n_experts 2D arrays of shape (input_dim, hidden_dim)\n    - Wg: Gating weight matrix, a 2D array of shape (hidden_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Top-k value for selecting experts per token\n    \n    Returns:\n    - A list of shape (batch_size, seq_len) with the MoE output, each entry rounded to 4 decimals.\n    \"\"\"\n\n    def softmax(x):\n        \"\"\"Compute the softmax of vector x.\"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)\n    (batch_size, seq_len, input_dim) = x.shape\n    hidden_dim = We[0].shape[1]\n    gating_scores = np.zeros((batch_size, seq_len, n_experts), dtype=np.float32)\n    expert_outputs = np.zeros((batch_size, seq_len, n_experts, hidden_dim), dtype=np.float32)\n    for i in range(n_experts):\n        gating_scores[:, :, i] = np.dot(x.reshape(batch_size * seq_len, input_dim), Wg[:, i]) + np.sum(We[i] * x, axis=-1)\n    for i in range(n_experts):\n        expert_outputs[:, :, i, :] = np.dot(x.reshape(batch_size * seq_len, input_dim), We[i].T).reshape(batch_size, seq_len, hidden_dim)\n    gating_probabilities = softmax(gating_scores.reshape(-1, n_experts)).reshape(batch_size, seq_len, n_experts)\n    topk_indices = np.argpartition(gating_probabilities, -top_k, axis=-1)[:, :, -top_k:]\n    selected_experts = np.take_along_axis(expert_outputs, topk_indices, axis=-2)\n    aggregated_output = np.sum(selected_experts * gating_probabilities, axis=-2)\n    rounded_output = np.round(aggregated_output, 4).tolist()\n    return rounded_output\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,) (1,4,4,4) ", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Apply Group Normalization on the input tensor X.\n    \n    Parameters:\n        X (np.ndarray): Input data with shape (batch_size, num_channels, height, width).\n        gamma (np.ndarray): Scale factor array for each group.\n        beta (np.ndarray): Offset/bias term array for each group.\n        num_groups (int): Number of groups to split the channels into.\n        epsilon (float): Small value added to variance to avoid division by zero.\n        \n    Returns:\n        list: Result after applying Group Normalization, reshaped and rounded to the nearest 4th decimal point.\n    \"\"\"\n    (batch_size, num_channels, height, width) = X.shape\n    channels_per_group = num_channels // num_groups\n    X_aggregated = X.reshape(batch_size, num_groups, channels_per_group, height, width)\n    mean = np.mean(X_aggregated, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_aggregated, axis=(2, 3, 4), keepdims=True)\n    normalized_X = (X_aggregated - mean) / np.sqrt(var + epsilon)\n    normalized_X = normalized_X.reshape(batch_size, num_channels, height, width)\n    result = gamma * normalized_X + beta\n    return result.astype(np.float32).reshape(-1).round(4).tolist()\nX = np.array([[[[1.0, -1.0, 1.0, -1.0], [1.0, -1.0, 1.0, -1.0], [1.0, -1.0, 1.0, -1.0], [1.0, -1.0, 1.0, -1.0]], [[-1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, 1.0]], [[1.0, -1.0, 1.0, -1.0], [1.0, -1.0, 1.0, -1.0], [1.0, -1.0, 1.0, -1.0], [1.0, -1.0, 1.0, -1.0]], [[1.0, -1.0, 1.0, -1.0], [1.0, -1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, 1.0]]]], dtype=np.float32)\ngamma = np.array([1, 1], dtype=np.float32)\nbeta = np.array([0, 0], dtype=np.float32)\nnum_groups = 2\nepsilon = 1e-05\nresult = group_normalization(X, gamma, beta, num_groups, epsilon)\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def gradient_f(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    prev_x = x + 1\n    iteration = 0\n    while np.abs(prev_x - x) > tolerance and iteration < max_iters:\n        prev_x = x\n        x = x - learning_rate * gradient_f(prev_x)\n        iteration += 1\n    return round(x, 4)\nstart_x = np.random.uniform(-5, 5)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: axis 3 is out of bounds for array of dimension 3", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float=10.0, gamma: float=50.0, beta: float=2.0) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh activation function to the input tensor x.\n    \n    The function normalizes the tensor using a learned scale (gamma) and shift (beta),\n    then applies tanh activation followed by a transformation with learned parameters alpha and beta.\n    \n    Args:\n        x (np.ndarray): Input tensor of shape (batch_size, channels, height, width).\n        alpha (float, optional): Scale parameter for dynamic tanh transformation. Defaults to 10.0.\n        gamma (float, optional): Learnable scaling factor for normalization. Defaults to 50.0.\n        beta (float, optional): Learnable offset for normalization. Defaults to 2.0.\n        \n    Returns:\n        list[float]: A list of floats representing the transformed values from the input tensor.\n    \"\"\"\n    mean = x.mean(axis=(1, 2, 3), keepdims=True)\n    std = x.std(axis=(1, 2, 3), keepdims=True)\n    normalized_x = (x - mean) / std\n    normalized_x = gamma * normalized_x + beta\n    transformed_x = alpha * np.tanh(normalized_x)\n    return transformed_x.tolist()\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
