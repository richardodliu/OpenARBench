{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the matrix and vector dimensions are compatible\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Calculate the dot product\n    result = []\n    for row in a:\n        row_result = sum(x * y for x, y in zip(row, b))\n        result.append(row_result)\n    \n    return result\n\n# Example usage:\n# Define a 3x2 matrix and a vector of length 2\nmatrix = [[1, 2], [3, 4], [5, 6]]\nvector = [2, 3]\n\n# Compute the dot product\noutput = matrix_dot_vector(matrix, vector)\nprint(output)  # Expected output: [8, 18, 28]\n```"}
{"task_id": 2, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that defines a function to compute the transpose of a given matrix:\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transposes the given matrix.\n\n    Parameters:\n    a (list[list[int|float]]): The input matrix.\n\n    Returns:\n    list[list[int|float]]: The transposed matrix.\n    \"\"\"\n    # Get the number of rows and columns\n    rows = len(a)\n    cols = len(a[0]) if rows > 0 else 0\n\n    # Initialize the transposed matrix with zeros\n    transposed = [[0 for _ in range(rows)] for _ in range(cols)]\n\n    # Fill the transposed matrix\n    for i in range(rows):\n        for j in range(cols):\n            transposed[j][i] = a[i][j]\n\n    return transposed\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a sample matrix\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n\n    # Compute the transpose\n    transposed_matrix = transpose_matrix(matrix)\n\n    # Print the original and transposed matrices\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    print(\"\\nTransposed Matrix:\")\n    for row in transposed_matrix:\n        print(row)\n```\n\nThis script defines the `transpose_matrix` function, which takes a matrix as input and returns its transpose. The function first determines the number of rows and columns in the input matrix, then initializes a new matrix with the transposed dimensions. It fills this new matrix by swapping the row and column indices of the elements from the original matrix. Finally, the script includes an example usage that demonstrates how to call the function and print both the original and transposed matrices."}
{"task_id": 3, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that defines the `reshape_matrix` function. This function takes a matrix `a` and a new shape `new_shape` as input, and returns the reshaped matrix if possible. If the reshaping is not possible, it returns an empty list `[]`.\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Convert the input list to a numpy array\n    arr = np.array(a)\n    \n    # Check if the total number of elements matches the required new shape\n    if arr.size != new_shape[0] * new_shape[1]:\n        return []\n    \n    # Reshape the array\n    reshaped_arr = arr.reshape(new_shape)\n    \n    # Convert the reshaped numpy array back to a Python list\n    reshaped_list = reshaped_arr.tolist()\n    \n    return reshaped_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6]]\n    new_shape = (3, 2)\n    print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2], [3, 4], [5, 6]]\n\n    matrix = [[1, 2], [3, 4]]\n    new_shape = (1, 5)\n    print(reshape_matrix(matrix, new_shape))  # Output: []\n```\n\n### Explanation:\n1. **Convert to Numpy Array**: The input list `a` is converted to a numpy array for easy manipulation.\n2. **Check Element Count**: We check if the total number of elements in the original matrix matches the total number of elements required for the new shape. If they don't match, reshaping is not possible, and we return an empty list `[]`.\n3. **Reshape the Array**: If the element count matches, we use numpy's `reshape` method to reshape the array.\n4. **Convert Back to List**: The reshaped numpy array is converted back to a Python list using the `tolist()` method.\n5. **Example Usage**: The script includes example usage to demonstrate how the function works.\n\nThis script ensures that the matrix is only reshaped if the total number of elements remains the same before and after reshaping."}
{"task_id": 4, "completion_id": 0, "completion": "Certainly! Below is a Python script that defines the `calculate_matrix_mean` function. This function takes a matrix and a mode as inputs and returns the mean values for either rows or columns based on the mode specified.\n\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of elements in a matrix either by row or by column.\n    \n    Parameters:\n    - matrix: A list of lists of floats representing the matrix.\n    - mode: A string, either 'row' or 'column', specifying the direction of mean calculation.\n    \n    Returns:\n    - A list of floats representing the mean values for each row or column.\n    \"\"\"\n    if mode == 'row':\n        # Calculate the mean for each row\n        row_means = [sum(row) / len(row) for row in matrix]\n        return row_means\n    elif mode == 'column':\n        # Transpose the matrix to calculate the mean for each column\n        transposed_matrix = list(zip(*matrix))\n        column_means = [sum(col) / len(col) for col in transposed_matrix]\n        return column_means\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n\n# Example usage:\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(\"Row means:\", calculate_matrix_mean(matrix, 'row'))\nprint(\"Column means:\", calculate_matrix_mean(matrix, 'column'))\n```\n\n### Explanation:\n- **Function Definition**: The function `calculate_matrix_mean` takes two parameters: `matrix` and `mode`.\n- **Row Mode**: If `mode` is 'row', the function calculates the mean of each row using a list comprehension.\n- **Column Mode**: If `mode` is 'column', the function first transposes the matrix using `zip(*matrix)` and then calculates the mean of each column.\n- **Error Handling**: If the `mode` is neither 'row' nor 'column', the function raises a `ValueError`.\n\n### Example Output:\nFor the provided matrix:\n```\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n```\n\nThe output will be:\n```\nRow means: [2.0, 5.0, 8.0]\nColumn means: [4.0, 5.0, 6.0]\n```\n\nThis script is self-contained and can be run directly to see the results."}
{"task_id": 5, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that defines a function to perform scalar multiplication on a matrix:\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of a matrix by a scalar value.\n\n    :param matrix: A 2D list (matrix) of integers or floats.\n    :param scalar: An integer or float scalar value.\n    :return: A new 2D list (matrix) where each element is the product of the original element and the scalar.\n    \"\"\"\n    # Initialize an empty list to store the result\n    result = []\n    \n    # Iterate over each row in the matrix\n    for row in matrix:\n        # Multiply each element in the row by the scalar and store the new row\n        new_row = [element * scalar for element in row]\n        # Append the new row to the result\n        result.append(new_row)\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a sample matrix\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    # Define a scalar\n    scalar = 2\n    \n    # Perform scalar multiplication\n    result = scalar_multiply(matrix, scalar)\n    \n    # Print the result\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(\"\\nScalar:\", scalar)\n    print(\"\\nResulting Matrix:\")\n    for row in result:\n        print(row)\n```\n\nThis script defines the `scalar_multiply` function, which takes a 2D list (matrix) and a scalar value as input and returns a new 2D list where each element is the product of the corresponding element in the original matrix and the scalar. The script also includes an example usage section to demonstrate how to use the function."}
{"task_id": 6, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that defines a function to calculate the eigenvalues of a 2x2 matrix and returns them sorted from highest to lowest.\n\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n        matrix (list[list[float | int]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n        list[float]: A list containing the eigenvalues sorted from highest to lowest.\n    \"\"\"\n    # Extract elements of the matrix\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    \n    # Calculate the trace and determinant\n    trace = a + d\n    det = a * d - b * c\n    \n    # Calculate the discriminant\n    discriminant = math.sqrt(trace**2 - 4 * det)\n    \n    # Calculate the eigenvalues\n    eigenvalue1 = (trace + discriminant) / 2\n    eigenvalue2 = (trace - discriminant) / 2\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[4, 2], [1, 3]]\n    eigenvalues = calculate_eigenvalues(matrix)\n    print(f\"The eigenvalues of the matrix {matrix} are: {eigenvalues}\")\n```\n\nThis script defines the `calculate_eigenvalues` function, which takes a 2x2 matrix as input and returns a list of its eigenvalues sorted from highest to lowest. The example usage at the bottom demonstrates how to use this function with a sample matrix."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transforms a given matrix A using the operation T^(-1) * A * S, where T and S are invertible matrices.\n    \n    Parameters:\n    A (list[list[int|float]]): The input matrix to be transformed.\n    T (list[list[int|float]]): The invertible matrix T.\n    S (list[list[int|float]]): The invertible matrix S.\n    \n    Returns:\n    list[list[int|float]]: The transformed matrix or -1 if T or S are not invertible.\n    \"\"\"\n    try:\n        # Convert lists to numpy arrays\n        A = np.array(A)\n        T = np.array(T)\n        S = np.array(S)\n        \n        # Check if T and S are invertible\n        T_inv = np.linalg.inv(T)\n        S = S  # S does not need inversion, but it's included for clarity\n        \n        # Perform the transformation\n        result = np.dot(np.dot(T_inv, A), S)\n        \n        # Round the result to the nearest 4th decimal\n        result_rounded = np.round(result, 4)\n        \n        # Convert the result back to a list and return\n        return result_rounded.tolist()\n    except np.linalg.LinAlgError:\n        # Return -1 if T or S are not invertible\n        return -1\n\n# Example usage:\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[2, 0], [0, 2]]\n\nprint(transform_matrix(A, T, S))  # Expected output: [[2.0, 4.0], [6.0, 8.0]]\n```"}
{"task_id": 8, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that defines a function to calculate the inverse of a 2x2 matrix. If the matrix is not invertible, the function returns `None`.\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[list[float]]: The inverse of the matrix, or None if the matrix is not invertible.\n    \"\"\"\n    # Extract the elements of the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    det = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if det == 0:\n        return None\n    \n    # Calculate the inverse\n    inv_matrix = [\n        [d / det, -b / det],\n        [-c / det, a / det]\n    ]\n    \n    return inv_matrix\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test cases\n    matrix1 = [[1, 2], [3, 4]]\n    matrix2 = [[1, 1], [1, 1]]\n    \n    print(\"Matrix 1:\", matrix1)\n    print(\"Inverse of Matrix 1:\", inverse_2x2(matrix1))\n    \n    print(\"\\nMatrix 2:\", matrix2)\n    print(\"Inverse of Matrix 2:\", inverse_2x2(matrix2))\n```\n\n### Explanation:\n1. **Function Definition**: The function `inverse_2x2` takes a 2x2 matrix as input and returns its inverse.\n2. **Matrix Elements**: The elements of the matrix are extracted into variables `a`, `b`, `c`, and `d`.\n3. **Determinant Calculation**: The determinant of the matrix is calculated using the formula \\( \\text{det} = ad - bc \\).\n4. **Invertibility Check**: If the determinant is zero, the matrix is not invertible, and the function returns `None`.\n5. **Inverse Calculation**: If the matrix is invertible, the inverse is calculated using the formula for the inverse of a 2x2 matrix:\n   \\[\n   \\begin{pmatrix}\n   a & b \\\\\n   c & d\n   \\end{pmatrix}^{-1} = \\frac{1}{\\text{det}} \\begin{pmatrix}\n   d & -b \\\\\n   -c & a\n   \\end{pmatrix}\n   \\]\n6. **Example Usage**: The script includes test cases to demonstrate the function's usage.\n\nThis script should work correctly for any 2x2 matrix and handle non-invertible matrices appropriately."}
{"task_id": 9, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that defines a function `matrixmul` to multiply two matrices. The function checks if the matrices can be multiplied (i.e., the number of columns in the first matrix equals the number of rows in the second matrix). If they can be multiplied, it performs the multiplication; otherwise, it returns -1.\n\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    # Get the dimensions of the matrices\n    rows_a = len(a)\n    cols_a = len(a[0]) if a else 0\n    rows_b = len(b)\n    cols_b = len(b[0]) if b else 0\n    \n    # Check if the matrices can be multiplied\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):  # or rows_b, since cols_a == rows_b\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\na = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\n\nb = [\n    [7, 8],\n    [9, 10],\n    [11, 12]\n]\n\nresult = matrixmul(a, b)\nprint(result)  # Output: [[58, 64], [139, 154]]\n\n# Example where matrices cannot be multiplied\nc = [\n    [1, 2, 3]\n]\n\nd = [\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nresult = matrixmul(c, d)\nprint(result)  # Output: -1\n```\n\n### Explanation:\n1. **Function Definition**: The function `matrixmul` takes two parameters `a` and `b`, which are lists of lists containing integers or floats.\n2. **Dimension Extraction**: The function extracts the dimensions of the matrices `a` and `b`.\n3. **Compatibility Check**: It checks if the number of columns in `a` is equal to the number of rows in `b`. If not, it returns -1.\n4. **Result Initialization**: It initializes a result matrix with zeros, with dimensions equal to the number of rows in `a` and the number of columns in `b`.\n5. **Matrix Multiplication**: It performs the matrix multiplication using nested loops and stores the result in the result matrix.\n6. **Example Usage**: The script includes example matrices and demonstrates how to call the function and print the result.\n\nThis script should work for any valid input matrices and handle cases where the matrices cannot be multiplied."}
{"task_id": 10, "completion_id": 0, "completion": "Certainly! Below is a Python script that defines a function `calculate_covariance_matrix` to compute the covariance matrix for a given set of vectors. The script also includes test cases to verify the correctness of the implementation.\n\n```python\nfrom typing import List\n\ndef mean(values: List[float]) -> float:\n    \"\"\"Calculate the mean of a list of numbers.\"\"\"\n    return sum(values) / len(values)\n\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    :param vectors: A list of lists, where each inner list represents a feature with its observations.\n    :return: A covariance matrix as a list of lists.\n    \"\"\"\n    n = len(vectors)\n    m = len(vectors[0])\n    \n    # Calculate the mean of each vector\n    means = [mean(vec) for vec in vectors]\n    \n    # Initialize the covariance matrix with zeros\n    cov_matrix = [[0.0] * n for _ in range(n)]\n    \n    # Calculate the covariance for each pair of vectors\n    for i in range(n):\n        for j in range(n):\n            for k in range(m):\n                cov_matrix[i][j] += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_matrix[i][j] /= (m - 1)\n    \n    return cov_matrix\n\n# Test cases\nif __name__ == \"__main__\":\n    # Test case 1\n    vectors1 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    expected1 = [\n        [1.0, 1.0],\n        [1.0, 1.0]\n    ]\n    assert calculate_covariance_matrix(vectors1) == expected1, \"Test case 1 failed\"\n\n    # Test case 2\n    vectors2 = [\n        [1, 3, 5],\n        [2, 4, 6],\n        [3, 6, 9]\n    ]\n    expected2 = [\n        [4.0, 4.0, 8.0],\n        [4.0, 4.0, 8.0],\n        [8.0, 8.0, 16.0]\n    ]\n    assert calculate_covariance_matrix(vectors2) == expected2, \"Test case 2 failed\"\n\n    # Test case 3\n    vectors3 = [\n        [1, 2],\n        [2, 3],\n        [3, 4]\n    ]\n    expected3 = [\n        [1.0, 1.0],\n        [1.0, 1.0]\n    ]\n    assert calculate_covariance_matrix(vectors3) == expected3, \"Test case 3 failed\"\n\n    print(\"All test cases passed!\")\n```\n\n### Explanation:\n1. **Mean Calculation**: The `mean` function calculates the mean of a list of numbers.\n2. **Covariance Matrix Calculation**:\n   - The function `calculate_covariance_matrix` takes a list of lists (`vectors`), where each inner list represents a feature with its observations.\n   - It first calculates the mean of each feature.\n   - It initializes a covariance matrix with zeros.\n   - It then iterates over each pair of features to calculate the covariance using the formula:\n     \\[\n     \\text{cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\mu_X)(Y_i - \\mu_Y)\n     \\]\n   - Finally, it returns the covariance matrix.\n\n### Test Cases:\n- **Test Case 1**: Simple 2x3 matrix.\n- **Test Case 2**: 3x3 matrix with linearly related features.\n- **Test Case 3**: 3x2 matrix with a simple linear relationship.\n\nThe script includes assertions to ensure the correctness of the covariance matrix calculation. If all test cases pass, it prints \"All test cases passed!\""}
{"task_id": 11, "completion_id": 0, "completion": "Certainly! Below is a Python script that implements the Jacobi method to solve a system of linear equations \\(Ax = b\\). The function iterates `n` times, rounds each intermediate solution to four decimal places, and returns the approximate solution \\(x\\) as a Python list.\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix (n x n).\n    b (np.ndarray): Right-hand side vector (n x 1).\n    n (int): Number of iterations.\n    \n    Returns:\n    list: Approximate solution vector x, rounded to 4 decimal places.\n    \"\"\"\n    # Initialize the solution vector with zeros\n    x = np.zeros_like(b)\n    \n    # Perform n iterations of the Jacobi method\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            sum_ax = sum(A[i][j] * x[j] for j in range(A.shape[1]) if j != i)\n            x_new[i] = (b[i] - sum_ax) / A[i][i]\n        \n        # Round the new solution vector to 4 decimal places\n        x_new = np.round(x_new, 4)\n        \n        # Update the solution vector\n        x = x_new\n    \n    # Convert the solution vector to a Python list and round to 4 decimal places\n    return x.tolist()\n\n# Example usage\nA = np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 4]])\nb = np.array([12, 12, 12])\nn = 100  # Number of iterations\n\nsolution = solve_jacobi(A, b, n)\nprint(solution)\n```\n\n### Explanation:\n1. **Initialization**:\n   - The solution vector `x` is initialized to zero.\n   \n2. **Iteration**:\n   - For each iteration, a new solution vector `x_new` is computed.\n   - For each equation (row in `A`), the current value of `x[i]` is updated using the formula:\n     \\[\n     x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} x_j^{(k)} \\right)\n     \\]\n   - The new value is rounded to 4 decimal places.\n   \n3. **Update**:\n   - The solution vector `x` is updated with the new values `x_new`.\n   \n4. **Output**:\n   - The final solution vector is converted to a Python list and returned.\n\nThis script should work for any square matrix `A` and corresponding vector `b`, provided the system is diagonally dominant or converges under the Jacobi method."}
{"task_id": 12, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that approximates the Singular Value Decomposition (SVD) for a 2x2 matrix using the Jacobian method. The script does not use the `numpy.linalg.svd` function to ensure that the SVD is computed manually.\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Ensure A is a 2x2 matrix\n    if A.shape != (2, 2):\n        raise ValueError(\"Matrix must be 2x2\")\n\n    # Compute A^T * A\n    A_tA = np.dot(A.T, A)\n\n    # Compute eigenvalues and eigenvectors of A^T * A\n    eigenvalues, eigenvectors = np.linalg.eig(A_tA)\n\n    # Sort eigenvalues and corresponding eigenvectors in descending order\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Singular values are the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n\n    # Compute U and V matrices\n    V = eigenvectors\n    U = np.dot(A, V) / singular_values\n\n    # Ensure U and V are orthogonal\n    U = U / np.linalg.norm(U, axis=0)\n    V = V / np.linalg.norm(V, axis=0)\n\n    # Round the results to 4 decimal places\n    singular_values = np.round(singular_values, 4)\n    U = np.round(U, 4)\n    V = np.round(V, 4)\n\n    return singular_values, U, V\n\n# Example usage\nA = np.array([[3, 4], [5, 6]])\nsingular_values, U, V = svd_2x2_singular_values(A)\nprint(\"Singular Values:\", singular_values)\nprint(\"U Matrix:\\n\", U)\nprint(\"V Matrix:\\n\", V)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the input matrix `A` is a 2x2 matrix.\n2. **Compute \\( A^T \\cdot A \\)**: This step is necessary to find the eigenvalues and eigenvectors.\n3. **Eigenvalue Decomposition**: The eigenvalues and eigenvectors of \\( A^T \\cdot A \\) are computed using `np.linalg.eig`.\n4. **Sort Eigenvalues and Eigenvectors**: The eigenvalues and corresponding eigenvectors are sorted in descending order to match the convention of SVD.\n5. **Compute Singular Values**: The singular values are the square roots of the eigenvalues.\n6. **Compute U and V Matrices**: The right singular vectors \\( V \\) are the eigenvectors of \\( A^T \\cdot A \\). The left singular vectors \\( U \\) are computed by multiplying \\( A \\) with \\( V \\) and normalizing by the singular values.\n7. **Orthogonalize U and V**: Ensure that \\( U \\) and \\( V \\) are orthogonal by normalizing their columns.\n8. **Round Results**: The results are rounded to 4 decimal places for readability.\n\nThis script provides a manual implementation of SVD for a 2x2 matrix using the Jacobian method, ensuring that the process is educational and not reliant on built-in SVD functions."}
{"task_id": 13, "completion_id": 0, "completion": "Certainly! Below is a Python script that calculates the determinant of a 4x4 matrix using Laplace's Expansion method. The function is implemented recursively to handle the computation of determinants for the 3x3 minor matrices.\n\n```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    def sub_matrix(matrix: list[list[int|float]], row: int, col: int) -> list[list[int|float]]:\n        \"\"\"Return the sub-matrix after removing the specified row and column.\"\"\"\n        return [[matrix[r][c] for c in range(len(matrix)) if c != col] for r in range(len(matrix)) if r != row]\n    \n    def determinant(matrix: list[list[int|float]]) -> float:\n        \"\"\"Calculate the determinant of a matrix using recursion.\"\"\"\n        size = len(matrix)\n        if size == 1:\n            return matrix[0][0]\n        elif size == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        else:\n            det = 0\n            for col in range(size):\n                sub_det = determinant(sub_matrix(matrix, 0, col))\n                det += ((-1) ** col) * matrix[0][col] * sub_det\n            return det\n    \n    # Ensure the input matrix is 4x4\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"The matrix must be 4x4.\")\n    \n    return determinant(matrix)\n\n# Example usage:\nmatrix = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(f\"Determinant of the 4x4 matrix: {determinant_4x4(matrix)}\")\n```\n\n### Explanation:\n1. **Sub-matrix Function**: The `sub_matrix` function creates a sub-matrix by removing the specified row and column from the original matrix. This is used to generate the minor matrices needed for Laplace's expansion.\n2. **Determinant Function**: The `determinant` function is a recursive function that calculates the determinant of a matrix. It handles base cases for 1x1 and 2x2 matrices directly and uses Laplace's expansion for larger matrices.\n3. **Input Validation**: The main function `determinant_4x4` ensures that the input matrix is indeed 4x4 before proceeding with the calculation.\n4. **Example Usage**: An example 4x4 matrix is provided, and the determinant is calculated and printed.\n\nThis script should work for any 4x4 matrix with integer or floating-point elements."}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    X (list[list[float]]): The feature matrix.\n    y (list[float]): The target vector.\n    \n    Returns:\n    list[float]: The coefficients of the linear regression model, rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X to account for the intercept term\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    \n    # Compute the normal equation\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = [round(coef, 4) for coef in theta]\n    \n    return theta_rounded\n\n# Example usage:\nX_example = [[1, 2], [2, 3], [4, 5]]\ny_example = [3, 5, 7]\ncoefficients = linear_regression_normal_equation(X_example, y_example)\nprint(coefficients)\n```\n\nThis script defines a function `linear_regression_normal_equation` that takes a feature matrix `X` and a target vector `y`, and returns the coefficients of the linear regression model using the normal equation method. The coefficients are rounded to four decimal places as required."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix with a column of ones for the intercept.\n    y (np.ndarray): Target vector.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations for gradient descent.\n    \n    Returns:\n    np.ndarray: Coefficients of the linear regression model as a list.\n    \"\"\"\n    # Initialize coefficients with zeros\n    m = X.shape[1]\n    theta = np.zeros(m)\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / len(y)\n        theta = theta - alpha * gradient\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert to list\n    return theta_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [1, 3], [1, 4]])\n    y = np.array([2, 3, 4])\n    alpha = 0.01\n    iterations = 1000\n    \n    # Perform linear regression\n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(\"Coefficients:\", coefficients)\n```\n\nThis script defines a function `linear_regression_gradient_descent` that performs linear regression using gradient descent. It takes as input the feature matrix `X`, the target vector `y`, the learning rate `alpha`, and the number of iterations. The function initializes the coefficients to zero, then iteratively updates them using the gradient descent algorithm. After the specified number of iterations, the coefficients are rounded to four decimal places and returned as a list. The example usage at the end demonstrates how to call this function with sample data."}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on a dataset using standardization and min-max normalization.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    \n    Returns:\n    - A tuple of two 2D lists: the first is the standardized data, and the second is the min-max normalized data.\n    \"\"\"\n    # Standardization\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    \n    # Min-Max Normalization\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_vals) / (max_vals - min_vals)\n    \n    # Rounding to 4 decimal places\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4).tolist()\n    \n    return standardized_data_rounded, min_max_normalized_data_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    data = np.array([\n        [1.0, 2.0, 3.0],\n        [4.0, 5.0, 6.0],\n        [7.0, 8.0, 9.0]\n    ])\n    \n    standardized, min_max_normalized = feature_scaling(data)\n    print(\"Standardized Data:\")\n    print(standardized)\n    print(\"\\nMin-Max Normalized Data:\")\n    print(min_max_normalized)\n```\n\n### Explanation:\n1. **Standardization**: This process involves subtracting the mean of each feature from each value and then dividing by the standard deviation of that feature. This scales the data to have a mean of 0 and a standard deviation of 1.\n2. **Min-Max Normalization**: This process involves subtracting the minimum value of each feature from each value and then dividing by the range (max - min) of that feature. This scales the data to a fixed range, typically [0, 1].\n3. **Rounding**: The results are rounded to the nearest 4th decimal place to meet the output constraints.\n4. **Conversion to List**: The final results are converted to lists using `numpy`'s `tolist()` method to ensure the output is in the required format."}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the closest centroid\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid_indices = np.argmin(distances, axis=0)\n        \n        # Update centroids based on the mean of the points assigned to each centroid\n        new_centroids = np.array([points[closest_centroid_indices == i].mean(axis=0) for i in range(k)])\n        \n        # Check if centroids have converged (i.e., no change in their positions)\n        if np.all(centroids == new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    # Round the final centroids to the nearest 4th decimal\n    final_centroids = [(round(c[0], 4), round(c[1], 4)) for c in centroids]\n    \n    return final_centroids\n\n# Example usage\nif __name__ == \"__main__\":\n    points = [(1.0, 1.0), (1.5, 2.0), (3.0, 4.0), (5.0, 7.0), (3.5, 5.0), (4.5, 5.0), (3.5, 4.5)]\n    k = 2\n    initial_centroids = [(1.0, 1.0), (5.0, 7.0)]\n    max_iterations = 100\n    \n    final_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\n    print(\"Final Centroids:\", final_centroids)\n```\n\nThis script defines a function `k_means_clustering` that performs k-Means clustering on a given set of points. It initializes centroids, iteratively assigns points to the nearest centroid, updates the centroids, and checks for convergence. The final centroids are rounded to the nearest 4th decimal place before being returned. The example usage at the end demonstrates how to call the function with sample data."}
{"task_id": 18, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements K-Fold Cross-Validation. The function `k_fold_cross_validation` takes a dataset and returns a list of train-test indices for each fold.\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Perform K-Fold Cross-Validation on the given dataset.\n\n    Parameters:\n    - X: np.ndarray, feature matrix of the dataset.\n    - y: np.ndarray, target vector of the dataset.\n    - k: int, number of folds (default is 5).\n    - shuffle: bool, whether to shuffle the data before splitting (default is True).\n    - random_seed: int, random seed for reproducibility (default is None).\n\n    Returns:\n    - folds: list of tuples, where each tuple contains two lists:\n             (train_indices, test_indices) for each fold.\n    \"\"\"\n    if shuffle:\n        np.random.seed(random_seed)\n        indices = np.random.permutation(len(X))\n    else:\n        indices = np.arange(len(X))\n\n    # Split the indices into k folds\n    fold_size = len(X) // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i < k - 1 else len(X)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        folds.append((train_indices, test_indices))\n\n    return folds\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n    y = np.array([0, 1, 0, 1, 0, 1])\n\n    # Perform 3-fold cross-validation\n    folds = k_fold_cross_validation(X, y, k=3, shuffle=True, random_seed=42)\n\n    # Print the train-test indices for each fold\n    for i, (train_indices, test_indices) in enumerate(folds):\n        print(f\"Fold {i+1}:\")\n        print(f\"Train Indices: {train_indices}\")\n        print(f\"Test Indices: {test_indices}\")\n        print()\n```\n\n### Explanation:\n1. **Shuffling**: If `shuffle` is `True`, the indices of the dataset are shuffled using a random seed for reproducibility. If `shuffle` is `False`, the indices are used in their original order.\n2. **Fold Size Calculation**: The size of each fold is calculated as the total number of samples divided by the number of folds `k`.\n3. **Splitting**: For each fold, the test indices are selected from the current segment, and the train indices are all other indices.\n4. **Returning Folds**: The function returns a list of tuples, where each tuple contains the train and test indices for one fold.\n\nThis implementation ensures that the dataset is evenly split into `k` folds, and each fold is used as a test set once while the remaining data serves as the training set."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given dataset.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    - A list of lists, where each inner list represents one of the top k principal components.\n    Each component is a list of floats, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Standardize the data\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    \n    # Compute the covariance matrix\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Compute the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n    \n    # Sort the eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the top k eigenvectors\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    \n    # Convert the eigenvectors to a list of lists and round the values\n    principal_components = np.round(top_k_eigenvectors.real, 4).tolist()\n    \n    return principal_components\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some sample data\n    np.random.seed(42)\n    data = np.random.rand(100, 5)  # 100 samples, 5 features\n    \n    # Perform PCA\n    k = 2\n    principal_components = pca(data, k)\n    \n    print(\"Principal Components:\")\n    for pc in principal_components:\n        print(pc)\n```\n\nThis script defines a function `pca` that performs Principal Component Analysis on a given dataset. It standardizes the data, computes the covariance matrix, finds the eigenvalues and eigenvectors, and returns the top `k` principal components. The results are rounded to the nearest 4th decimal. The example usage at the end demonstrates how to use the function with a randomly generated dataset."}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef entropy(class_probabilities):\n    \"\"\"Calculate the entropy from a list of class probabilities.\"\"\"\n    return -sum(p * math.log2(p) for p in class_probabilities if p != 0)\n\ndef class_probabilities(examples, target_attr):\n    \"\"\"Calculate the probabilities of each class in the dataset.\"\"\"\n    counts = Counter(example[target_attr] for example in examples)\n    total = len(examples)\n    return [count / total for count in counts.values()]\n\ndef partition_by_attribute_value(examples, attribute, value):\n    \"\"\"Return a subset of examples where the attribute has the specified value.\"\"\"\n    return [example for example in examples if example[attribute] == value]\n\ndef split_information(examples, attribute):\n    \"\"\"Calculate the split information for a given attribute.\"\"\"\n    values = set(example[attribute] for example in examples)\n    weighted_entropy = sum(\n        (len(partition) / len(examples)) * entropy(class_probabilities(partition, 'label'))\n        for value in values\n        for partition in [partition_by_attribute_value(examples, attribute, value)]\n    )\n    return weighted_entropy\n\ndef majority_vote(examples, target_attr):\n    \"\"\"Return the most common value for a given attribute.\"\"\"\n    counts = Counter(example[target_attr] for example in examples)\n    return counts.most_common(1)[0][0]\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    # Base case: no more attributes to split or all examples have the same label\n    if not examples or not attributes:\n        return {'leaf': majority_vote(examples, target_attr)}\n    \n    # Check if all examples have the same label\n    unique_labels = {example[target_attr] for example in examples}\n    if len(unique_labels) == 1:\n        return {'leaf': unique_labels.pop()}\n    \n    # Calculate the information gain for each attribute\n    base_entropy = entropy(class_probabilities(examples, target_attr))\n    information_gains = {\n        attribute: base_entropy - split_information(examples, attribute)\n        for attribute in attributes\n    }\n    \n    # Choose the attribute with the highest information gain\n    best_attribute = max(information_gains, key=information_gains.get)\n    \n    # Remove the chosen attribute from the list of attributes\n    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n    \n    # Create a new decision tree node\n    tree = {best_attribute: {}}\n    \n    # Split the dataset by the values of the best attribute\n    for value in set(example[best_attribute] for example in examples):\n        # Recursively build the subtree\n        subset = partition_by_attribute_value(examples, best_attribute, value)\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    \n    return tree\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample dataset\n    examples = [\n        {'outlook': 'sunny', 'temp': 'hot', 'humidity': 'high', 'windy': False, 'label': 'no'},\n        {'outlook': 'sunny', 'temp': 'hot', 'humidity': 'high', 'windy': True, 'label': 'no'},\n        {'outlook': 'overcast', 'temp': 'hot', 'humidity': 'high', 'windy': False, 'label': 'yes'},\n        {'outlook': 'rainy', 'temp': 'mild', 'humidity': 'high', 'windy': False, 'label': 'yes'},\n        {'outlook': 'rainy', 'temp': 'cool', 'humidity': 'normal', 'windy': False, 'label': 'yes'},\n        {'outlook': 'rainy', 'temp': 'cool', 'humidity': 'normal', 'windy': True, 'label': 'no'},\n        {'outlook': 'overcast', 'temp': 'cool', 'humidity': 'normal', 'windy': True, 'label': 'yes'},\n        {'outlook': 'sunny', 'temp': 'mild', 'humidity': 'high', 'windy': False, 'label': 'no'},\n        {'outlook': 'sunny', 'temp': 'cool', 'humidity': 'normal', 'windy': False, 'label': 'yes'},\n        {'outlook': 'rainy', 'temp': 'mild', 'humidity': 'normal', 'windy': False, 'label': 'yes'},\n        {'outlook': 'sunny', 'temp': 'mild', 'humidity': 'normal', 'windy': True, 'label': 'yes'},\n        {'outlook': 'overcast', 'temp': 'mild', 'humidity': 'high', 'windy': True, 'label': 'yes'},\n        {'outlook': 'overcast', 'temp': 'hot', 'humidity': 'normal', 'windy': False, 'label': 'yes'},\n        {'outlook': 'rainy', 'temp': 'mild', 'humidity': 'high', 'windy': True, 'label': 'no'}\n    ]\n    attributes = ['outlook', 'temp', 'humidity', 'windy']\n    target_attr = 'label'\n    \n    decision_tree = learn_decision_tree(examples, attributes, target_attr)\n    print(decision_tree)\n```\n\nThis script defines a function `learn_decision_tree` that builds a decision tree using the provided examples and attributes. It uses entropy and information gain to determine the best attribute to split on at each step of the recursion. The resulting decision tree is represented as a nested dictionary, which can be used for classification."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Train a kernel SVM classifier using a deterministic version of the Pegasos algorithm.\n    \n    Parameters:\n    - data: 2D NumPy array where each row is a data sample and each column is a feature.\n    - labels: 1D NumPy array where each entry corresponds to the label of the sample.\n    - kernel: Type of kernel to use ('linear' or 'rbf').\n    - lambda_val: Regularization parameter.\n    - iterations: Number of iterations for training.\n    - sigma: Parameter for RBF kernel.\n    \n    Returns:\n    - alpha: List of alpha coefficients.\n    - b: Bias term.\n    \"\"\"\n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0\n    \n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n    \n    def rbf_kernel(x1, x2, sigma):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * (sigma ** 2)))\n    \n    if kernel == 'linear':\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i, j] = linear_kernel(data[i], data[j])\n    elif kernel == 'rbf':\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i, j] = rbf_kernel(data[i], data[j], sigma)\n    \n    for t in range(1, iterations + 1):\n        eta = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            prediction = np.sign(np.sum(alpha * labels * K[:, i]) + b)\n            if labels[i] * prediction < 1:\n                alpha[i] = alpha[i] + 1\n        alpha = (1 - eta * lambda_val) * alpha\n        support_vectors = alpha > 0\n        b = b + eta * (1 - np.sum(alpha[support_vectors] * labels[support_vectors] * K[support_vectors, :].dot(labels)))\n    \n    # Round the results to 4 decimal places and convert to lists\n    alpha = np.round(alpha, 4).tolist()\n    b = np.round(b, 4)\n    \n    return alpha, b\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some synthetic data\n    np.random.seed(0)\n    X = np.random.randn(10, 2)\n    y = np.array([1 if x[0] + x[1] > 0 else -1 for x in X])\n    \n    # Train the SVM\n    alphas, bias = pegasos_kernel_svm(X, y, kernel='linear', lambda_val=0.01, iterations=1000)\n    print(\"Alphas:\", alphas)\n    print(\"Bias:\", bias)\n```\n\nThis script defines a function `pegasos_kernel_svm` that trains a kernel SVM classifier using a deterministic version of the Pegasos algorithm. The function supports both linear and RBF kernels. The example usage at the end demonstrates how to generate synthetic data, train the SVM, and print the resulting alpha coefficients and bias term."}
{"task_id": 22, "completion_id": 0, "completion": "Certainly! Below is the Python script that defines a function `sigmoid` which computes the output of the sigmoid activation function for a given input `z` and returns the result rounded to four decimal places.\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): Input value\n    \n    Returns:\n    float: Output of the sigmoid function, rounded to four decimal places\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)\n\n# Example usage:\nprint(sigmoid(0))  # Should print 0.5000\nprint(sigmoid(5))  # Should print 0.9933\nprint(sigmoid(-5)) # Should print 0.0067\n```\n\nThis script includes the necessary import statement, the function definition, and some example usage to demonstrate how the function works."}
{"task_id": 23, "completion_id": 0, "completion": "Certainly! Below is the Python script that implements the softmax activation function and returns the results rounded to four decimal places.\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation for a given list of scores.\n    \n    Parameters:\n    scores (list[float]): A list of numerical scores.\n    \n    Returns:\n    list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    # Calculate the exponentials of the scores\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Calculate the sum of the exponentials\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax values\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    \n    # Round the results to four decimal places\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    \n    return rounded_softmax_values\n\n# Example usage\nif __name__ == \"__main__\":\n    scores = [1.0, 2.0, 3.0]\n    print(softmax(scores))\n```\n\nThis script defines the `softmax` function, which takes a list of floating-point numbers as input and returns a list of their softmax values, rounded to four decimal places. The example usage at the bottom demonstrates how to call this function."}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: A list of feature vectors (each vector represents multiple features for an example).\n    :param labels: A list of true binary labels (0 or 1) corresponding to each feature vector.\n    :param weights: A list of weights, one for each feature in the feature vectors.\n    :param bias: The bias term for the neuron.\n    :return: A tuple containing the predicted probabilities (after sigmoid activation) and the mean squared error.\n             Both the probabilities and the error are rounded to four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    features = np.array(features)\n    labels = np.array(labels)\n    weights = np.array(weights)\n    \n    # Compute the weighted sum of inputs plus the bias\n    z = np.dot(features, weights) + bias\n    \n    # Apply the sigmoid activation function\n    predictions = sigmoid(z)\n    \n    # Calculate the mean squared error\n    mse = np.mean((predictions - labels) ** 2)\n    \n    # Round the predictions and mse to 4 decimal places\n    predictions_rounded = np.round(predictions, 4).tolist()\n    mse_rounded = round(mse, 4)\n    \n    return predictions_rounded, mse_rounded\n\n# Example usage:\nfeatures = [[0.5, -0.2], [0.3, 0.1], [-0.4, 0.6]]\nlabels = [1, 0, 1]\nweights = [0.8, -0.5]\nbias = 0.1\n\npredictions, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predictions)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis script defines a `single_neuron_model` function that simulates a single neuron with a sigmoid activation function. It takes lists of feature vectors, labels, weights, and a bias as input, and returns the predicted probabilities and the mean squared error, both rounded to four decimal places. The `sigmoid` function is defined to compute the sigmoid activation, which is used within the main function to produce the predictions. The example usage at the end demonstrates how to call the function with sample data."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error loss function.\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    # Initialize weights and bias\n    weights = initial_weights.copy()\n    bias = initial_bias\n    \n    # List to store the MSE for each epoch\n    mse_history = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        z = np.dot(features, weights) + bias\n        y_pred = sigmoid(z)\n        \n        # Compute the MSE loss\n        loss = mse_loss(labels, y_pred)\n        mse_history.append(round(loss, 4))\n        \n        # Backward pass\n        dL_dy_pred = 2 * (y_pred - labels)\n        dy_pred_dz = y_pred * (1 - y_pred)\n        dz_dw = features\n        \n        # Gradients\n        dL_dz = dL_dy_pred * dy_pred_dz\n        dL_dw = np.dot(dz_dw.T, dL_dz)\n        dL_db = np.sum(dL_dz)\n        \n        # Update weights and bias\n        weights -= learning_rate * dL_dw\n        bias -= learning_rate * dL_db\n    \n    # Return the final weights, bias, and MSE history\n    return weights, bias, mse_history\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    features = np.array([[0.5, 0.7], [0.3, 0.4], [0.8, 0.9]])\n    labels = np.array([1, 0, 1])\n    initial_weights = np.array([0.1, 0.2])\n    initial_bias = 0.0\n    learning_rate = 0.1\n    epochs = 100\n    \n    # Train the neuron\n    final_weights, final_bias, mse_history = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n    \n    print(\"Final Weights:\", final_weights.tolist())\n    print(\"Final Bias:\", round(final_bias, 4))\n    print(\"MSE History:\", mse_history)\n```"}
{"task_id": 26, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the `Value` class with the required autograd operations:\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        # Topological sort all of the children in the graph\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # Go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example usage:\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a * b\ne = d + c\nf = e.relu()\nf.backward()\n\nprint(f\"a.grad = {a.grad}\")\nprint(f\"b.grad = {b.grad}\")\nprint(f\"c.grad = {c.grad}\")\nprint(f\"d.grad = {d.grad}\")\nprint(f\"e.grad = {e.grad}\")\nprint(f\"f.grad = {f.grad}\")\n```\n\n### Explanation:\n1. **Initialization (`__init__`)**:\n   - Initializes the value with `data`, `grad` (gradient), and `_backward` function which is a no-op by default.\n   - `_prev` keeps track of the nodes that were used to compute this node.\n   - `_op` stores the operation that produced this node.\n\n2. **Addition (`__add__`)**:\n   - Adds two `Value` objects or a `Value` object and a scalar.\n   - Computes the gradient for each input during backpropagation.\n\n3. **Multiplication (`__mul__`)**:\n   - Multiplies two `Value` objects or a `Value` object and a scalar.\n   - Computes the gradient for each input during backpropagation.\n\n4. **ReLU Activation (`relu`)**:\n   - Applies the ReLU activation function.\n   - Computes the gradient for the input during backpropagation.\n\n5. **Backward Propagation (`backward`)**:\n   - Performs a topological sort to ensure that we compute gradients in the correct order.\n   - Sets the gradient of the output node to 1 and then propagates the gradients backward through the graph.\n\n6. **String Representation (`__repr__`)**:\n   - Provides a string representation of the `Value` object for easier debugging.\n\n### Example Usage:\n- The example demonstrates creating `Value` objects, performing operations, and computing gradients using the `backward` method. The gradients of the input variables are printed at the end."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C for R^3.\n    \n    Parameters:\n    - B: A list of lists representing the basis vectors of B.\n    - C: A list of lists representing the basis vectors of C.\n    \n    Returns:\n    - A list of lists representing the transformation matrix P from B to C.\n    \"\"\"\n    # Convert the basis lists to numpy arrays\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    \n    # Compute the inverse of the matrix C\n    C_inv = np.linalg.inv(C_matrix)\n    \n    # The transformation matrix P from B to C is given by C_inv * B\n    P = np.dot(C_inv, B_matrix)\n    \n    # Round the results to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert the matrix back to a list of lists\n    return P_rounded.tolist()\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\nP = transform_basis(B, C)\nprint(P)\n```\n\nThis script defines a function `transform_basis` that takes two lists of lists as input, representing the basis vectors for R^3 in bases B and C. It computes the transformation matrix P that transforms coordinates from basis B to basis C. The result is rounded to the nearest 4th decimal and returned as a list of lists. The example usage demonstrates how to call the function and print the transformation matrix."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Ensure A is a 2x2 matrix\n    assert A.shape == (2, 2), \"Input matrix must be 2x2\"\n    \n    # Step 1: Compute A^T A\n    A_t_A = A.T @ A\n    \n    # Step 2: Perform eigendecomposition on A^T A\n    eigenvalues, eigenvectors = np.linalg.eigh(A_t_A)\n    \n    # Step 3: Construct the singular values and the diagonal matrix S\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    \n    # Step 4: Use the eigenvectors of A^T A as the columns of V\n    V = eigenvectors\n    \n    # Step 5: Compute U\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] > 0:\n            U[:, i] = (A @ V[:, i]) / singular_values[i]\n    \n    # Normalize U to ensure it is orthogonal\n    U = np.linalg.qr(U)[0]\n    \n    # Round the results to 4 decimal places\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert to lists\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    \n    return U_list, S_list, V_list\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis script defines a function `svd_2x2` that computes the Singular Value Decomposition (SVD) of a 2x2 matrix using the method based on eigendecomposition of \\(A^T A\\). The function returns the matrices \\(U\\), \\(S\\), and \\(V\\) as Python lists, with all results rounded to the nearest 4th decimal."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles two numpy arrays in unison (maintaining the correspondence between X and y).\n\n    Parameters:\n    X (numpy.ndarray): Input data array.\n    y (numpy.ndarray): Target data array.\n    seed (int, optional): Seed for the random number generator to ensure reproducibility.\n\n    Returns:\n    tuple: A tuple containing two lists, the shuffled X and y arrays.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random permutation of indices\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    \n    # Shuffle X and y using the generated indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    \n    # Shuffle data\n    X_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\n    \n    print(\"Shuffled X:\", X_shuffled)\n    print(\"Shuffled y:\", y_shuffled)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of data from the input arrays X and y.\n    \n    Parameters:\n    - X: numpy array containing the features.\n    - y: numpy array containing the labels (optional).\n    - batch_size: integer specifying the batch size.\n    \n    Yields:\n    - Batches of (X, y) if y is provided.\n    - Batches of X if y is not provided.\n    \"\"\"\n    n_samples = X.shape[0]\n    start = 0\n    while start < n_samples:\n        end = min(start + batch_size, n_samples)\n        if y is not None:\n            yield (X[start:end].tolist(), y[start:end].tolist())\n        else:\n            yield X[start:end].tolist()\n        start = end\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.random.rand(100, 10)  # 100 samples, 10 features each\n    y = np.random.randint(2, size=(100,))  # Binary labels for 100 samples\n    \n    # Using the batch iterator\n    for batch in batch_iterator(X, y, batch_size=32):\n        print(\"Batch of X:\", batch[0])\n        print(\"Batch of y:\", batch[1])\n        \n    # Using the batch iterator without labels\n    for batch in batch_iterator(X, batch_size=32):\n        print(\"Batch of X:\", batch)\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset into two subsets based on whether the value of a specified feature\n    is greater than or equal to a given threshold.\n    \n    Parameters:\n    - X: A 2D numpy array representing the dataset where each row is a sample and each column is a feature.\n    - feature_i: An integer representing the index of the feature to split the dataset on.\n    - threshold: A float representing the threshold value for the feature.\n    \n    Returns:\n    - Two lists: The first list contains samples where the value of the specified feature is >= threshold,\n                 and the second list contains samples where the value of the specified feature is < threshold.\n    \"\"\"\n    # Split the dataset based on the condition\n    X_true = X[X[:, feature_i] >= threshold]\n    X_false = X[X[:, feature_i] < threshold]\n    \n    # Convert the numpy arrays back to lists\n    X_true_list = X_true.tolist()\n    X_false_list = X_false.tolist()\n    \n    return X_true_list, X_false_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample dataset\n    data = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n        [10, 11, 12]\n    ])\n    \n    feature_index = 1  # Index of the feature to split on\n    threshold_value = 5  # The threshold value\n    \n    # Call the function\n    true_subset, false_subset = divide_on_feature(data, feature_index, threshold_value)\n    \n    print(\"Subset where feature value >= threshold:\")\n    print(true_subset)\n    print(\"\\nSubset where feature value < threshold:\")\n    print(false_subset)\n```\n\n### Explanation:\n- **Function Definition**: The `divide_on_feature` function takes three parameters: the dataset `X`, the feature index `feature_i`, and the threshold value `threshold`.\n- **Condition Check**: It uses boolean indexing to filter the dataset into two parts: `X_true` for samples where the feature value is greater than or equal to the threshold, and `X_false` for samples where the feature value is less than the threshold.\n- **Conversion to List**: The filtered numpy arrays are converted to lists using the `tolist()` method to meet the output constraints.\n- **Example Usage**: The example demonstrates how to use the function with a sample dataset, specifying a feature index and a threshold value. It then prints the resulting subsets."}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - degree: integer, the degree of the polynomial features\n    \n    Returns:\n    - poly_X: 2D numpy array containing the polynomial features\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Generate all combinations of feature indices\n    comb = combinations_with_replacement(range(n_features), degree)\n    # Initialize an empty list to store the polynomial features\n    poly_features = []\n    for c in comb:\n        # For each combination, compute the product of the corresponding features\n        feature = np.prod(X[:, c], axis=1)\n        poly_features.append(feature)\n    # Convert the list of features to a 2D numpy array\n    poly_X = np.array(poly_features).T\n    # Return the result as a Python list\n    return poly_X.tolist()\n\n# Example usage\nX = np.array([[2, 3], [4, 5]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis script defines a function `polynomial_features` that takes a 2D numpy array `X` and an integer `degree` as inputs. It generates all possible combinations of the features up to the specified degree and computes the product of the features for each combination. The resulting polynomial features are returned as a list."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array representing the feature matrix of the dataset.\n    - y: 1D numpy array representing the target vector of the dataset.\n    - n_subsets: Integer specifying the number of subsets to generate.\n    - replacements: Boolean indicating whether to sample with or without replacement.\n    - seed: Integer for reproducibility of the random sampling.\n    \n    Returns:\n    - A list of tuples, where each tuple contains a subset (X_subset, y_subset) of the dataset.\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    \n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        \n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a simple dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    \n    # Generate 3 random subsets with replacement\n    subsets_with_replacement = get_random_subsets(X, y, 3, replacements=True)\n    print(\"Subsets with replacement:\", subsets_with_replacement)\n    \n    # Generate 3 random subsets without replacement\n    subsets_without_replacement = get_random_subsets(X, y, 3, replacements=False)\n    print(\"Subsets without replacement:\", subsets_without_replacement)\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of a 1D numpy array of integer values.\n    \n    Parameters:\n    - x: 1D numpy array of integers.\n    - n_col: Optional integer specifying the number of columns in the one-hot encoded array.\n             If None, it will be inferred from the maximum value in x + 1.\n    \n    Returns:\n    - A list of lists representing the one-hot encoded data.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Initialize an array of zeros with shape (len(x), n_col)\n    one_hot = np.zeros((x.size, n_col))\n    \n    # Set the appropriate elements to 1\n    one_hot[np.arange(x.size), x] = 1\n    \n    # Convert the numpy array to a list of lists\n    return one_hot.tolist()\n\n# Example usage:\nx = np.array([0, 2, 1, 3])\nencoded_x = to_categorical(x)\nprint(encoded_x)\n# Expected output: [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\n```"}
{"task_id": 35, "completion_id": 0, "completion": "Certainly! Below is the Python script that converts a 1D numpy array into a diagonal matrix and returns it as a Python list:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix and return it as a Python list.\n    \n    Parameters:\n    x (numpy.ndarray): A 1D numpy array.\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    # Create a diagonal matrix from the input vector\n    diag_matrix = np.diag(x)\n    \n    # Convert the numpy 2D array to a Python list\n    result_list = diag_matrix.tolist()\n    \n    return result_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a 1D numpy array\n    vector = np.array([1, 2, 3, 4])\n    \n    # Convert the vector to a diagonal matrix\n    diagonal_matrix = make_diagonal(vector)\n    \n    # Print the result\n    print(diagonal_matrix)\n```\n\n### Explanation:\n- **np.diag(x)**: This function creates a diagonal matrix from the input 1D array `x`.\n- **tolist()**: This method converts the 2D numpy array to a Python list of lists.\n- The function `make_diagonal` takes a 1D numpy array as input and returns the diagonal matrix as a Python list.\n\n### Example Output:\nIf you run the example provided in the script, the output will be:\n```\n[[1, 0, 0, 0],\n [0, 2, 0, 0],\n [0, 0, 3, 0],\n [0, 0, 0, 4]]\n```\n\nThis represents the diagonal matrix where the elements of the input vector are on the diagonal, and all other elements are zero."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    - y_true: 1D numpy array of true labels\n    - y_pred: 1D numpy array of predicted labels\n    \n    Returns:\n    - float: accuracy score rounded to 4 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\nprint(accuracy_score(y_true, y_pred))  # Expected output: 0.8\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): A 2D numpy array. If None, Y is set to X.\n    \n    Returns:\n    list: The correlation matrix as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Calculate the correlation matrix\n    corr_matrix = np.corrcoef(X, rowvar=False, y=Y)\n    \n    # Round the values to 4 decimal places\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    \n    # Convert the numpy array to a Python list\n    corr_matrix_list = corr_matrix_rounded.tolist()\n    \n    return corr_matrix_list\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\nY = np.array([[5, 6], [7, 8]])\nprint(calculate_correlation_matrix(X))  # Correlation matrix of X with itself\nprint(calculate_correlation_matrix(X, Y))  # Correlation matrix of X with Y\n```"}
{"task_id": 38, "completion_id": 0, "completion": "Certainly! Below is a Python script that implements the `adaboost_fit` function for an AdaBoost classifier. The function initializes sample weights, finds the best thresholds for each feature, calculates the error, updates the weights, and returns a list of classifiers with their parameters.\n\n```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    \n    # Initialize weights\n    w = np.full(n_samples, (1 / n_samples))\n    \n    classifiers = []\n    \n    for _ in range(n_clf):\n        # Find the best weak classifier\n        best_error = float('inf')\n        best_threshold = None\n        best_feature_idx = None\n        best_polarity = None\n        \n        for feature_idx in range(n_features):\n            # Sort data based on the current feature\n            X_sorted = sorted(zip(X[:, feature_idx], y, w), key=lambda x: x[0])\n            sorted_X, sorted_y, sorted_w = zip(*X_sorted)\n            \n            # Calculate cumulative sums of weights for positive and negative classes\n            cum_sum_pos = np.cumsum([w_i if y_i == 1 else 0 for w_i, y_i in zip(sorted_w, sorted_y)])\n            cum_sum_neg = np.cumsum([w_i if y_i == -1 else 0 for w_i, y_i in zip(sorted_w, sorted_y)])\n            \n            # Calculate errors for all possible thresholds\n            errors = np.concatenate(([cum_sum_neg[-1]], cum_sum_pos + cum_sum_neg[-1] - cum_sum_neg))\n            threshold_candidates = np.concatenate(([-np.inf], [x[0] for x in X_sorted], [np.inf]))\n            \n            # Find the best threshold\n            min_error_idx = np.argmin(errors)\n            min_error = errors[min_error_idx]\n            best_threshold_candidate = threshold_candidates[min_error_idx]\n            \n            if min_error < best_error:\n                best_error = min_error\n                best_threshold = best_threshold_candidate\n                best_feature_idx = feature_idx\n                best_polarity = 1 if min_error_idx == 0 or sorted_y[min_error_idx - 1] == 1 else -1\n        \n        # Calculate alpha\n        epsilon = best_error / sum(w)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        \n        # Update weights\n        predictions = np.ones(n_samples)\n        predictions[X[:, best_feature_idx] < best_threshold] *= -1\n        predictions *= best_polarity\n        w *= np.exp(-alpha * y * predictions)\n        w /= np.sum(w)\n        \n        # Store the classifier\n        classifiers.append({\n            'polarity': best_polarity,\n            'threshold': best_threshold,\n            'feature_idx': best_feature_idx,\n            'alpha': alpha\n        })\n    \n    return classifiers\n\n# Example usage\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 1, -1, -1])\nn_clf = 3\n\nclassifiers = adaboost_fit(X, y, n_clf)\nprint(classifiers)\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the weights `w` for each sample to be equal.\n2. **Finding the Best Weak Classifier**:\n   - For each feature, we sort the data based on the feature values.\n   - We calculate the cumulative sums of weights for positive and negative classes.\n   - We compute the error for each possible threshold and select the one with the minimum error.\n3. **Alpha Calculation**: We calculate the weight `alpha` for the best weak classifier using the formula \\( \\alpha = 0.5 \\log \\left( \\frac{1 - \\epsilon}{\\epsilon} \\right) \\).\n4. **Weight Update**: We update the weights of the samples based on the prediction errors.\n5. **Store the Classifier**: We store the parameters of the best weak classifier in a list.\n\nThis implementation ensures that the results are rounded to the nearest 4th decimal as required."}
{"task_id": 39, "completion_id": 0, "completion": "Certainly! Below is the Python script that implements the `log_softmax` function as described:\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list) -> list:\n    \"\"\"\n    Compute the log-softmax of a 1D numpy array of scores.\n    \n    Parameters:\n    scores (list): A list of scores.\n    \n    Returns:\n    list: The log-softmax of the input scores, rounded to 4 decimal places.\n    \"\"\"\n    # Convert the list to a numpy array\n    scores_array = np.array(scores)\n    \n    # Compute the maximum score for numerical stability\n    max_score = np.max(scores_array)\n    \n    # Subtract the maximum score from each score to avoid overflow\n    shifted_scores = scores_array - max_score\n    \n    # Compute the exponentials of the shifted scores\n    exp_scores = np.exp(shifted_scores)\n    \n    # Compute the sum of the exponentials\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Compute the log-softmax values\n    log_softmax_values = shifted_scores - np.log(sum_exp_scores)\n    \n    # Round the results to 4 decimal places\n    log_softmax_values_rounded = np.round(log_softmax_values, 4)\n    \n    # Convert the result back to a list\n    return log_softmax_values_rounded.tolist()\n\n# Example usage\nscores = [1.0, 2.0, 3.0]\nresult = log_softmax(scores)\nprint(result)\n```\n\n### Explanation:\n1. **Input Handling**: The function takes a list of scores as input.\n2. **Conversion to Numpy Array**: The list is converted to a numpy array for efficient computation.\n3. **Numerical Stability**: To avoid numerical overflow, the maximum score is subtracted from each score.\n4. **Exponentiation**: The shifted scores are exponentiated.\n5. **Sum of Exponentials**: The sum of these exponentials is computed.\n6. **Log-Softmax Calculation**: The log-softmax values are calculated by subtracting the logarithm of the sum of exponentials from the shifted scores.\n7. **Rounding**: The results are rounded to 4 decimal places.\n8. **Output**: The final result is converted back to a Python list and returned.\n\nThis implementation ensures numerical stability and provides the log-softmax values as required."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n\n            self.W -= self.optimizer.update(W, grad_w)\n            self.w0 -= self.optimizer.update(self.w0, grad_w0)\n\n        accum_grad = np.dot(accum_grad, W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example Optimizer (Simple Gradient Descent)\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, param, grad):\n        return self.learning_rate * grad\n\n# Example Usage\nif __name__ == \"__main__\":\n    # Create a dense layer with 3 units and input shape (2,)\n    dense_layer = Dense(n_units=3, input_shape=(2,))\n    dense_layer.initialize(SGD(learning_rate=0.01))\n\n    # Example input data\n    X = np.array([[1, 2], [3, 4]])\n\n    # Forward pass\n    output = dense_layer.forward_pass(X)\n    print(\"Forward Pass Output:\", output.tolist())\n\n    # Backward pass (example accumulated gradient)\n    accum_grad = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    backward_output = dense_layer.backward_pass(accum_grad)\n    print(\"Backward Pass Output:\", backward_output.tolist())\n\n    # Output shape\n    print(\"Output Shape:\", dense_layer.output_shape())\n\n    # Parameters\n    print(\"Number of Parameters:\", dense_layer.parameters())\n```"}
{"task_id": 41, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements a simple 2D convolutional layer according to the given specifications:\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Get the dimensions of the input matrix and the kernel\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate the output dimensions based on the padding and stride\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    \n    # Initialize the output matrix with zeros\n    output_matrix = np.zeros((output_height, output_width))\n    \n    # Pad the input matrix\n    padded_input = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    \n    # Perform the convolution\n    for i in range(output_height):\n        for j in range(output_width):\n            # Extract the region of the input matrix that the kernel will be applied to\n            input_region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            \n            # Apply the kernel to the input region and sum the result\n            output_matrix[i, j] = np.sum(input_region * kernel)\n    \n    # Round the results to the nearest 4th decimal\n    output_matrix = np.round(output_matrix, 4)\n    \n    # Convert the output matrix to a Python list\n    output_list = output_matrix.tolist()\n    \n    return output_list\n\n# Example usage\nif __name__ == \"__main__\":\n    input_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    kernel = np.array([[1, 0], [0, 1]])\n    padding = 1\n    stride = 1\n    \n    result = simple_conv2d(input_matrix, kernel, padding, stride)\n    print(result)\n```\n\n### Explanation:\n1. **Input Matrix and Kernel**: The function takes an input matrix and a kernel as 2D NumPy arrays.\n2. **Padding and Stride**: The `padding` parameter adds zeros around the input matrix, and the `stride` parameter determines how many steps the kernel moves in each direction.\n3. **Output Dimensions**: The output dimensions are calculated based on the input dimensions, padding, and stride.\n4. **Padded Input**: The input matrix is padded with zeros to handle the edges.\n5. **Convolution**: The function iterates over the input matrix, applying the kernel to each region and summing the results.\n6. **Rounding**: The results are rounded to the nearest 4th decimal.\n7. **Conversion to List**: The final output is converted to a Python list using `tolist()`.\n\nThis script should meet the requirements of the task."}
{"task_id": 42, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the ReLU activation function:\n\n```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function to the input.\n\n    Parameters:\n    z (float): A single floating-point number.\n\n    Returns:\n    float: The result of applying the ReLU function to the input.\n    \"\"\"\n    return max(0, z)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    test_values = [-1.0, 0.0, 1.0, 5.5]\n    \n    for value in test_values:\n        print(f\"relu({value}) = {relu(value)}\")\n```\n\nThis script defines the `relu` function, which takes a single float `z` and returns the maximum of `0` and `z`. The `if __name__ == \"__main__\":` block includes some test cases to demonstrate the function's behavior."}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n    w (np.ndarray): Coefficients of shape (n_features,).\n    y_true (np.ndarray): True labels of shape (n_samples,).\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    # Predicted values\n    y_pred = X @ w\n    # Mean Squared Error\n    mse = np.mean((y_pred - y_true) ** 2)\n    # Regularization term\n    reg_term = alpha * np.sum(w ** 2)\n    # Total loss\n    total_loss = mse + reg_term\n    \n    return round(total_loss, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, 0.5])\n    y_true = np.array([1, 2, 3])\n    alpha = 0.1\n    \n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```\n\nThis script defines the `ridge_loss` function, which calculates the Ridge Regression loss given the feature matrix `X`, coefficients `w`, true labels `y_true`, and regularization parameter `alpha`. The function computes the mean squared error (MSE) and adds a regularization term to it. The total loss is then rounded to four decimal places and returned. The example usage at the bottom demonstrates how to call this function with a sample dataset."}
{"task_id": 44, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the Leaky ReLU activation function:\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n    \n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    print(leaky_relu(10.0))       # Should print 10.0\n    print(leaky_relu(-5.0))       # Should print -0.05\n    print(leaky_relu(-5.0, 0.1))  # Should print -0.5\n    print(leaky_relu(0.0))        # Should print 0.0\n```\n\nThis script defines the `leaky_relu` function, which takes an input `z` and an optional parameter `alpha`. It returns the value of `z` if `z` is positive or zero, and `alpha * z` if `z` is negative. The example usage section demonstrates how to call the function with different inputs."}
{"task_id": 45, "completion_id": 0, "completion": "Certainly! Below is the Python script that defines a function `kernel_function` to compute the linear kernel between two input vectors `x1` and `x2`.\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two vectors.\n\n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n\n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define two vectors\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n\n    # Compute the linear kernel\n    result = kernel_function(x1, x2)\n    print(f\"The linear kernel between {x1} and {x2} is: {result}\")\n```\n\n### Explanation:\n- **Importing numpy**: We import the `numpy` library to use its efficient array operations.\n- **Function Definition**: The `kernel_function` takes two numpy arrays `x1` and `x2` as input and returns their dot product using `np.dot`.\n- **Example Usage**: The script includes an example where two vectors `x1` and `x2` are defined, and the linear kernel is computed and printed.\n\nThis script is self-contained and can be run directly to see the output."}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n    \n    Parameters:\n    y_true (numpy.ndarray): Array of true binary labels.\n    y_pred (numpy.ndarray): Array of predicted binary labels.\n    \n    Returns:\n    float: Precision score.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0  # To avoid division by zero\n    precision_score = true_positives / (true_positives + false_positives)\n    return precision_score\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1])\n    y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n    print(\"Precision:\", precision(y_true, y_pred))\n```\n\nThis script defines a function `precision` that calculates the precision score based on the true and predicted labels provided as numpy arrays. It includes a simple check to prevent division by zero and an example usage to demonstrate how the function works."}
{"task_id": 47, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the `gradient_descent` function for Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent using Mean Squared Error (MSE) as the loss function.\n\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent for linear regression using the specified method.\n    \n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - y: numpy array of shape (n_samples,)\n    - weights: numpy array of shape (n_features,)\n    - learning_rate: float, the step size for each iteration\n    - n_iterations: int, the number of iterations to perform\n    - batch_size: int, the number of samples to use per batch (default is 1)\n    - method: str, the type of gradient descent ('batch', 'mini_batch', 'stochastic')\n    \n    Returns:\n    - weights: numpy array of shape (n_features,) representing the learned weights\n    - losses: list of floats representing the MSE loss at each iteration\n    \"\"\"\n    n_samples, n_features = X.shape\n    losses = []\n    \n    for i in range(n_iterations):\n        if method == 'batch':\n            # Batch Gradient Descent\n            y_pred = np.dot(X, weights)\n            error = y_pred - y\n            gradient = (2 / n_samples) * np.dot(X.T, error)\n            weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            # Mini-Batch Gradient Descent\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n            y_pred = np.dot(X_batch, weights)\n            error = y_pred - y_batch\n            gradient = (2 / batch_size) * np.dot(X_batch.T, error)\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            # Stochastic Gradient Descent\n            indices = np.random.permutation(n_samples)\n            for idx in indices:\n                X_i = X[idx].reshape(1, -1)\n                y_i = y[idx].reshape(1, -1)\n                y_pred = np.dot(X_i, weights)\n                error = y_pred - y_i\n                gradient = (2 / 1) * np.dot(X_i.T, error)\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose from 'batch', 'mini_batch', or 'stochastic'.\")\n        \n        # Calculate and store the loss\n        y_pred = np.dot(X, weights)\n        loss = np.mean((y_pred - y) ** 2)\n        losses.append(round(loss, 4))\n    \n    return weights.tolist(), losses\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some synthetic data\n    np.random.seed(0)\n    X = np.random.rand(100, 3)\n    true_weights = np.array([1.5, -2.0, 3.0])\n    y = np.dot(X, true_weights) + np.random.randn(100) * 0.5\n    \n    # Initialize weights\n    initial_weights = np.zeros(X.shape[1])\n    \n    # Hyperparameters\n    learning_rate = 0.01\n    n_iterations = 100\n    batch_size = 10\n    \n    # Perform gradient descent\n    final_weights, losses = gradient_descent(X, y, initial_weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n    \n    print(\"Final Weights:\", final_weights)\n    print(\"Losses:\", losses)\n```\n\n### Explanation:\n1. **Function Definition**: The `gradient_descent` function takes input features `X`, target values `y`, initial weights, learning rate, number of iterations, batch size, and the method of gradient descent.\n2. **Gradient Descent Variants**:\n   - **Batch Gradient Descent**: Uses the entire dataset to compute the gradient.\n   - **Mini-Batch Gradient Descent**: Uses a subset (batch) of the dataset to compute the gradient.\n   - **Stochastic Gradient Descent**: Uses a single sample at a time to compute the gradient.\n3. **Loss Calculation**: The Mean Squared Error (MSE) is calculated at each iteration and stored in the `losses` list.\n4. **Output**: The function returns the learned weights and the list of losses, both rounded to 4 decimal places.\n\nThis script provides a comprehensive implementation of the three gradient descent variants and demonstrates how to use it with synthetic data."}
{"task_id": 48, "completion_id": 0, "completion": "Certainly! Below is the Python script that implements the Reduced Row Echelon Form (RREF) function. This function will convert a given matrix into its RREF and return it as a Python list.\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix (list of lists or numpy array): The input matrix.\n\n    Returns:\n    list of lists: The matrix in RREF form.\n    \"\"\"\n\n    # Convert the input matrix to a NumPy array for easier manipulation\n    A = np.array(matrix, dtype=float)\n    num_rows, num_cols = A.shape\n\n    # Initialize the pivot row and column\n    pivot_row = 0\n    pivot_col = 0\n\n    while pivot_row < num_rows and pivot_col < num_cols:\n        # Find the row with the maximum absolute value in the current column\n        max_row = pivot_row + np.argmax(np.abs(A[pivot_row:, pivot_col]))\n\n        # Swap the current row with the row that has the maximum value\n        A[[pivot_row, max_row]] = A[[max_row, pivot_row]]\n\n        # If the pivot element is still zero, move to the next column\n        if A[pivot_row, pivot_col] == 0:\n            pivot_col += 1\n            continue\n\n        # Normalize the pivot row so that the pivot element is 1\n        A[pivot_row] /= A[pivot_row, pivot_col]\n\n        # Make all other elements in the current column zero\n        for row in range(num_rows):\n            if row != pivot_row:\n                A[row] -= A[row, pivot_col] * A[pivot_row]\n\n        # Move to the next row and column\n        pivot_row += 1\n        pivot_col += 1\n\n    # Convert the resulting NumPy array back to a Python list\n    return A.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nresult = rref(matrix)\nprint(\"RREF of the matrix:\")\nfor row in result:\n    print(row)\n```\n\n### Explanation:\n1. **Input Conversion**: The input matrix is converted to a NumPy array for easier manipulation.\n2. **Pivot Selection**: The algorithm iterates through the matrix, selecting the pivot element in each column. If the pivot element is zero, it moves to the next column.\n3. **Row Swapping**: If the pivot element is not the largest in its column, the rows are swapped to ensure the largest absolute value is at the pivot position.\n4. **Normalization**: The pivot row is normalized so that the pivot element is 1.\n5. **Elimination**: All other elements in the current column are made zero by subtracting appropriate multiples of the pivot row from the other rows.\n6. **Output Conversion**: The resulting NumPy array is converted back to a Python list using `tolist()`.\n\nThis function should correctly convert any given matrix into its Reduced Row Echelon Form (RREF)."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    # Initialize m and v to zero\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    \n    # Current estimate of the parameters\n    x = x0\n    \n    # Adam optimization loop\n    for t in range(1, num_iterations + 1):\n        # Compute the gradient at the current point\n        g = grad(x)\n        \n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n        \n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1 ** t)\n        \n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update the parameters\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Return the optimized parameters, rounded to 4 decimal places\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# Initial guess\nx0 = np.array([1.0, 2.0])\n\n# Run the Adam optimizer\noptimized_params = adam_optimizer(f, grad, x0, num_iterations=1000)\nprint(optimized_params)\n```\n\nThis script defines the Adam optimizer as specified and includes an example usage with a simple quadratic function and its gradient. The optimized parameters are returned as a Python list with values rounded to four decimal places."}
{"task_id": 50, "completion_id": 0, "completion": "Certainly! Below is the Python script that implements Lasso Regression using Gradient Descent. The function `l1_regularization_gradient_descent` takes the input features `X`, target values `y`, regularization parameter `alpha`, learning rate, maximum number of iterations, and tolerance for convergence. It returns the optimized weights and bias after performing gradient descent with L1 regularization.\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Initialize weights and bias\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    \n    for iteration in range(max_iter):\n        # Compute predictions\n        y_pred = np.dot(X, weights) + bias\n        \n        # Compute gradients\n        dw = (-1 / n_samples) * np.dot(X.T, (y - y_pred)) + (alpha * np.sign(weights))\n        db = (-1 / n_samples) * np.sum(y - y_pred)\n        \n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        \n        # Check for convergence\n        if np.linalg.norm(dw) < tol and np.abs(db) < tol:\n            break\n    \n    # Round the results to the nearest 4th decimal\n    weights_rounded = np.round(weights, 4).tolist()\n    bias_rounded = round(bias, 4)\n    \n    return (weights_rounded, bias_rounded)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([3, 7, 11])\n    \n    # Call the function\n    weights, bias = l1_regularization_gradient_descent(X, y, alpha=0.1, learning_rate=0.01, max_iter=1000, tol=1e-4)\n    \n    print(\"Optimized Weights:\", weights)\n    print(\"Optimized Bias:\", bias)\n```\n\n### Explanation:\n1. **Initialization**: The weights (`weights`) and bias (`bias`) are initialized to zero.\n2. **Gradient Calculation**:\n   - The predictions (`y_pred`) are computed using the current weights and bias.\n   - The gradients for the weights (`dw`) and bias (`db`) are calculated. The L1 regularization term (`alpha * np.sign(weights)`) is added to the weight gradient.\n3. **Weight and Bias Update**: The weights and bias are updated using the gradients and the learning rate.\n4. **Convergence Check**: The loop breaks if the norm of the weight gradient and the absolute value of the bias gradient are both below the specified tolerance (`tol`).\n5. **Rounding and Returning**: The final weights and bias are rounded to the nearest 4th decimal and returned as a tuple.\n\nThis script provides a complete implementation of Lasso Regression using Gradient Descent, including the necessary steps for feature selection via L1 regularization."}
{"task_id": 51, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that calculates the Optimal String Alignment (OSA) distance between two given strings:\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    # Initialize the DP table\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    \n    # Base cases: distance from empty string to source/target\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    \n    # Fill the DP table\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(\n                dp[i - 1][j] + 1,      # Deletion\n                dp[i][j - 1] + 1,      # Insertion\n                dp[i - 1][j - 1] + cost # Substitution\n            )\n            \n            # Check for transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)  # Transposition\n    \n    return dp[len_source][len_target]\n\n# Example usage\nsource = \"caper\"\ntarget = \"acer\"\ndistance = OSA(source, target)\nprint(f\"The OSA distance between '{source}' and '{target}' is {distance}.\")\n```\n\n### Explanation:\n1. **Initialization**:\n   - We initialize a 2D array `dp` where `dp[i][j]` represents the OSA distance between the first `i` characters of `source` and the first `j` characters of `target`.\n   - The base cases handle the distance from an empty string to any prefix of the other string.\n\n2. **Filling the DP Table**:\n   - For each cell `dp[i][j]`, we compute the minimum cost of transforming the first `i` characters of `source` to the first `j` characters of `target` using the four operations: insertion, deletion, substitution, and transposition.\n   - The cost for substitution is 0 if the characters are the same, otherwise 1.\n   - We also check for transpositions when the current characters and their immediate predecessors match the target string's corresponding positions.\n\n3. **Result**:\n   - The final result is found in `dp[len_source][len_target]`, which gives the OSA distance between the entire `source` and `target` strings.\n\nThis script will correctly compute the OSA distance between any two input strings."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List of true binary labels (0 or 1).\n    - y_pred: List of predicted binary labels (0 or 1).\n    \n    Returns:\n    - Recall value rounded to three decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # True positives (TP): where both actual and predicted are 1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # False negatives (FN): where actual is 1 but predicted is 0\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall\n    if tp + fn == 0:\n        return 0.0\n    recall_value = tp / (tp + fn)\n    \n    return round(recall_value, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 0]\n\nprint(f\"Recall: {recall(y_true, y_pred)}\")\n```\n\nThis script defines the `recall` function, which calculates the recall metric for binary classification. It handles the case where the denominator is zero by returning 0.0. The example usage demonstrates how to use the function with sample data."}
{"task_id": 53, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the self-attention mechanism. The function `self_attention` takes input matrix `X` and weight matrices `W_q`, `W_k`, and `W_v` for the query, key, and value transformations, respectively. The output is the self-attention result, rounded to four decimal places and returned as a Python list.\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    X (numpy.ndarray): Input data of shape (sequence_length, input_dim).\n    W_q (numpy.ndarray): Query weight matrix of shape (input_dim, d_model).\n    W_k (numpy.ndarray): Key weight matrix of shape (input_dim, d_model).\n    W_v (numpy.ndarray): Value weight matrix of shape (input_dim, d_model).\n    \n    Returns:\n    list: Self-attention output as a Python list with values rounded to 4 decimal places.\n    \"\"\"\n    # Linear transformations to get queries, keys, and values\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Attention scores\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    \n    # Softmax to get attention weights\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    \n    # Weighted sum to get the final output\n    output = np.dot(attention_weights, V)\n    \n    # Round the output to 4 decimal places and convert to a Python list\n    output_rounded = np.round(output, 4).tolist()\n    \n    return output_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input data and weight matrices\n    X = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n    W_q = np.array([[0.1, 0.2], [0.3, 0.4]])\n    W_k = np.array([[0.5, 0.6], [0.7, 0.8]])\n    W_v = np.array([[0.9, 1.0], [1.1, 1.2]])\n    \n    # Compute self-attention\n    attention_output = self_attention(X, W_q, W_k, W_v)\n    \n    # Print the result\n    print(attention_output)\n```\n\n### Explanation:\n1. **Linear Transformations**: The input matrix `X` is transformed into query (`Q`), key (`K`), and value (`V`) matrices using the respective weight matrices `W_q`, `W_k`, and `W_v`.\n2. **Attention Scores**: The dot product of `Q` and the transpose of `K` is scaled by the square root of the dimension of `K` to get the attention scores.\n3. **Softmax**: The attention scores are passed through a softmax function to get the attention weights.\n4. **Weighted Sum**: The attention weights are used to compute a weighted sum of the value matrix `V` to get the final output.\n5. **Rounding and Conversion**: The final output is rounded to four decimal places and converted to a Python list using `tolist()`.\n\nThis implementation is self-contained and can be run directly to see the output of the self-attention mechanism."}
{"task_id": 54, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements a simple RNN cell according to the provided specifications:\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell to process a sequence of input vectors and produce the final hidden state.\n    \n    Parameters:\n    - input_sequence: A list of lists, where each inner list is an input vector.\n    - initial_hidden_state: The initial hidden state as a list of floats.\n    - Wx: The weight matrix for input-to-hidden connections.\n    - Wh: The weight matrix for hidden-to-hidden connections.\n    - b: The bias vector.\n    \n    Returns:\n    - The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    h_t = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Process each input vector in the sequence\n    for x_t in input_sequence:\n        # Compute the new hidden state\n        h_t = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_t) + b)\n    \n    # Round the final hidden state to four decimal places\n    final_hidden_state = np.round(h_t, 4).tolist()\n    \n    return final_hidden_state\n\n# Example usage:\ninput_sequence = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.2]\n\nfinal_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\nprint(final_hidden_state)\n```\n\nThis script defines the `rnn_forward` function that processes a sequence of input vectors and returns the final hidden state. The function uses the tanh activation function and rounds the final hidden state to four decimal places before returning it. The example usage at the end demonstrates how to call the function with sample data."}
{"task_id": 55, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the `translate_object` function to apply a 2D translation matrix to a set of points:\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Apply a 2D translation matrix to a set of points.\n    \n    Parameters:\n    - points: List of [x, y] coordinates.\n    - tx: Translation distance in the x direction.\n    - ty: Translation distance in the y direction.\n    \n    Returns:\n    - A new list of points after applying the translation matrix.\n    \"\"\"\n    # Convert the list of points to a NumPy array\n    points_array = np.array(points)\n    \n    # Create the translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    \n    # Add a column of ones to the points array to make it homogeneous\n    homogeneous_points = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n    \n    # Apply the translation matrix\n    translated_homogeneous_points = np.dot(homogeneous_points, translation_matrix.T)\n    \n    # Convert back to 2D points by removing the last column\n    translated_points = translated_homogeneous_points[:, :2]\n    \n    # Convert the result back to a Python list\n    translated_points_list = translated_points.tolist()\n    \n    return translated_points_list\n\n# Example usage\nif __name__ == \"__main__\":\n    points = [[1, 2], [3, 4], [5, 6]]\n    tx = 10\n    ty = 20\n    translated_points = translate_object(points, tx, ty)\n    print(\"Original Points:\", points)\n    print(\"Translated Points:\", translated_points)\n```\n\n### Explanation:\n1. **Convert Points to NumPy Array**: The input list of points is converted to a NumPy array for easier manipulation.\n2. **Create Translation Matrix**: A 3x3 translation matrix is created with the given translation distances `tx` and `ty`.\n3. **Homogeneous Coordinates**: A column of ones is added to the points array to convert it to homogeneous coordinates, which allows us to use matrix multiplication for translation.\n4. **Apply Translation Matrix**: The translation matrix is applied to the homogeneous points using matrix multiplication.\n5. **Convert Back to 2D Points**: The result is converted back to 2D points by removing the last column (the homogeneous coordinate).\n6. **Convert to List**: The final result is converted back to a Python list using `tolist()`.\n\nThis script includes an example usage section to demonstrate how the function works."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of the first distribution (P)\n    - sigma_p: Standard deviation of the first distribution (P)\n    - mu_q: Mean of the second distribution (Q)\n    - sigma_q: Standard deviation of the second distribution (Q)\n    \n    Returns:\n    - The KL divergence as a floating-point number.\n    \"\"\"\n    # KL(P || Q) = 0.5 * ( (sigma_p / sigma_q)^2 + (mu_q - mu_p)^2 / sigma_q^2 - 1 + log(sigma_q^2 / sigma_p^2) )\n    term1 = (sigma_p / sigma_q) ** 2\n    term2 = ((mu_p - mu_q) / sigma_q) ** 2\n    term3 = np.log((sigma_q / sigma_p) ** 2)\n    kl_divergence = 0.5 * (term1 + term2 - 1 + term3)\n    return kl_divergence\n\n# Example usage\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2\n\nresult = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(f\"KL Divergence: {result}\")\n```\n\nThis script defines a function `kl_divergence_normal` that computes the KL divergence between two normal distributions with given means and standard deviations. It then demonstrates how to use this function with example values."}
{"task_id": 57, "completion_id": 0, "completion": "Certainly! Below is the Python implementation of the Gauss-Seidel method for solving a system of linear equations:\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the system of linear equations Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    - A: 2D numpy array, the coefficient matrix\n    - b: 1D numpy array, the right-hand side vector\n    - n: int, the number of iterations\n    - x_ini: 1D numpy array or None, initial guess for the solution vector (default is None)\n    \n    Returns:\n    - x: 1D list, the approximated solution vector after n iterations\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Initialize the solution vector\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform Gauss-Seidel iteration\n    for _ in range(n):\n        for i in range(len(b)):\n            # Calculate the sum of the terms involving already updated x values\n            s1 = np.dot(A[i, :i], x[:i])\n            # Calculate the sum of the terms involving not yet updated x values\n            s2 = np.dot(A[i, i+1:], x[i+1:])\n            # Update the current x value\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    \n    # Round the results to 4 decimal places and convert to a list\n    x_rounded = np.round(x, 4).tolist()\n    \n    return x_rounded\n\n# Example usage:\nA = [[4, 1, 2],\n     [3, 5, 1],\n     [1, 1, 3]]\nb = [1, 2, 3]\nn = 100\nx_ini = [0, 0, 0]\n\nsolution = gauss_seidel(A, b, n, x_ini)\nprint(solution)\n```\n\n### Explanation:\n1. **Input Conversion**: The input matrices and vectors are converted to NumPy arrays to facilitate element-wise operations.\n2. **Initialization**: If no initial guess (`x_ini`) is provided, the solution vector `x` is initialized to a vector of zeros. Otherwise, it is set to the provided initial guess.\n3. **Gauss-Seidel Iteration**:\n   - For each iteration, the solution vector `x` is updated.\n   - For each equation (row in `A`), the sum of the terms involving already updated `x` values (`s1`) and the sum of the terms involving not yet updated `x` values (`s2`) are calculated.\n   - The current `x` value is updated using the formula \\( x_i = \\frac{b_i - s1 - s2}{A_{ii}} \\).\n4. **Rounding and Conversion**: The final solution vector is rounded to 4 decimal places and converted to a list before being returned.\n\nThis implementation ensures that the solution vector is iteratively refined over the specified number of iterations, providing an approximate solution to the system of linear equations."}
{"task_id": 58, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the Gaussian Elimination method with partial pivoting to solve a system of linear equations \\(Ax = b\\). The solution vector \\(x\\) is returned with results rounded to the nearest 4th decimal.\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix of the system.\n    b (np.ndarray): Right-hand side vector of the system.\n    \n    Returns:\n    list: Solution vector x, rounded to 4 decimal places.\n    \"\"\"\n    n = len(b)\n    A = np.column_stack((A, b))  # Combine A and b into one augmented matrix\n\n    for i in range(n):\n        # Partial pivoting\n        max_row = i\n        for j in range(i + 1, n):\n            if abs(A[j, i]) > abs(A[max_row, i]):\n                max_row = j\n        A[[i, max_row]] = A[[max_row, i]]  # Swap rows\n\n        # Elimination\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n\n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (A[i, -1] - np.dot(A[i, :-1], x)) / A[i, i]\n\n    # Round the results to 4 decimal places\n    x_rounded = np.round(x, 4)\n\n    return x_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[2, 1, -1],\n                  [-3, -1, 2],\n                  [-2, 1, 2]], dtype=float)\n    b = np.array([8, -11, -3], dtype=float)\n    \n    solution = gaussian_elimination(A, b)\n    print(f\"Solution: {solution}\")\n```\n\n### Explanation:\n1. **Partial Pivoting**: Before performing the elimination step, we ensure numerical stability by swapping rows to place the largest pivot element at the diagonal position.\n2. **Elimination**: We transform the matrix into an upper triangular form by eliminating the elements below the diagonal.\n3. **Backward Substitution**: We solve the resulting upper triangular system starting from the bottom row.\n4. **Rounding**: The final solution vector is rounded to 4 decimal places and returned as a Python list.\n\nThis implementation should work for any square system of linear equations provided in the form of matrices \\(A\\) and vector \\(b\\)."}
{"task_id": 59, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the LSTM network as described in the task. The script includes the `LSTM` class with the `forward` method to process a sequence of inputs and return the hidden states, final hidden state, and final cell state.\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        hidden_states = []\n\n        for t in range(len(x)):\n            combined_input = np.concatenate((x[t], h_t), axis=0)\n\n            # Compute the forget gate\n            ft = self.sigmoid(np.dot(self.Wf, combined_input) + self.bf)\n\n            # Compute the input gate\n            it = self.sigmoid(np.dot(self.Wi, combined_input) + self.bi)\n\n            # Compute the candidate cell state\n            cct = self.tanh(np.dot(self.Wc, combined_input) + self.bc)\n\n            # Update the cell state\n            c_t = ft * c_t + it * cct\n\n            # Compute the output gate\n            ot = self.sigmoid(np.dot(self.Wo, combined_input) + self.bo)\n\n            # Compute the new hidden state\n            h_t = ot * self.tanh(c_t)\n\n            # Append the current hidden state to the list\n            hidden_states.append(h_t)\n\n        # Convert hidden states to list and round to 4 decimal places\n        hidden_states = [np.round(state, 4).tolist() for state in hidden_states]\n        final_hidden_state = np.round(h_t, 4).tolist()\n        final_cell_state = np.round(c_t, 4).tolist()\n\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage\nif __name__ == \"__main__\":\n    input_size = 3\n    hidden_size = 2\n    seq_length = 5\n\n    # Generate random input sequence\n    x = [np.random.randn(input_size, 1) for _ in range(seq_length)]\n\n    # Initialize the LSTM\n    lstm = LSTM(input_size, hidden_size)\n\n    # Initial hidden and cell states\n    initial_hidden_state = np.zeros((hidden_size, 1))\n    initial_cell_state = np.zeros((hidden_size, 1))\n\n    # Forward pass\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\n    print(\"Hidden States:\", hidden_states)\n    print(\"Final Hidden State:\", final_hidden_state)\n    print(\"Final Cell State:\", final_cell_state)\n```\n\n### Explanation:\n1. **Initialization**:\n   - The `__init__` method initializes the weights and biases for the forget gate (`Wf`, `bf`), input gate (`Wi`, `bi`), candidate cell state (`Wc`, `bc`), and output gate (`Wo`, `bo`).\n\n2. **Activation Functions**:\n   - `sigmoid` and `tanh` functions are defined to be used in the computation of gates and cell states.\n\n3. **Forward Pass**:\n   - The `forward` method processes the input sequence `x` and updates the hidden state `h_t` and cell state `c_t` at each time step.\n   - The combined input is created by concatenating the current input `x[t]` and the previous hidden state `h_t`.\n   - The forget gate, input gate, candidate cell state, and output gate are computed using the respective weights and biases.\n   - The cell state `c_t` is updated using the forget gate, input gate, and candidate cell state.\n   - The hidden state `h_t` is updated using the output gate and the updated cell state.\n   - The hidden states at each time step are stored in a list.\n\n4. **Output**:\n   - The hidden states, final hidden state, and final cell state are rounded to 4 decimal places and converted to lists using `numpy`'s `tolist()` method.\n\nThis implementation provides a basic LSTM network that can be used to process sequences of inputs and return the necessary states."}
{"task_id": 60, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the `compute_tf_idf` function according to the given requirements:\n\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"The corpus cannot be empty.\")\n    \n    # Step 1: Compute Term Frequency (TF)\n    def compute_tf(doc):\n        tf_dict = {}\n        doc_length = len(doc)\n        for word in doc:\n            if word in tf_dict:\n                tf_dict[word] += 1\n            else:\n                tf_dict[word] = 1\n        for word in tf_dict:\n            tf_dict[word] /= doc_length\n        return tf_dict\n    \n    # Step 2: Compute Document Frequency (DF)\n    def compute_df(corpus):\n        df_dict = {}\n        for doc in corpus:\n            unique_words = set(doc)\n            for word in unique_words:\n                if word in df_dict:\n                    df_dict[word] += 1\n                else:\n                    df_dict[word] = 1\n        return df_dict\n    \n    # Step 3: Compute Inverse Document Frequency (IDF)\n    def compute_idf(df_dict, n_docs):\n        idf_dict = {}\n        for word, df in df_dict.items():\n            idf_dict[word] = np.log((n_docs + 1) / (df + 1)) + 1\n        return idf_dict\n    \n    # Step 4: Compute TF-IDF\n    def compute_tfidf(tf_dict, idf_dict):\n        tfidf_dict = {}\n        for word, tf in tf_dict.items():\n            if word in idf_dict:\n                tfidf_dict[word] = tf * idf_dict[word]\n            else:\n                tfidf_dict[word] = 0\n        return tfidf_dict\n    \n    # Step 5: Main computation\n    n_docs = len(corpus)\n    df_dict = compute_df(corpus)\n    idf_dict = compute_idf(df_dict, n_docs)\n    tfidf_scores = []\n    \n    for doc in corpus:\n        tf_dict = compute_tf(doc)\n        tfidf_dict = compute_tfidf(tf_dict, idf_dict)\n        doc_scores = [tfidf_dict.get(word, 0) for word in query]\n        tfidf_scores.append(doc_scores)\n    \n    # Round the results to 5 decimal places\n    tfidf_scores = np.array(tfidf_scores).round(4).tolist()\n    \n    return tfidf_scores\n\n# Example usage\ncorpus = [\n    ['word1', 'word2', 'word3'],\n    ['word2', 'word3', 'word4', 'word5'],\n    ['word1', 'word3', 'word5']\n]\n\nquery = ['word1', 'word2', 'word3', 'word6']\n\ntfidf_scores = compute_tf_idf(corpus, query)\nprint(tfidf_scores)\n```\n\n### Explanation:\n1. **Term Frequency (TF)**: The `compute_tf` function calculates the frequency of each word in a document relative to the total number of words in that document.\n2. **Document Frequency (DF)**: The `compute_df` function counts the number of documents in which each word appears.\n3. **Inverse Document Frequency (IDF)**: The `compute_idf` function calculates the IDF for each word, applying smoothing to avoid division by zero.\n4. **TF-IDF Calculation**: The `compute_tfidf` function multiplies the TF and IDF values for each word in the query.\n5. **Main Computation**: The main part of the function iterates over each document, computes the TF-IDF scores for the query words, and collects the results.\n6. **Rounding and Returning**: The final TF-IDF scores are rounded to four decimal places and returned as a list of lists.\n\nThis implementation ensures that the function is robust and handles edge cases such as empty documents and query terms not present in the corpus."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives, False Positives, and False Negatives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        f_score = 0\n    else:\n        f_score = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_score, 3)\n\n# Example usage\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n\nbeta = 1.0\nprint(f\"F-Score: {f_score(y_true, y_pred, beta)}\")\n```\n\nThis script defines a function `f_score` that calculates the F-Score for binary classification tasks. It takes true labels (`y_true`), predicted labels (`y_pred`), and a beta value as input. The function returns the F-Score rounded to three decimal places. The example usage demonstrates how to use the function with sample data."}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        h = np.zeros((T + 1, self.hidden_size, 1))  # Hidden states, h[-1] is the initial state\n        y = np.zeros((T, self.W_hy.shape[0], 1))    # Outputs\n\n        for t in range(T):\n            x = input_sequence[t].reshape(-1, 1)\n            h[t] = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h[t-1]) + self.b_h)\n            y[t] = np.dot(self.W_hy, h[t]) + self.b_y\n\n        return y, h\n\n    def backward(self, input_sequence, expected_output, outputs, h, learning_rate=0.01):\n        \"\"\"\n        Backward pass through the RNN using BPTT.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(h[0])\n\n        for t in reversed(range(T)):\n            dy = (outputs[t] - expected_output[t].reshape(-1, 1)) / T\n            dW_hy += np.dot(dy, h[t].T)\n            db_y += dy\n\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - h[t] * h[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, input_sequence[t].reshape(-1, 1).T)\n            dW_hh += np.dot(dh_raw, h[t-1].T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Clip gradients to prevent explosion\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n\n        # Update parameters\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, input_sequence, expected_output, learning_rate=0.01, epochs=100):\n        \"\"\"\n        Train the RNN on a given sequence and expected output.\n        \"\"\"\n        for epoch in range(epochs):\n            outputs, h = self.forward(input_sequence)\n            loss = 0.5 * np.sum((outputs - expected_output.reshape(outputs.shape)) ** 2)\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n            self.backward(input_sequence, expected_output, outputs, h, learning_rate)\n\n# Example usage\nif __name__ == \"__main__\":\n    input_size = 3\n    hidden_size = 10\n    output_size = 2\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n\n    # Generate a simple sequence and its corresponding expected output\n    input_sequence = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    expected_output = np.array([[4, 5], [7, 8], [10, 11]])\n\n    rnn.train(input_sequence, expected_output, learning_rate=0.01, epochs=1000)\n```\n\n### Explanation:\n- **Initialization**: The `__init__` method initializes the weights and biases of the RNN. The weights are initialized with small random values to break symmetry, and the biases are initialized to zero.\n- **Forward Pass**: The `forward` method processes a sequence of inputs and computes the hidden states and outputs at each time step. The activation function used for the hidden states is the hyperbolic tangent (`tanh`).\n- **Backward Pass**: The `backward` method performs backpropagation through time (BPTT) to compute the gradients of the loss with respect to the weights and biases. It then updates the parameters using gradient descent.\n- **Training**: The `train` method combines the forward and backward passes to train the RNN on a given sequence and expected output. It prints the loss at each epoch to monitor the training progress.\n\nThis implementation is designed to handle a simple sequence prediction task, where the RNN learns to predict the next item in a sequence. The loss function used is the mean squared error (MSE), and the gradients are clipped to prevent exploding gradients."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    m = len(b)\n    if x0 is None:\n        x0 = np.zeros(m)\n    \n    xk = x0\n    rk = b - A @ xk\n    pk = rk.copy()\n    \n    for k in range(n):\n        Apk = A @ pk\n        rkrk = np.dot(rk, rk)\n        \n        alpha = rkrk / np.dot(pk, Apk)\n        xk = xk + alpha * pk\n        rk_new = rk - alpha * Apk\n        \n        beta = np.dot(rk_new, rk_new) / rkrk\n        pk = rk_new + beta * pk\n        \n        if np.linalg.norm(rk_new) < tol:\n            break\n        \n        rk = rk_new\n    \n    return np.round(xk, 8).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[4, 1], [1, 3]], dtype=float)\n    b = np.array([1, 2], dtype=float)\n    n = 100\n    x = conjugate_gradient(A, b, n)\n    print(f\"Solution vector x: {x}\")\n```\n\nThis script defines the `conjugate_gradient` function that implements the Conjugate Gradient method for solving the linear system \\(Ax = b\\). The function takes a symmetric positive-definite matrix \\(A\\), a vector \\(b\\), and other parameters to control the iteration process and convergence. The solution vector \\(x\\) is returned after rounding to 8 decimal places and converting to a list. An example usage is provided to demonstrate how to call the function."}
{"task_id": 64, "completion_id": 0, "completion": "Certainly! Below is the self-contained Python script that implements the Gini Impurity calculation for a set of classes:\n\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Calculate the total number of elements\n    n = len(y)\n    \n    # Return 0 if the list is empty\n    if n == 0:\n        return 0.0\n    \n    # Count the frequency of each class label\n    unique, counts = np.unique(y, return_counts=True)\n    class_frequencies = dict(zip(unique, counts))\n    \n    # Calculate the probability of each class\n    probabilities = [count / n for count in class_frequencies.values()]\n    \n    # Calculate Gini Impurity\n    gini = 1 - sum(p ** 2 for p in probabilities)\n    \n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y = [0, 0, 1, 1, 1, 2, 2, 2, 2]\n    print(f\"Gini Impurity: {gini_impurity(y)}\")\n```\n\n### Explanation:\n1. **Importing Libraries**: We import `numpy` for efficient array operations.\n2. **Function Definition**: The function `gini_impurity` takes a list of class labels `y` and returns the Gini Impurity.\n3. **Edge Case Handling**: If the input list is empty, the function returns 0.0.\n4. **Counting Class Frequencies**: We use `np.unique` to get the unique class labels and their counts.\n5. **Calculating Probabilities**: We compute the probability of each class by dividing the count of each class by the total number of elements.\n6. **Gini Impurity Calculation**: The Gini Impurity is calculated as \\(1 - \\sum(p_i^2)\\), where \\(p_i\\) is the probability of class \\(i\\).\n7. **Rounding**: The result is rounded to three decimal places.\n8. **Example Usage**: An example usage of the function is provided at the end to demonstrate how it works.\n\nThis script should meet the requirements of the task."}
{"task_id": 65, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the conversion of a dense matrix to its Compressed Row Sparse (CSR) format.\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    \n    for row in dense_matrix:\n        non_zero_count = 0\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(col_index)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    \n    return values, col_indices, row_pointers\n\n# Example usage\nif __name__ == \"__main__\":\n    dense_matrix = [\n        [1, 0, 0, 0],\n        [0, 2, 3, 0],\n        [4, 0, 0, 5],\n        [0, 6, 0, 0]\n    ]\n    \n    values, col_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\n    \n    print(\"Values array:\", values)\n    print(\"Column indices array:\", col_indices)\n    print(\"Row pointer array:\", row_pointers)\n```\n\n### Explanation:\n1. **Values Array**: This list stores all the non-zero elements of the dense matrix in row-major order.\n2. **Column Indices Array**: This list stores the column index for each non-zero element in the values array.\n3. **Row Pointer Array**: This list stores the cumulative number of non-zero elements up to the start of each row. The length of this array is one more than the number of rows in the dense matrix, with the last element being the total number of non-zero elements.\n\n### Example Output:\nFor the given `dense_matrix`:\n```python\ndense_matrix = [\n    [1, 0, 0, 0],\n    [0, 2, 3, 0],\n    [4, 0, 0, 5],\n    [0, 6, 0, 0]\n]\n```\nThe output will be:\n```\nValues array: [1, 2, 3, 4, 5, 6]\nColumn indices array: [0, 1, 2, 0, 3, 1]\nRow pointer array: [0, 1, 3, 5, 6]\n```\n\nThis output correctly represents the CSR format of the given dense matrix."}
{"task_id": 66, "completion_id": 0, "completion": "```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    \n    # Convert lists to numpy arrays for easier computation\n    v = np.array(v)\n    L = np.array(L)\n    \n    # Calculate the dot product of v and L\n    dot_product_vL = np.dot(v, L)\n    \n    # Calculate the dot product of L with itself\n    dot_product_LL = np.dot(L, L)\n    \n    # Compute the scalar projection of v onto L\n    scalar_projection = dot_product_vL / dot_product_LL\n    \n    # Compute the vector projection of v onto L\n    vector_projection = scalar_projection * L\n    \n    # Round the result to three decimal places and convert back to list\n    return [round(x, 3) for x in vector_projection.tolist()]\n\n# Example usage:\nv = [2, 3]\nL = [4, 1]\nprint(orthogonal_projection(v, L))  # Output should be [3.632, 0.908]\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    # Initialize the lists to store the values, row indices, and column pointers\n    values = []\n    row_indices = []\n    col_pointers = [0]  # The first element is always 0\n\n    # Iterate over each column in the dense matrix\n    for col in range(len(dense_matrix[0])):\n        # Iterate over each row in the current column\n        for row in range(len(dense_matrix)):\n            # Check if the current element is non-zero\n            if dense_matrix[row][col] != 0:\n                # Append the non-zero value and its row index to the respective lists\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        # Append the current length of the values list to the column pointers\n        col_pointers.append(len(values))\n\n    return values, row_indices, col_pointers\n\n# Example usage\ndense_matrix = [\n    [0, 0, 0, 0],\n    [5, 8, 0, 0],\n    [0, 0, 3, 0],\n    [0, 6, 0, 0]\n]\n\nvalues, row_indices, col_pointers = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointers:\", col_pointers)\n```\n\nThis script defines the `compressed_col_sparse_matrix` function that converts a given dense matrix into its CSC representation. It iterates over each column and then each row within that column to identify non-zero elements, which are then stored in the `values` and `row_indices` lists. The `col_pointers` list keeps track of the starting index of each column in the `values` list. The example usage demonstrates how to call the function and print the resulting CSC representation."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Finds the basis vectors that span the column space (image) of a matrix A.\n    \n    Parameters:\n    - A: A numpy array representing the matrix.\n    \n    Returns:\n    - A list of lists, where each inner list represents a basis vector of the column space,\n      rounded to 8 decimal places.\n    \"\"\"\n    # Convert the input matrix to a NumPy array if it isn't one already\n    A = np.array(A, dtype=float)\n    \n    # Perform row reduction to find the pivot columns\n    _, indices = np.linalg.qr(A, mode='reduced', pivoting=True)\n    \n    # Extract the pivot columns from the original matrix\n    basis_vectors = A[:, indices]\n    \n    # Round the values to 8 decimal places and convert to a list\n    basis_vectors_rounded = np.round(basis_vectors, decimals=8).tolist()\n    \n    return basis_vectors_rounded\n\n# Example usage:\nA = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nprint(matrix_image(A))\n```\n\n### Explanation:\n1. **QR Decomposition**: The function uses QR decomposition with column pivoting (`np.linalg.qr` with `pivoting=True`) to identify the pivot columns. These pivot columns form a basis for the column space of the matrix `A`.\n2. **Extracting Columns**: The pivot columns are extracted from the original matrix `A`.\n3. **Rounding and Conversion**: The values in the basis vectors are rounded to 8 decimal places and converted to a list of lists for the final output.\n\nThis approach ensures that the basis vectors are correctly identified and presented in a format that meets the specified constraints."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression analysis.\n    \n    Parameters:\n    - y_true: array-like, true values of the target variable.\n    - y_pred: array-like, predicted values of the target variable.\n    \n    Returns:\n    - float, R-squared value rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the mean of the true values\n    y_mean = np.mean(y_true)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - y_mean) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    return round(r2, 3)\n\n# Example usage\ny_true = [1, 2, 3, 4, 5]\ny_pred = [1.1, 2.2, 2.8, 4.1, 4.9]\n\nprint(f\"R-squared: {r_squared(y_true, y_pred)}\")\n```\n\nThis script defines a function `r_squared` that calculates the R-squared value for a given set of true and predicted values. It uses NumPy for efficient numerical operations and returns the result rounded to three decimal places. The example usage at the end demonstrates how to call the function with sample data."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    \n    row_length = len(img[0])\n    total_brightness = 0\n    pixel_count = 0\n    \n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    \n    if pixel_count == 0:\n        return -1\n    \n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)\n\n# Example usage:\nimage = [\n    [200, 220, 230],\n    [210, 215, 225],\n    [220, 225, 230]\n]\n\nprint(calculate_brightness(image))  # Output: 218.33\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (array-like): The actual values.\n    y_pred (array-like): The predicted values.\n    \n    Returns:\n    float: The RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    TypeError: If the input is not an array-like structure.\n    \"\"\"\n    # Check if inputs are array-like\n    if not (isinstance(y_true, (list, np.ndarray)) and isinstance(y_pred, (list, np.ndarray))):\n        raise TypeError(\"Both y_true and y_pred must be array-like structures.\")\n    \n    # Convert inputs to numpy arrays for consistency\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate RMSE\n    rmse_value = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    \n    # Return RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(f\"RMSE: {rmse(y_true, y_pred)}\")\n```\n\nThis script defines a function `rmse` that calculates the Root Mean Square Error between two arrays of actual and predicted values. It includes error handling for mismatched array shapes, empty arrays, and invalid input types. The RMSE value is returned rounded to three decimal places."}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n    \n    Parameters:\n    - y_true: numpy array of true binary labels.\n    - y_pred: numpy array of predicted binary labels.\n    \n    Returns:\n    - float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection and union\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    \n    # Handle edge case where both arrays are all zeros\n    if union == 0:\n        return 1.0 if intersection == 0 else 0.0\n    \n    # Calculate the Jaccard Index\n    jaccard = intersection / union\n    \n    return round(jaccard, 3)\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 1, 0]\n\nprint(jaccard_index(y_true, y_pred))  # Output: 0.333\n```"}
{"task_id": 73, "completion_id": 0, "completion": "Certainly! Below is the self-contained Python script that implements the `dice_score` function according to the provided specifications:\n\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: numpy array of true binary labels.\n    - y_pred: numpy array of predicted binary labels.\n    \n    Returns:\n    - float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Ensure the input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if the input arrays have the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"y_true and y_pred must have the same length.\")\n    \n    # Calculate the number of true positives, false positives, and false negatives\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    true_positives = np.sum(y_true == 1)\n    predicted_positives = np.sum(y_pred == 1)\n    \n    # Calculate the Dice Score\n    if true_positives + predicted_positives == 0:\n        return 0.0  # Avoid division by zero\n    dice = (2 * intersection) / (true_positives + predicted_positives)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0, 1, 0, 0]\n    y_pred = [1, 0, 1, 0, 0, 1, 0, 1]\n    \n    score = dice_score(y_true, y_pred)\n    print(f\"Dice Score: {score}\")\n```\n\n### Explanation:\n1. **Input Validation**: The function first ensures that `y_true` and `y_pred` are numpy arrays and checks if they have the same length. If not, it raises a `ValueError`.\n2. **Intersection Calculation**: It calculates the number of true positives (where both `y_true` and `y_pred` are 1).\n3. **True Positives and Predicted Positives**: It calculates the total number of true positives in `y_true` and the total number of predicted positives in `y_pred`.\n4. **Dice Score Calculation**: The Dice Score is calculated using the formula provided. If there are no true or predicted positives, the function returns 0.0 to avoid division by zero.\n5. **Rounding**: The final Dice Score is rounded to 3 decimal places before returning.\n\nThis script includes an example usage section to demonstrate how to call the `dice_score` function and print the result."}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_hypervector(seed, dim):\n    \"\"\"Generate a hypervector with a given seed and dimension.\"\"\"\n    np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\n\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"Bind two hypervectors together.\"\"\"\n    return hv1 * hv2\n\ndef bundle_hypervectors(hvs):\n    \"\"\"Bundle multiple hypervectors together.\"\"\"\n    return np.sum(hvs, axis=0)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim: The dimensionality of the hypervectors.\n    - random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    - A composite hypervector representing the entire row.\n    \"\"\"\n    feature_hvs = []\n    for feature, value in row.items():\n        # Generate hypervector for the feature name\n        feature_seed = random_seeds[feature]\n        feature_hv = create_hypervector(feature_seed, dim)\n        \n        # Generate hypervector for the value using the same feature seed\n        value_seed = hash((feature_seed, value)) % (2**32)\n        value_hv = create_hypervector(value_seed, dim)\n        \n        # Bind the feature and value hypervectors\n        bound_hv = bind_hypervectors(feature_hv, value_hv)\n        \n        # Append the bound hypervector to the list\n        feature_hvs.append(bound_hv)\n    \n    # Bundle all feature hypervectors to create the composite hypervector\n    composite_hv = bundle_hypervectors(np.array(feature_hvs))\n    \n    # Convert the composite hypervector to a list and return\n    return composite_hv.tolist()\n\n# Example usage\nrow = {'age': 30, 'gender': 'male', 'income': 50000}\ndim = 1000\nrandom_seeds = {'age': 42, 'gender': 43, 'income': 44}\n\ncomposite_hv = create_row_hv(row, dim, random_seeds)\nprint(composite_hv)\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n    \n    Parameters:\n        data (list of lists): Each inner list contains [y_true, y_pred] for one observation.\n        \n    Returns:\n        list of lists: A 2x2 confusion matrix.\n    \"\"\"\n    # Initialize the confusion matrix\n    # [[TP, FP], [FN, TN]]\n    matrix = [[0, 0], [0, 0]]\n    \n    for true_label, pred_label in data:\n        if true_label == 1 and pred_label == 1:\n            matrix[0][0] += 1  # True Positive\n        elif true_label == 0 and pred_label == 1:\n            matrix[0][1] += 1  # False Positive\n        elif true_label == 1 and pred_label == 0:\n            matrix[1][0] += 1  # False Negative\n        elif true_label == 0 and pred_label == 0:\n            matrix[1][1] += 1  # True Negative\n    \n    return matrix\n\n# Example usage\ndata = [\n    [1, 1],\n    [0, 1],\n    [1, 0],\n    [0, 0],\n    [1, 1],\n    [0, 0]\n]\n\nprint(confusion_matrix(data))\n# Expected Output:\n# [[2, 1], [1, 2]]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: Numpy array representing the first vector.\n    - v2: Numpy array representing the second vector.\n    \n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n    \"\"\"\n    # Ensure both vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Both vectors must have the same shape.\")\n    \n    # Calculate the dot product of the two vectors\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitudes of the two vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Check for zero magnitude to avoid division by zero\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    # Calculate the cosine similarity\n    cos_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return the result rounded to three decimal places\n    return round(cos_sim, 3)\n\n# Example usage:\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\nprint(cosine_similarity(v1, v2))  # Output: 0.974\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Initialize counters\n    TP = FP = TN = FN = 0\n    \n    # Count True Positives, False Positives, True Negatives, and False Negatives\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 1 and p == 0:\n            FN += 1\n    \n    # Calculate confusion matrix\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    \n    # Calculate accuracy\n    total = TP + FP + TN + FN\n    accuracy = (TP + TN) / total if total > 0 else 0.0\n    \n    # Calculate precision\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    \n    # Calculate recall\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n    \n    # Calculate F1 Score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    # Calculate specificity\n    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n    \n    # Calculate negative predictive value\n    negative_predictive_value = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n    \n    # Round the results to three decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage:\nactual = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\npredicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n\nconfusion_matrix, accuracy, f1_score, specificity, npv = performance_metrics(actual, predicted)\n\nprint(\"Confusion Matrix:\", confusion_matrix)\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1_score)\nprint(\"Specificity:\", specificity)\nprint(\"Negative Predictive Value:\", npv)\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n    - data: A list or NumPy array of numerical values.\n    \n    Returns:\n    - A dictionary containing mean, median, mode, variance, standard deviation,\n      percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    # Convert input to NumPy array for consistency\n    data = np.array(data)\n    \n    # Calculate mean\n    mean = np.mean(data)\n    \n    # Calculate median\n    median = np.median(data)\n    \n    # Calculate mode\n    mode = stats.mode(data)[0][0]\n    \n    # Calculate variance\n    variance = np.var(data)\n    \n    # Calculate standard deviation\n    std_dev = np.std(data)\n    \n    # Calculate percentiles\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    \n    # Calculate interquartile range\n    iqr = percentile_75 - percentile_25\n    \n    # Create dictionary with results\n    result = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(std_dev, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(iqr, 4)\n    }\n    \n    return result\n\n# Example usage\ndata = [1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(descriptive_statistics(data))\n```\n\nThis script defines a function `descriptive_statistics` that calculates and returns the specified descriptive statistics for a given dataset. The function uses NumPy and SciPy libraries for efficient computation and handles the rounding of floating-point numbers to four decimal places where appropriate."}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binom_coeff = math.comb(n, k)\n    \n    # Calculate the probability using the binomial distribution formula\n    prob = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to 5 decimal places\n    return round(prob, 5)\n\n# Example usage:\n# Calculate the probability of getting exactly 3 heads in 5 coin flips with a fair coin\nprint(binomial_probability(5, 3, 0.5))\n```\n\nThis code defines a function `binomial_probability` that calculates the probability of getting exactly `k` successes in `n` trials, where each trial has a success probability of `p`. The function uses the binomial distribution formula and returns the result rounded to five decimal places. The example usage demonstrates how to use the function to calculate the probability of getting exactly 3 heads in 5 coin flips with a fair coin."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    exponent = -((x - mean) ** 2) / (2 * (std_dev ** 2))\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\n\n# Example usage\nx = 1.0\nmean = 0.0\nstd_dev = 1.0\nprint(normal_pdf(x, mean, std_dev))  # Output should be 0.24197\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    # Poisson distribution formula: P(X = k) = (lam^k * e^(-lam)) / k!\n    probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)\n\n# Example usage:\n# Calculate the probability of observing 3 events when the average rate is 2\nprint(poisson_probability(3, 2))\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    contrast = max_val - min_val\n    return contrast\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample grayscale image\n    img = np.array([\n        [120, 150, 80],\n        [70, 200, 160],\n        [90, 180, 220]\n    ])\n    \n    # Calculate the contrast\n    contrast = calculate_contrast(img)\n    print(f\"Contrast of the image: {contrast}\")\n```\n\nThis script defines a function `calculate_contrast` that takes a 2D NumPy array representing a grayscale image and returns the contrast of the image, which is calculated as the difference between the maximum and minimum pixel values. The example usage demonstrates how to use this function with a sample image."}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    # Ensure the vectors are 1D and have the same length\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1D arrays.\")\n    if len(vec1) != len(vec2):\n        raise ValueError(\"Both vectors must have the same length.\")\n    \n    # Calculate the dot product using NumPy's dot function\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\n\n# Example usage\nif __name__ == \"__main__\":\n    vec1 = np.array([1, 2, 3])\n    vec2 = np.array([4, 5, 6])\n    result = calculate_dot_product(vec1, vec2)\n    print(f\"The dot product of {vec1} and {vec2} is {result}\")\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 1:\n        return []\n    \n    # Convert the input list to a NumPy array for easier manipulation\n    data_array = np.array(data)\n    \n    # Generate all possible combinations of indices up to the specified degree\n    n_features = len(data)\n    multi_indices = list(combinations_with_replacement(range(n_features), degree))\n    \n    # Initialize the output list\n    transformed_data = []\n    \n    # Compute the polynomial features for each combination\n    for indices in multi_indices:\n        product = 1.0\n        for i in indices:\n            product *= data_array[i]\n        transformed_data.append(product)\n    \n    # Round the results to 8 decimal places\n    transformed_data = np.round(transformed_data, 8).tolist()\n    \n    return [transformed_data]\n\n# Example usage:\ndata = [1.0, 2.0]\ndegree = 2\nprint(phi_transform(data, degree))\n```\n\nThis script defines the `phi_transform` function, which takes a list of numerical values and a degree as inputs. It then generates polynomial features up to the specified degree and returns them as a nested list. The output is rounded to 8 decimal places for precision. The example usage demonstrates how to call the function with a sample dataset and degree."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Create a matrix of shape (position, d_model) filled with zeros\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    \n    # Calculate the positional encoding values\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            angle = pos / np.power(10000, (i / d_model))\n            pos_enc[pos, i] = np.sin(angle)\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = np.cos(angle)\n    \n    return pos_enc.tolist()\n\n# Example usage:\nposition = 5\nd_model = 4\nprint(pos_encoding(position, d_model))\n```\n\nThis Python function implements the positional encoding as described in the Transformer architecture. It checks for invalid input parameters and returns -1 if the conditions are not met. The positional encoding is calculated using sine and cosine functions for even and odd indices of the model dimensionality, respectively. The resulting array is of type `np.float16` and is converted to a list before being returned."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    # Check for underfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # Otherwise, consider it a good fit\n    else:\n        return 0\n\n# Example usage:\n# print(model_fit_quality(0.95, 0.65))  # Expected output: 1 (Overfitting)\n# print(model_fit_quality(0.6, 0.58))   # Expected output: -1 (Underfitting)\n# print(model_fit_quality(0.85, 0.83))  # Expected output: 0 (Good fit)\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameters\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round and convert to list\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n\n# Example usage\nparameter = np.array([1.0, 2.0])\ngrad = np.array([0.1, 0.2])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 1\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(\"Updated Parameter:\", updated_parameter)\nprint(\"Updated First Moment (m):\", updated_m)\nprint(\"Updated Second Moment (v):\", updated_v)\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims=True)\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\n\ndef multi_head_attention(q, k, v, n_head, mask=None):\n    d_k = q.shape[-1] // n_head\n    q = q.reshape(q.shape[0], n_head, d_k).transpose(1, 0, 2)\n    k = k.reshape(k.shape[0], n_head, d_k).transpose(1, 0, 2)\n    v = v.reshape(v.shape[0], n_head, d_k).transpose(1, 0, 2)\n    \n    scores = np.matmul(q, k.transpose(0, 2, 1)) / np.sqrt(d_k)\n    if mask is not None:\n        scores += mask * -1e9\n    weights = softmax(scores)\n    output = np.matmul(weights, v).transpose(1, 0, 2).reshape(q.shape[0], -1)\n    return output\n\ndef feed_forward(x, w1, b1, w2, b2):\n    hidden = gelu(np.dot(x, w1) + b1)\n    output = np.dot(hidden, w2) + b2\n    return output\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    n_ctx = hparams[\"n_ctx\"]\n    n_head = hparams[\"n_head\"]\n    \n    # Encode the prompt\n    input_ids = encoder.encode(prompt)\n    input_len = len(input_ids)\n    \n    # Initialize the token embeddings and positional embeddings\n    token_embeddings = params[\"wte\"][input_ids]\n    pos_embeddings = params[\"wpe\"][:input_len]\n    x = token_embeddings + pos_embeddings\n    \n    # Layer normalization before the first block\n    x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n    \n    # Generate tokens\n    for _ in range(n_tokens_to_generate):\n        # Attention mechanism\n        q = k = v = x\n        attn_output = multi_head_attention(q, k, v, n_head)\n        \n        # Feed-forward network\n        ff_output = feed_forward(attn_output, np.random.rand(10, 10), np.zeros(10), np.random.rand(10, 3), np.zeros(3))\n        \n        # Apply layer normalization\n        x = layer_norm(ff_output, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n        \n        # Get the next token ID\n        logits = np.dot(x[-1], params[\"wte\"].T)\n        next_token_id = np.argmax(logits)\n        \n        # Append the next token to the sequence\n        input_ids.append(next_token_id)\n        \n        # Update the token and positional embeddings\n        token_embeddings = params[\"wte\"][input_ids[-1]]\n        pos_embeddings = params[\"wpe\"][len(input_ids)-1]\n        x = np.vstack((x, token_embeddings + pos_embeddings))\n    \n    # Decode the generated token IDs back to text\n    generated_text = encoder.decode(input_ids)\n    return generated_text\n\n# Example usage\nprompt = \"hello world\"\ngenerated_text = gen_text(prompt, n_tokens_to_generate=10)\nprint(generated_text)\n```\n\nThis script defines a simplified version of the GPT-2 text generation process. It includes token and positional embeddings, multi-head attention, a feed-forward network, and layer normalization. The `gen_text` function takes a prompt and generates a specified number of tokens, outputting the resulting text."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    \n    # Convert the crystal values into a numpy array for easier manipulation\n    crystal_values = np.array(crystal_values).reshape(-1, 1)\n    \n    # Compute the dot product (similarity) between each pair of crystals\n    scores = np.dot(crystal_values, crystal_values.T)\n    \n    # Apply the softmax function along the rows to get the attention weights\n    attention_weights = np.apply_along_axis(softmax, 1, scores)\n    \n    # Compute the final weighted patterns for each crystal\n    weighted_patterns = np.dot(attention_weights, crystal_values)\n    \n    # Flatten the result and round to 4 decimal places\n    weighted_patterns = np.round(weighted_patterns.flatten(), 4)\n    \n    return weighted_patterns.tolist()\n\n# Example usage:\nn = 3\ncrystal_values = [1, 2, 3]\ndimension = 1  # This parameter is not used in the current implementation but can be expanded for more complex scenarios\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\nThis script defines a function `pattern_weaver` that takes the number of crystals `n`, a list of their values `crystal_values`, and a dimension parameter `dimension` (which is currently not used but can be expanded for more complex scenarios). It computes the self-attention mechanism for the crystals and returns the final weighted patterns, rounded to 4 decimal places. The `softmax` function is implemented to ensure numerical stability during the computation of attention weights."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given query over a corpus of documents.\n    \n    :param corpus: List of strings, where each string is a document.\n    :param query: String representing the query.\n    :param k1: Parameter controlling non-linear term frequency scaling.\n    :param b: Parameter controlling the impact of document length.\n    :return: List of BM25 scores for each document, rounded to 3 decimal places.\n    \"\"\"\n    # Preprocess the corpus and query\n    corpus = [doc.lower().split() for doc in corpus]\n    query = query.lower().split()\n    \n    N = len(corpus)  # Total number of documents\n    avg_dl = np.mean([len(doc) for doc in corpus])  # Average document length\n    \n    # Term Frequency (TF) for each document\n    tf = [Counter(doc) for doc in corpus]\n    \n    # Inverse Document Frequency (IDF) for each term in the query\n    idf = {}\n    for term in query:\n        df = sum(1 for doc in corpus if term in doc)\n        idf[term] = np.log((N - df + 0.5) / (df + 0.5)) + 1\n    \n    # BM25 score calculation\n    scores = []\n    for i, doc in enumerate(corpus):\n        score = 0\n        for term in query:\n            if term in doc:\n                freq = tf[i][term]\n                numerator = idf[term] * freq * (k1 + 1)\n                denominator = freq + k1 * (1 - b + b * (len(doc) / avg_dl))\n                score += numerator / denominator\n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage\ncorpus = [\n    \"the quick brown fox jumps over the lazy dog\",\n    \"the quick brown dog jumps over the lazy cat\",\n    \"a quick brown dog jumps over a lazy fox\",\n    \"the quick brown cat jumps over the lazy dog\"\n]\n\nquery = \"quick brown dog\"\n\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n\nThis script defines a function `calculate_bm25_scores` that calculates the BM25 scores for a given query over a corpus of documents. The function preprocesses the corpus and query, computes the term frequencies, inverse document frequencies, and finally calculates the BM25 scores for each document. The scores are then rounded to three decimal places and returned as a list."}
{"task_id": 91, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that calculates the F1 score from predicted and true labels:\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n    TP = sum((a == 1 and b == 1) for a, b in zip(y_true, y_pred))\n    FP = sum((a == 0 and b == 1) for a, b in zip(y_true, y_pred))\n    FN = sum((a == 1 and b == 0) for a, b in zip(y_true, y_pred))\n\n    # Calculate Precision and Recall\n    if TP + FP == 0:\n        precision = 0\n    else:\n        precision = TP / (TP + FP)\n\n    if TP + FN == 0:\n        recall = 0\n    else:\n        recall = TP / (TP + FN)\n\n    # Calculate F1 Score\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n\n    return round(f1_score, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 1]\n\nf1_score = calculate_f1_score(y_true, y_pred)\nprint(f\"F1 Score: {f1_score}\")\n```\n\nThis script defines a function `calculate_f1_score` that takes two lists of labels (`y_true` and `y_pred`) and returns the F1 score rounded to three decimal places. The example usage at the bottom demonstrates how to use this function."}
{"task_id": 92, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that addresses the task of predicting the power grid consumption for day 15, including removing the daily fluctuation, performing linear regression, and adding a 5% safety margin.\n\n```python\nimport math\nfrom scipy import stats\nimport numpy as np\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    \n    # 2) Perform linear regression on the detrended data.\n    x = np.array(range(1, len(detrended_data) + 1))\n    y = np.array(detrended_data)\n    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n    \n    # 3) Predict day 15's base consumption.\n    day_15_base_consumption = slope * 15 + intercept\n    \n    # 4) Add the day 15 fluctuation back.\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    \n    # 5) Round, then add a 5% safety margin (rounded up).\n    safety_margin = 1.05 * day_15_consumption\n    final_consumption = math.ceil(safety_margin)\n    \n    # 6) Return the final integer.\n    return final_consumption\n\n# Example usage:\nconsumption_data = [120, 130, 140, 150, 160, 170, 180, 190, 200, 210]\npredicted_consumption = power_grid_forecast(consumption_data)\nprint(f\"Predicted consumption for day 15: {predicted_consumption}\")\n```\n\n### Explanation:\n1. **Subtract the Daily Fluctuation**: We calculate the fluctuation for each day using the formula \\(10 \\times \\sin(2\\pi i / 10)\\) and subtract it from the corresponding consumption data point.\n2. **Linear Regression**: We use the `scipy.stats.linregress` function to perform linear regression on the detrended data. This gives us the slope and intercept of the best-fit line.\n3. **Predict Day 15's Base Consumption**: Using the slope and intercept from the linear regression, we predict the base consumption for day 15.\n4. **Add Back the Fluctuation**: We calculate the fluctuation for day 15 and add it back to the predicted base consumption.\n5. **Safety Margin**: We add a 5% safety margin to the total consumption and round it up to the nearest integer.\n6. **Return the Final Integer**: The final predicted consumption is returned as an integer.\n\nThis script provides a comprehensive solution to the task, ensuring the colony's future power needs are met with a safety margin."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate the absolute differences between true and predicted values\n    abs_errors = np.abs(y_true - y_pred)\n    # Calculate the mean of the absolute errors\n    mean_abs_error = np.mean(abs_errors)\n    # Return the mean absolute error rounded to 3 decimal places\n    return round(mean_abs_error, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    y_pred = np.array([1.1, 2.1, 2.9, 4.1, 4.9])\n    print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")\n```\n\nThis script defines a function `mae` that calculates the Mean Absolute Error between two NumPy arrays representing the true and predicted values. The function computes the absolute differences between the corresponding elements of the two arrays, averages these differences, and returns the result rounded to three decimal places. The example usage at the bottom demonstrates how to use the function with sample data."}
{"task_id": 94, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script implementing the multi-head attention mechanism:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the query (Q), key (K), and value (V) matrices.\n    \n    :param X: Input tensor of shape (batch_size, seq_len, d_model)\n    :param W_q: Weight matrix for queries of shape (d_model, d_model)\n    :param W_k: Weight matrix for keys of shape (d_model, d_model)\n    :param W_v: Weight matrix for values of shape (d_model, d_model)\n    :return: Tuple of (Q, K, V) each of shape (batch_size, seq_len, d_model)\n    \"\"\"\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    :param Q: Query matrix of shape (batch_size, seq_len, d_model)\n    :param K: Key matrix of shape (batch_size, seq_len, d_model)\n    :param V: Value matrix of shape (batch_size, seq_len, d_model)\n    :return: Self-attention output of shape (batch_size, seq_len, d_model)\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.matmul(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention on the input tensor.\n    \n    :param X: Input tensor of shape (batch_size, seq_len, d_model)\n    :param W_q: Weight matrix for queries of shape (d_model, d_model)\n    :param W_k: Weight matrix for keys of shape (d_model, d_model)\n    :param W_v: Weight matrix for values of shape (d_model, d_model)\n    :param n_heads: Number of attention heads\n    :return: List of the multi-head attention output, reshaped and rounded to 4 decimal places\n    \"\"\"\n    batch_size, seq_len, d_model = X.shape\n    head_dim = d_model // n_heads\n    \n    # Split the weight matrices into multiple heads\n    W_q_heads = np.split(W_q, n_heads, axis=1)\n    W_k_heads = np.split(W_k, n_heads, axis=1)\n    W_v_heads = np.split(W_v, n_heads, axis=1)\n    \n    outputs = []\n    for i in range(n_heads):\n        Q, K, V = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        output = self_attention(Q, K, V)\n        outputs.append(output)\n    \n    # Concatenate the outputs from all heads\n    concatenated_output = np.concatenate(outputs, axis=-1)\n    \n    # Reshape and round the final output\n    final_output = np.round(concatenated_output, decimals=4).reshape(batch_size, seq_len, d_model).tolist()\n    \n    return final_output\n\n# Example usage\nif __name__ == \"__main__\":\n    batch_size, seq_len, d_model = 2, 3, 4\n    n_heads = 2\n    X = np.random.rand(batch_size, seq_len, d_model)\n    W_q = np.random.rand(d_model, d_model)\n    W_k = np.random.rand(d_model, d_model)\n    W_v = np.random.rand(d_model, d_model)\n    \n    result = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(result)\n```\n\n### Explanation:\n1. **`compute_qkv` Function**:\n   - Computes the query (Q), key (K), and value (V) matrices by multiplying the input tensor `X` with the respective weight matrices `W_q`, `W_k`, and `W_v`.\n\n2. **`self_attention` Function**:\n   - Computes the self-attention mechanism by calculating the attention scores, applying the softmax function to get the attention weights, and then multiplying these weights with the value matrix `V`.\n\n3. **`multi_head_attention` Function**:\n   - Splits the weight matrices into multiple heads.\n   - For each head, it computes the Q, K, V matrices and applies the self-attention mechanism.\n   - Concatenates the outputs from all heads.\n   - Reshapes and rounds the final output to 4 decimal places and returns it as a Python list.\n\nThis implementation ensures that the multi-head attention mechanism is correctly applied to the input tensor, and the results are returned in the specified format."}
{"task_id": 95, "completion_id": 0, "completion": "```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Initialize contingency table\n    a, b, c, d = 0, 0, 0, 0\n    \n    # Populate the contingency table\n    for i in range(len(x)):\n        if x[i] == 1 and y[i] == 1:\n            a += 1\n        elif x[i] == 1 and y[i] == 0:\n            b += 1\n        elif x[i] == 0 and y[i] == 1:\n            c += 1\n        elif x[i] == 0 and y[i] == 0:\n            d += 1\n    \n    # Calculate the Phi coefficient\n    numerator = (a * d - b * c)\n    denominator = ((a + b) * (c + d) * (a + c) * (b + d)) ** 0.5\n    \n    if denominator == 0:\n        return 0.0  # Avoid division by zero\n    \n    phi = numerator / denominator\n    \n    # Return the result rounded to 4 decimal places\n    return round(phi, 4)\n\n# Example usage\nx = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\ny = [1, 1, 0, 1, 0, 0, 1, 0, 1, 0]\n\nprint(phi_corr(x, y))  # Output: 0.2929\n```"}
{"task_id": 96, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Hard Sigmoid activation function in Python:\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # Compute the Hard Sigmoid value\n    result = min(max(0, 0.2 * x + 0.5), 1)\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test the function with some example inputs\n    test_values = [-3.0, -1.0, 0.0, 1.0, 3.0]\n    for value in test_values:\n        print(f\"hard_sigmoid({value}) = {hard_sigmoid(value)}\")\n```\n\nThis function `hard_sigmoid` takes a single floating-point number as input and returns the Hard Sigmoid of that input. The Hard Sigmoid function is defined as:\n\n\\[ \\text{Hard Sigmoid}(x) = \\min(\\max(0, 0.2x + 0.5), 1) \\]\n\nThis ensures that the output is clamped between 0 and 1, making it a computationally efficient approximation of the standard sigmoid function. The example usage at the bottom demonstrates how to call the function with various input values."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n\n# Example usage\nprint(elu(3.5))  # Should output 3.5\nprint(elu(-1.0))  # Should output -0.6321\nprint(elu(-2.0, 0.5))  # Should output -0.3935\n```"}
{"task_id": 98, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the PReLU (Parametric ReLU) activation function:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test the PReLU function with different inputs\n    test_values = [-1.0, 0.0, 1.0, -2.5, 3.7]\n    alpha_value = 0.25\n\n    for value in test_values:\n        result = prelu(value, alpha_value)\n        print(f\"PReLU({value}) with alpha={alpha_value} = {result}\")\n```\n\nThis script defines the `prelu` function, which takes an input `x` and an optional `alpha` parameter. The function returns `x` if `x` is non-negative, and `alpha * x` if `x` is negative. The example usage at the bottom demonstrates how to use the `prelu` function with various input values and a specified `alpha` value."}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    # To avoid numerical overflow, use the identity:\n    # log(1 + e^x) = x if x is large, otherwise use the direct formula\n    if x > 20:  # This threshold helps to avoid overflow in exp(x)\n        return round(x, 4)\n    else:\n        return round(math.log1p(math.exp(x)), 4)\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    test_values = [-10, -1, 0, 1, 10, 20, 50]\n    for value in test_values:\n        print(f\"softplus({value}) = {softplus(value)}\")\n```\n\nThis implementation of the `softplus` function includes a check to handle large values of `x` to prevent numerical overflow. For very large values of `x`, `log(1 + e^x)` is approximately equal to `x`, which is used to safely compute the result. The results are rounded to four decimal places as specified."}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n\n# Example usage:\nprint(softsign(2.5))  # Expected output: 0.7143\nprint(softsign(-3.0)) # Expected output: -0.7500\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert lists to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    # Compute the clipped surrogate objective\n    L_clip = np.minimum(rhos * A, np.clip(rhos, 1 - epsilon, 1 + epsilon) * A)\n\n    # Compute the KL divergence between the old policy and the reference policy\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n\n    # Compute the final GRPO objective\n    objective = np.mean(L_clip) - beta * kl_divergence\n\n    # Round the result to the nearest 6th decimal\n    return round(objective, 6)\n\n# Example usage\nrhos = [1.2, 0.8, 1.5, 0.9]\nA = [0.5, -0.3, 0.7, -0.4]\npi_theta_old = [0.2, 0.3, 0.4, 0.1]\npi_theta_ref = [0.25, 0.35, 0.45, 0.15]\n\nresult = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\nprint(f\"GRPO Objective Value: {result}\")\n```\n\nThis script defines the `grpo_objective` function, which computes the Group Relative Policy Optimization (GRPO) objective function. It takes the likelihood ratios, advantage estimates, old policy probabilities, reference policy probabilities, and optional clipping and KL divergence penalty parameters as inputs. The function returns the computed GRPO objective value, rounded to the nearest 6th decimal. The example usage at the end demonstrates how to call this function with sample data."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    # Swish is defined as x * sigmoid(x)\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    return round(x * sigmoid_x, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [0.5, -2.0, 3.0, -1.5]\n    for value in test_values:\n        print(f\"Swish({value}) = {swish(value)}\")\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * (alpha * math.exp(x) - alpha), 4)\n\n# Example usage:\n# print(selu(0.5))  # Output: 0.5253\n# print(selu(-0.5)) # Output: -0.544\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    z = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    \n    # Convert probabilities to binary predictions using a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Convert the numpy array to a list\n    return predictions.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    weights = np.array([0.5, -0.5])\n    bias = 0.1\n\n    # Get predictions\n    predictions = predict_logistic(X, weights, bias)\n    print(predictions)  # Output will depend on the input data and model parameters\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(z):\n    \"\"\"Compute the softmax function.\"\"\"\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return e_z / e_z.sum(axis=1, keepdims=True)\n\ndef cross_entropy_loss(y_true, y_pred):\n    \"\"\"Compute the cross entropy loss.\"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Parameters:\n    X : np.ndarray, input features of shape (N, D)\n    y : np.ndarray, one-hot encoded labels of shape (N, C)\n    learning_rate : float, step size for gradient descent\n    iterations : int, number of iterations for gradient descent\n\n    Returns:\n    B : list[float], CxM updated parameter vector rounded to 4 floating points\n    losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    N, D = X.shape\n    C = y.shape[1]\n    \n    # Initialize weights randomly\n    B = np.random.randn(D, C)\n    \n    losses = []\n    \n    for i in range(iterations):\n        # Compute predictions\n        z = X @ B\n        y_pred = softmax(z)\n        \n        # Compute loss\n        loss = cross_entropy_loss(y, y_pred)\n        losses.append(round(loss, 4))\n        \n        # Compute gradients\n        grad_B = X.T @ (y_pred - y) / N\n        \n        # Update weights\n        B -= learning_rate * grad_B\n    \n    # Round the final weights and losses\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    \n    return B_rounded, losses_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some example data\n    np.random.seed(42)\n    X = np.random.randn(100, 10)\n    y = np.random.randint(0, 3, size=(100,))\n    y_one_hot = np.eye(3)[y]\n\n    # Train the model\n    B, losses = train_softmaxreg(X, y_one_hot, learning_rate=0.01, iterations=1000)\n    \n    print(\"Optimized Coefficients:\", B)\n    print(\"Losses Over Iterations:\", losses)\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef binary_cross_entropy_loss(y_true, y_pred):\n    \"\"\"Binary Cross Entropy Loss function.\"\"\"\n    epsilon = 1e-15  # To avoid log(0) or log(1)\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Initialize weights and bias\n    n_features = X.shape[1]\n    weights = np.zeros(n_features)\n    bias = 0\n    \n    # Lists to store the loss and weights over iterations\n    loss_history = []\n    weights_history = []\n    \n    for i in range(iterations):\n        # Linear combination of inputs and weights\n        z = np.dot(X, weights) + bias\n        \n        # Apply sigmoid activation function\n        y_pred = sigmoid(z)\n        \n        # Compute the loss\n        loss = binary_cross_entropy_loss(y, y_pred)\n        loss_history.append(loss)\n        \n        # Compute gradients\n        dw = (1 / X.shape[0]) * np.dot(X.T, (y_pred - y))\n        db = (1 / X.shape[0]) * np.sum(y_pred - y)\n        \n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        \n        # Store the current weights\n        weights_history.append(weights.copy())\n    \n    # Convert the final weights and loss history to lists and round to 4 decimal places\n    final_weights = [round(w, 4) for w in weights.tolist()]\n    final_bias = round(bias, 4)\n    final_loss_history = [round(l, 4) for l in loss_history]\n    \n    return final_weights, final_bias, final_loss_history\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some example data\n    np.random.seed(42)\n    X = np.random.rand(100, 2)\n    y = np.random.randint(0, 2, size=100)\n    \n    # Train the logistic regression model\n    final_weights, final_bias, final_loss_history = train_logreg(X, y, learning_rate=0.1, iterations=1000)\n    \n    print(\"Final Weights:\", final_weights)\n    print(\"Final Bias:\", final_bias)\n    print(\"Loss History (last 5):\", final_loss_history[-5:])\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \n    Parameters:\n    X (np.ndarray): Input data of shape (batch_size, seq_length, d_model).\n    W_q (np.ndarray): Weight matrix for queries of shape (d_model, d_model).\n    W_k (np.ndarray): Weight matrix for keys of shape (d_model, d_model).\n    W_v (np.ndarray): Weight matrix for values of shape (d_model, d_model).\n    \n    Returns:\n    tuple: A tuple containing the computed Q, K, and V matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n    Q (np.ndarray): Query matrix of shape (batch_size, seq_length, d_model).\n    K (np.ndarray): Key matrix of shape (batch_size, seq_length, d_model).\n    V (np.ndarray): Value matrix of shape (batch_size, seq_length, d_model).\n    mask (np.ndarray): Attention mask of shape (batch_size, 1, seq_length, seq_length).\n    \n    Returns:\n    np.ndarray: The output of the masked self-attention mechanism.\n    \"\"\"\n    # Compute the dot product between Q and K\n    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(K.shape[-1])\n    \n    # Apply the mask\n    scores += mask * -1e9\n    \n    # Compute the attention weights\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of V\n    output = np.matmul(attention_weights, V)\n    \n    return output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    batch_size = 2\n    seq_length = 3\n    d_model = 4\n    \n    # Random input data\n    X = np.random.rand(batch_size, seq_length, d_model)\n    \n    # Random weight matrices\n    W_q = np.random.rand(d_model, d_model)\n    W_k = np.random.rand(d_model, d_model)\n    W_v = np.random.rand(d_model, d_model)\n    \n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Random attention mask\n    mask = np.triu(np.ones((batch_size, 1, seq_length, seq_length)), k=1)\n    \n    # Compute masked attention\n    attention_output = masked_attention(Q, K, V, mask)\n    \n    print(attention_output)\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nimport math\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is calculated using the Shannon entropy formula, which measures\n    the unpredictability of the information content. A lower entropy indicates\n    less disorder (more uniform distribution), while a higher entropy indicates\n    more disorder (less uniform distribution).\n    \"\"\"\n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the probability of each color\n    probabilities = [count / total_apples for count in color_counts.values()]\n    \n    # Calculate the Shannon entropy\n    entropy = -sum(p * math.log(p, 2) for p in probabilities)\n    \n    # Round the result to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Test cases\nprint(disorder([0,0,0,0]))  # Expected: 0.0\nprint(disorder([1,1,0,0]))  # Expected: 1.0\nprint(disorder([0,1,2,3]))  # Expected: 2.0\nprint(disorder([0,0,1,1,2,2,3,3]))  # Expected: 2.0\n```"}
{"task_id": 109, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements Layer Normalization for sequence data as described:\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Layer Normalization on an input tensor.\n    \n    Parameters:\n    - X: np.ndarray, shape (batch_size, seq_length, feature_dim)\n        Input tensor.\n    - gamma: np.ndarray, shape (feature_dim,)\n        Scaling parameter.\n    - beta: np.ndarray, shape (feature_dim,)\n        Shifting parameter.\n    - epsilon: float, default=1e-5\n        Small value to avoid division by zero.\n    \n    Returns:\n    - normalized_X: list\n        Normalized input tensor, rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    # Compute the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scaling and shifting\n    X_normalized = gamma * X_normalized + beta\n    \n    # Round to 5 decimal places and convert to list\n    normalized_X = np.round(X_normalized, decimals=5).tolist()\n    \n    return normalized_X\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    X = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    gamma = np.array([1.0, 1.0])\n    beta = np.array([0.0, 0.0])\n    \n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    \n    # Print the result\n    print(normalized_X)\n```\n\n### Explanation:\n1. **Input Parameters**:\n   - `X`: A 3D numpy array of shape `(batch_size, seq_length, feature_dim)` representing the input tensor.\n   - `gamma`: A 1D numpy array of shape `(feature_dim,)` representing the scaling parameter.\n   - `beta`: A 1D numpy array of shape `(feature_dim,)` representing the shifting parameter.\n   - `epsilon`: A small float value to prevent division by zero during normalization.\n\n2. **Normalization Process**:\n   - Compute the mean and variance of the input tensor across the feature dimension.\n   - Normalize the input tensor using the computed mean and variance.\n   - Scale and shift the normalized tensor using the `gamma` and `beta` parameters.\n\n3. **Output**:\n   - The normalized tensor is rounded to 5 decimal places and converted to a list before being returned.\n\nThis implementation ensures that the normalization is performed correctly and efficiently, and the output is formatted as specified."}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR score for evaluating machine translation quality.\n    \n    Parameters:\n    reference (str): The reference translation.\n    candidate (str): The candidate translation.\n    alpha (float): The parameter to balance precision and recall.\n    beta (int): The parameter to control the penalty for fragmentation.\n    gamma (float): The parameter to control the penalty weight.\n    \n    Returns:\n    float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    \n    def unigrams(text):\n        \"\"\"Return a list of unigrams from the text.\"\"\"\n        return text.lower().split()\n    \n    def chunk_pairs(ref_unigrams, cand_unigrams):\n        \"\"\"Find matching chunks between reference and candidate unigrams.\"\"\"\n        ref_matched = [False] * len(ref_unigrams)\n        cand_matched = [False] * len(cand_unigrams)\n        for i, ru in enumerate(ref_unigrams):\n            for j, cu in enumerate(cand_unigrams):\n                if ru == cu and not ref_matched[i] and not cand_matched[j]:\n                    ref_matched[i] = True\n                    cand_matched[j] = True\n        return sum(ref_matched), sum(cand_matched)\n    \n    def fragmentation(ref_unigrams, cand_unigrams, ref_matched, cand_matched):\n        \"\"\"Calculate the fragmentation penalty.\"\"\"\n        fragments = 0\n        last_match = -1\n        for i, matched in enumerate(cand_matched):\n            if matched:\n                if i > last_match + 1:\n                    fragments += 1\n                last_match = i\n        return fragments\n    \n    ref_unigrams = unigrams(reference)\n    cand_unigrams = unigrams(candidate)\n    \n    # Calculate matches\n    ref_matches, cand_matches = chunk_pairs(ref_unigrams, cand_unigrams)\n    precision = cand_matches / len(cand_unigrams) if len(cand_unigrams) > 0 else 0\n    recall = ref_matches / len(ref_unigrams) if len(ref_unigrams) > 0 else 0\n    \n    # Calculate F-mean\n    if precision + recall > 0:\n        f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n    else:\n        f_mean = 0\n    \n    # Calculate fragmentation penalty\n    frag_penalty = gamma * fragmentation(ref_unigrams, cand_unigrams, ref_matches, cand_matches)\n    \n    # Calculate final METEOR score\n    meteor = f_mean * (beta / (beta + frag_penalty))\n    \n    return round(meteor, 3)\n\n# Example usage\nreference = \"the cat is on the mat\"\ncandidate = \"there is a cat on the mat\"\nprint(meteor_score(reference, candidate))\n```\n\nThis script defines a function `meteor_score` that calculates the METEOR score for evaluating the quality of machine translations. It takes into account unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation. The function returns the METEOR score rounded to three decimal places."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) for two events.\n    \n    Parameters:\n    - joint_counts: int, the number of times both events occur together.\n    - total_counts_x: int, the total occurrences of event X.\n    - total_counts_y: int, the total occurrences of event Y.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - pmi: float, the PMI rounded to 3 decimal places.\n    \"\"\"\n    # Calculate probabilities\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    \n    # Avoid division by zero\n    if p_x == 0 or p_y == 0 or p_xy == 0:\n        return 0.0\n    \n    # Compute PMI\n    pmi = np.log(p_xy / (p_x * p_y))\n    \n    # Return PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    joint_counts = 10\n    total_counts_x = 50\n    total_counts_y = 70\n    total_samples = 1000\n    pmi = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)\n    print(f\"PMI: {pmi}\")\n```\n\nThis script defines a function `compute_pmi` that calculates the Pointwise Mutual Information based on the provided counts. It handles edge cases where probabilities might be zero and returns the PMI value rounded to three decimal places. The example usage at the bottom demonstrates how to call this function with sample data."}
{"task_id": 112, "completion_id": 0, "completion": "Certainly! Below is a Python script that implements the Min-Max Normalization function as described:\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    if min_val == max_val:\n        # If all values are the same, return a list of 0.0\n        return [0.0] * len(x)\n    \n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [10, 20, 30, 40, 50]\n    normalized_data = min_max(data)\n    print(normalized_data)\n```\n\n### Explanation:\n1. **Function Definition**: The function `min_max` takes a list of integers `x` and returns a list of floats.\n2. **Edge Case Handling**:\n   - If the input list is empty, it returns an empty list.\n   - If all values in the list are the same, the function returns a list of zeros with the same length as the input list.\n3. **Min-Max Normalization**:\n   - Calculate the minimum (`min_val`) and maximum (`max_val`) values in the list.\n   - Normalize each value in the list using the formula \\((\\text{value} - \\text{min\\_val}) / (\\text{max\\_val} - \\text{min\\_val})\\).\n   - Round each normalized value to 4 decimal places.\n4. **Example Usage**:\n   - The script includes an example usage where it normalizes a sample list of integers and prints the result.\n\nThis script ensures that the feature values are scaled to the range [0, 1], which is useful for various machine learning algorithms."}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    A simple residual block that processes the input through two weight layers,\n    applies ReLU activations, and adds the original input via a shortcut connection\n    before a final ReLU activation.\n    \n    Parameters:\n    - x (np.ndarray): Input 1D array.\n    - w1 (np.ndarray): First weight matrix for the first layer.\n    - w2 (np.ndarray): Second weight matrix for the second layer.\n    \n    Returns:\n    - np.ndarray: Output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # First layer with ReLU activation\n    z1 = relu(np.dot(w1, x))\n    \n    # Second layer with ReLU activation\n    z2 = relu(np.dot(w2, z1))\n    \n    # Shortcut connection and final ReLU activation\n    output = relu(z2 + x)\n    \n    # Return the output rounded to 4 decimal places and converted to a list\n    return np.round(output, 4).tolist()\n\n# Example usage:\nx = np.array([1, 2, 3])\nw1 = np.array([[0.5, 0.1, 0.2], [0.3, 0.4, 0.6]])\nw2 = np.array([[0.7, 0.8], [0.9, 0.2]])\n\noutput = residual_block(x, w1, w2)\nprint(output)\n```\n\n### Explanation:\n- **ReLU Activation**: The `relu` function applies element-wise ReLU activation, which sets all negative values to zero.\n- **First Layer**: The input `x` is multiplied by the first weight matrix `w1`, and ReLU activation is applied.\n- **Second Layer**: The output from the first layer is multiplied by the second weight matrix `w2`, and ReLU activation is applied again.\n- **Shortcut Connection**: The original input `x` is added to the output of the second layer, and a final ReLU activation is applied to ensure non-linearity.\n- **Output**: The final output is rounded to 4 decimal places and converted to a list before being returned.\n\nThis implementation ensures that the residual block maintains the properties of a deep learning residual network, allowing for the training of deeper networks by mitigating the vanishing gradient problem."}
{"task_id": 114, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the Global Average Pooling function as described:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array representing feature maps.\n    \n    Parameters:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,) where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    # Ensure the input is a 3D array\n    if x.ndim != 3:\n        raise ValueError(\"Input must be a 3D array of shape (height, width, channels)\")\n    \n    # Calculate the average across the height and width dimensions\n    result = np.mean(x, axis=(0, 1))\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample 3D array with shape (4, 4, 3)\n    x = np.random.rand(4, 4, 3)\n    \n    # Apply Global Average Pooling\n    pooled_output = global_avg_pool(x)\n    \n    print(\"Original array shape:\", x.shape)\n    print(\"Pooled output shape:\", pooled_output.shape)\n    print(\"Pooled output:\", pooled_output)\n```\n\nThis script defines the `global_avg_pool` function, which takes a 3D NumPy array as input and returns a 1D array where each element is the average of all values in the corresponding feature map. The example usage demonstrates how to use this function with a randomly generated 3D array."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array in BCHW format.\n    \n    Parameters:\n    X (np.ndarray): Input array of shape (N, C, H, W).\n    gamma (np.ndarray): Scale parameter of shape (C,).\n    beta (np.ndarray): Shift parameter of shape (C,).\n    epsilon (float): Small constant for numerical stability.\n    \n    Returns:\n    list: Normalized and scaled array, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Extract dimensions\n    N, C, H, W = X.shape\n    \n    # Compute mean and variance across batch and spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and shift\n    out = gamma.reshape(1, C, 1, 1) * X_norm + beta.reshape(1, C, 1, 1)\n    \n    # Round and convert to list\n    return np.round(out, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Dummy data\n    X = np.random.randn(2, 3, 4, 4)\n    gamma = np.array([1.0, 2.0, 3.0])\n    beta = np.array([0.1, 0.2, 0.3])\n    \n    # Perform batch normalization\n    normalized_X = batch_normalization(X, gamma, beta)\n    print(normalized_X)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    # Apply the power rule: d/dx [c * x^n] = n * c * x^(n-1)\n    derivative_value = n * c * x**(n - 1)\n    return round(derivative_value, 4)\n\n# Example usage:\n# To test the function, you can use these values:\n# c = 3, x = 2, n = 4\n# The expected output should be 96.0000\nprint(poly_term_derivative(3, 2, 4))\n```\n\nThis script defines a function `poly_term_derivative` that calculates the derivative of a polynomial term at a specified point using the power rule. It takes three parameters: the coefficient `c`, the point `x` where the derivative is evaluated, and the exponent `n`. The result is rounded to four decimal places for precision. The example usage demonstrates how to call the function and what output to expect."}
{"task_id": 117, "completion_id": 0, "completion": "Certainly! Below is a Python script that implements the function `orthonormal_basis` to compute an orthonormal basis for a list of 2D vectors using the Gram-Schmidt process. The function takes a list of 2D vectors and a tolerance value to determine linear independence. The output is a list of orthonormal vectors, each rounded to 4 decimal places and converted to a list format.\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    - vectors: List of 2D vectors.\n    - tol: Tolerance value to determine linear independence.\n    \n    Returns:\n    - A list of orthonormal vectors that span the same subspace as the input vectors.\n    \"\"\"\n    # Convert the list of vectors to a NumPy array\n    vectors = np.array(vectors)\n    \n    # Initialize an empty list to store the orthonormal basis\n    orthonormal_basis = []\n    \n    for v in vectors:\n        # Project v onto the subspace spanned by the current orthonormal basis\n        w = v.copy()\n        for u in orthonormal_basis:\n            proj = np.dot(v, u) * u\n            w -= proj\n        \n        # Check if w is non-zero\n        norm_w = np.linalg.norm(w)\n        if norm_w > tol:\n            # Normalize w and add it to the orthonormal basis\n            u = w / norm_w\n            orthonormal_basis.append(u)\n    \n    # Round the orthonormal basis to 4 decimal places and convert to list format\n    orthonormal_basis = [np.round(u, decimals=4).tolist() for u in orthonormal_basis]\n    \n    return orthonormal_basis\n\n# Example usage:\nvectors = [[1, 1], [2, 2], [3, 4]]\nresult = orthonormal_basis(vectors)\nprint(result)\n```\n\n### Explanation:\n1. **Input Conversion**: The input list of vectors is converted to a NumPy array for easier manipulation.\n2. **Initialization**: An empty list `orthonormal_basis` is initialized to store the resulting orthonormal vectors.\n3. **Gram-Schmidt Process**:\n   - For each vector `v` in the input list:\n     - Subtract the projection of `v` onto the subspace spanned by the current orthonormal basis to get `w`.\n     - Check if `w` is non-zero using the tolerance value `tol`.\n     - If `w` is non-zero, normalize `w` to get `u` and add `u` to the orthonormal basis.\n4. **Output Formatting**: Each vector in the orthonormal basis is rounded to 4 decimal places and converted to a list format before being returned.\n\nThis script should meet the requirements and constraints specified in the task."}
{"task_id": 118, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the function to compute the cross product of two 3-dimensional vectors, rounds the result to 4 decimal places, and returns it as a list:\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (list or np.array): First 3D vector.\n    b (list or np.array): Second 3D vector.\n    \n    Returns:\n    list: The cross product of the two vectors, rounded to 4 decimal places.\n    \"\"\"\n    # Convert input lists to numpy arrays for computation\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product\n    cross_result = np.cross(a, b)\n    \n    # Round the result to 4 decimal places\n    rounded_result = np.round(cross_result, 4)\n    \n    # Convert the result to a list\n    result_list = rounded_result.tolist()\n    \n    return result_list\n\n# Example usage\nvector_a = [1, 2, 3]\nvector_b = [4, 5, 6]\n\nresult = cross_product(vector_a, vector_b)\nprint(f\"The cross product of {vector_a} and {vector_b} is: {result}\")\n```\n\n### Explanation:\n1. **Function Definition**: The function `cross_product` takes two parameters, `a` and `b`, which are the 3-dimensional vectors.\n2. **Input Conversion**: The input vectors are converted to NumPy arrays to leverage NumPy's efficient vector operations.\n3. **Cross Product Calculation**: The `np.cross` function is used to compute the cross product of the two vectors.\n4. **Rounding**: The result is rounded to 4 decimal places using `np.round`.\n5. **Conversion to List**: The rounded result is converted to a list using `tolist()`.\n6. **Return**: The function returns the final result as a list.\n\n### Example Usage:\nThe example usage demonstrates how to call the `cross_product` function with two vectors and print the result."}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): Coefficient matrix (must be square).\n    b (np.array): Constant vector.\n    \n    Returns:\n    list: Solution vector x if a unique solution exists, otherwise -1.\n    \"\"\"\n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # Check if the system has a unique solution\n    if det_A == 0:\n        return -1\n    \n    # Number of variables\n    n = len(b)\n    \n    # Initialize the solution vector\n    x = np.zeros(n)\n    \n    # Compute the determinants for each variable\n    for i in range(n):\n        # Create a new matrix by replacing the i-th column of A with b\n        A_i = A.copy()\n        A_i[:, i] = b\n        # Calculate the determinant of the new matrix\n        det_A_i = np.linalg.det(A_i)\n        # Cramer's rule: x_i = det(A_i) / det(A)\n        x[i] = det_A_i / det_A\n    \n    # Round the results to the nearest 4th decimal place\n    x_rounded = np.round(x, 4)\n    \n    # Convert the solution vector to a list\n    return x_rounded.tolist()\n\n# Example usage:\nA = np.array([[2, 1], [5, 7]])\nb = np.array([2, 3])\nsolution = cramers_rule(A, b)\nprint(solution)  # Output: [0.1667, 0.3333]\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the input lists are of the same length and not empty\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    \n    # Convert lists to numpy arrays for element-wise operations\n    p_array = np.array(p)\n    q_array = np.array(q)\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p_array * q_array))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc)\n    \n    # Return the result rounded to 4 decimal places\n    return round(bd, 4)\n\n# Example usage:\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.4, 0.3, 0.2, 0.1]\nprint(bhattacharyya_distance(p, q))  # Expected output: 0.6931\n```"}
{"task_id": 121, "completion_id": 0, "completion": "Certainly! Below is a Python script that defines a function `vector_sum` which takes two lists as input and returns their element-wise sum if they are of the same length. If the lengths are different, it returns -1.\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    # Check if both vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute the element-wise sum\n    result = [x + y for x, y in zip(a, b)]\n    \n    return result\n\n# Example usage:\nvector_a = [1, 2, 3]\nvector_b = [4, 5, 6]\n\n# Should print [5, 7, 9]\nprint(vector_sum(vector_a, vector_b))\n\nvector_c = [1, 2]\nvector_d = [3, 4, 5]\n\n# Should print -1\nprint(vector_sum(vector_c, vector_d))\n```\n\nThis script includes:\n- A function `vector_sum` that checks if the input lists are of the same length.\n- If they are, it computes the element-wise sum using a list comprehension.\n- If they are not, it returns -1.\n- Example usage to demonstrate the function's behavior with compatible and incompatible vectors."}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient using the REINFORCE algorithm.\n    \n    Parameters:\n    - theta: A 2D NumPy array of shape (num_states, num_actions).\n    - episodes: A list of episodes, where each episode is a list of (state, action, reward) tuples.\n    \n    Returns:\n    - A list of gradients, reshaped to match the shape of the input theta.\n    \"\"\"\n    num_states, num_actions = theta.shape\n    policy_gradients = np.zeros_like(theta, dtype=float)\n    \n    for episode in episodes:\n        returns = []\n        G = 0\n        for _, _, reward in reversed(episode):\n            G += reward\n            returns.insert(0, G)\n        \n        for t, (state, action, _) in enumerate(episode):\n            log_policy_gradient = np.zeros_like(theta[state])\n            log_policy_gradient[action] = 1\n            policy = np.exp(theta[state]) / np.sum(np.exp(theta[state]))\n            log_policy_gradient -= policy\n            policy_gradients[state] += log_policy_gradient * returns[t]\n    \n    # Average the gradients over all episodes\n    policy_gradients /= len(episodes)\n    \n    # Round to the nearest 4th decimal\n    policy_gradients = np.round(policy_gradients, 4)\n    \n    return policy_gradients.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    theta = np.array([[0.1, 0.2], [0.3, 0.4]])\n    episodes = [\n        [(0, 0, 1), (1, 1, 2)],\n        [(0, 1, 0.5), (1, 0, 1.5)]\n    ]\n    gradients = compute_policy_gradient(theta, episodes)\n    print(gradients)\n```\n\nThis script defines the `compute_policy_gradient` function, which implements the REINFORCE algorithm to compute the policy gradient. The function takes a 2D NumPy array `theta` and a list of episodes, where each episode is a list of (state, action, reward) tuples. It returns the average gradient of the log-policy multiplied by the return at each time step, rounded to the nearest 4th decimal and reshaped to a Python list."}
{"task_id": 123, "completion_id": 0, "completion": "Certainly! Below is a Python script that calculates the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer. The script defines a function `compute_efficiency` which takes the number of experts, the number of active experts, and the input/output dimensions as arguments. It then computes the FLOPs for both the MoE and dense layers and calculates the savings percentage.\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Parameters:\n    - n_experts: Total number of experts\n    - k_active: Number of active experts\n    - d_in: Input dimension\n    - d_out: Output dimension\n\n    Returns:\n    - savings_percentage: Percentage of computational cost savings (rounded to 1 decimal place)\n    \"\"\"\n\n    # FLOPs for a dense layer\n    flops_dense = 2 * d_in * d_out\n\n    # FLOPs for the gating network (assuming a simple linear layer)\n    flops_gating = 2 * d_in * n_experts\n\n    # FLOPs for the active experts\n    flops_experts = k_active * (2 * d_in * d_out)\n\n    # Total FLOPs for the MoE layer\n    flops_moe = flops_gating + flops_experts\n\n    # Calculate the savings percentage\n    savings_percentage = ((flops_dense - flops_moe) / flops_dense) * 100\n\n    # Round to the nearest 1th decimal\n    savings_percentage = round(savings_percentage, 1)\n\n    return savings_percentage\n\n# Example usage:\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 1024\n\nsavings = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Computational cost savings: {savings}%\")\n```\n\n### Explanation:\n1. **Dense Layer FLOPs**: The dense layer performs a matrix multiplication followed by an addition, resulting in \\(2 \\times d_{\\text{in}} \\times d_{\\text{out}}\\) FLOPs.\n2. **Gating Network FLOPs**: The gating network is assumed to be a simple linear layer that outputs a probability distribution over the experts, resulting in \\(2 \\times d_{\\text{in}} \\times n_{\\text{experts}}\\) FLOPs.\n3. **Active Experts FLOPs**: Each active expert performs a matrix multiplication followed by an addition, resulting in \\(2 \\times d_{\\text{in}} \\times d_{\\text{out}}\\) FLOPs per active expert. Since there are \\(k_{\\text{active}}\\) active experts, the total FLOPs for the active experts is \\(k_{\\text{active}} \\times 2 \\times d_{\\text{in}} \\times d_{\\text{out}}\\).\n4. **Total MoE Layer FLOPs**: The total FLOPs for the MoE layer is the sum of the FLOPs from the gating network and the active experts.\n5. **Savings Percentage**: The savings percentage is calculated as the ratio of the difference in FLOPs between the dense layer and the MoE layer to the FLOPs of the dense layer, multiplied by 100.\n\nThis script provides a clear and concise way to calculate the computational efficiency of an MoE layer compared to a dense layer."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return e_x / e_x.sum(axis=1, keepdims=True)\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K Gating function for Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    - X: Input data matrix of shape (batch_size, input_dim).\n    - W_g: Weight matrix for gating network of shape (input_dim, num_experts).\n    - W_noise: Weight matrix for noise of shape (input_dim, num_experts).\n    - N: Pre-sampled noise matrix of shape (batch_size, num_experts).\n    - k: Sparsity parameter, number of experts to activate.\n    \n    Returns:\n    - A list of the final gating probabilities matrix of shape (batch_size, num_experts),\n      with only the top-k probabilities being non-zero, rounded to 4 decimal places.\n    \"\"\"\n    # Compute the gating logits\n    logits = X @ W_g + N * (X @ W_noise)\n    \n    # Apply softmax to get gating probabilities\n    gating_probs = softmax(logits)\n    \n    # Get the top-k indices for each row\n    topk_indices = np.argpartition(gating_probs, -k, axis=1)[:, -k:]\n    \n    # Create a mask for top-k elements\n    mask = np.zeros_like(gating_probs)\n    np.put_along_axis(mask, topk_indices, 1, axis=1)\n    \n    # Apply the mask to the gating probabilities\n    topk_gating_probs = gating_probs * mask\n    \n    # Normalize the top-k gating probabilities\n    topk_gating_probs /= np.sum(topk_gating_probs, axis=1, keepdims=True)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(topk_gating_probs, 4).tolist()\n\n# Example usage:\nX = np.array([[0.1, 0.6, 0.3], [0.2, 0.8, 0.5]])\nW_g = np.array([[0.5, 0.1], [0.3, 0.2], [0.4, 0.7]])\nW_noise = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nN = np.array([[0.05, -0.03], [-0.01, 0.02]])\nk = 1\n\nresult = noisy_topk_gating(X, W_g, W_noise, N, k)\nprint(result)\n```\n\nThis script defines the `noisy_topk_gating` function, which implements the Noisy Top-K Gating mechanism for Mixture-of-Experts (MoE) models. The function takes an input matrix `X`, gating weight matrix `W_g`, noise weight matrix `W_noise`, pre-sampled noise matrix `N`, and a sparsity parameter `k`. It returns the final gating probabilities matrix, where only the top-k probabilities are non-zero, rounded to 4 decimal places and converted to a list."}
{"task_id": 125, "completion_id": 0, "completion": "Certainly! Below is a Python script that implements a Sparse Mixture of Experts (MoE) layer using softmax gating and top-k routing. The script includes the necessary imports and a function to compute the MoE output.\n\n```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute the softmax of vector x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims=True)\n\ndef top_k(gate_values, k):\n    \"\"\"Select the top-k values and their indices from gate_values.\"\"\"\n    sorted_indices = np.argsort(gate_values, axis=-1)[:, ::-1]\n    top_k_indices = sorted_indices[:, :k]\n    top_k_values = np.take_along_axis(gate_values, top_k_indices, axis=-1)\n    return top_k_values, top_k_indices\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Compute the Mixture of Experts (MoE) output.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim).\n    - We: Expert weight matrices of shape (n_experts, input_dim, expert_output_dim).\n    - Wg: Gating weight matrix of shape (input_dim, n_experts).\n    - n_experts: Number of experts.\n    - top_k: Number of top experts to select per token.\n    \n    Returns:\n    - moe_output: Final MoE output as a list.\n    \"\"\"\n    batch_size, input_dim = x.shape\n    expert_output_dim = We.shape[2]\n    \n    # Compute gating scores\n    gate_scores = x @ Wg  # (batch_size, n_experts)\n    \n    # Apply softmax to get gating probabilities\n    gate_probs = softmax(gate_scores)  # (batch_size, n_experts)\n    \n    # Select top-k experts\n    top_k_values, top_k_indices = top_k(gate_probs, top_k)  # (batch_size, top_k), (batch_size, top_k)\n    \n    # Normalize top-k values\n    normalized_top_k_values = top_k_values / np.sum(top_k_values, axis=-1, keepdims=True)  # (batch_size, top_k)\n    \n    # Initialize the output tensor\n    moe_output = np.zeros((batch_size, expert_output_dim), dtype=np.float32)\n    \n    # Compute the output for each token\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            expert_output = x[i] @ We[expert_idx]  # (1, expert_output_dim)\n            moe_output[i] += normalized_top_k_values[i, j] * expert_output\n    \n    # Round the results to the nearest 4th decimal\n    moe_output = np.round(moe_output, 4)\n    \n    # Convert to a list\n    moe_output_list = moe_output.tolist()\n    \n    return moe_output_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input dimensions\n    batch_size = 2\n    input_dim = 3\n    expert_output_dim = 4\n    n_experts = 5\n    top_k = 2\n    \n    # Generate random input data\n    x = np.random.randn(batch_size, input_dim)\n    We = np.random.randn(n_experts, input_dim, expert_output_dim)\n    Wg = np.random.randn(input_dim, n_experts)\n    \n    # Compute MoE output\n    moe_output = moe(x, We, Wg, n_experts, top_k)\n    \n    print(\"MoE Output:\", moe_output)\n```\n\n### Explanation:\n1. **Input Parameters**:\n   - `x`: Input tensor of shape `(batch_size, input_dim)`.\n   - `We`: Expert weight matrices of shape `(n_experts, input_dim, expert_output_dim)`.\n   - `Wg`: Gating weight matrix of shape `(input_dim, n_experts)`.\n   - `n_experts`: Number of experts.\n   - `top_k`: Number of top experts to select per token.\n\n2. **Gating Scores**:\n   - Compute the gating scores by multiplying the input tensor `x` with the gating weight matrix `Wg`.\n\n3. **Softmax Gating**:\n   - Apply the softmax function to the gating scores to obtain the gating probabilities.\n\n4. **Top-k Selection**:\n   - Select the top-k experts based on the gating probabilities.\n\n5. **Normalization**:\n   - Normalize the top-k values to ensure they sum to 1.\n\n6. **Expert Transformations**:\n   - For each token in the batch, apply the selected experts' transformations and aggregate the results weighted by the normalized gating probabilities.\n\n7. **Output**:\n   - Round the final output to the nearest 4th decimal.\n   - Convert the output to a list using `numpy`'s `tolist()` method.\n\nThis implementation ensures that the MoE layer operates efficiently and correctly according to the specified constraints."}
{"task_id": 126, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements Group Normalization as described:\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor with shape (B, C, H, W).\n\n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Scale parameter of shape (C,)\n    - beta: Shift parameter of shape (C,)\n    - num_groups: Number of groups for normalization\n    - epsilon: Small constant for numerical stability\n\n    Returns:\n    - Normalized output tensor of shape (B, C, H, W)\n    \"\"\"\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"Number of channels must be divisible by number of groups\"\n\n    # Reshape input into groups\n    group_size = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, group_size, H, W)\n\n    # Compute mean and variance for each group\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n\n    # Normalize the input\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n\n    # Reshape back to original shape\n    X_normalized = X_normalized.reshape(B, C, H, W)\n\n    # Apply scale and shift parameters\n    X_normalized = gamma.reshape(1, C, 1, 1) * X_normalized + beta.reshape(1, C, 1, 1)\n\n    # Round the results to the nearest 4th decimal\n    X_normalized_rounded = np.round(X_normalized, decimals=4)\n\n    # Convert to a list and return\n    return X_normalized_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    B, C, H, W = 2, 6, 3, 3\n    num_groups = 2\n    X = np.random.randn(B, C, H, W)\n    gamma = np.random.randn(C)\n    beta = np.random.randn(C)\n\n    normalized_X = group_normalization(X, gamma, beta, num_groups)\n    print(normalized_X)\n```\n\n### Explanation:\n1. **Input Validation**: Ensure that the number of channels `C` is divisible by the number of groups `num_groups`.\n2. **Reshape Input**: Reshape the input tensor `X` to have an additional dimension for the groups.\n3. **Compute Mean and Variance**: Calculate the mean and variance for each group across the spatial dimensions.\n4. **Normalize**: Normalize the input tensor using the computed mean and variance.\n5. **Reshape Back**: Reshape the normalized tensor back to its original shape.\n6. **Apply Scale and Shift**: Multiply by the scale parameter `gamma` and add the shift parameter `beta`.\n7. **Round Results**: Round the results to the nearest 4th decimal place.\n8. **Convert to List**: Convert the final result to a Python list using `tolist()` and return it.\n\nThis script provides a complete implementation of Group Normalization and includes an example usage section to demonstrate how to call the function."}
{"task_id": 127, "completion_id": 0, "completion": "Certainly! Below is a Python script that implements the function `find_treasure` to find the minimum of the function \\( f(x) = x^4 - 3x^3 + 2 \\) using gradient descent:\n\n```python\nimport numpy as np\n\ndef f(x: float) -> float:\n    \"\"\"The function to minimize.\"\"\"\n    return x**4 - 3*x**3 + 2\n\ndef df(x: float) -> float:\n    \"\"\"The derivative of the function f(x).\"\"\"\n    return 4*x**3 - 9*x**2\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Finds the value of x where f(x) reaches its minimum using gradient descent.\n    \n    Parameters:\n    - start_x: The initial guess for x.\n    - learning_rate: The step size for each iteration.\n    - tolerance: The threshold for stopping the optimization.\n    - max_iters: The maximum number of iterations to perform.\n    \n    Returns:\n    - The value of x where f(x) reaches its minimum, rounded to 4 decimal places.\n    \"\"\"\n    x = start_x\n    for i in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        \n        # Check if the change in x is smaller than the tolerance\n        if abs(new_x - x) < tolerance:\n            break\n        \n        x = new_x\n    \n    return round(x, 4)\n\n# Example usage:\nstart_x = np.random.rand() * 10  # Random initial position between 0 and 10\nmin_x = find_treasure(start_x)\nprint(f\"The minimum is found at x = {min_x}\")\n```\n\n### Explanation:\n1. **Function Definitions**:\n   - `f(x)`: This function computes the value of the given polynomial \\( f(x) = x^4 - 3x^3 + 2 \\).\n   - `df(x)`: This function computes the derivative of \\( f(x) \\), which is \\( 4x^3 - 9x^2 \\). The derivative is used to determine the direction of the steepest descent.\n\n2. **Gradient Descent**:\n   - The `find_treasure` function initializes `x` with the starting value `start_x`.\n   - In each iteration, it computes the gradient at the current `x` and updates `x` using the gradient descent update rule: \\( x_{\\text{new}} = x - \\text{learning_rate} \\times \\text{gradient} \\).\n   - The loop continues until the change in `x` is smaller than the specified tolerance or the maximum number of iterations is reached.\n\n3. **Output**:\n   - The function returns the value of `x` where the minimum is found, rounded to 4 decimal places.\n\n4. **Example Usage**:\n   - A random initial position `start_x` is generated between 0 and 10.\n   - The `find_treasure` function is called with this initial position, and the result is printed.\n\nThis script should effectively find the minimum of the given function starting from any random initial position."}
{"task_id": 128, "completion_id": 0, "completion": "Certainly! Below is the self-contained Python script implementing the Dynamic Tanh (DyT) function as described:\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implement the Dynamic Tanh (DyT) function.\n    \n    Parameters:\n    - x: Input numpy array.\n    - alpha: Scaling factor.\n    - gamma: Parameter controlling the squashing behavior.\n    - beta: Bias term.\n    \n    Returns:\n    - A list of floats representing the output of the DyT function, rounded to 4 decimal places.\n    \"\"\"\n    # Apply the DyT function\n    dyt_output = alpha * np.tanh(gamma * x + beta)\n    \n    # Round the output to 4 decimal places\n    dyt_output_rounded = np.round(dyt_output, 4)\n    \n    # Convert the numpy array to a list\n    result_list = dyt_output_rounded.tolist()\n    \n    return result_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input array\n    x = np.array([0.5, -1.2, 3.7, -2.8, 1.0])\n    \n    # Example parameters\n    alpha = 1.0\n    gamma = 0.5\n    beta = 0.1\n    \n    # Compute the DyT output\n    dyt_result = dynamic_tanh(x, alpha, gamma, beta)\n    \n    # Print the result\n    print(dyt_result)\n```\n\n### Explanation:\n1. **Function Definition**: The `dynamic_tanh` function takes an input array `x` and three parameters: `alpha`, `gamma`, and `beta`.\n2. **DyT Calculation**: The function computes the DyT output using the formula `alpha * tanh(gamma * x + beta)`.\n3. **Rounding**: The output values are rounded to 4 decimal places using `np.round`.\n4. **Conversion to List**: The numpy array is converted to a Python list using `tolist()`.\n5. **Example Usage**: The script includes an example usage where an input array and parameters are provided, and the DyT function is called and printed.\n\nThis script should fulfill the requirements specified in the task."}
