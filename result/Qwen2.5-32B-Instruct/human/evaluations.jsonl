{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Args:\n    a: A list of lists (matrix) containing integers or floats.\n    b: A list (vector) containing integers or floats.\n    \n    Returns:\n    A list representing the resulting vector if the operation is valid,\n    or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        sum_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(sum_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list of list of int|float): The input matrix.\n    \n    Returns:\n    list of list of int|float: The transposed matrix.\n    \"\"\"\n    num_rows = len(a)\n    num_cols = len(a[0]) if a else 0\n    transposed_matrix = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed_matrix[j][i] = a[i][j]\n    return transposed_matrix\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Takes a 2D list (matrix) 'a' and tries to reshape it into a new shape specified by 'new_shape'.\n    If the reshape operation is not possible due to size mismatch, returns an empty list.\n    \n    :param a: List of lists representing the input matrix.\n    :param new_shape: A tuple indicating the desired new shape of the matrix.\n    :return: The reshaped matrix as a list of lists if possible, otherwise an empty list.\n    \"\"\"\n    array = np.array(a)\n    original_shape = array.shape\n    if original_shape[0] * original_shape[1] != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    reshaped_list = reshaped_array.tolist()\n    return reshaped_list\nnew_shape = (2, 3)\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    \"\"\"\n    Calculates the mean of a matrix either by row or by column based on the mode specified.\n    \n    Parameters:\n        matrix (List[List[float]]): A 2D list representing the matrix.\n        mode (str): A string indicating if the mean should be calculated per 'row' or per 'column'.\n        \n    Returns:\n        List[float]: A list of means calculated based on the requested mode.\n    \"\"\"\n    if mode.lower() == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode.lower() == 'column':\n        transposed_matrix = zip(*matrix)\n        means = [sum(col) / len(col) for col in transposed_matrix]\n    else:\n        raise ValueError(\"Invalid mode provided. Mode must be 'row' or 'column'.\")\n    return means\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the provided scalar value.\n    \n    Args:\n        matrix (list of list of int/float): The matrix to be multiplied.\n        scalar (int/float): The scalar value to multiply the matrix by.\n        \n    Returns:\n        list of list of int/float: The resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculates the eigenvalues of a 2x2 matrix and returns them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists.\n    :return: A list containing the eigenvalues of the matrix, sorted from highest to lowest.\n    \"\"\"\n    matrix_np = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    return sorted(eigenvalues, reverse=True)\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        assert T_np.shape[0] == T_np.shape[1], 'Matrix T must be a square matrix'\n        assert S_np.shape[0] == S_np.shape[1], 'Matrix S must be a square matrix'\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        transformed_A = np.dot(np.dot(T_inv, A_np), S_inv)\n        transformed_A_rounded = np.around(transformed_A, decimals=4).tolist()\n        return transformed_A_rounded\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 0], [0, 1]]\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n        matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n        \n    Returns:\n        list[list[float]]: The inverse of the matrix, or None if the matrix is not invertible.\n    \"\"\"\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of vectors.\n    \n    Args:\n    vectors (list of list of float): A list of lists where each inner list \n                                     represents a feature with its observations.\n    \n    Returns:\n    list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    data = np.array(vectors)\n    data = data.T\n    cov_matrix = np.cov(data, bias=True)\n    return cov_matrix.tolist()\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations given by Ax = b using the Jacobi iterative method.\n    \n    :param A: Coefficient matrix (np.ndarray)\n    :param b: Right-hand side vector (np.ndarray)\n    :param n: Number of iterations (int)\n    :return: The approximate solution vector x (list), rounded to 4 decimal places.\n    \"\"\"\n    N = len(b)\n    x = np.zeros_like(b, dtype=np.float64)\n    for i in range(n):\n        x_new = np.copy(x)\n        for j in range(N):\n            s1 = np.dot(A[j, :j], x_new[:j])\n            s2 = np.dot(A[j, j + 1:], x[j + 1:])\n            x_new[j] = (b[j] - s1 - s2) / A[j, j]\n            x_new[j] = round(x_new[j], 4)\n        if np.allclose(x, x_new, atol=0.0001, rtol=0.0):\n            break\n        x = x_new\n    return x.tolist()\nA = np.array([[4.0, 1.0, 2.0, 2.0], [3.0, 5.0, 1.0, 1.0], [1.0, 1.0, 3.0, 1.0], [1.0, 2.0, 1.0, 6.0]])\nb = np.array([5.0, 6.0, 5.0, 2.0])\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    max_iter = 1000\n    tolerance = 1e-05\n    A = A.astype(float)\n    for _ in range(max_iter):\n        delta = A[0, 1]\n        if abs(delta) < tolerance:\n            break\n        theta = 0.5 * np.arctan2(2 * delta, A[1, 1] - A[0, 0])\n        c = np.cos(theta)\n        s = np.sin(theta)\n        T = np.array([[c, -s], [s, c]])\n        A = T.T @ A @ T\n    singular_values = np.sqrt(np.diag(A))\n    singular_values = np.round(singular_values, decimals=4)\n    return tuple(singular_values)\nA = np.array([[1, 2], [3, 4]], dtype=float)\nsingular_values = svd_2x2_singular_values(A)\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor_matrix: list[list[int | float]]) -> float:\n        \"\"\"\n        Helper function to calculate the determinant of a 3x3 matrix.\n        :param minor_matrix: A 3x3 matrix (list of lists) for which the determinant is calculated.\n        :return: The determinant value as a float.\n        \"\"\"\n        det = minor_matrix[0][0] * (minor_matrix[1][1] * minor_matrix[2][2] - minor_matrix[1][2] * minor_matrix[2][1]) - minor_matrix[0][1] * (minor_matrix[1][0] * minor_matrix[2][2] - minor_matrix[1][2] * minor_matrix[2][0]) + minor_matrix[0][2] * (minor_matrix[1][0] * minor_matrix[2][1] - minor_matrix[1][1] * minor_matrix[2][0])\n        return det\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('The input matrix must be a 4x4 matrix')\n    total_det = 0\n    for col_index in range(4):\n        sign = (-1) ** (col_index % 2)\n        minor_matrix = [row[:col_index] + row[col_index + 1:] for row in matrix[1:]]\n        total_det += sign * matrix[0][col_index] * determinant_3x3(minor_matrix)\n    return total_det\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n        X: A list of lists representing the feature set including x0=1.\n        y: A list representing the target variable.\n        \n    Returns:\n        A list of coefficients for the linear regression model.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n    theta_rounded = [round(x, 4) for x in theta]\n    return theta_rounded\nX = [[1, 2, 3], [1, 3, 4], [1, 4, 5], [1, 5, 6]]\ny = [4, 5, 6, 7]\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent on the given data.\n    \n    :param X: NumPy array containing feature values with a column of ones for the intercept.\n    :param y: NumPy array containing target values.\n    :param alpha: Learning rate.\n    :param iterations: Number of iterations for the gradient descent algorithm.\n    :return: List of coefficients for the linear regression model.\n    \"\"\"\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        gradient = X.T.dot(errors) / len(y)\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on a 2D NumPy array using standardization and min-max normalization.\n    \n    Parameters:\n    data : np.ndarray\n        A 2D NumPy array where each row is a sample and each column is a feature.\n        \n    Returns:\n    tuple\n        A tuple containing two elements: the first is a 2D list of standardized data,\n        and the second is a 2D list of data normalized by min-max.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    norm_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data = np.round(standardized_data, 4)\n    norm_data = np.round(norm_data, 4)\n    standardized_list = standardized_data.tolist()\n    norm_list = norm_data.tolist()\n    return (standardized_list, norm_list)\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        for i in range(k):\n            if np.any(labels == i):\n                centroids_array[i] = points_array[labels == i].mean(axis=0)\n    return [tuple(np.round(centroid, decimals=4)) for centroid in centroids_array]\npoints = [(1.5, 2.0), (2.5, 3.0), (4.0, 3.0), (7.0, 7.0), (8.0, 6.0)]\nk = 2\ninitial_centroids = [(1.0, 1.0), (8.0, 8.0)]\nmax_iterations = 100\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits the input data into k folds for performing cross-validation.\n\n    Parameters:\n    X (np.ndarray): The feature matrix.\n    y (np.ndarray): The target vector.\n    k (int): Number of folds.\n    shuffle (bool): Shuffle data before splitting into folds.\n    random_seed (int): Seed for reproducible shuffling.\n\n    Returns:\n    list: A list of tuples containing train and test indices for each fold.\n    \"\"\"\n    assert len(X) == len(y), 'X and y must have the same length.'\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    combined = np.hstack((X, y.reshape(-1, 1)))\n    if shuffle:\n        np.random.shuffle(combined)\n    n_samples = len(combined)\n    fold_size = n_samples // k\n    folds_indices = []\n    for i in range(k):\n        test_start = i * fold_size\n        test_end = test_start + fold_size\n        if i == k - 1:\n            test_end = n_samples\n        test_indices = np.arange(test_start, test_end)\n        train_indices = np.delete(np.arange(n_samples), test_indices)\n        folds_indices.append((train_indices, test_indices))\n    return folds_indices\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform PCA on a dataset and return the top k principal components.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a sample and each column is a feature.\n    - k: Number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the top k principal components.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    result = [list(map(lambda x: round(x, 4), component)) for component in top_k_eigenvectors.T]\n    return result\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef calculate_entropy(examples, target_attr):\n    \"\"\"Calculate the entropy of the given examples with respect to the target attribute.\"\"\"\n    counts = Counter((example[target_attr] for example in examples))\n    entropy = 0.0\n    for count in counts.values():\n        prob = count / len(examples)\n        entropy -= prob * math.log2(prob)\n    return entropy\ndef calculate_information_gain(examples, attribute, target_attr):\n    \"\"\"Calculate the information gain for a given attribute.\"\"\"\n    total_entropy = calculate_entropy(examples, target_attr)\n    partitioned_data = {}\n    for example in examples:\n        value = example[attribute]\n        if value not in partitioned_data:\n            partitioned_data[value] = []\n        partitioned_data[value].append(example)\n    weighted_entropy = sum((len(subset) / len(examples) * calculate_entropy(subset, target_attr) for subset in partitioned_data.values()))\n    return total_entropy - weighted_entropy\ndef find_best_attribute(examples, attributes, target_attr):\n    \"\"\"Find the attribute that provides the most information gain.\"\"\"\n    best_attribute = max(attributes, key=lambda attr: calculate_information_gain(examples, attr, target_attr))\n    return best_attribute\ndef plurality_value(examples, target_attr):\n    \"\"\"Return the most common value among the target attribute.\"\"\"\n    counts = Counter((example[target_attr] for example in examples))\n    return counts.most_common(1)[0][0]\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Build a decision tree recursively.\n    \n    :param examples: List of dictionaries, where each dictionary represents an example with attribute-value pairs.\n    :param attributes: List of strings representing the attribute names.\n    :param target_attr: String representing the name of the target attribute.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples:\n        return plurality_value(examples, target_attr)\n    if len(set((example[target_attr] for example in examples))) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return plurality_value(examples, target_attr)\n    best_attribute = find_best_attribute(examples, attributes, target_attr)\n    tree = {best_attribute: {}}\n    for value in set((example[best_attribute] for example in examples)):\n        subset = [example for example in examples if example[best_attribute] == value]\n        subtree = learn_decision_tree(subset, [attr for attr in attributes if attr != best_attribute], target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2, kernel='linear', sigma=1.0):\n    \"\"\"\n    Compute the kernel function between two points.\n    \n    :param x1: First point.\n    :param x2: Second point.\n    :param kernel: Type of kernel to use ('linear' or 'RBF').\n    :param sigma: Standard deviation for the RBF kernel.\n    :return: Kernel value.\n    \"\"\"\n    if kernel == 'linear':\n        return np.dot(x1, x2)\n    elif kernel == 'RBF':\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError('Unsupported kernel type.')\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    t = 0\n    for epoch in range(iterations):\n        for i in range(n_samples):\n            t += 1\n            eta_t = 1 / (t * lambda_val)\n            sum_ = 0\n            for j in range(n_samples):\n                sum_ += alpha[j] * labels[j] * kernel_function(data[i], data[j], kernel, sigma)\n            if labels[i] * sum_ < 1:\n                alpha[i] = (1 - eta_t * lambda_val) * alpha[i] + eta_t * labels[i]\n            else:\n                alpha[i] = max(0, (1 - eta_t * lambda_val) * alpha[i])\n    support_vectors = alpha > 0\n    b_sum = 0\n    count = 0\n    for i in range(n_samples):\n        if support_vectors[i]:\n            sum_ = 0\n            for j in range(n_samples):\n                sum_ += alpha[j] * labels[j] * kernel_function(data[i], data[j], kernel, sigma)\n            b_sum += labels[i] - sum_\n            count += 1\n    b = b_sum / count if count > 0 else 0.0\n    return {'alpha': np.round(alpha, 4).tolist(), 'bias': np.round(b, 4)}\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input value z.\n    \n    The formula for the sigmoid function is 1 / (1 + e^(-z)).\n    \n    Args:\n        z (float): The input value to the sigmoid function.\n    \n    Returns:\n        float: The output of the sigmoid function, rounded to 4 decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of a given list of scores.\n    \n    Args:\n        scores (list of float): A list of scores for which softmax needs to be computed.\n        \n    Returns:\n        list of float: A list containing softmax values of the given scores, each rounded to 4 decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid activation function value for a given input.\"\"\"\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulate a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n    - features: A list of feature vectors, where each vector represents the feature values.\n    - labels: The true binary labels corresponding to each feature vector.\n    - weights: The weight parameters for each feature.\n    - bias: The bias parameter.\n\n    Returns:\n    - A tuple containing:\n        - A list of predicted probabilities after applying the sigmoid activation function.\n        - The mean squared error between the predicted probabilities and the true labels, rounded to four decimal places.\n    \"\"\"\n    predictions = []\n    for feature_vec in features:\n        weighted_sum = sum((w * f for (w, f) in zip(weights, feature_vec))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n    mse = sum(((p - l) ** 2 for (p, l) in zip(predictions, labels))) / len(labels)\n    mse_rounded = round(mse, 4)\n    return ([round(pred, 4) for pred in predictions], mse_rounded)\nfeatures = [[0.5, 0.8], [0.6, 0.1]]\nlabels = [1, 0]\nweights = [0.2, 0.4]\nbias = 0.1\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"Mean Squared Error loss function.\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        errors = labels - predictions\n        gradient_weights = np.dot(errors.T, features)\n        gradient_bias = np.sum(errors)\n        weights += learning_rate * gradient_weights\n        bias += learning_rate * gradient_bias\n        mse = mean_squared_error(labels, predictions)\n        mse_values.append(round(mse, 4))\n    return (weights.tolist(), round(bias, 4), mse_values)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": false, "result": "failed: Singular matrix", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C for R^3.\n    \n    Parameters:\n    - B: A list of lists representing the basis vectors in basis B.\n    - C: A list of lists representing the basis vectors in basis C.\n    \n    Returns:\n    - A list of lists representing the transformation matrix P from B to C.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    C_inv = np.linalg.inv(C_array)\n    P = np.dot(C_inv, B_array)\n    P_rounded = np.round(P, decimals=4).tolist()\n    return P_rounded\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nP = transform_basis(B, C)\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the SVD of a 2x2 matrix using eigenvalues and eigenvectors.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 matrix.\n        \n    Returns:\n        tuple: A tuple containing U, S, and V matrices of the SVD of A.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(V)\n    for i in range(U.shape[1]):\n        if singular_values[i] > 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n        else:\n            U[:, i] = np.cross([1, 0], V[:, 1])\n            U[:, i] /= np.linalg.norm(U[:, i])\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of X and corresponding elements in y while maintaining their correspondence.\n    \n    :param X: Numpy array containing features.\n    :param y: Numpy array containing labels.\n    :param seed: Optional; sets the seed for the random number generator for reproducibility.\n    :return: Two lists, shuffled X and y, each reshaped into a list using numpy's tolist() method.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert X.shape[0] == len(y), 'X and y must have the same number of rows.'\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    return (shuffled_X.tolist(), shuffled_y.tolist())\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of X or (X, y) pairs.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array containing features.\n    y (np.ndarray, optional): 1D or 2D numpy array containing labels. Defaults to None.\n    batch_size (int): Size of each batch. Defaults to 64.\n    \n    Yields:\n    tuple: A tuple of (batch_X, batch_y) when y is not None, else batch_X.\n    \n    Note:\n    - The last batch might be smaller if the total number of samples is not divisible by batch_size.\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = min(start_idx + batch_size, n_samples)\n        excerpt = indices[start_idx:end_idx]\n        batch_X = X[excerpt].tolist()\n        if y is not None:\n            batch_y = y[excerpt].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the data X into two subsets based on whether the value in the specified feature column (feature_i)\n    is greater than or equal to the given threshold.\n    \n    :param X: Numpy array representing the dataset with features and samples.\n    :param feature_i: Integer index of the feature to compare against the threshold.\n    :param threshold: Value to compare the feature values against.\n    :return: Two lists representing the subsets of the dataset that meet and do not meet the condition.\n    \"\"\"\n    X = np.array(X)\n    subset_ge = X[X[:, feature_i] >= threshold].tolist()\n    subset_lt = X[X[:, feature_i] < threshold].tolist()\n    return (subset_ge, subset_lt)\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates a new feature matrix consisting of all polynomial combinations of the features\n    with degree less than or equal to the specified degree. For example, if an input sample is\n    two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    Parameters:\n    - X: numpy.ndarray, shape (n_samples, n_features)\n        The input samples.\n    - degree: int\n        The degree of the polynomial features.\n        \n    Returns:\n    - poly_array: list\n        The matrix of polynomial features.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    indices = np.arange(n_features)\n    poly_indices = []\n    for deg in range(degree + 1):\n        poly_indices += [np.array(idx) for idx in combinations_with_replacement(indices, deg)]\n    poly_indices = poly_indices[1:]\n    poly_features = np.empty((n_samples, len(poly_indices)))\n    for (i, idx) in enumerate(poly_indices):\n        poly_features[:, i] = np.prod(X[:, idx], axis=1)\n    return poly_features.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of random subsets from the dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    y (np.ndarray): 1D numpy array representing the labels.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether sampling is done with or without replacements.\n    seed (int): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    list: A list of tuples, each containing a subset of X and corresponding y.\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    total_samples = len(y)\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(total_samples, total_samples, replace=True)\n        else:\n            indices = np.random.choice(total_samples, total_samples, replace=False)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integers into a one-hot encoded format.\n    \n    Parameters:\n    - x (np.array): A 1D numpy array of integers.\n    - n_col (int, optional): The number of columns for the one-hot encoded array. \n    \n    Returns:\n    - list: A list of lists, each inner list represents a one-hot encoded vector.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    - x: A 1D numpy array.\n    \n    Returns:\n    - A 2D numpy array representing the diagonal matrix.\n      The output is returned as a python list.\n    \"\"\"\n    n = len(x)\n    diag_matrix = np.zeros((n, n))\n    np.fill_diagonal(diag_matrix, x)\n    return diag_matrix.tolist()\nx = np.array([1, 2, 3])\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculates the accuracy score between the true labels and the predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): True labels as a 1D numpy array.\n    y_pred (numpy.ndarray): Predicted labels as a 1D numpy array.\n    \n    Returns:\n    float: The accuracy score, rounded to the nearest 4th decimal.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be equal.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array\n    - Y (optional): 2D numpy array\n    \n    Returns:\n    - corr_matrix: 2D numpy array representing the correlation matrix\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered) / (X.shape[0] - 1)\n    std_dev_X = np.std(X, axis=0, ddof=1)\n    std_dev_Y = np.std(Y, axis=0, ddof=1)\n    std_dev_outer = np.outer(std_dev_X, std_dev_Y)\n    std_dev_outer[std_dev_outer == 0] = np.nan\n    corr_matrix = cov_matrix / std_dev_outer\n    corr_matrix = np.nan_to_num(corr_matrix)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[7, 8], [9, 10], [11, 12]])\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the AdaBoost classifier fitting method.\n    \n    Parameters:\n    X (np.array): Training data of shape (n_samples, n_features).\n    y (np.array): Target values of shape (n_samples,).\n    n_clf (int): Number of weak classifiers.\n    \n    Returns:\n    list: A list containing dictionaries with the classifier parameters.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    clfs = []\n    w = np.full(n_samples, 1 / n_samples)\n    for _ in range(n_clf):\n        clf_error = float('inf')\n        clf_threshold = 0\n        clf_feature_index = 0\n        clf_polarity = 1\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            sorted_values = np.unique(feature_values)\n            for threshold in sorted_values:\n                polarity_options = [1, -1]\n                for p in polarity_options:\n                    pred = np.ones(n_samples)\n                    if p == 1:\n                        pred[feature_values < threshold] = -1\n                    else:\n                        pred[feature_values > threshold] = -1\n                    error = np.sum(w * (pred != y))\n                    if error < clf_error:\n                        clf_error = error\n                        clf_threshold = threshold\n                        clf_feature_index = feature_idx\n                        clf_polarity = p\n        clf_alpha = 0.5 * np.log((1 - clf_error) / max(clf_error, 1e-16))\n        pred = np.ones(n_samples)\n        if clf_polarity == 1:\n            pred[X[:, clf_feature_index] < clf_threshold] = -1\n        else:\n            pred[X[:, clf_feature_index] > clf_threshold] = -1\n        w *= np.exp(-clf_alpha * y * pred)\n        w /= np.sum(w)\n        clfs.append({'alpha': round(float(clf_alpha), 4), 'threshold': round(float(clf_threshold), 4), 'index': int(clf_feature_index), 'polarity': int(clf_polarity)})\n    return clfs\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The log-softmax function for each score i is defined as:\n    log_softmax(i) = score[i] - log(sum(exp(score[j]) for j in range(len(score))))\n    \n    :param scores: A list of scores.\n    :return: A list of log-softmax values corresponding to the input scores.\n    \"\"\"\n    scores_arr = np.array(scores)\n    scores_max = np.max(scores_arr)\n    log_softmax_values = scores_arr - scores_max - np.log(np.sum(np.exp(scores_arr - scores_max)))\n    return np.round(log_softmax_values, decimals=4).tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X @ self.W + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        W_grad = self.layer_input.T @ accum_grad\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, W_grad)\n            self.w0 = self.optimizer_w0.update(self.w0, w0_grad)\n        return accum_grad @ self.W.T\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a 2D convolution on the given input matrix using the provided kernel, padding, and stride.\n    \n    Parameters:\n    input_matrix (np.ndarray): The input matrix to perform the convolution on.\n    kernel (np.ndarray): The convolutional kernel to use.\n    padding (int): The padding size for the input matrix.\n    stride (int): The stride value for the convolution operation.\n    \n    Returns:\n    list: A list of the output of the convolution operation, rounded to 4 decimal places.\n    \"\"\"\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    padded_input = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    output_h = int((input_h + 2 * padding - kernel_h) / stride) + 1\n    output_w = int((input_w + 2 * padding - kernel_w) / stride) + 1\n    output = np.zeros((output_h, output_w))\n    for h in range(0, output_h):\n        for w in range(0, output_w):\n            row_start = h * stride\n            row_end = row_start + kernel_h\n            col_start = w * stride\n            col_end = col_start + kernel_w\n            roi = padded_input[row_start:row_end, col_start:col_end]\n            result = np.sum(roi * kernel)\n            output[h, w] = round(result, 4)\n    return output.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 0\nstride = 1\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Args:\n        z: A float number to which the ReLU function will be applied.\n        \n    Returns:\n        A float, which is the result of applying the ReLU function to z.\n    \"\"\"\n    return max(0, z)\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss.\n    \n    :param X: np.ndarray, 2D array representing the feature matrix.\n    :param w: np.ndarray, 1D array representing the coefficients.\n    :param y_true: np.ndarray, 1D array representing the true labels.\n    :param alpha: float, the regularization parameter.\n    :return: float, the Ridge loss.\n    \"\"\"\n    predictions = X.dot(w)\n    mse = np.mean((predictions - y_true) ** 2)\n    regularization_term = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization_term\n    return round(ridge_loss_value, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport sys\ndef leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float): The slope for the negative part of the function. Defaults to 0.01.\n    \n    Returns:\n    float: The output after applying the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n    \n    Parameters:\n    - x1: A numpy array representing the first vector.\n    - x2: A numpy array representing the second vector.\n    \n    Returns:\n    The linear kernel value which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision score between two binary label arrays.\n    \n    Parameters:\n    - y_true: np.ndarray, the true binary labels.\n    - y_pred: np.ndarray, the predicted binary labels.\n    \n    Returns:\n    - float, the precision score.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    if TP + FP == 0:\n        return 1.0\n    else:\n        return TP / (TP + FP)\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Implements different variants of Gradient Descent with Mean Squared Error (MSE) as the loss function.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D array of input features.\n    y (numpy.ndarray): A 1D array of target values.\n    weights (numpy.ndarray): Initial weights for the model.\n    learning_rate (float): Learning rate for updating weights.\n    n_iterations (int): Number of iterations for training.\n    batch_size (int): Size of the mini-batch. Default is 1 (Stochastic).\n    method (str): Specifies the method to use: 'stochastic', 'batch', or 'mini-batch'.\n    \n    Returns:\n    list: A list of weights obtained after performing gradient descent.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if method == 'batch':\n        batch_size = n_samples\n    for _ in range(n_iterations):\n        for i in range(0, n_samples, batch_size):\n            end = i + batch_size\n            if method == 'stochastic':\n                end = i + 1\n            X_batch = X[i:end]\n            y_batch = y[i:end]\n            predictions = np.dot(X_batch, weights)\n            errors = predictions - y_batch\n            gradients = np.dot(X_batch.T, errors) / len(X_batch)\n            weights -= learning_rate * gradients\n    return np.round(weights, decimals=4).tolist()\nX = np.array([[1, 1], [1, 2], [2, 3]])\ny = np.array([6, 8, 9])\nweights = np.array([0, 0])\nlearning_rate = 0.001\nn_iterations = 1000\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n\n    :param matrix: A 2D numpy array representing the input matrix.\n    :return: A 2D list representing the matrix in RREF.\n    \"\"\"\n    if not isinstance(matrix, np.ndarray):\n        matrix = np.array(matrix)\n    (rows, cols) = matrix.shape\n    lead = 0\n    rowCount = rows\n    for r in range(rowCount):\n        if lead >= cols:\n            return matrix.tolist()\n        i = r\n        while abs(matrix[i, lead]) < 1e-10:\n            i += 1\n            if i == rowCount:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return matrix.tolist()\n        matrix[[i, r]] = matrix[[r, i]]\n        lv = matrix[r, lead]\n        matrix[r] = matrix[r] / lv\n        for j in range(rowCount):\n            if j != r:\n                lv = matrix[j, lead]\n                matrix[j] = matrix[j] - lv * matrix[r]\n        lead += 1\n    return matrix.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    p = np.copy(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(p)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        p -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(p, decimals=4).tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    prev_cost = np.inf\n    for iteration in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = -2 / n_samples * np.dot(X.T, y - y_pred) + alpha * np.sign(weights)\n        db = -2 / n_samples * np.sum(y - y_pred)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        cost = 1 / (2 * n_samples) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(weights))\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    weights = np.round(weights, decimals=4).tolist()\n    return (weights, round(bias, 4))\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    \n    :param source: First input string.\n    :param target: Second input string.\n    :return: Minimum number of edits needed to convert source to target.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)\n    return dp[len_source][len_target]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for a binary classification problem.\n    \n    Parameters:\n    y_true (list): List of true binary labels.\n    y_pred (list): List of predicted binary labels.\n    \n    Returns:\n    float: The recall score rounded to three decimal places. \n           If TP + FN is 0, returns 0.0 to avoid division by zero.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_score = tp / (tp + fn)\n        return round(recall_score, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: module 'numpy' has no attribute 'softmax'", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention for a given input sequence X and learnable parameters W_q, W_k, W_v.\n    \n    Parameters:\n    - X: Input sequence tensor of shape (seq_length, embedding_dim).\n    - W_q: Learnable matrix for transforming X into query vectors, shape (embedding_dim, d_k).\n    - W_k: Learnable matrix for transforming X into key vectors, shape (embedding_dim, d_k).\n    - W_v: Learnable matrix for transforming X into value vectors, shape (embedding_dim, d_v).\n    \n    Returns:\n    - A python list representing the self-attention output tensor after applying the softmax operation\n      on the scaled dot-product of queries and keys, followed by multiplication with values, and finally\n      rounding to the nearest 4th decimal.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    d_k = W_k.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    context = np.dot(attention_weights, V)\n    context_rounded = np.round(context, 4)\n    return context_rounded.tolist()\nX = np.array([[1, 0], [0, 1], [1, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 0], [0, 1]])\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell that processes a sequence of input vectors and returns the final hidden state.\n    \n    :param input_sequence: List of input vectors for each time step.\n    :param initial_hidden_state: Initial hidden state of the RNN.\n    :param Wx: Weight matrix for input-to-hidden connections.\n    :param Wh: Weight matrix for hidden-to-hidden connections.\n    :param b: Bias vector added to the hidden state at each time step.\n    :return: Final hidden state after processing the entire sequence, rounded to 4 decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    current_hidden_state = initial_hidden_state\n    for x_t in input_sequence:\n        current_hidden_state = np.tanh(np.dot(x_t, Wx) + np.dot(current_hidden_state, Wh) + b)\n    final_hidden_state = np.round(current_hidden_state, decimals=4).tolist()\n    return final_hidden_state\ninput_sequence = [[1.0], [2.0], [3.0]]\ninitial_hidden_state = [0.0]\nWx = [[0.5]]\nWh = [[0.7]]\nb = [0.1]\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a list of points.\n    \n    :param points: List of [x, y] coordinates.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: Translated list of points.\n    \"\"\"\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    hom_points = np.ones((len(points), 3))\n    hom_points[:, :2] = points\n    translated_hom_points = np.dot(hom_points, translation_matrix.T)\n    translated_points = (translated_hom_points / translated_hom_points[:, -1].reshape(-1, 1))[:, :2].tolist()\n    return translated_points\npoints_example = [[1, 2], [3, 4], [5, 6]]\ntx_example = 2\nty_example = 3\ntranslated_points = translate_object(points_example, tx_example, ty_example)\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculates the Kullback-Leibler (KL) divergence between two univariate \n    normal distributions.\n    \n    Parameters:\n        mu_p (float): Mean of the first normal distribution (P).\n        sigma_p (float): Standard deviation of the first normal distribution (P).\n        mu_q (float): Mean of the second normal distribution (Q).\n        sigma_q (float): Standard deviation of the second normal distribution (Q).\n        \n    Returns:\n        float: KL divergence between P and Q.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_div = np.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n    return kl_div\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the system Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n        A (numpy.ndarray): The coefficient matrix, assumed to be square.\n        b (numpy.ndarray): The right-hand side vector.\n        n (int): Number of iterations to perform.\n        x_ini (numpy.ndarray, optional): Initial guess for x. Defaults to a zero vector.\n    \n    Returns:\n        list: Approximated solution vector x, rounded to 4 decimal places.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square'\n    assert A.shape[0] == len(b), 'Size mismatch between A and b'\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = x_ini\n    for _ in range(n):\n        for i in range(len(x)):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, decimals=4).tolist()\nA = np.array([[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]])\nb = np.array([16, 15, 14, 13])\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear equation system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n        A (list of lists): The coefficient matrix of the system of linear equations.\n        b (list): The constant terms vector of the system of linear equations.\n    \n    Returns:\n        list: The solution vector x rounded to the nearest 4th decimal.\n    \"\"\"\n    n = len(A)\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    Ab = np.column_stack((A, b))\n    for i in range(n):\n        max_element_index = np.argmax(abs(Ab[i:, i])) + i\n        Ab[[i, max_element_index]] = Ab[[max_element_index, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j][i] / Ab[i][i]\n            for k in range(i, n + 1):\n                Ab[j][k] -= factor * Ab[i][k]\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        x[i] = round((Ab[i][-1] - np.dot(Ab[i][:-1], x)) / Ab[i][i], 4)\n    return x.tolist()\nA = [[2, 1, -1], [3, -1, -2], [-1, 0.5, -1]]\nb = [8, -11, -3]\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: tuple index out of range", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    @staticmethod\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    @staticmethod\n    def tanh(x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        (num_samples, seq_length, input_dim) = (x.shape[0], x.shape[1], x.shape[2])\n        hidden_states = []\n        (current_h, current_c) = (initial_hidden_state, initial_cell_state)\n        for t in range(seq_length):\n            input_t = x[:, t, :]\n            combined_input = np.concatenate((input_t, current_h), axis=1).T\n            f_t = self.sigmoid(self.Wf.dot(combined_input) + self.bf)\n            i_t = self.sigmoid(self.Wi.dot(combined_input) + self.bi)\n            c_tilda = self.tanh(self.Wc.dot(combined_input) + self.bc)\n            o_t = self.sigmoid(self.Wo.dot(combined_input) + self.bo)\n            current_c = f_t * current_c + i_t * c_tilda\n            current_h = o_t * self.tanh(current_c)\n            hidden_states.append(current_h.T)\n        final_hidden_state = hidden_states[-1]\n        final_cell_state = current_c\n        hidden_states_rounded = [np.round(state, decimals=4).tolist() for state in hidden_states]\n        final_hidden_state_rounded = np.round(final_hidden_state, decimals=4).tolist()\n        final_cell_state_rounded = np.round(final_cell_state.T, decimals=4).tolist()\n        return (hidden_states_rounded, final_hidden_state_rounded, final_cell_state_rounded)\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom collections import Counter\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus cannot be empty.')\n    num_docs = len(corpus)\n    tf_idf_scores = []\n    idf = {}\n    for term in set(query):\n        df = sum((1 for doc in corpus if term in doc))\n        idf[term] = np.log((num_docs + 1) / (df + 1))\n    for doc in corpus:\n        doc_counter = Counter(doc)\n        doc_tfidf = []\n        for term in query:\n            tf = doc_counter[term] / len(doc) if len(doc) > 0 else 0\n            tfidf = tf * idf.get(term, 0)\n            doc_tfidf.append(round(tfidf, 4))\n        tf_idf_scores.append(doc_tfidf)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['this', 'is', 'the', 'first', 'document'], ['this', 'document', 'is', 'the', 'second', 'document'], ['and', 'this', 'is', 'the', 'third', 'one'], ['is', 'this', 'the', 'fourth', 'one']]\nquery = ['document', 'one', 'is']\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    beta_sq = beta ** 2\n    f_beta = (1 + beta_sq) * (precision * recall) / (beta_sq * precision + recall) if precision + recall > 0 else 0\n    return round(f_beta, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: 'SimpleRNN' object has no attribute 'forward'", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def activation_derivative(self, x):\n        return x * (1 - x)\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        :param input_sequence: List of input vectors.\n        :return: Outputs, last inputs, and hidden states for each time step.\n        \"\"\"\n        hiddens = [np.zeros((self.hidden_size, 1))]\n        outputs = []\n        last_inputs = []\n        for t in range(len(input_sequence)):\n            x = np.array(input_sequence[t]).reshape(-1, 1)\n            h_prev = hiddens[-1]\n            h_t = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y_t = np.dot(self.W_hy, h_t) + self.b_y\n            hiddens.append(h_t)\n            outputs.append(y_t)\n            last_inputs.append(x)\n        return (outputs, last_inputs, hiddens[1:])\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT.\n        :param input_sequence: List of input vectors.\n        :param expected_output: Expected output for each time step.\n        :param outputs: Outputs computed in the forward pass.\n        :param last_inputs: Inputs at each time step computed in the forward pass.\n        :param last_hiddens: Hidden states computed in the forward pass.\n        :param learning_rate: Learning rate for weight updates.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[-1])\n        for t in reversed(range(len(input_sequence))):\n            x_t = last_inputs[t]\n            h_t = last_hiddens[t]\n            h_prev = last_hiddens[t - 1] if t > 0 else np.zeros_like(last_hiddens[0])\n            y_pred = outputs[t]\n            dy = y_pred - np.array(expected_output[t]).reshape(-1, 1)\n            dW_hy += np.dot(dy, h_t.T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - h_t ** 2) * dh\n            dW_xh += np.dot(dh_raw, x_t.T)\n            db_h += dh_raw\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n            self.W_xh -= learning_rate * dW_xh\n            self.W_hh -= learning_rate * dW_hh\n            self.W_hy -= learning_rate * dW_hy\n            self.b_h -= learning_rate * db_h\n            self.b_y -= learning_rate * db_y\ninput_size = 5\nhidden_size = 3\noutput_size = 5\ninput_sequence = [np.random.rand(input_size) for _ in range(10)]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    r = np.copy(b - np.dot(A, x0))\n    p = np.copy(r)\n    rsold = np.dot(r, r)\n    for k in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x0 += alpha * p\n        r -= alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x0, decimals=8).tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    y = np.array(y)\n    (unique_classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for (i, row) in enumerate(dense_matrix):\n        num_non_zeros = 0\n        for (j, elem) in enumerate(row):\n            if elem != 0:\n                values.append(elem)\n                col_indices.append(j)\n                num_non_zeros += 1\n        row_pointers.append(row_pointers[-1] + num_non_zeros)\n    return (values, col_indices, row_pointers)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    projection_vector = scalar_projection * L\n    projection_vector_rounded = np.round(projection_vector, 3).tolist()\n    return projection_vector_rounded\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for c in range(num_cols):\n        for r in range(num_rows):\n            if dense_matrix[r][c] != 0:\n                values.append(dense_matrix[r][c])\n                row_indices.append(r)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A by finding the basis vectors\n    that span the column space. The function identifies independent columns in row echelon form\n    and returns these columns from the original matrix.\n    \n    Parameters:\n    A (np.ndarray): The input matrix for which we want to find the column space.\n    \n    Returns:\n    list: A list of lists representing the basis vectors that span the column space of A,\n          rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A)\n    (ref, _) = row_echelon_form(A)\n    pivot_columns = set()\n    for row in range(ref.shape[0]):\n        for col in range(ref.shape[1]):\n            if ref[row, col] != 0:\n                pivot_columns.add(col)\n                break\n    basis_vectors = A[:, list(pivot_columns)]\n    return np.round(basis_vectors, 8).tolist()\ndef row_echelon_form(A):\n    \"\"\"\n    Converts a matrix A into its row echelon form.\n    \n    Parameters:\n    A (np.ndarray): The input matrix.\n    \n    Returns:\n    tuple: A tuple containing the row echelon form of A and the number of non-zero rows.\n    \"\"\"\n    A = A.copy()\n    (m, n) = A.shape\n    (row, col) = (0, 0)\n    while row < m and col < n:\n        max_row = np.argmax(np.abs(A[row:, col])) + row\n        if A[max_row, col] == 0:\n            col += 1\n            continue\n        A[[row, max_row]] = A[[max_row, row]]\n        for below_row in range(row + 1, m):\n            factor = A[below_row, col] / A[row, col]\n            A[below_row] -= factor * A[row]\n        row += 1\n        col += 1\n    non_zero_rows = np.count_nonzero(np.max(np.abs(A), axis=1))\n    return (A, non_zero_rows)\nA = np.array([[2, 4, 1], [-1, -2, 1], [3, 6, 0]])\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculates the R-squared (coefficient of determination) between the true and predicted values.\n    \n    Parameters:\n    - y_true: Array of true values.\n    - y_pred: Array of predicted values.\n    \n    Returns:\n    - The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    mean_y_true = np.mean(y_true)\n    sst = np.sum((y_true - mean_y_true) ** 2)\n    ssr = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ssr / sst\n    return round(r2, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    expected_length = len(img[0])\n    for row in img:\n        if len(row) != expected_length:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    avg_brightness = total_brightness / total_pixels\n    return round(avg_brightness, 2)\nimg = [[128, 255, 64], [96, 0, 192]]\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the true values (y_true)\n    and the predicted values (y_pred).\n    \n    Parameters:\n    y_true : array-like\n        The actual values.\n    y_pred : array-like\n        The predicted values.\n        \n    Returns:\n    float\n        The RMSE value rounded to three decimal places.\n        \n    Raises:\n    ValueError\n        If y_true and y_pred have different lengths or are empty.\n    TypeError\n        If y_true or y_pred are not array-like.\n    \"\"\"\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n    except Exception as e:\n        raise TypeError('Inputs y_true and y_pred must be array-like.') from e\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same length.')\n    squared_errors = (y_true - y_pred) ** 2\n    rmse_value = np.sqrt(np.mean(squared_errors))\n    return round(rmse_value, 3)\ny_true = [1.0, 2.0, 3.0, 4.0, 5.0]\ny_pred = [1.2, 1.8, 3.1, 3.9, 5.2]\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays y_true and y_pred.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: A binary np.ndarray representing the ground truth labels.\n    - y_pred: A binary np.ndarray representing the predicted labels.\n    \n    Returns:\n    - A float representing the Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    sum_true_pred = np.sum(y_true) + np.sum(y_pred)\n    if sum_true_pred == 0:\n        return 1.0\n    score = 2 * intersection / sum_true_pred\n    return round(score, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: Seed must be between 0 and 2**32 - 1", "solution": "import numpy as np\nimport numpy as np\ndef create_hypervector(seed, dim):\n    \"\"\"Generate a binary hypervector with a given seed and dimension.\"\"\"\n    np.random.seed(seed)\n    return np.where(np.random.rand(dim) > 0.5, 1, -1)\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"Bind two hypervectors using element-wise multiplication and normalize.\"\"\"\n    return hv1 * hv2\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Create a composite hypervector for a dataset row using HDC.\n    \n    Parameters:\n    - row (dict): A dictionary representing a dataset row.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): Seeds for each feature to ensure reproducibility.\n    \n    Returns:\n    - list: A composite hypervector representing the row.\n    \"\"\"\n    row_hv = np.ones(dim)\n    for (feature_name, value) in row.items():\n        feature_hv = create_hypervector(random_seeds[feature_name], dim)\n        value_hv = create_hypervector(hash(str(value)), dim)\n        bound_hv = bind_hypervectors(feature_hv, value_hv)\n        row_hv = bind_hypervectors(row_hv, bound_hv)\n    return row_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification based on input data.\n    \n    Parameters:\n        data (list of lists): Each element is [y_true, y_pred], where y_true is the true label and y_pred is the prediction.\n    \n    Returns:\n        list of lists: A 2x2 confusion matrix [[TP, FP], [FN, TN]].\n    \"\"\"\n    TP = FP = FN = TN = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            TP += 1\n        elif y_true == 0 and y_pred == 1:\n            FP += 1\n        elif y_true == 1 and y_pred == 0:\n            FN += 1\n        else:\n            TN += 1\n    matrix = [[TP, FP], [FN, TN]]\n    return matrix\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: numpy array, first input vector\n    - v2: numpy array, second input vector\n    \n    Returns:\n    - float, cosine similarity rounded to three decimal places\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Vectors must not have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return round(dot_product / (norm_v1 * norm_v2), 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "from collections import Counter\nfrom collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of 'actual' and 'predicted' lists must be the same.\")\n    confusion_matrix = [[0, 0], [0, 0]]\n    for (act, pred) in zip(actual, predicted):\n        if act == 1 and pred == 1:\n            confusion_matrix[0][0] += 1\n        elif act == 0 and pred == 0:\n            confusion_matrix[1][1] += 1\n        elif act == 0 and pred == 1:\n            confusion_matrix[1][0] += 1\n        elif act == 1 and pred == 0:\n            confusion_matrix[0][1] += 1\n    tp = confusion_matrix[0][0]\n    tn = confusion_matrix[1][1]\n    fp = confusion_matrix[1][0]\n    fn = confusion_matrix[0][1]\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = round(tp / (tp + fp), 3) if tp + fp != 0 else 0\n    recall = round(tp / (tp + fn), 3) if tp + fn != 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall != 0 else 0\n    specificity = round(tn / (tn + fp), 3) if tn + fp != 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if tn + fn != 0 else 0\n    return (np.array(confusion_matrix), accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 1, 0, 0, 1, 0, 0, 0, 1, 0]\npredicted = [1, 0, 1, 0, 0, 0, 1, 0, 1, 0]\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: invalid index to scalar variable.", "solution": "import numpy as np\nimport numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n        data (list or numpy.ndarray): A list or NumPy array of numerical values.\n        \n    Returns:\n        dict: A dictionary containing various descriptive statistics metrics.\n    \"\"\"\n    data = np.array(data)\n    mean = round(np.mean(data), 4)\n    median = round(np.median(data), 4)\n    mode = stats.mode(data).mode[0]\n    variance = round(np.var(data, ddof=1), 4)\n    std_dev = round(np.std(data, ddof=1), 4)\n    percentile_25 = round(np.percentile(data, 25), 4)\n    percentile_50 = round(np.percentile(data, 50), 4)\n    percentile_75 = round(np.percentile(data, 75), 4)\n    iqr = round(np.percentile(data, 75) - np.percentile(data, 25), 4)\n    stats_dict = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': std_dev, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}\n    return stats_dict\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nprobability = binomial_probability(5, 3, 0.5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate (mean) of occurrences in a fixed interval, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability rounded to 5 decimal places\n    \"\"\"\n    try:\n        prob = math.exp(-lam) * lam ** k / math.factorial(k)\n        return round(prob, 5)\n    except Exception as e:\n        return None\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast value which is the difference between the maximum and minimum pixel values in the image.\n    \"\"\"\n    img_array = np.array(img)\n    contrast_value = np.max(img_array) - np.min(img_array)\n    return contrast_value\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        raise ValueError('Both vectors must be of the same length.')\n    result = np.dot(vec1, vec2)\n    return result\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list containing the transformed features of each data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_np = np.array(data).reshape(-1, 1)\n    transformed_data = np.empty((data_np.shape[0], 0))\n    for d in range(1, degree + 1):\n        transformed_data = np.concatenate((transformed_data, data_np ** d), axis=1)\n    transformed_data_rounded = np.round(transformed_data, 8).tolist()\n    return transformed_data_rounded\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for a transformer model with given position and model dimension.\n    \n    Args:\n        position (int): The maximum position encoded.\n        d_model (int): Dimensionality of the model.\n        \n    Returns:\n        List of float: Positional encoding list or -1 if input is invalid.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    position_enc = np.arange(0, position, dtype=np.float16).reshape(-1, 1)\n    div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float16) * -(np.log(10000.0) / d_model))\n    pe[:, 0::2] = np.sin(position_enc * div_term)\n    pe[:, 1::2] = np.cos(position_enc * div_term)\n    return pe.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())\nparameter = np.array([1.5, -0.3])\ngrad = np.array([0.1, -0.2])\nm = np.array([0.05, -0.01])\nv = np.array([0.01, 0.005])\nt = 1\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: shapes (1,1024) and (1,10) not aligned: 1024 (dim 1) != 1 (dim 0)", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    token_embeddings = params['wte'][token_ids]\n    pos_embeddings = params['wpe'][:len(token_ids)]\n    x = token_embeddings + pos_embeddings\n    attention_weights = np.random.rand(len(token_ids), hparams['n_ctx'])\n    x = np.dot(attention_weights, x)\n    ff_weights = np.random.rand(10, 10)\n    x = np.maximum(x @ ff_weights, 0)\n    (g, b) = (params['ln_f']['g'], params['ln_f']['b'])\n    mean = np.mean(x, axis=-1, keepdims=True)\n    var = np.var(x, axis=-1, keepdims=True)\n    x = g * (x - mean) / np.sqrt(var + 1e-05) + b\n    for _ in range(n_tokens_to_generate):\n        next_token_id = np.random.randint(0, len(encoder.encoder_dict))\n        x = np.concatenate((x, params['wte'][next_token_id].reshape(1, -1)), axis=0)\n        token_ids.append(next_token_id)\n    generated_text = encoder.decode(token_ids)\n    return generated_text\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    np.random.seed(42)\n    weights_q = np.random.rand(dimension, dimension)\n    weights_k = np.random.rand(dimension, dimension)\n    weights_v = np.random.rand(dimension, dimension)\n    crystal_values = np.array(crystal_values).reshape(-1, dimension)\n    queries = np.dot(crystal_values, weights_q)\n    keys = np.dot(crystal_values, weights_k)\n    values = np.dot(crystal_values, weights_v)\n    attention_scores = np.dot(queries, keys.T)\n    attention_probs = softmax(attention_scores)\n    weighted_values = np.dot(attention_probs, values)\n    final_patterns = np.mean(weighted_values, axis=1)\n    final_patterns_rounded = [round(value, 4) for value in final_patterns]\n    return final_patterns_rounded\ncrystal_values = [1, 2, 3, 4, 5]\ndimension = 2\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'split'", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for documents in the corpus given a query.\n    \n    :param corpus: List of strings, where each string represents a document.\n    :param query: String representing the search query.\n    :param k1: Positive float representing term frequency saturation constant.\n    :param b: Constant between 0 and 1 for document length normalization.\n    :return: List of BM25 scores for each document in the corpus, rounded to 3 decimal places.\n    \"\"\"\n    corpus_words = [doc.split() for doc in corpus]\n    query_words = query.split()\n    avgdl = sum([len(doc) for doc in corpus_words]) / len(corpus)\n    tf_doc = [Counter(doc) for doc in corpus_words]\n    df = Counter((word for doc in tf_doc for word in doc.keys()))\n    idf = {word: np.log((len(corpus) - df[word] + 0.5) / (df[word] + 0.5)) for word in df}\n    scores = []\n    for (idx, doc) in enumerate(corpus_words):\n        dl = len(doc)\n        score = 0\n        for q_word in set(query_words):\n            if q_word not in tf_doc[idx]:\n                continue\n            tf = tf_doc[idx][q_word]\n            numerator = idf[q_word] * tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * dl / avgdl)\n            score += numerator / denominator\n        scores.append(score)\n    return [round(score, 3) for score in scores]\ncorpus = ['the cat sat on the mat', 'a cat is sitting on the mat', 'there is a cat sitting on a mat']\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = fp = fn = 0\n    set_true = set(y_true)\n    set_pred = set(y_pred)\n    for label in set_true.union(set_pred):\n        if label in set_true and label in set_pred:\n            tp += min(y_true.count(label), y_pred.count(label))\n        elif label not in set_true and label in set_pred:\n            fp += y_pred.count(label)\n        elif label in set_true and label not in set_pred:\n            fn += y_true.count(label)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import math\nPI = 3.14159\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = np.array([i for i in range(1, len(consumption_data) + 1)]).reshape(-1, 1)\n    fluctuation = [10 * math.sin(2 * PI * i / 10) for i in range(1, len(consumption_data) + 1)]\n    detrended_consumption = [consumption_data[i] - fluctuation[i] for i in range(len(consumption_data))]\n    model = LinearRegression()\n    model.fit(days, detrended_consumption)\n    future_day = np.array([15]).reshape(-1, 1)\n    base_consumption_day_15 = model.predict(future_day)[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    rounded_prediction = np.ceil(predicted_consumption_day_15)\n    safety_margin_prediction = np.ceil(rounded_prediction * 1.05)\n    return int(safety_margin_prediction)\nconsumption_data = [102, 110, 108, 115, 120, 127, 130, 135, 140, 145]\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('y_true and y_pred must be instances of numpy.ndarray')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Length of y_true and y_pred must be equal')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices given the input matrix X and weight matrices for Q, K, V.\n\n    Parameters:\n    - X: Input data matrix of shape (batch_size, seq_len, input_dim).\n    - W_q: Weight matrix for the query transformation of shape (input_dim, d_q).\n    - W_k: Weight matrix for the key transformation of shape (input_dim, d_k).\n    - W_v: Weight matrix for the value transformation of shape (input_dim, d_v).\n\n    Returns:\n    - A tuple (Q, K, V) representing the computed Query, Key, and Value matrices.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute scaled dot product self-attention.\n\n    Parameters:\n    - Q: Query matrix of shape (batch_size, seq_len, d_q).\n    - K: Key matrix of shape (batch_size, seq_len, d_k).\n    - V: Value matrix of shape (batch_size, seq_len, d_v).\n\n    Returns:\n    - The result of self-attention computation.\n    \"\"\"\n    d_k = K.shape[-1]\n    assert d_k != 0, 'd_k must be non-zero'\n    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attn_dist = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    context = np.matmul(attn_dist, V)\n    return context\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute multi-head attention.\n\n    Parameters:\n    - X: Input data matrix of shape (batch_size, seq_len, input_dim).\n    - W_q: Weight matrix for the query transformation of shape (input_dim, d_q).\n    - W_k: Weight matrix for the key transformation of shape (input_dim, d_k).\n    - W_v: Weight matrix for the value transformation of shape (input_dim, d_v).\n    - n_heads: Number of attention heads.\n\n    Returns:\n    - The output of multi-head attention computation, a python list.\n    \"\"\"\n    (batch_size, seq_len, _) = X.shape\n    d_model = X.shape[-1]\n    assert d_model % n_heads == 0, 'Model dimension must be divisible by the number of heads'\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    d_head = d_model // n_heads\n    Q_heads = Q.reshape(batch_size, seq_len, n_heads, d_head).transpose(0, 2, 1, 3)\n    K_heads = K.reshape(batch_size, seq_len, n_heads, d_head).transpose(0, 2, 1, 3)\n    V_heads = V.reshape(batch_size, seq_len, n_heads, d_head).transpose(0, 2, 1, 3)\n    attention_heads = [self_attention(Q_heads[:, i], K_heads[:, i], V_heads[:, i]) for i in range(n_heads)]\n    concat_heads = np.concatenate(attention_heads, axis=-1)\n    rounded_results = np.round(concat_heads, decimals=4)\n    return rounded_results.tolist()\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    n11 = np.sum((x == 1) & (y == 1))\n    n10 = np.sum((x == 1) & (y == 0))\n    n01 = np.sum((x == 0) & (y == 1))\n    n00 = np.sum((x == 0) & (y == 0))\n    numerator = n11 * n00 - n10 * n01\n    denominator = np.sqrt((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01))\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 0, 1, 1, 0, 0]\ny = [1, 0, 0, 1, 0, 1, 0, 1]\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place.\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for /: 'list' and 'list'", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    rho_max = np.maximum(rhos, 1)\n    rho_min = np.minimum(rhos, 1)\n    grpo_obj = np.mean(np.minimum(clipped_rhos * A, rho_max * A) - np.minimum(rho_min * A, A))\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj -= beta * kl_divergence\n    return round(grpo_obj, 6)\nrhos = [1.2, 0.8, 1.5, 0.7]\nA = [2.0, -1.0, 3.0, -2.5]\npi_theta_old = [0.4, 0.6, 0.3, 0.7]\npi_theta_ref = [0.35, 0.65, 0.25, 0.75]\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": false, "result": "failed: test case failed: selu(1.0)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": false, "result": "failed: test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the sigmoid of z.\n    \n    Args:\n        z: A scalar or numpy array of any size.\n\n    Returns:\n        Sigmoid of z\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)", "solution": "import numpy as np\nimport numpy as np\ndef softmax(z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the softmax function for each row of the input.\"\"\"\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return e_z / np.sum(e_z, axis=1, keepdims=True)\ndef cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the cross-entropy loss.\"\"\"\n    return -np.sum(y_true * np.log(y_pred + 1e-08)) / len(y_true)\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = len(np.unique(y))\n    y_one_hot = np.eye(n_classes)[y]\n    B = np.zeros((n_features, n_classes))\n    losses = []\n    for _ in range(iterations):\n        linear_model = X @ B\n        predictions = softmax(linear_model)\n        error = predictions - y_one_hot\n        gradient = X.T @ error / n_samples\n        B -= learning_rate * gradient\n        loss = cross_entropy_loss(y_one_hot, predictions)\n        losses.append(round(loss, 4))\n    B_rounded = np.round(B, 4).tolist()\n    return (B_rounded, losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef compute_loss(y: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute binary cross entropy loss.\"\"\"\n    epsilon = 1e-12\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    bce = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n    return round(bce, 4)\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    losses = []\n    for _ in range(iterations):\n        y_pred = sigmoid(np.dot(X, weights) + bias)\n        loss = compute_loss(y, y_pred)\n        losses.append(loss)\n        dw = 1 / m * np.dot(X.T, y_pred - y)\n        db = 1 / m * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    coefficients = np.concatenate([weights, [bias]]).tolist()\n    return (coefficients, losses)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores += mask * -1000000000.0\n    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.dot(weights, V)\n    return output.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\nfrom collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon entropy to measure the disorder of the distribution of apple colors.\n    \"\"\"\n    counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in counts.values()]\n    entropy = -sum((p * math.log2(p) for p in probabilities))\n    return round(entropy, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform layer normalization on a 3D numpy array (batch_size x seq_length x feature_dim).\n    \n    Parameters:\n    - X: Input data as a numpy array of shape (batch_size, seq_length, feature_dim).\n    - gamma: Scaling parameter (shape: feature_dim).\n    - beta: Shifting parameter (shape: feature_dim).\n    - epsilon: Small constant added to avoid division by zero.\n    \n    Returns:\n    - A numpy array of the same shape as X, but layer-normalized and scaled/shifted.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    std = np.sqrt(var + epsilon)\n    X_norm = (X - mean) / std\n    X_scaled_shifted = gamma * X_norm + beta\n    return np.round(X_scaled_shifted, decimals=5).tolist()\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef harmonic_mean(precision, recall, beta=3):\n    \"\"\"Calculate the harmonic mean used in METEOR score.\"\"\"\n    if precision == 0 or recall == 0:\n        return 0\n    return (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\ndef fragmentation_penalty(reference_tokens, candidate_tokens):\n    \"\"\"Calculate the fragmentation penalty based on the number of unigrams matched.\"\"\"\n    ref_counter = Counter(reference_tokens)\n    can_counter = Counter(candidate_tokens)\n    total_matched = 0\n    max_matched = 0\n    last_match_index = -1\n    for (i, token) in enumerate(candidate_tokens):\n        if ref_counter[token] > 0 and can_counter[token] > 0:\n            total_matched += 1\n            if i == last_match_index + 1:\n                max_matched += 1\n            last_match_index = i\n            ref_counter[token] -= 1\n            can_counter[token] -= 1\n    return max_matched / total_matched if total_matched > 0 else 1\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for a given reference and candidate sentences.\n    \n    Parameters:\n    reference (str): The reference sentence.\n    candidate (str): The candidate sentence.\n    alpha (float): Weight of brevity penalty.\n    beta (int): Beta parameter for the F-score.\n    gamma (float): Gamma parameter for the fragmentation penalty.\n    \n    Returns:\n    float: The computed METEOR score, rounded to three decimal places.\n    \"\"\"\n    reference_tokens = reference.split()\n    candidate_tokens = candidate.split()\n    ref_count = Counter(reference_tokens)\n    can_count = Counter(candidate_tokens)\n    total_matches = sum((ref_count & can_count).values())\n    if total_matches == 0:\n        return 0.0\n    precision = total_matches / len(candidate_tokens)\n    recall = total_matches / len(reference_tokens)\n    f_mean = harmonic_mean(precision, recall, beta)\n    brevity_penalty = min(1, np.exp(1 - len(reference_tokens) / len(candidate_tokens))) if len(candidate_tokens) != 0 else 0\n    fragmentation_pen = fragmentation_penalty(reference_tokens, candidate_tokens)\n    meteor_s = brevity_penalty * (f_mean * (1 - gamma) + gamma * fragmentation_pen)\n    meteor_s *= max(0, (total_matches - alpha * len(reference_tokens)) / (1 - alpha))\n    return round(min(meteor_s, 1), 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) for two events.\n    \n    :param joint_counts: The number of times both events x and y occur together.\n    :param total_counts_x: The total number of occurrences of event x.\n    :param total_counts_y: The total number of occurrences of event y.\n    :param total_samples: The total number of samples.\n    :return: A float value representing PMI, rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return 0.0\n    expected = total_counts_x * total_counts_y / total_samples\n    pmi = np.log2(joint_counts / expected)\n    return round(pmi, 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Applies Min-Max Normalization to the given list of integers.\n    Each value in the list is scaled to a range [0, 1].\n    \n    Parameters:\n    x (list of int): The list of integers to normalize.\n    \n    Returns:\n    list of float: A new list containing the normalized values rounded to 4 decimal places.\n    \"\"\"\n    if not x or max(x) == min(x):\n        return [0.0] * len(x)\n    x_min = min(x)\n    x_max = max(x)\n    normalized_x = [(i - x_min) / (x_max - x_min) for i in x]\n    return [round(i, 4) for i in normalized_x]\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Implements a simple residual block with shortcut connection using NumPy.\n    \n    Args:\n    x : np.ndarray - A 1D input array.\n    w1 : np.ndarray - Weight matrix for the first layer.\n    w2 : np.ndarray - Weight matrix for the second layer.\n    \n    Returns:\n    list - The output of the residual block after the final ReLU activation,\n           rounded to 4 decimal places.\n    \"\"\"\n    assert x.shape[0] == w1.shape[0], \"Input and first weight matrix dimensions don't match.\"\n    assert w1.shape[1] == w2.shape[0], \"Weight matrices dimensions don't match.\"\n    out = np.maximum(np.dot(x, w1), 0)\n    out = np.maximum(np.dot(out, w2), 0)\n    out += x\n    out = np.maximum(out, 0)\n    return np.round(out, 4).tolist()\nx = np.array([1, 2, 3])\nw1 = np.array([[0.5, 1.0], [0.3, 0.7], [1.0, 0.2]])\nw2 = np.array([[0.8, 0.5], [0.2, 0.9]])\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Applies Global Average Pooling over the input tensor.\n\n    Parameters:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels)\n\n    Returns:\n    np.ndarray: A 1D NumPy array of shape (channels,) where each element\n                represents the average value of the corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Applies Batch Normalization to a 4D BCHW input NumPy array.\n    \n    Parameters:\n    X (np.ndarray): A 4D input array in BCHW format.\n    gamma (np.ndarray): Scale parameter for each channel.\n    beta (np.ndarray): Shift parameter for each channel.\n    epsilon (float): Small constant to prevent division by zero.\n    \n    Returns:\n    np.ndarray: Normalized 4D output array.\n    \"\"\"\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    x_norm = (X - mean) / np.sqrt(var + epsilon)\n    out = x_norm * gamma.reshape(1, -1, 1, 1) + beta.reshape(1, -1, 1, 1)\n    return np.round(out, decimals=4).tolist()\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c*x^n at a given point x.\n    \n    Parameters:\n    - c (float): Coefficient of the polynomial term.\n    - x (float): Point at which to evaluate the derivative.\n    - n (float): Exponent of the polynomial term.\n\n    Returns:\n    - float: Value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    :param vectors: List of 2D vectors represented as lists.\n    :param tol: Tolerance value to determine linear independence.\n    :return: List of orthonormal vectors that span the same subspace as the input vectors.\n    \"\"\"\n\n    def normalize(vector):\n        norm = np.linalg.norm(vector)\n        if norm < tol:\n            raise ValueError('Cannot normalize a zero vector')\n        return vector / norm\n    V = np.array(vectors).T\n    (Q, _) = np.linalg.qr(V)\n    ortho_basis = []\n    for col in Q.T:\n        if not ortho_basis:\n            ortho_basis.append(normalize(col))\n        else:\n            projection = sum((np.dot(col, basis) * basis for basis in ortho_basis))\n            difference = col - projection\n            if np.linalg.norm(difference) > tol:\n                ortho_basis.append(normalize(difference))\n    return [np.round(vec, 4).tolist() for vec in ortho_basis]\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (np.ndarray): First input vector.\n    b (np.ndarray): Second input vector.\n    \n    Returns:\n    list: A list containing the components of the resulting vector,\n          rounded to 4 decimal places.\n    \"\"\"\n    result = np.cross(a, b)\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): The coefficient matrix, must be square.\n    b (np.array): The constant vector.\n    \n    Returns:\n    list or int: The solution vector x as a list if unique solution exists, otherwise returns -1.\n    \"\"\"\n    det_A = np.linalg.det(A)\n    n = len(b)\n    if abs(det_A) < 1e-10:\n        return -1\n    solutions = []\n    for i in range(n):\n        Ai = np.copy(A)\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        solutions.append(round(det_Ai / det_A, 4))\n    return np.array(solutions).tolist()\nA = np.array([[3, 2], [1, -1]])\nb = np.array([7, 1])\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions p and q.\n    \n    Parameters:\n    p (list of floats): A list representing the first probability distribution.\n    q (list of floats): A list representing the second probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance rounded to 4 decimal places, or 0.0 if inputs are invalid.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    bc = sum((np.sqrt(a * b) for (a, b) in zip(p, q)))\n    bd = -np.log(bc)\n    return round(bd, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\ndef vector_sum(a: List[int | float], b: List[int | float]) -> List[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Parameters:\n    a (List[int|float]): First vector as a list of integers or floats.\n    b (List[int|float]): Second vector as a list of integers or floats.\n\n    Returns:\n    List[int|float]: A new vector (list) where each element is the sum of the elements at the same index in a and b.\n    int: Returns -1 if the vectors have different lengths.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: axis 1 is out of bounds for array of dimension 0", "solution": "import numpy as np\nimport numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1).reshape(-1, 1)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list:\n    \"\"\"\n    Computes the policy gradient for the given episodes using the REINFORCE algorithm.\n    \n    Parameters:\n    - theta: A 2D numpy array representing the policy parameters.\n    - episodes: A list of episodes where each episode is a list of tuples (state, action, reward).\n    \n    Returns:\n    - The average gradient of the log-policy for each state-action pair.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    total_episodes = len(episodes)\n\n    def get_return(rewards):\n        return sum(rewards)\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        returns = [get_return(rewards[i:]) for i in range(len(rewards))]\n        policy_probs = softmax(theta[states])\n        for (t, (s, a, r)) in enumerate(zip(states, actions, returns)):\n            grad_theta_s_a = np.zeros((num_states, num_actions))\n            grad_theta_s_a[s, a] = 1\n            gradient += r * (grad_theta_s_a - policy_probs[t][a]) / total_episodes\n    return np.around(gradient, decimals=4).tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Compute the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): Number of experts in the MoE layer.\n    k_active (int): Number of active experts during computation.\n    d_in (int): Input dimension for each expert.\n    d_out (int): Output dimension for each expert.\n    \n    Returns:\n    float: Percentage of FLOP savings of the MoE layer over a dense layer, rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_per_matrix_multiply = d_in * d_out\n    flops_dense_layer = flops_per_matrix_multiply\n    flops_moe_layer = k_active * flops_per_matrix_multiply\n    flop_savings = flops_dense_layer - flops_moe_layer\n    if flop_savings <= 0:\n        return 0.0\n    savings_percentage = flop_savings / flops_dense_layer * 100\n    return round(savings_percentage, 1)\nk_active = 4\nd_in = 512\nd_out = 512\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implementation of the Noisy Top-K Gating Mechanism for MoE models.\n    \n    Parameters:\n    - X: Input matrix of shape (batch_size, feature_dim)\n    - W_g: Weight matrix for gating of shape (feature_dim, num_experts)\n    - W_noise: Weight matrix for noise injection of shape (1, num_experts)\n    - N: Pre-sampled noise matrix of shape (batch_size, num_experts)\n    - k: Sparsity constraint, number of top experts to be selected\n    \n    Returns:\n    - Gating probabilities matrix of shape (batch_size, num_experts) as a python list\n    \"\"\"\n    logits = X @ W_g\n    noise = N * W_noise\n    noisy_logits = logits + noise\n    maxes = np.max(noisy_logits, axis=-1, keepdims=True)\n    exps = np.exp(noisy_logits - maxes)\n    sum_exps = np.sum(exps, axis=-1, keepdims=True)\n    probs = exps / sum_exps\n    sorted_probs = np.sort(probs, axis=-1)[:, ::-1]\n    topk_probs = sorted_probs[:, :k]\n    threshold = topk_probs[:, -1:]\n    mask = (probs >= threshold).astype(float)\n    masked_probs = probs * mask\n    row_sums = np.sum(masked_probs, axis=-1, keepdims=True)\n    normalized_probs = masked_probs / row_sums\n    result = np.around(normalized_probs, decimals=4)\n    return result.tolist()\nX = np.array([[0.5, 0.3], [0.2, 0.7]])\nW_g = np.array([[0.8, 0.6], [0.4, 0.9]])\nW_noise = np.array([[0.1, 0.2]])\nN = np.random.randn(*X.shape[0:1], W_g.shape[-1])\nk = 1\nresult = noisy_topk_gating(X, W_g, W_noise, N, k)\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: too many values to unpack (expected 2)", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n        x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n        We (np.ndarray): Expert weight matrix of shape (input_dim, n_experts, output_dim).\n        Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n        n_experts (int): Number of experts.\n        top_k (int): How many top experts to route each token to.\n        \n    Returns:\n        np.ndarray: The aggregated output tensor after applying MoE.\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    (_, _, output_dim) = We.shape\n    gating_scores = x @ Wg\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=-1, keepdims=True)\n    topk_indices = np.argsort(-gating_probs, axis=-1)[:, :, :top_k]\n    topk_gating_probs = np.take_along_axis(gating_probs, topk_indices, axis=-1)\n    normalized_topk_gating_probs = topk_gating_probs / np.sum(topk_gating_probs, axis=-1, keepdims=True)\n    output = np.zeros((batch_size, top_k, output_dim))\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_index = topk_indices[i, 0, j]\n            output[i, j] = x[i] @ We[:, expert_index, :]\n    final_output = np.sum(normalized_topk_gating_probs[..., np.newaxis] * output, axis=1)\n    return np.round(final_output, decimals=4).tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    G = num_groups\n    assert C % G == 0, 'Number of channels cannot be evenly divided by the number of groups.'\n    x_group = X.reshape((B, G, -1))\n    mean = np.mean(x_group, axis=2, keepdims=True)\n    var = np.var(x_group, axis=2, keepdims=True)\n    x_norm = (x_group - mean) / np.sqrt(var + epsilon)\n    x_norm = x_norm.reshape(B, C, H, W)\n    x_out = x_norm * gamma + beta\n    x_out_list = np.round(x_out, 4).tolist()\n    return x_out_list\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        \"\"\"Function to minimize.\"\"\"\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        \"\"\"Derivative of the function.\"\"\"\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    iters = 0\n    while True:\n        gradient = df(x)\n        x_next = x - learning_rate * gradient\n        if abs(x_next - x) < tolerance or iters >= max_iters:\n            break\n        x = x_next\n        iters += 1\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) activation function which serves as a normalization-free\n    alternative for transformers. The function applies a modified tanh transformation to the input \n    array x, controlled by parameters alpha, gamma, and beta.\n    \n    Parameters:\n    - x: A numpy array of arbitrary shape, representing the inputs to be transformed.\n    - alpha: A scaling factor that influences the amplitude of the output.\n    - gamma: A parameter that shifts the input values before applying the tanh function.\n    - beta: An additional scaling factor applied post-tanh transformation.\n\n    Returns:\n    - A list of floats, each representing the DyT-transformed value of the corresponding input,\n      rounded to 4 decimal places.\n    \"\"\"\n    transformed = alpha * np.tanh(gamma * x + beta)\n    result_list = np.round(transformed, decimals=4).tolist()\n    return result_list\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
