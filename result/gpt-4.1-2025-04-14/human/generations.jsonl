{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if matrix 'a' is empty\n    if not a:\n        # If 'a' is empty, return empty list if b is also empty, else -1\n        return [] if len(b) == 0 else -1\n\n    num_cols = len(a[0])\n\n    # Check if all rows in 'a' have the same number of columns\n    for row in a:\n        if len(row) != num_cols:\n            return -1\n\n    # Check if the number of columns matches the length of vector 'b'\n    if num_cols != len(b):\n        return -1\n\n    # Compute the dot product for each row\n    result = []\n    for row in a:\n        dot = sum(x * y for x, y in zip(row, b))\n        result.append(dot)\n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Valid case\n    mat = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    vec = [7, 8, 9]\n    print(matrix_dot_vector(mat, vec))  # Output: [50, 122]\n\n    # Invalid case (dimension mismatch)\n    mat2 = [\n        [1, 2],\n        [3, 4]\n    ]\n    vec2 = [5, 6, 7]\n    print(matrix_dot_vector(mat2, vec2))  # Output: -1\n\n    # Empty matrix and empty vector\n    print(matrix_dot_vector([], []))  # Output: []\n\n    # Matrix with inconsistent row lengths\n    mat3 = [\n        [1, 2, 3],\n        [4, 5]\n    ]\n    vec3 = [1, 2, 3]\n    print(matrix_dot_vector(mat3, vec3))  # Output: -1\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n\n    Args:\n        a (list[list[int|float]]): The input matrix.\n\n    Returns:\n        list[list[int|float]]: The transposed matrix.\n    \"\"\"\n    if not a:\n        return []\n    # Check if all rows have the same length\n    row_length = len(a[0])\n    for row in a:\n        if len(row) != row_length:\n            raise ValueError(\"All rows must have the same length.\")\n    # Transpose using zip and list comprehension\n    return [list(row) for row in zip(*a)]\n\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    matrix1 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    print(\"Original matrix:\")\n    for row in matrix1:\n        print(row)\n    print(\"Transposed matrix:\")\n    for row in transpose_matrix(matrix1):\n        print(row)\n\n    matrix2 = [\n        [1.0, 2.5],\n        [3.1, 4.2],\n        [5.3, 6.4]\n    ]\n    print(\"\\nOriginal matrix:\")\n    for row in matrix2:\n        print(row)\n    print(\"Transposed matrix:\")\n    for row in transpose_matrix(matrix2):\n        print(row)\n\n    # Edge case: empty matrix\n    matrix3 = []\n    print(\"\\nOriginal matrix:\", matrix3)\n    print(\"Transposed matrix:\", transpose_matrix(matrix3))\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Convert input list to numpy array\n    arr = np.array(a)\n    # Check if total elements match\n    if arr.size != new_shape[0] * new_shape[1]:\n        return []\n    try:\n        reshaped = arr.reshape(new_shape)\n        return reshaped.tolist()\n    except Exception:\n        return []\n\n# Example usage:\nif __name__ == \"__main__\":\n    mat = [[1,2,3],[4,5,6]]\n    print(reshape_matrix(mat, (3,2)))  # [[1, 2], [3, 4], [5, 6]]\n    print(reshape_matrix(mat, (2,3)))  # [[1, 2, 3], [4, 5, 6]]\n    print(reshape_matrix(mat, (4,2)))  # []\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix by row or by column.\n\n    Args:\n        matrix (list[list[float]]): The input matrix.\n        mode (str): 'row' to calculate mean by row, 'column' to calculate mean by column.\n\n    Returns:\n        list[float]: List of means according to the specified mode.\n\n    Raises:\n        ValueError: If mode is not 'row' or 'column', or if the matrix is empty or irregular.\n    \"\"\"\n    if not matrix or not all(isinstance(row, list) and row for row in matrix):\n        raise ValueError(\"Matrix must be a non-empty list of non-empty lists.\")\n\n    num_cols = len(matrix[0])\n    if any(len(row) != num_cols for row in matrix):\n        raise ValueError(\"All rows in the matrix must have the same number of columns.\")\n\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        return [sum(matrix[row_idx][col_idx] for row_idx in range(len(matrix))) / len(matrix)\n                for col_idx in range(num_cols)]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    mat = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    print(\"Row means:\", calculate_matrix_mean(mat, 'row'))      # Output: [2.0, 5.0, 8.0]\n    print(\"Column means:\", calculate_matrix_mean(mat, 'column'))  # Output: [4.0, 5.0, 6.0]\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the given scalar.\n\n    Args:\n        matrix (list[list[int|float]]): The input matrix.\n        scalar (int|float): The scalar to multiply with.\n\n    Returns:\n        list[list[int|float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    mat = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    s = 3\n    result = scalar_multiply(mat, s)\n    print(\"Original matrix:\")\n    for row in mat:\n        print(row)\n    print(f\"\\nMatrix after multiplying by scalar {s}:\")\n    for row in result:\n        print(row)\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n        matrix: A 2x2 matrix represented as a list of lists.\n\n    Returns:\n        A list of eigenvalues sorted from highest to lowest.\n    \"\"\"\n    if (\n        not isinstance(matrix, list)\n        or len(matrix) != 2\n        or any(not isinstance(row, list) or len(row) != 2 for row in matrix)\n    ):\n        raise ValueError(\"Input must be a 2x2 matrix (list of two lists, each of length 2).\")\n\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Characteristic equation: \u03bb^2 - (a+d)\u03bb + (ad-bc) = 0\n    trace = a + d\n    determinant = a * d - b * c\n\n    # Quadratic formula: \u03bb = [trace \u00b1 sqrt(trace^2 - 4*determinant)] / 2\n    discriminant = trace ** 2 - 4 * determinant\n\n    # To handle possible floating point errors for very small negative discriminant\n    if discriminant < 0:\n        # Complex eigenvalues\n        import cmath\n        sqrt_disc = cmath.sqrt(discriminant)\n    else:\n        sqrt_disc = discriminant ** 0.5\n\n    lambda1 = (trace + sqrt_disc) / 2\n    lambda2 = (trace - sqrt_disc) / 2\n\n    # Return as floats (or complex if needed), sorted from highest to lowest\n    eigenvalues = [lambda1, lambda2]\n    eigenvalues.sort(key=lambda x: (x.real if hasattr(x, 'real') else x), reverse=True)\n    return eigenvalues\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[4, 2], [1, 3]]\n    print(\"Eigenvalues:\", calculate_eigenvalues(matrix))\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[float]]:\n    try:\n        # Convert input lists to numpy arrays of type float\n        A = np.array(A, dtype=float)\n        T = np.array(T, dtype=float)\n        S = np.array(S, dtype=float)\n        \n        # Check if T and S are square and invertible\n        if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n            return -1\n        \n        # Check if dimensions are compatible for multiplication\n        if T.shape[0] != A.shape[0] or S.shape[1] != A.shape[1]:\n            return -1\n        \n        # Check invertibility by determinant\n        if np.isclose(np.linalg.det(T), 0) or np.isclose(np.linalg.det(S), 0):\n            return -1\n        \n        # Compute T^{-1}\n        T_inv = np.linalg.inv(T)\n        \n        # Perform the transformation: T^{-1} * A * S\n        result = T_inv @ A @ S\n        \n        # Round to 4 decimal places\n        result = np.round(result, 4)\n        \n        # Convert to list of lists\n        return result.tolist()\n    except Exception:\n        return -1\n```\n"}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    Returns None if the matrix is not invertible.\n    \"\"\"\n    # Check if input is a 2x2 matrix\n    if (\n        not isinstance(matrix, list) or\n        len(matrix) != 2 or\n        not all(isinstance(row, list) and len(row) == 2 for row in matrix)\n    ):\n        raise ValueError(\"Input must be a 2x2 matrix (list of two lists, each of length 2).\")\n    \n    a, b = matrix[0]\n    c, d = matrix[1]\n    det = a * d - b * c\n\n    if det == 0:\n        return None\n\n    inv_det = 1 / det\n    inverse = [\n        [ d * inv_det, -b * inv_det],\n        [-c * inv_det,  a * inv_det]\n    ]\n    return inverse\n\n# Example usage:\nif __name__ == \"__main__\":\n    m1 = [[4, 7], [2, 6]]\n    m2 = [[1, 2], [2, 4]]  # Not invertible\n\n    print(\"Inverse of m1:\", inverse_2x2(m1))\n    print(\"Inverse of m2:\", inverse_2x2(m2))\n```\n"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if a and b are non-empty and are proper matrices\n    if not a or not b or not all(isinstance(row, list) and row for row in a) or not all(isinstance(row, list) and row for row in b):\n        return -1\n\n    # Number of columns in a must equal number of rows in b\n    num_rows_a = len(a)\n    num_cols_a = len(a[0])\n    num_rows_b = len(b)\n    num_cols_b = len(b[0])\n\n    # Check all rows in a have the same length\n    if any(len(row) != num_cols_a for row in a):\n        return -1\n    # Check all rows in b have the same length\n    if any(len(row) != num_cols_b for row in b):\n        return -1\n\n    if num_cols_a != num_rows_b:\n        return -1\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(num_cols_b)] for _ in range(num_rows_a)]\n\n    # Matrix multiplication\n    for i in range(num_rows_a):\n        for j in range(num_cols_b):\n            for k in range(num_cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    a = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    b = [\n        [7, 8],\n        [9, 10],\n        [11, 12]\n    ]\n    print(matrixmul(a, b))  # Output: [[58, 64], [139, 154]]\n\n    # Incompatible matrices\n    c = [\n        [1, 2],\n        [3, 4]\n    ]\n    d = [\n        [5, 6, 7]\n    ]\n    print(matrixmul(c, d))  # Output: -1\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of feature vectors.\n\n    Args:\n        vectors (list[list[float]]): List of features, each a list of observations.\n                                     Each inner list is a feature (length n), and there are m features.\n\n    Returns:\n        list[list[float]]: Covariance matrix as a list of lists (m x m).\n    \"\"\"\n    if not vectors or not all(isinstance(vec, list) and vec for vec in vectors):\n        raise ValueError(\"Input must be a non-empty list of non-empty lists.\")\n\n    num_features = len(vectors)\n    num_observations = len(vectors[0])\n\n    # Check all features have the same number of observations\n    if any(len(vec) != num_observations for vec in vectors):\n        raise ValueError(\"All features must have the same number of observations.\")\n\n    # Compute means for each feature\n    means = [sum(vec) / num_observations for vec in vectors]\n\n    # Center the data\n    centered = [\n        [x - mean for x in vec]\n        for vec, mean in zip(vectors, means)\n    ]\n\n    # Compute covariance matrix\n    cov_matrix = []\n    for i in range(num_features):\n        row = []\n        for j in range(num_features):\n            # Cov(X, Y) = sum((x_i - mean_x)*(y_i - mean_y)) / (n - 1)\n            cov = sum(centered[i][k] * centered[j][k] for k in range(num_observations)) / (num_observations - 1)\n            row.append(cov)\n        cov_matrix.append(row)\n    return cov_matrix\n\n# ------------------- Test Cases -------------------\n\ndef print_matrix(matrix):\n    for row in matrix:\n        print(\"  \", [\"{:.4f}\".format(x) for x in row])\n\ndef test_covariance_matrix():\n    # Test 1: Simple 2D example\n    vectors1 = [\n        [2.1, 2.5, 3.6, 4.0],  # Feature 1\n        [8.0, 10.0, 12.0, 14.0]  # Feature 2\n    ]\n    print(\"Test 1: Simple 2D example\")\n    cov1 = calculate_covariance_matrix(vectors1)\n    print_matrix(cov1)\n    # Expected: Covariance matrix close to [[0.6167, 2.5333], [2.5333, 6.6667]]\n\n    # Test 2: 3 features, 5 observations\n    vectors2 = [\n        [1, 2, 3, 4, 5],\n        [5, 4, 3, 2, 1],\n        [2, 2, 2, 2, 2]\n    ]\n    print(\"\\nTest 2: 3 features, 5 observations\")\n    cov2 = calculate_covariance_matrix(vectors2)\n    print_matrix(cov2)\n    # Expected: Feature 3 has zero variance, covariances with others are zero\n\n    # Test 3: Single feature\n    vectors3 = [\n        [1, 2, 3, 4]\n    ]\n    print(\"\\nTest 3: Single feature\")\n    cov3 = calculate_covariance_matrix(vectors3)\n    print_matrix(cov3)\n    # Expected: [[1.6667]]\n\n    # Test 4: Features with negative correlation\n    vectors4 = [\n        [1, 2, 3, 4, 5],\n        [10, 8, 6, 4, 2]\n    ]\n    print(\"\\nTest 4: Negative correlation\")\n    cov4 = calculate_covariance_matrix(vectors4)\n    print_matrix(cov4)\n    # Expected: Off-diagonal elements negative\n\n    # Test 5: Error on mismatched lengths\n    try:\n        print(\"\\nTest 5: Mismatched lengths\")\n        vectors5 = [\n            [1, 2, 3],\n            [4, 5]\n        ]\n        calculate_covariance_matrix(vectors5)\n    except ValueError as e:\n        print(\"  Caught expected error:\", e)\n\nif __name__ == \"__main__\":\n    test_covariance_matrix()\n```\n"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the linear system Ax = b using the Jacobi iterative method.\n\n    Parameters:\n        A (np.ndarray): Coefficient matrix (must be square).\n        b (np.ndarray): Right-hand side vector.\n        n (int): Number of iterations.\n\n    Returns:\n        list: Approximate solution vector x, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    x = np.zeros_like(b)\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    for _ in range(n):\n        x_new = (b - np.dot(R, x)) / D\n        x_new = np.round(x_new, 4)\n        x = x_new\n    return x.reshape(-1).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example system:\n    # 10x + 2y - 1z = 27\n    # -3x - 6y + 2z = -61.5\n    # x + y + 5z = -21.5\n    A = np.array([[10, 2, -1],\n                  [-3, -6, 2],\n                  [1, 1, 5]])\n    b = np.array([27, -61.5, -21.5])\n    n = 25\n    solution = solve_jacobi(A, b, n)\n    print(\"Approximate solution:\", solution)\n```\n**How it works:**\n- The function initializes the solution vector `x` to zeros.\n- For each iteration, it computes the new `x` using the Jacobi update rule.\n- Each intermediate solution is rounded to 4 decimal places.\n- The final result is returned as a Python list.\n\n**You can replace `A`, `b`, and `n` in the example with your own system to test.**"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2x2 matrix A using the Jacobi method.\n    Returns the singular values (sigma1, sigma2) rounded to 4 decimals, with sigma1 >= sigma2.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n\n    # Compute A^T A\n    ATA = A.T @ A\n\n    # ATA is symmetric: [[a, b], [b, d]]\n    a = ATA[0, 0]\n    b = ATA[0, 1]\n    d = ATA[1, 1]\n\n    # Compute eigenvalues of ATA (which are the squared singular values)\n    # Characteristic equation: \u03bb^2 - (a + d)\u03bb + (ad - b^2) = 0\n    trace = a + d\n    det = a * d - b * b\n    temp = np.sqrt((trace / 2) ** 2 - det)\n    eig1 = trace / 2 + temp\n    eig2 = trace / 2 - temp\n\n    # Singular values are sqrt of eigenvalues, ensure non-negative\n    sigma1 = np.sqrt(max(eig1, 0))\n    sigma2 = np.sqrt(max(eig2, 0))\n\n    # Sort in descending order\n    singular_values = sorted([sigma1, sigma2], reverse=True)\n    # Round to 4 decimals\n    singular_values = tuple([round(s, 4) for s in singular_values])\n    return singular_values\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[3, 1], [1, 3]])\n    print(svd_2x2_singular_values(A))  # Output: (4.0, 2.0)\n```\n**Explanation:**\n- The function computes the singular values of a 2x2 matrix by finding the eigenvalues of \\(A^T A\\), which are the squared singular values.\n- It then takes the square roots, sorts them in descending order, and rounds to 4 decimals.\n- No use of `np.linalg.svd` or similar black-box SVD routines."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n\n    Args:\n        matrix (list[list[int|float]]): A 4x4 matrix.\n\n    Returns:\n        float: The determinant of the matrix.\n    \"\"\"\n    def determinant_3x3(m: list[list[int|float]]) -> float:\n        # For a 3x3 matrix:\n        # |a b c|\n        # |d e f|\n        # |g h i|\n        # det = a(ei - fh) - b(di - fg) + c(dh - eg)\n        a, b, c = m[0]\n        d, e, f = m[1]\n        g, h, i = m[2]\n        return (\n            a * (e * i - f * h)\n            - b * (d * i - f * g)\n            + c * (d * h - e * g)\n        )\n\n    def minor(matrix, row, col):\n        # Remove the specified row and column from the matrix\n        return [\n            [matrix[i][j] for j in range(4) if j != col]\n            for i in range(4) if i != row\n        ]\n\n    # Check input is 4x4\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input must be a 4x4 matrix.\")\n\n    det = 0.0\n    for col in range(4):\n        sign = (-1) ** col\n        minor_matrix = minor(matrix, 0, col)\n        det += sign * matrix[0][col] * determinant_3x3(minor_matrix)\n    return det\n\n# Example usage and test\nif __name__ == \"__main__\":\n    mat = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [2, 6, 4, 8],\n        [3, 1, 1, 2]\n    ]\n    print(\"Determinant:\", determinant_4x4(mat))  # Should print a float value\n\n    # Test with identity matrix (determinant should be 1)\n    identity = [\n        [1,0,0,0],\n        [0,1,0,0],\n        [0,0,1,0],\n        [0,0,0,1]\n    ]\n    print(\"Determinant (identity):\", determinant_4x4(identity))  # Should print 1.0\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert input lists to numpy arrays\n    X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float)\n    \n    # Add intercept term (column of ones)\n    ones = np.ones((X.shape[0], 1))\n    X_b = np.hstack([ones, X])\n    \n    # Normal equation: theta = (X^T X)^(-1) X^T y\n    XtX = X_b.T @ X_b\n    try:\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n        # If not invertible, use pseudo-inverse\n        XtX_inv = np.linalg.pinv(XtX)\n    theta = XtX_inv @ X_b.T @ y\n    \n    # Round to 4 decimal places\n    theta_rounded = [round(float(val), 4) for val in theta]\n    return theta_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = [[1, 2], [2, 3], [4, 5]]\n    y = [3, 5, 9]\n    print(linear_regression_normal_equation(X, y))\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m, n = X.shape  # m: number of samples, n: number of features (including intercept)\n    theta = np.zeros((n, 1))  # Initialize coefficients to zeros, shape (n, 1)\n    y = y.reshape(-1, 1)      # Ensure y is column vector\n\n    for _ in range(iterations):\n        predictions = X @ theta                # Shape: (m, 1)\n        errors = predictions - y               # Shape: (m, 1)\n        gradient = (1/m) * (X.T @ errors)      # Shape: (n, 1)\n        theta = theta - alpha * gradient       # Update coefficients\n\n    # Round to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.reshape(-1).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: y = 2 + 3*x\n    X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])  # Add column of ones for intercept\n    y = np.array([5, 8, 11, 14])\n    alpha = 0.01\n    iterations = 1000\n    coeffs = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(coeffs)  # Should be close to [2.0, 3.0]\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization: (x - mean) / std\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    # Avoid division by zero: if std == 0, set to 1 (feature is constant)\n    stds_safe = np.where(stds == 0, 1, stds)\n    standardized = (data - means) / stds_safe\n\n    # Min-Max Normalization: (x - min) / (max - min)\n    mins = np.min(data, axis=0)\n    maxs = np.max(data, axis=0)\n    ranges = maxs - mins\n    # Avoid division by zero: if range == 0, set to 1 (feature is constant)\n    ranges_safe = np.where(ranges == 0, 1, ranges)\n    minmax = (data - mins) / ranges_safe\n\n    # Round to 4 decimals\n    standardized = np.round(standardized, 4)\n    minmax = np.round(minmax, 4)\n\n    # Convert to lists\n    standardized_list = standardized.tolist()\n    minmax_list = minmax.tolist()\n\n    return standardized_list, minmax_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data: 4 samples, 3 features\n    data = np.array([\n        [1.0, 200.0, 3.0],\n        [2.0, 300.0, 6.0],\n        [3.0, 400.0, 9.0],\n        [4.0, 500.0, 12.0]\n    ])\n    std_scaled, minmax_scaled = feature_scaling(data)\n    print(\"Standardized:\")\n    for row in std_scaled:\n        print(row)\n    print(\"\\nMin-Max Normalized:\")\n    for row in minmax_scaled:\n        print(row)\n```\n"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-means clustering.\n\n    Args:\n        points: List of data points, each a tuple (x, y, ...)\n        k: Number of clusters\n        initial_centroids: List of initial centroid positions, each a tuple (x, y, ...)\n        max_iterations: Maximum number of iterations\n\n    Returns:\n        List of final centroids, each rounded to 4 decimal places\n    \"\"\"\n    # Convert points and centroids to numpy arrays for vectorized operations\n    X = np.array(points, dtype=float)\n    centroids = np.array(initial_centroids, dtype=float)\n\n    for iteration in range(max_iterations):\n        # Step 1: Assign points to the nearest centroid\n        # Compute distances from each point to each centroid\n        # Shape: (num_points, k)\n        distances = np.linalg.norm(X[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\n        # Assign each point to the closest centroid\n        labels = np.argmin(distances, axis=1)\n\n        # Step 2: Update centroids\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            # Get all points assigned to cluster i\n            cluster_points = X[labels == i]\n            if len(cluster_points) > 0:\n                # Update centroid as mean of assigned points\n                new_centroids[i] = cluster_points.mean(axis=0)\n            else:\n                # If a cluster gets no points, keep the centroid unchanged\n                new_centroids[i] = centroids[i]\n\n        # Check for convergence (if centroids do not change)\n        if np.allclose(centroids, new_centroids, atol=1e-8):\n            break\n\n        centroids = new_centroids\n\n    # Round centroids to 4 decimal places and convert to list of tuples\n    centroids_rounded = [tuple(np.round(c, 4)) for c in centroids]\n    return centroids_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    points = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\n    k = 2\n    initial_centroids = [(1.0, 2.0), (5.0, 8.0)]\n    max_iterations = 100\n    final_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\n    print(final_centroids)\n```\n"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n\n    Parameters:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n        y (np.ndarray): Target array of shape (n_samples,)\n        k (int): Number of folds (default=5)\n        shuffle (bool): Whether to shuffle the data before splitting (default=True)\n        random_seed (int or None): Random seed for reproducibility (default=None)\n\n    Returns:\n        List[Tuple[np.ndarray, np.ndarray]]: List of (train_indices, test_indices) for each fold\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n\n    if shuffle:\n        rng = np.random.RandomState(random_seed)\n        rng.shuffle(indices)\n\n    # Compute fold sizes\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1  # Distribute the remainder\n\n    current = 0\n    splits = []\n    for fold_size in fold_sizes:\n        start, stop = current, current + fold_size\n        test_indices = indices[start:stop]\n        train_indices = np.concatenate([indices[:start], indices[stop:]])\n        splits.append((train_indices, test_indices))\n        current = stop\n\n    return splits\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create dummy data\n    X = np.arange(20).reshape(10, 2)\n    y = np.arange(10)\n\n    splits = k_fold_cross_validation(X, y, k=3, shuffle=True, random_seed=42)\n    for i, (train_idx, test_idx) in enumerate(splits):\n        print(f\"Fold {i+1}:\")\n        print(\"  Train indices:\", train_idx)\n        print(\"  Test indices: \", test_idx)\n        print()\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the data (zero mean, unit variance)\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0, ddof=1)\n    standardized_data = (data - mean) / std\n\n    # Step 2: Compute the covariance matrix\n    n_samples = standardized_data.shape[0]\n    cov_matrix = np.dot(standardized_data.T, standardized_data) / (n_samples - 1)\n\n    # Step 3: Compute eigenvalues and eigenvectors\n    eigvals, eigvecs = np.linalg.eigh(cov_matrix)  # eigh for symmetric matrices\n\n    # Step 4: Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigvals)[::-1]\n    top_indices = sorted_indices[:k]\n    principal_components = eigvecs[:, top_indices]\n\n    # Step 5: Round results to 4 decimals and convert to list of lists\n    principal_components = np.round(principal_components, 4)\n    return principal_components.T.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data: 5 samples, 3 features\n    data = np.array([\n        [2.5, 2.4, 1.2],\n        [0.5, 0.7, 0.3],\n        [2.2, 2.9, 1.7],\n        [1.9, 2.2, 1.1],\n        [3.1, 3.0, 1.9]\n    ])\n    k = 2\n    pcs = pca(data, k)\n    print(\"Top\", k, \"principal components:\")\n    for i, pc in enumerate(pcs, 1):\n        print(f\"PC{i}:\", pc)\n```\n**How it works:**\n- The function standardizes the input data.\n- Computes the covariance matrix.\n- Finds eigenvalues and eigenvectors.\n- Selects the top `k` eigenvectors (principal components) corresponding to the largest eigenvalues.\n- Rounds the results to 4 decimal places and returns them as a list of lists (each list is a principal component vector).\n\n**Example output:**\n```\nTop 2 principal components:\nPC1: [0.5774, 0.5774, 0.5774]\nPC2: [0.8165, -0.4082, -0.4082]\n```\n*(Note: Output will vary depending on the input data.)*"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom copy import deepcopy\n\ndef entropy(examples, target_attr):\n    \"\"\"Calculate the entropy of the set for the target attribute.\"\"\"\n    total = len(examples)\n    if total == 0:\n        return 0\n    value_counts = Counter(example[target_attr] for example in examples)\n    ent = 0.0\n    for count in value_counts.values():\n        p = count / total\n        if p > 0:\n            ent -= p * math.log2(p)\n    return ent\n\ndef information_gain(examples, attr, target_attr):\n    \"\"\"Calculate the information gain of splitting on attr.\"\"\"\n    total = len(examples)\n    if total == 0:\n        return 0\n    attr_values = set(example[attr] for example in examples)\n    weighted_entropy = 0.0\n    for value in attr_values:\n        subset = [ex for ex in examples if ex[attr] == value]\n        weighted_entropy += (len(subset) / total) * entropy(subset, target_attr)\n    return entropy(examples, target_attr) - weighted_entropy\n\ndef majority_value(examples, target_attr):\n    \"\"\"Return the most common target attribute value among examples.\"\"\"\n    if not examples:\n        return None\n    value_counts = Counter(example[target_attr] for example in examples)\n    return value_counts.most_common(1)[0][0]\n\ndef all_same_class(examples, target_attr):\n    \"\"\"Check if all examples have the same class.\"\"\"\n    if not examples:\n        return True\n    first_value = examples[0][target_attr]\n    return all(example[target_attr] == first_value for example in examples)\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Recursively learn a decision tree using entropy and information gain.\n    Returns a nested dictionary representing the tree.\n    \"\"\"\n    # Base cases\n    if not examples:\n        return None  # No examples to split\n    if all_same_class(examples, target_attr):\n        return {'label': examples[0][target_attr]}\n    if not attributes:\n        return {'label': majority_value(examples, target_attr)}\n\n    # Choose the best attribute to split on\n    gains = [(attr, information_gain(examples, attr, target_attr)) for attr in attributes]\n    best_attr, best_gain = max(gains, key=lambda x: x[1])\n\n    if best_gain == 0:\n        # No information gain, return majority class\n        return {'label': majority_value(examples, target_attr)}\n\n    tree = {'attribute': best_attr, 'nodes': {}}\n    attr_values = set(example[best_attr] for example in examples)\n    for value in attr_values:\n        subset = [ex for ex in examples if ex[best_attr] == value]\n        if not subset:\n            subtree = {'label': majority_value(examples, target_attr)}\n        else:\n            # Remove the used attribute for the subtree\n            remaining_attrs = [a for a in attributes if a != best_attr]\n            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        tree['nodes'][value] = subtree\n    return tree\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple dataset: Play Tennis\n    examples = [\n        {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n        {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n        {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'No'},\n        {'Outlook': 'Overcast', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n        {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Overcast', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    ]\n    attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n    target_attr = 'PlayTennis'\n\n    tree = learn_decision_tree(examples, attributes, target_attr)\n    import pprint\n    pprint.pprint(tree)\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos Kernel SVM implementation.\n    Args:\n        data: (n_samples, n_features) numpy array.\n        labels: (n_samples,) numpy array, values in {+1, -1}.\n        kernel: 'linear' or 'rbf'.\n        lambda_val: regularization parameter.\n        iterations: number of passes over the data.\n        sigma: RBF kernel width (only used if kernel='rbf').\n    Returns:\n        (alpha_list, bias): alpha coefficients and bias, both as python lists rounded to 4 decimals.\n    \"\"\"\n    n_samples = data.shape[0]\n    y = labels.astype(float).reshape(-1)\n    # Precompute kernel matrix\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'rbf':\n        X_norm = np.sum(data ** 2, axis=1)\n        K = np.exp(-(X_norm[:, None] + X_norm[None, :] - 2 * data @ data.T) / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel. Use 'linear' or 'rbf'.\")\n\n    alpha = np.zeros(n_samples)\n    b = 0.0\n\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        # Compute decision function for all samples\n        f = (alpha * y) @ K + b  # shape: (n_samples,)\n        # Find indices where constraint is violated\n        idx = np.where(y * f < 1)[0]\n        # Gradient step for alpha\n        # Pegasos update: alpha_i = (1 - eta * lambda) * alpha_i + (eta / n) * sum_{j in idx} y_j * K_{i,j}\n        # But in dual, the update is:\n        # For all i: alpha_i = (1 - eta * lambda) * alpha_i\n        alpha = (1 - eta * lambda_val) * alpha\n        if len(idx) > 0:\n            # For each j in idx, update alpha_j\n            alpha[idx] += eta / len(idx)\n        # Project alpha to be >= 0 (since SVM dual variables are >= 0)\n        alpha = np.maximum(alpha, 0)\n        # Update bias as mean over margin errors\n        if len(idx) > 0:\n            b = np.mean(y[idx] - ((alpha * y) @ K[:, idx]))\n        else:\n            b = 0.0\n\n    # Round results to 4 decimals and return as lists\n    alpha_rounded = np.round(alpha, 4).reshape(-1).tolist()\n    b_rounded = float(np.round(b, 4))\n    return alpha_rounded, b_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple XOR dataset (not linearly separable)\n    X = np.array([[0,0],[0,1],[1,0],[1,1]])\n    y = np.array([-1,1,1,-1])\n    alpha, bias = pegasos_kernel_svm(X, y, kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5)\n    print(\"Alpha:\", alpha)\n    print(\"Bias:\", bias)\n```\n**Notes:**\n- The function supports both linear and RBF kernels.\n- The alpha coefficients and bias are rounded to 4 decimals and returned as Python lists.\n- The update is deterministic: all samples are used in every iteration.\n- The bias is updated as the mean error over margin-violating samples.\n- The example at the end demonstrates usage on a simple XOR dataset."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for the input z.\n\n    Args:\n        z (float): The input value.\n\n    Returns:\n        float: The sigmoid output rounded to four decimal places.\n    \"\"\"\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    return round(sigmoid_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2, 0, 2]\n    for val in test_values:\n        print(f\"sigmoid({val}) = {sigmoid(val)}\")\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    # Numerical stability: subtract max score before exponentiating\n    max_score = max(scores)\n    exp_scores = [math.exp(s - max_score) for s in scores]\n    sum_exp = sum(exp_scores)\n    softmax_vals = [round(e / sum_exp, 4) for e in exp_scores]\n    return softmax_vals\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_scores = [2.0, 1.0, 0.1]\n    print(softmax(test_scores))  # Output: [0.659, 0.2424, 0.0986]\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation for binary classification.\n\n    Args:\n        features: List of feature vectors (list of lists of floats).\n        labels: List of true binary labels (0 or 1).\n        weights: List of weights (one per feature).\n        bias: Bias term (float).\n\n    Returns:\n        Tuple of:\n            - List of predicted probabilities (rounded to 4 decimals).\n            - Mean squared error (rounded to 4 decimals).\n    \"\"\"\n    # Convert to numpy arrays for vectorized operations\n    X = np.array(features)  # shape: (n_samples, n_features)\n    w = np.array(weights)   # shape: (n_features,)\n    b = bias\n\n    # Linear combination: z = Xw + b\n    z = np.dot(X, w) + b  # shape: (n_samples,)\n\n    # Sigmoid activation\n    sigmoid = lambda x: 1 / (1 + math.exp(-x))\n    probs = np.array([sigmoid(val) for val in z])\n\n    # Round probabilities to 4 decimals\n    probs_rounded = np.round(probs, 4)\n\n    # Compute mean squared error\n    labels_arr = np.array(labels)\n    mse = np.mean((probs_rounded - labels_arr) ** 2)\n    mse_rounded = round(mse, 4)\n\n    # Return as per constraints\n    return probs_rounded.reshape(-1).tolist(), mse_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    features = [[0.5, 1.2], [1.0, -0.7], [0.3, 0.8]]\n    labels = [1, 0, 1]\n    weights = [0.8, -0.5]\n    bias = 0.1\n    probs, mse = single_neuron_model(features, labels, weights, bias)\n    print(\"Predicted probabilities:\", probs)\n    print(\"Mean squared error:\", mse)\n```\n"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Trains a single neuron with sigmoid activation using MSE loss and gradient descent.\n\n    Args:\n        features (np.ndarray): Shape (n_samples, n_features)\n        labels (np.ndarray): Shape (n_samples,)\n        initial_weights (np.ndarray): Shape (n_features,)\n        initial_bias (float): Initial bias\n        learning_rate (float): Learning rate for gradient descent\n        epochs (int): Number of epochs\n\n    Returns:\n        (np.ndarray, float, list[float]): Updated weights, bias, and list of MSEs per epoch (rounded to 4 decimals)\n    \"\"\"\n    # Ensure correct shapes\n    features = np.array(features)\n    labels = np.array(labels)\n    weights = np.array(initial_weights, dtype=np.float64)\n    bias = float(initial_bias)\n    n_samples = features.shape[0]\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    mse_list = []\n\n    for epoch in range(epochs):\n        # Forward pass\n        z = np.dot(features, weights) + bias  # shape: (n_samples,)\n        y_pred = sigmoid(z)                   # shape: (n_samples,)\n\n        # Compute MSE loss\n        mse = np.mean((y_pred - labels) ** 2)\n        mse_list.append(round(mse, 4))\n\n        # Backpropagation\n        # dL/dy_pred = 2 * (y_pred - labels) / n_samples\n        dL_dy_pred = 2 * (y_pred - labels) / n_samples  # shape: (n_samples,)\n\n        # dy_pred/dz = sigmoid(z) * (1 - sigmoid(z))\n        dy_pred_dz = y_pred * (1 - y_pred)              # shape: (n_samples,)\n\n        # dL/dz = dL/dy_pred * dy_pred/dz\n        dL_dz = dL_dy_pred * dy_pred_dz                 # shape: (n_samples,)\n\n        # dL/dw = sum over samples of (dL/dz * x)\n        dL_dw = np.dot(features.T, dL_dz)               # shape: (n_features,)\n\n        # dL/db = sum over samples of dL/dz\n        dL_db = np.sum(dL_dz)                           # scalar\n\n        # Update weights and bias\n        weights -= learning_rate * dL_dw\n        bias -= learning_rate * dL_db\n\n        # Round weights and bias to 4 decimals after each epoch\n        weights = np.round(weights, 4)\n        bias = round(bias, 4)\n\n    # Final rounding and conversion\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    return weights.tolist(), bias, mse_list\n```\n"}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        # Topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test: (a + b) * c, with ReLU\n    a = Value(2.0)\n    b = Value(-3.0)\n    c = Value(10.0)\n    d = a + b\n    e = d * c\n    f = e.relu()\n    f.backward()\n    print(f\"a: {a}\")\n    print(f\"b: {b}\")\n    print(f\"c: {c}\")\n    print(f\"d: {d}\")\n    print(f\"e: {e}\")\n    print(f\"f: {f}\")\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C in R^3.\n    That is, for a vector v, [v]_C = P [v]_B.\n\n    Args:\n        B: 3x3 list of basis vectors for B (each column is a basis vector).\n        C: 3x3 list of basis vectors for C (each column is a basis vector).\n\n    Returns:\n        3x3 list of floats: the transformation matrix P, rounded to 4 decimals.\n    \"\"\"\n    B_mat = np.array(B, dtype=float)\n    C_mat = np.array(C, dtype=float)\n    # Each basis is given as a list of vectors; we treat them as columns\n    # So, to get the change of basis matrix from B to C:\n    # P = C^{-1} * B\n    try:\n        C_inv = np.linalg.inv(C_mat)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"Basis C is not invertible (vectors are linearly dependent).\")\n    P = np.dot(C_inv, B_mat)\n    P = np.round(P, 4)\n    return P.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Standard basis\n    B = [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n    # Another basis\n    C = [\n        [2, 1, 0],\n        [1, 1, 1],\n        [0, 1, 2]\n    ]\n    P = transform_basis(B, C)\n    print(P)\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix A using eigendecomposition of A^T A.\n    Returns (U, S, V) where:\n        - U: 2x2 orthogonal matrix\n        - S: 2x2 diagonal matrix of singular values\n        - V: 2x2 orthogonal matrix\n    All results are rounded to 4 decimal places and returned as lists.\n    \"\"\"\n    # Step 1: Compute A^T A\n    AtA = A.T @ A\n\n    # Step 2: Eigendecomposition of A^T A\n    eigvals, eigvecs = np.linalg.eigh(AtA)  # eigh for symmetric matrices\n\n    # Step 3: Sort eigenvalues and eigenvectors in descending order\n    idx = np.argsort(eigvals)[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n\n    # Step 4: Singular values are sqrt of eigenvalues (ensure non-negative)\n    singular_vals = np.sqrt(np.clip(eigvals, 0, None))\n\n    # Step 5: Form S matrix (2x2 diagonal)\n    S = np.zeros((2,2))\n    S[0,0] = singular_vals[0]\n    S[1,1] = singular_vals[1]\n\n    # Step 6: V is the matrix of eigenvectors (already orthonormal)\n    V = eigvecs\n\n    # Step 7: Compute U columns: u_i = (A v_i) / sigma_i\n    U = np.zeros((2,2))\n    for i in range(2):\n        sigma = singular_vals[i]\n        v = V[:,i]\n        Av = A @ v\n        if sigma > 1e-10:\n            u = Av / sigma\n        else:\n            # If sigma is zero, pick a vector orthogonal to the previous u\n            # For 2x2, just pick a perpendicular vector\n            if i == 1:\n                u = np.array([-U[1,0], U[0,0]])\n                # Normalize\n                norm = np.linalg.norm(u)\n                if norm > 1e-10:\n                    u = u / norm\n                else:\n                    u = np.zeros_like(u)\n            else:\n                u = np.zeros_like(Av)\n        U[:,i] = u\n\n    # Step 8: Ensure U and V are proper rotations (determinant +1)\n    # (Optional, but for SVD uniqueness)\n    if np.linalg.det(U) < 0:\n        U[:,1] *= -1\n    if np.linalg.det(V) < 0:\n        V[:,1] *= -1\n\n    # Step 9: Round results to 4 decimals\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n\n    # Step 10: Return as lists\n    return U.tolist(), S.tolist(), V.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[3, 1], [1, 3]])\n    U, S, V = svd_2x2(A)\n    print(\"U =\", U)\n    print(\"S =\", S)\n    print(\"V =\", V)\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle X and y in unison, maintaining correspondence between samples.\n\n    Parameters:\n        X (np.ndarray): Feature array of shape (n_samples, ...).\n        y (np.ndarray): Label array of shape (n_samples, ...).\n        seed (int, optional): Random seed for reproducibility.\n\n    Returns:\n        tuple: (shuffled_X_list, shuffled_y_list)\n            shuffled_X_list: list, X shuffled and converted to list\n            shuffled_y_list: list, y shuffled and converted to list\n    \"\"\"\n    if seed is not None:\n        rng = np.random.RandomState(seed)\n    else:\n        rng = np.random\n\n    assert len(X) == len(y), \"X and y must have the same number of samples\"\n    indices = np.arange(len(X))\n    rng.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([10, 20, 30, 40])\n    X_shuf, y_shuf = shuffle_data(X, y, seed=42)\n    print(\"Shuffled X:\", X_shuf)\n    print(\"Shuffled y:\", y_shuf)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Iterates over X (and optionally y) in batches of batch_size.\n    Yields batches as python lists (using numpy's tolist()).\n\n    Parameters:\n        X (np.ndarray): Input data.\n        y (np.ndarray, optional): Target data.\n        batch_size (int): Size of each batch.\n\n    Yields:\n        If y is None:\n            batch_X (list): Batch of X as a python list.\n        Else:\n            (batch_X, batch_y): Tuple of batches as python lists.\n    \"\"\"\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        batch_X = X[start:end].tolist()\n        if y is not None:\n            batch_y = y[start:end].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    X = np.arange(100).reshape(50, 2)\n    y = np.arange(50)\n\n    print(\"Batches with X and y:\")\n    for batch_X, batch_y in batch_iterator(X, y, batch_size=20):\n        print(\"X batch:\", batch_X)\n        print(\"y batch:\", batch_y)\n        print(\"---\")\n\n    print(\"Batches with X only:\")\n    for batch_X in batch_iterator(X, batch_size=20):\n        print(\"X batch:\", batch_X)\n        print(\"---\")\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset X into two subsets based on whether the value of feature_i is >= threshold.\n\n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - feature_i: int, index of the feature to split on\n    - threshold: value to compare against\n\n    Returns:\n    - X_1: list, samples where X[:, feature_i] >= threshold\n    - X_2: list, samples where X[:, feature_i] < threshold\n    \"\"\"\n    X = np.asarray(X)\n    mask = X[:, feature_i] >= threshold\n    X_1 = X[mask]\n    X_2 = X[~mask]\n    return X_1.tolist(), X_2.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample dataset: 5 samples, 3 features\n    X = np.array([\n        [2.5, 1.0, 3.3],\n        [1.2, 3.4, 2.2],\n        [3.1, 0.5, 1.8],\n        [0.7, 2.2, 4.1],\n        [2.9, 1.8, 2.7]\n    ])\n    feature_i = 0\n    threshold = 2.5\n\n    X_1, X_2 = divide_on_feature(X, feature_i, threshold)\n    print(\"Samples where feature[0] >= 2.5:\")\n    print(X_1)\n    print(\"Samples where feature[0] < 2.5:\")\n    print(X_2)\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for input array X up to the specified degree.\n\n    Parameters:\n    X (np.ndarray): Input 2D array of shape (n_samples, n_features)\n    degree (int): The degree of the polynomial features\n\n    Returns:\n    list: 2D list of shape (n_samples, n_output_features) with polynomial features\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Generate all combinations of feature indices with replacement\n    combs = []\n    for d in range(degree + 1):\n        combs.extend(combinations_with_replacement(range(n_features), d))\n    # For each sample, compute the product for each combination\n    features = []\n    for row in X:\n        row_features = []\n        for comb in combs:\n            if len(comb) == 0:\n                row_features.append(1.0)\n            else:\n                prod = 1.0\n                for idx in comb:\n                    prod *= row[idx]\n                row_features.append(prod)\n        features.append(row_features)\n    return np.array(features).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[2, 3], [4, 5]])\n    degree = 2\n    poly_feats = polynomial_features(X, degree)\n    print(poly_feats)\n    # Output:\n    # [\n    #   [1.0, 2.0, 3.0, 4.0, 6.0, 9.0],\n    #   [1.0, 4.0, 5.0, 16.0, 20.0, 25.0]\n    # ]\n```"}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n\n    Parameters:\n        X (np.ndarray): 2D array of features (shape: [n_samples, n_features])\n        y (np.ndarray): 1D array of labels (shape: [n_samples])\n        n_subsets (int): Number of random subsets to generate\n        replacements (bool): Whether to sample with replacement\n        seed (int): Random seed for reproducibility\n\n    Returns:\n        list: List of n_subsets tuples, each (X_subset, y_subset), where\n              X_subset and y_subset are lists (converted from numpy arrays)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples = X.shape[0]\n    subsets = []\n\n    for _ in range(n_subsets):\n        if replacements:\n            indices = rng.integers(0, n_samples, size=n_samples)\n        else:\n            indices = rng.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        # Convert to lists as per output constraints\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    n_subsets = 3\n\n    # With replacement\n    subsets_with_replacement = get_random_subsets(X, y, n_subsets, replacements=True, seed=123)\n    print(\"With replacement:\")\n    for i, (X_sub, y_sub) in enumerate(subsets_with_replacement):\n        print(f\"Subset {i+1}:\")\n        print(\"X:\", X_sub)\n        print(\"y:\", y_sub)\n\n    # Without replacement\n    subsets_without_replacement = get_random_subsets(X, y, n_subsets, replacements=False, seed=123)\n    print(\"\\nWithout replacement:\")\n    for i, (X_sub, y_sub) in enumerate(subsets_without_replacement):\n        print(f\"Subset {i+1}:\")\n        print(\"X:\", X_sub)\n        print(\"y:\", y_sub)\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encode a 1D numpy array of integer values.\n\n    Parameters:\n    x (np.ndarray): 1D array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array.\n\n    Returns:\n    list: One-hot encoded 2D list.\n    \"\"\"\n    x = np.asarray(x).flatten()\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.size, n_col), dtype=int)\n    one_hot[np.arange(x.size), x] = 1\n    return one_hot.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    arr = np.array([0, 2, 1, 3])\n    print(to_categorical(arr))           # Output: [[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]\n    print(to_categorical(arr, n_col=5))  # Output: [[1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0]]\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix and returns it as a Python list.\n\n    Parameters:\n    x (np.ndarray): 1D numpy array\n\n    Returns:\n    list: 2D list representing the diagonal matrix\n    \"\"\"\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    arr = np.array([1, 2, 3])\n    result = make_diagonal(arr)\n    print(result)  # Output: [[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n\n    Parameters:\n    y_true (np.ndarray): 1D array of true labels\n    y_pred (np.ndarray): 1D array of predicted labels\n\n    Returns:\n    float: Accuracy score rounded to 4 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    if y_true.ndim != 1:\n        raise ValueError(\"y_true and y_pred must be 1D arrays.\")\n    total = y_true.size\n    if total == 0:\n        raise ValueError(\"y_true and y_pred must not be empty.\")\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / total\n    return round(accuracy, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0])\n    y_pred = np.array([1, 0, 0, 1, 0])\n    print(accuracy_score(y_true, y_pred))  # Output: 0.8\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between columns of X and columns of Y.\n    If Y is None, compute the correlation matrix of X with itself.\n\n    Parameters:\n        X (np.ndarray): 2D array of shape (n_samples, n_features_X)\n        Y (np.ndarray, optional): 2D array of shape (n_samples, n_features_Y)\n\n    Returns:\n        list: 2D list of correlation coefficients rounded to 4 decimals\n    \"\"\"\n    X = np.asarray(X)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y)\n        if X.shape[0] != Y.shape[0]:\n            raise ValueError(\"X and Y must have the same number of rows (samples).\")\n\n    # Center the columns\n    X_mean = X - np.mean(X, axis=0, keepdims=True)\n    Y_mean = Y - np.mean(Y, axis=0, keepdims=True)\n\n    # Compute standard deviations\n    X_std = np.std(X, axis=0, ddof=0)\n    Y_std = np.std(Y, axis=0, ddof=0)\n\n    # Avoid division by zero\n    X_std[X_std == 0] = np.nan\n    Y_std[Y_std == 0] = np.nan\n\n    # Compute covariance matrix\n    cov = (X_mean.T @ Y_mean) / X.shape[0]\n\n    # Outer product of stds for normalization\n    denom = np.outer(X_std, Y_std)\n\n    corr = cov / denom\n\n    # Replace nan with 0 (if any column is constant)\n    corr = np.nan_to_num(corr)\n\n    # Round to 4 decimals\n    corr = np.round(corr, 4)\n\n    # Convert to list\n    return corr.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]])\n    Y = np.array([[9, 8],\n                  [6, 5],\n                  [3, 2]])\n    print(\"Correlation matrix of X with itself:\")\n    print(calculate_correlation_matrix(X))\n    print(\"Correlation matrix of X with Y:\")\n    print(calculate_correlation_matrix(X, Y))\n```\n"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Fit AdaBoost with decision stumps (one-level trees).\n    Args:\n        X: numpy array, shape (n_samples, n_features)\n        y: numpy array, shape (n_samples,), labels must be in {-1, 1}\n        n_clf: int, number of classifiers\n    Returns:\n        List of classifiers, each is a dict:\n            {\n                'feature': int,\n                'threshold': float,\n                'polarity': int,\n                'alpha': float\n            }\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Ensure y is in {-1, 1}\n    y = np.where(y == 0, -1, y)\n    # Initialize weights\n    w = np.ones(n_samples) / n_samples\n    classifiers = []\n\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_clf = {}\n\n        # Loop over all features\n        for feature_i in range(n_features):\n            X_column = X[:, feature_i]\n            # Get all unique thresholds (midpoints between sorted values)\n            thresholds = np.unique(X_column)\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    # Predict: polarity * (x > threshold) -> 1 else -1\n                    preds = np.ones(n_samples)\n                    if polarity == 1:\n                        preds[X_column <= threshold] = -1\n                    else:\n                        preds[X_column > threshold] = -1\n\n                    # Compute weighted error\n                    misclassified = preds != y\n                    error = np.sum(w * misclassified)\n\n                    # If error > 0.5, flip polarity\n                    if error > 0.5:\n                        error = 1 - error\n                        polarity = -polarity\n\n                    if error < min_error:\n                        min_error = error\n                        best_clf = {\n                            'feature': feature_i,\n                            'threshold': float(threshold),\n                            'polarity': polarity,\n                            'preds': preds.copy()\n                        }\n\n        # Avoid division by zero\n        min_error = max(min_error, 1e-10)\n        # Compute alpha\n        alpha = 0.5 * math.log((1 - min_error) / min_error)\n        alpha = round(alpha, 4)\n\n        # Store classifier\n        classifiers.append({\n            'feature': best_clf['feature'],\n            'threshold': round(best_clf['threshold'], 4),\n            'polarity': best_clf['polarity'],\n            'alpha': alpha\n        })\n\n        # Update weights\n        # w = w * exp(-alpha * y * h(x))\n        w = w * np.exp(-alpha * y * best_clf['polarity'] * (\n            2 * (X[:, best_clf['feature']] > best_clf['threshold']) - 1\n        ))\n        # For polarity == -1, flip the sign of the comparison\n        if best_clf['polarity'] == -1:\n            w = w * np.exp(-2 * alpha * y * -1 * (\n                2 * (X[:, best_clf['feature']] > best_clf['threshold']) - 1\n            ))\n\n        # Normalize\n        w = w / np.sum(w)\n\n    return classifiers\n```\n**Usage Example:**\n```python\n# Example usage\nX = np.array([[1,2],[2,1],[3,4],[4,3]])\ny = np.array([1,1,-1,-1])\nclfs = adaboost_fit(X, y, 2)\nprint(clfs)\n```\n**Note:**  \n- The function expects `y` to be in `{0,1}` or `{-1,1}`; it will convert `0` to `-1`.\n- All returned values are rounded to 4 decimal places as required.\n- Each classifier is a dict with keys: `'feature'`, `'threshold'`, `'polarity'`, `'alpha'`."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    # For numerical stability, subtract the max from scores\n    max_score = np.max(scores)\n    shifted_scores = scores - max_score\n    exp_shifted = np.exp(shifted_scores)\n    sum_exp = np.sum(exp_shifted)\n    log_sum_exp = np.log(sum_exp)\n    log_softmax_vals = shifted_scores - log_sum_exp\n    # Round to 4 decimals\n    log_softmax_vals = np.round(log_softmax_vals, 4)\n    return log_softmax_vals.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    scores = [2.0, 1.0, 0.1]\n    print(log_softmax(scores))  # Output: [-0.417, -1.417, -2.317]\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n# Simple SGD optimizer for demonstration\nclass SGD:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n\n    def update(self, param, grad):\n        return param - self.lr * grad\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self, optimizer=SGD()):\n        # If input_shape is not set, raise error\n        if self.input_shape is None:\n            raise ValueError(\"Input shape must be defined before initialization.\")\n        input_dim = self.input_shape[0]\n        limit = 1.0 / math.sqrt(input_dim)\n        # Uniform initialization\n        self.W = np.random.uniform(-limit, limit, (input_dim, self.n_units))\n        self.W = np.round(self.W, 4)\n        self.w0 = np.zeros((1, self.n_units))\n        self.w0 = np.round(self.w0, 4)\n        # Optimizers (deepcopy to avoid shared state)\n        self.W_optimizer = copy.deepcopy(optimizer)\n        self.w0_optimizer = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        # W: input_dim * n_units, w0: n_units\n        if self.W is None or self.w0 is None:\n            return 0\n        return int(np.prod(self.W.shape) + np.prod(self.w0.shape))\n\n    def forward_pass(self, X, training=True):\n        # Save input for backward\n        self.layer_input = X\n        # X: (batch_size, input_dim) or (input_dim,)\n        # W: (input_dim, n_units)\n        # w0: (1, n_units)\n        # Output: (batch_size, n_units) or (n_units,)\n        out = np.dot(X, self.W) + self.w0\n        out = np.round(out, 4)\n        return out.tolist() if out.ndim == 1 else np.round(out, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        # accum_grad: (batch_size, n_units) or (n_units,)\n        X = self.layer_input\n        if X is None:\n            raise ValueError(\"No input cached for backward pass.\")\n        # Ensure 2D\n        X = np.atleast_2d(X)\n        accum_grad = np.atleast_2d(accum_grad)\n        batch_size = X.shape[0]\n\n        # Gradients\n        dW = np.dot(X.T, accum_grad) / batch_size  # (input_dim, n_units)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True) / batch_size  # (1, n_units)\n\n        # Gradient w.r.t input\n        grad_input = np.dot(accum_grad, self.W.T)  # (batch_size, input_dim)\n        grad_input = np.round(grad_input, 4)\n\n        # Update weights if trainable\n        if self.trainable:\n            self.W = self.W_optimizer.update(self.W, dW)\n            self.W = np.round(self.W, 4)\n            self.w0 = self.w0_optimizer.update(self.w0, dw0)\n            self.w0 = np.round(self.w0, 4)\n\n        # Return as list, shape: (batch_size, input_dim) or (input_dim,)\n        if grad_input.shape[0] == 1:\n            return grad_input.flatten().tolist()\n        else:\n            return grad_input.tolist()\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example usage and test\nif __name__ == \"__main__\":\n    # Set random seed for reproducibility\n    np.random.seed(42)\n    # Create Dense layer with 3 units, input shape (4,)\n    dense = Dense(3, input_shape=(4,))\n    dense.initialize(optimizer=SGD(lr=0.1))\n    print(\"Parameters:\", dense.parameters())\n    print(\"Output shape:\", dense.output_shape())\n\n    # Forward pass with a single input\n    x = np.array([1.0, 2.0, 3.0, 4.0])\n    out = dense.forward_pass(x)\n    print(\"Forward pass output (single):\", out)\n\n    # Forward pass with batch input\n    X_batch = np.array([[1.0, 2.0, 3.0, 4.0],\n                        [0.5, 1.5, 2.5, 3.5]])\n    out_batch = dense.forward_pass(X_batch)\n    print(\"Forward pass output (batch):\", out_batch)\n\n    # Backward pass (single)\n    grad = np.array([0.1, -0.2, 0.3])\n    grad_input = dense.backward_pass(grad)\n    print(\"Backward pass grad_input (single):\", grad_input)\n\n    # Backward pass (batch)\n    grad_batch = np.array([[0.1, -0.2, 0.3],\n                           [0.05, -0.1, 0.15]])\n    grad_input_batch = dense.backward_pass(grad_batch)\n    print(\"Backward pass grad_input (batch):\", grad_input_batch)\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Performs a simple 2D convolution (single channel, no bias, no dilation).\n    Args:\n        input_matrix (np.ndarray): 2D input array.\n        kernel (np.ndarray): 2D kernel array.\n        padding (int): Number of zero-padding layers to add to input (same on all sides).\n        stride (int): Stride of the convolution.\n    Returns:\n        list: 2D list of the convolved output, rounded to 4 decimals.\n    \"\"\"\n    # Pad the input\n    if padding > 0:\n        padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    else:\n        padded_input = input_matrix.copy()\n    \n    in_h, in_w = padded_input.shape\n    k_h, k_w = kernel.shape\n\n    # Calculate output dimensions\n    out_h = ((in_h - k_h) // stride) + 1\n    out_w = ((in_w - k_w) // stride) + 1\n\n    output = np.zeros((out_h, out_w), dtype=np.float64)\n\n    # Flip the kernel for convolution\n    kernel_flipped = np.flipud(np.fliplr(kernel))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            # Extract the current region\n            region = padded_input[i*stride:i*stride+k_h, j*stride:j*stride+k_w]\n            # Element-wise multiplication and sum\n            conv_sum = np.sum(region * kernel_flipped)\n            # Round to 4 decimals\n            output[i, j] = np.round(conv_sum, 4)\n\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    inp = np.array([[1, 2, 3],\n                    [4, 5, 6],\n                    [7, 8, 9]], dtype=np.float64)\n    ker = np.array([[1, 0],\n                    [0, -1]], dtype=np.float64)\n    result = simple_conv2d(inp, ker, padding=1, stride=1)\n    print(result)\n```"}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function.\n\n    Parameters:\n        z (float): Input value.\n\n    Returns:\n        float: Output after applying ReLU (max(0, z)).\n    \"\"\"\n    return max(0.0, z)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    test_values = [-3.5, 0.0, 2.7, -0.1, 5.0]\n    for val in test_values:\n        print(f\"relu({val}) = {relu(val)}\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss (MSE + L2 regularization).\n\n    Parameters:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n        w (np.ndarray): Coefficient vector of shape (n_features,)\n        y_true (np.ndarray): True labels of shape (n_samples,)\n        alpha (float): Regularization parameter\n\n    Returns:\n        float: Ridge loss rounded to 4 decimal places\n    \"\"\"\n    # Compute predictions\n    y_pred = X @ w\n    # Compute Mean Squared Error\n    mse = np.mean((y_true - y_pred) ** 2)\n    # Compute L2 regularization term\n    l2_reg = alpha * np.sum(w ** 2)\n    # Ridge loss\n    loss = mse + l2_reg\n    # Round to 4 decimal places\n    return np.round(loss, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, -0.2])\n    y_true = np.array([1.0, 2.0, 3.0])\n    alpha = 0.1\n    print(ridge_loss(X, w, y_true, alpha))  # Example output\n```\n"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input z.\n\n    Parameters:\n        z (float): The input value.\n        alpha (float, optional): Slope for negative inputs. Default is 0.01.\n\n    Returns:\n        float or int: The result after applying Leaky ReLU.\n    \"\"\"\n    return z if z >= 0 else alpha * z\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    test_values = [-5.0, -1.0, 0.0, 2.0, 10.0]\n    alphas = [0.01, 0.1]\n\n    for alpha in alphas:\n        print(f\"Testing with alpha = {alpha}:\")\n        for z in test_values:\n            result = leaky_relu(z, alpha)\n            print(f\"leaky_relu({z}, alpha={alpha}) = {result}\")\n        print()\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two input vectors.\n\n    Parameters:\n        x1 (array-like): First input vector.\n        x2 (array-like): Second input vector.\n\n    Returns:\n        float: The linear kernel (dot product) of x1 and x2.\n    \"\"\"\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n    return np.dot(x1, x2)\n\n# Example usage:\nif __name__ == \"__main__\":\n    a = [1, 2, 3]\n    b = [4, 5, 6]\n    print(\"Linear kernel between a and b:\", kernel_function(a, b))  # Output: 32\n```\n"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n\n    Parameters:\n    y_true (np.ndarray): Array of true binary labels (0 or 1).\n    y_pred (np.ndarray): Array of predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision score.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # True Positives: predicted 1 and actually 1\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    # False Positives: predicted 1 and actually 0\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    \n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0  # Define precision as 0 when no positive predictions\n    return tp / denominator\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0])\n    y_pred = np.array([1, 0, 1, 0, 0, 1, 1])\n    print(\"Precision:\", precision(y_true, y_pred))\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent optimization (batch, mini-batch, or stochastic) for linear regression using MSE loss.\n\n    Parameters:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n        y (np.ndarray): Target vector of shape (n_samples,) or (n_samples, 1)\n        weights (np.ndarray): Initial weights of shape (n_features,) or (n_features, 1)\n        learning_rate (float): Learning rate for gradient descent\n        n_iterations (int): Number of iterations\n        batch_size (int): Size of mini-batch (used for mini-batch and SGD)\n        method (str): 'batch', 'mini-batch', or 'sgd'\n\n    Returns:\n        list: Final weights as a python list, rounded to 4 decimals\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y).reshape(-1, 1)\n    weights = np.array(weights).reshape(-1, 1)\n    n_samples = X.shape[0]\n\n    for it in range(n_iterations):\n        if method == 'batch':\n            # Use all samples\n            X_batch = X\n            y_batch = y\n            y_pred = X_batch @ weights\n            error = y_pred - y_batch\n            grad = (2 / n_samples) * (X_batch.T @ error)\n            weights -= learning_rate * grad\n\n        elif method == 'sgd':\n            # Shuffle data\n            indices = np.random.permutation(n_samples)\n            for idx in indices:\n                X_i = X[idx].reshape(1, -1)\n                y_i = y[idx].reshape(1, 1)\n                y_pred = X_i @ weights\n                error = y_pred - y_i\n                grad = 2 * (X_i.T @ error)\n                weights -= learning_rate * grad\n\n        elif method == 'mini-batch':\n            # Shuffle data\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                X_batch = X_shuffled[start:end]\n                y_batch = y_shuffled[start:end]\n                m = X_batch.shape[0]\n                y_pred = X_batch @ weights\n                error = y_pred - y_batch\n                grad = (2 / m) * (X_batch.T @ error)\n                weights -= learning_rate * grad\n        else:\n            raise ValueError(\"method must be 'batch', 'mini-batch', or 'sgd'\")\n\n    # Round to 4 decimals and return as python list\n    weights = np.round(weights, 4)\n    return weights.reshape(-1).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 15, 23])\n    weights = np.array([0.0, 0.0])\n    learning_rate = 0.01\n    n_iterations = 1000\n\n    # Batch Gradient Descent\n    w_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n    print(\"Batch GD Weights:\", w_batch)\n\n    # Stochastic Gradient Descent\n    w_sgd = gradient_descent(X, y, weights, learning_rate, n_iterations, method='sgd')\n    print(\"SGD Weights:\", w_sgd)\n\n    # Mini-Batch Gradient Descent\n    w_mini = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=2, method='mini-batch')\n    print(\"Mini-Batch GD Weights:\", w_mini)\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix to its Reduced Row Echelon Form (RREF).\n    Args:\n        matrix: 2D list or numpy array\n    Returns:\n        RREF of the matrix as a python list (using numpy's tolist())\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    rows, cols = A.shape\n    row = 0  # current row\n\n    for col in range(cols):\n        # Find the pivot row\n        pivot_row = None\n        for r in range(row, rows):\n            if not np.isclose(A[r, col], 0):\n                pivot_row = r\n                break\n\n        if pivot_row is None:\n            continue  # No pivot in this column, move to next column\n\n        # Swap current row with pivot_row\n        if pivot_row != row:\n            A[[row, pivot_row]] = A[[pivot_row, row]]\n\n        # Normalize the pivot row\n        pivot_val = A[row, col]\n        A[row] = A[row] / pivot_val\n\n        # Eliminate all other entries in this column\n        for r in range(rows):\n            if r != row and not np.isclose(A[r, col], 0):\n                A[r] = A[r] - A[r, col] * A[row]\n\n        row += 1\n        if row >= rows:\n            break\n\n    # Clean up very small numbers (e.g., -0.0, 1e-15) to 0\n    A[np.abs(A) < 1e-12] = 0.0\n\n    return A.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    mat = [\n        [1, 2, -1, -4],\n        [2, 3, -1, -11],\n        [-2, 0, -3, 22]\n    ]\n    rref_mat = rref(mat)\n    for row in rref_mat:\n        print(row)\n```\n**How it works:**\n- The function uses numpy for efficient row operations and floating point arithmetic.\n- It iterates through each column, finds the pivot (first nonzero entry in the current column at or below the current row), swaps rows if necessary, normalizes the pivot row, and eliminates all other entries in the pivot column.\n- After all steps, it cleans up very small values to zero for neatness.\n- The result is returned as a Python list using `tolist()`.\n\n**Tested Example Output:**\n```\n[1.0, 0.0, 0.0, -8.0]\n[0.0, 1.0, 0.0, 1.0]\n[0.0, 0.0, 1.0, -2.0]\n```"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n\n    for i in range(num_iterations):\n        t += 1\n        g = np.array(grad(x))\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Round to 4 decimals and return as list\n    x_rounded = np.round(x, 4)\n    return x_rounded.reshape(-1).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: minimize f(x) = (x-3)^2 + (y+1)^2\n    def f(params):\n        x, y = params\n        return (x - 3) ** 2 + (y + 1) ** 2\n\n    def grad(params):\n        x, y = params\n        return np.array([2 * (x - 3), 2 * (y + 1)])\n\n    x0 = [0.0, 0.0]\n    result = adam_optimizer(f, grad, x0, learning_rate=0.1, num_iterations=1000)\n    print(\"Optimized parameters:\", result)\n```"}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(\n    X: np.array, \n    y: np.array, \n    alpha: float = 0.1, \n    learning_rate: float = 0.01, \n    max_iter: int = 1000, \n    tol: float = 1e-4\n) -> tuple:\n    \"\"\"\n    Implements Lasso Regression (L1 Regularization) using Gradient Descent.\n\n    Parameters:\n        X (np.array): Feature matrix of shape (n_samples, n_features)\n        y (np.array): Target vector of shape (n_samples,)\n        alpha (float): Regularization parameter\n        learning_rate (float): Learning rate for gradient descent\n        max_iter (int): Maximum number of iterations\n        tol (float): Tolerance for stopping criterion\n\n    Returns:\n        tuple: (weights, bias) where weights is a list of shape (n_features,) and bias is a float\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Initialize weights and bias\n    w = np.zeros(n_features)\n    b = 0.0\n\n    y = y.flatten()  # Ensure y is 1D\n\n    for iteration in range(max_iter):\n        # Compute predictions\n        y_pred = X.dot(w) + b\n\n        # Compute gradients\n        error = y_pred - y\n        grad_w = (1/n_samples) * X.T.dot(error) + alpha * np.sign(w)\n        grad_b = (1/n_samples) * np.sum(error)\n\n        # Save old weights for convergence check\n        w_old = w.copy()\n        b_old = b\n\n        # Update weights and bias\n        w = w - learning_rate * grad_w\n        b = b - learning_rate * grad_b\n\n        # Check convergence (L2 norm of weight change and bias change)\n        if np.linalg.norm(w - w_old, ord=2) < tol and abs(b - b_old) < tol:\n            break\n\n    # Round results to 4 decimal places\n    w = np.round(w, 4)\n    b = np.round(b, 4)\n\n    return w.reshape(-1).tolist(), float(b)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test case\n    np.random.seed(42)\n    X = np.random.randn(100, 3)\n    true_w = np.array([1.5, 0.0, -2.0])\n    y = X @ true_w + 0.5 + np.random.randn(100) * 0.1\n\n    weights, bias = l1_regularization_gradient_descent(X, y, alpha=0.2, learning_rate=0.05, max_iter=2000)\n    print(\"Weights:\", weights)\n    print(\"Bias:\", bias)\n```\n**Notes:**\n- The L1 penalty is implemented via `alpha * np.sign(w)`.\n- The weights and bias are rounded to 4 decimal places as required.\n- The function returns Python lists using `.tolist()`.\n- The example at the bottom demonstrates usage and can be removed if only the function is needed."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Compute the Optimal String Alignment (OSA) distance between two strings.\n    Allowed operations: insertion, deletion, substitution, and transposition of adjacent characters.\n    Each operation has a cost of 1.\n    \"\"\"\n    m, n = len(source), len(target)\n    # Initialize the (m+1) x (n+1) distance matrix\n    d = np.zeros((m+1, n+1), dtype=int)\n\n    # Base cases: transforming from empty string\n    for i in range(m+1):\n        d[i][0] = i\n    for j in range(n+1):\n        d[0][j] = j\n\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            cost = 0 if source[i-1] == target[j-1] else 1\n\n            # Compute costs for deletion, insertion, substitution\n            deletion = d[i-1][j] + 1\n            insertion = d[i][j-1] + 1\n            substitution = d[i-1][j-1] + cost\n\n            d[i][j] = min(deletion, insertion, substitution)\n\n            # Check for transposition\n            if i > 1 and j > 1 and source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                d[i][j] = min(d[i][j], d[i-2][j-2] + 1)\n\n    return d[m][n]\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    print(OSA(\"caper\", \"acer\"))      # Output: 2\n    print(OSA(\"abcdef\", \"abcfde\"))   # Output: 2 (transpose 'd' and 'f', then 'e' and 'd')\n    print(OSA(\"kitten\", \"sitting\"))  # Output: 3\n    print(OSA(\"ca\", \"ac\"))           # Output: 1 (transpose)\n    print(OSA(\"\", \"\"))               # Output: 0\n    print(OSA(\"a\", \"\"))              # Output: 1\n    print(OSA(\"\", \"a\"))              # Output: 1\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate recall for binary classification.\n\n    Parameters:\n    y_true (list): True binary labels (0 or 1).\n    y_pred (list): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # True Positives: predicted 1 and actually 1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    # False Negatives: predicted 0 but actually 1\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    recall_value = tp / denominator\n    return round(recall_value, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0, 1, 0]\n    y_pred = [1, 0, 0, 1, 0, 1, 1]\n    print(\"Recall:\", recall(y_true, y_pred))  # Output: Recall: 0.75\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism.\n\n    Args:\n        X: Input sequence, shape (seq_len, d_model)\n        W_q: Query weight matrix, shape (d_model, d_k)\n        W_k: Key weight matrix, shape (d_model, d_k)\n        W_v: Value weight matrix, shape (d_model, d_v)\n\n    Returns:\n        Self-attention output, shape (seq_len, d_v), as a python list rounded to 4 decimals.\n    \"\"\"\n    # Compute Q, K, V\n    Q = np.dot(X, W_q)  # (seq_len, d_k)\n    K = np.dot(X, W_k)  # (seq_len, d_k)\n    V = np.dot(X, W_v)  # (seq_len, d_v)\n\n    # Compute attention scores\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # (seq_len, seq_len)\n\n    # Softmax\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)  # (seq_len, seq_len)\n\n    # Weighted sum of values\n    output = np.dot(attention_weights, V)  # (seq_len, d_v)\n\n    # Round to 4 decimals\n    output = np.round(output, 4)\n\n    # Return as python list\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    seq_len = 3\n    d_model = 4\n    d_k = 4\n    d_v = 4\n\n    X = np.random.rand(seq_len, d_model)\n    W_q = np.random.rand(d_model, d_k)\n    W_k = np.random.rand(d_model, d_k)\n    W_v = np.random.rand(d_model, d_v)\n\n    result = self_attention(X, W_q, W_k, W_v)\n    print(result)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(\n    input_sequence: list[list[float]],\n    initial_hidden_state: list[float],\n    Wx: list[list[float]],\n    Wh: list[list[float]],\n    b: list[float]\n) -> list[float]:\n    \"\"\"\n    Processes a sequence of input vectors through a simple RNN cell.\n\n    Args:\n        input_sequence: List of input vectors (each is a list of floats).\n        initial_hidden_state: Initial hidden state vector (list of floats).\n        Wx: Input-to-hidden weight matrix (list of lists).\n        Wh: Hidden-to-hidden weight matrix (list of lists).\n        b: Bias vector (list of floats).\n\n    Returns:\n        Final hidden state as a list of floats, rounded to 4 decimal places.\n    \"\"\"\n    # Convert all inputs to numpy arrays\n    h = np.array(initial_hidden_state, dtype=np.float64)\n    Wx = np.array(Wx, dtype=np.float64)\n    Wh = np.array(Wh, dtype=np.float64)\n    b = np.array(b, dtype=np.float64)\n\n    for x in input_sequence:\n        x = np.array(x, dtype=np.float64)\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n\n    # Round to 4 decimal places and convert to list\n    h_rounded = np.round(h, 4)\n    return h_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example parameters\n    input_sequence = [\n        [0.5, -0.2],\n        [0.1, 0.4],\n        [-0.3, 0.8]\n    ]\n    initial_hidden_state = [0.0, 0.0]\n    Wx = [\n        [0.2, 0.4],\n        [0.3, 0.1]\n    ]\n    Wh = [\n        [0.5, -0.3],\n        [0.8, 0.2]\n    ]\n    b = [0.1, -0.2]\n\n    final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(final_hidden_state)\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a set of points.\n\n    Args:\n        points (list): List of [x, y] coordinates.\n        tx (float): Translation in x direction.\n        ty (float): Translation in y direction.\n\n    Returns:\n        list: New list of translated [x, y] coordinates.\n    \"\"\"\n    # Convert points to numpy array and add homogeneous coordinate\n    pts = np.array(points)\n    ones = np.ones((pts.shape[0], 1))\n    pts_hom = np.hstack([pts, ones])  # shape (N, 3)\n\n    # Create 3x3 translation matrix\n    T = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n\n    # Apply translation\n    translated_pts_hom = pts_hom @ T.T  # shape (N, 3)\n\n    # Discard the homogeneous coordinate and return as list\n    translated_pts = translated_pts_hom[:, :2]\n    return translated_pts.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    points = [[1, 2], [3, 4], [5, 6]]\n    tx, ty = 2, -1\n    translated = translate_object(points, tx, ty)\n    print(\"Original points:\", points)\n    print(\"Translated points:\", translated)\n```\n"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence D_KL(P || Q) between two normal distributions:\n    P ~ N(mu_p, sigma_p^2)\n    Q ~ N(mu_q, sigma_q^2)\n    \n    Parameters:\n        mu_p (float): Mean of distribution P\n        sigma_p (float): Standard deviation of distribution P (must be > 0)\n        mu_q (float): Mean of distribution Q\n        sigma_q (float): Standard deviation of distribution Q (must be > 0)\n        \n    Returns:\n        float: The KL divergence D_KL(P || Q)\n    \"\"\"\n    # Ensure standard deviations are positive\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n    \n    # KL divergence formula for univariate normals\n    # D_KL(P || Q) = log(sigma_q/sigma_p) + [sigma_p^2 + (mu_p - mu_q)^2] / (2*sigma_q^2) - 0.5\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl = term1 + term2 - 0.5\n    return kl\n\n# Example usage:\nif __name__ == \"__main__\":\n    mu_p = 0.0\n    sigma_p = 1.0\n    mu_q = 1.0\n    sigma_q = 2.0\n    kl = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n    print(f\"KL divergence D_KL(P || Q): {kl}\")\n```\n"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = len(b)\n    if x_ini is None:\n        x = np.zeros(N)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])  # Use updated values\n            s2 = np.dot(A[i, i+1:], x[i+1:])  # Use old values\n            if A[i, i] == 0:\n                raise ZeroDivisionError(f\"Zero diagonal element at row {i}\")\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    # Round to 4 decimals\n    x = np.round(x, 4)\n    return x.reshape(-1).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = [[4, 1, 2],\n         [3, 5, 1],\n         [1, 1, 3]]\n    b = [4,7,3]\n    n = 10\n    result = gauss_seidel(A, b, n)\n    print(result)\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).reshape(-1, 1)\n    n = len(b)\n\n    # Forward elimination with partial pivoting\n    for i in range(n):\n        # Partial pivoting\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        if A[max_row, i] == 0:\n            raise ValueError(\"Matrix is singular or nearly singular.\")\n        if max_row != i:\n            # Swap rows in A\n            A[[i, max_row]] = A[[max_row, i]]\n            # Swap corresponding entries in b\n            b[[i, max_row]] = b[[max_row, i]]\n\n        # Eliminate entries below the pivot\n        for j in range(i+1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n\n    # Backward substitution\n    x = np.zeros((n, 1))\n    for i in range(n-1, -1, -1):\n        if A[i, i] == 0:\n            raise ValueError(\"Matrix is singular or nearly singular.\")\n        x[i] = (b[i] - np.dot(A[i, i+1:], x[i+1:, 0])) / A[i, i]\n\n    # Round to 4 decimals and return as list\n    x = np.round(x.flatten(), 4)\n    return x.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = [[2, 1, -1],\n         [-3, -1, 2],\n         [-2, 1, 2]]\n    b = [8, -11, -3]\n    solution = gaussian_elimination(A, b)\n    print(solution)  # Output: [2.0, 3.0, -1.0]\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef round4(x):\n    # Rounds a numpy array to 4 decimals\n    return np.round(x, 4)\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        Args:\n            x: numpy array of shape (seq_len, input_size)\n            initial_hidden_state: numpy array of shape (hidden_size, 1)\n            initial_cell_state: numpy array of shape (hidden_size, 1)\n        Returns:\n            hidden_states: list of hidden states at each time step (seq_len, hidden_size)\n            final_hidden_state: hidden state after last time step (hidden_size, 1)\n            final_cell_state: cell state after last time step (hidden_size, 1)\n        \"\"\"\n        seq_len = x.shape[0]\n        h = initial_hidden_state\n        c = initial_cell_state\n        hidden_states = []\n\n        for t in range(seq_len):\n            x_t = x[t].reshape(-1, 1)  # (input_size, 1)\n            concat = np.vstack((h, x_t))  # (hidden_size + input_size, 1)\n\n            f_t = sigmoid(self.Wf @ concat + self.bf)  # forget gate\n            i_t = sigmoid(self.Wi @ concat + self.bi)  # input gate\n            c_hat_t = np.tanh(self.Wc @ concat + self.bc)  # candidate cell\n            o_t = sigmoid(self.Wo @ concat + self.bo)  # output gate\n\n            c = f_t * c + i_t * c_hat_t  # new cell state\n            h = o_t * np.tanh(c)         # new hidden state\n\n            # Round to 4 decimals and flatten to 1D\n            h_rounded = round4(h).flatten()\n            hidden_states.append(h_rounded.tolist())\n\n        final_hidden_state = round4(h).flatten().tolist()\n        final_cell_state = round4(c).flatten().tolist()\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    input_size = 3\n    hidden_size = 2\n    seq_len = 4\n\n    # Random input sequence\n    x = np.random.randn(seq_len, input_size)\n    h0 = np.zeros((hidden_size, 1))\n    c0 = np.zeros((hidden_size, 1))\n\n    lstm = LSTM(input_size, hidden_size)\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, h0, c0)\n\n    print(\"Hidden states at each time step:\")\n    for i, h in enumerate(hidden_states):\n        print(f\"t={i}: {h}\")\n    print(\"Final hidden state:\", final_hidden_state)\n    print(\"Final cell state:\", final_cell_state)\n```\n"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF scores for the query words in each document of the corpus.\n\n    Args:\n        corpus: List of documents, where each document is a list of words.\n        query: List of words for which to compute TF-IDF scores.\n\n    Returns:\n        List of lists containing the TF-IDF scores for the query words in each document,\n        rounded to 4 decimal places.\n    \"\"\"\n    # Handle empty corpus\n    if not corpus:\n        return []\n\n    N = len(corpus)\n    # Precompute document frequencies (df) for each query term\n    df = {}\n    for term in query:\n        df[term] = 0\n        for doc in corpus:\n            if term in doc:\n                df[term] += 1\n\n    # Compute IDF with smoothing: idf = log((N + 1) / (df + 1)) + 1\n    idf = {}\n    for term in query:\n        idf[term] = np.log((N + 1) / (df[term] + 1)) + 1\n\n    # Compute TF and TF-IDF for each document\n    tf_idf_matrix = []\n    for doc in corpus:\n        doc_len = len(doc)\n        # Count term frequencies in the document\n        tf = {}\n        for word in doc:\n            tf[word] = tf.get(word, 0) + 1\n        row = []\n        for term in query:\n            # Term frequency: count of term in doc / total words in doc\n            if doc_len == 0:\n                tf_val = 0.0\n            else:\n                tf_val = tf.get(term, 0) / doc_len\n            tf_idf_val = tf_val * idf[term]\n            # Round to 4 decimal places\n            tf_idf_val = round(tf_idf_val, 4)\n            row.append(tf_idf_val)\n        tf_idf_matrix.append(row)\n\n    # Convert to numpy array and then to list as per instructions\n    return np.array(tf_idf_matrix).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    corpus = [\n        [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n        [\"the\", \"dog\", \"barked\"],\n        [],\n        [\"the\", \"cat\", \"barked\", \"at\", \"the\", \"dog\"]\n    ]\n    query = [\"cat\", \"dog\", \"barked\", \"mat\", \"fish\"]\n    result = compute_tf_idf(corpus, query)\n    for row in result:\n        print(row)\n```\n**Explanation:**\n- Smoothing is applied in the IDF calculation to avoid division by zero.\n- Handles empty corpus and empty documents.\n- Rounds results to 4 decimal places.\n- Returns a list of lists as required."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Ensure input arrays are numpy arrays and flattened\n    y_true = np.asarray(y_true).flatten()\n    y_pred = np.asarray(y_pred).flatten()\n\n    # Calculate True Positives (TP), False Positives (FP), False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n\n    # Precision and Recall calculation with zero-division handling\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n\n    # F-Score calculation with zero-division handling\n    beta_sq = beta ** 2\n    denom = (beta_sq * precision) + recall\n    if denom == 0:\n        fscore = 0.0\n    else:\n        fscore = (1 + beta_sq) * (precision * recall) / denom\n\n    return round(fscore, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0])\n    y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0])\n    beta = 1\n    print(\"F1-Score:\", f_score(y_true, y_pred, beta))  # Should print the F1-Score rounded to 3 decimals\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.output_size = output_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x_seq):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Args:\n            x_seq: list or array of input vectors (each shape: input_size x 1)\n        Returns:\n            outputs: list of output vectors (each shape: output_size x 1)\n            last_inputs: list of input vectors at each time step\n            last_hiddens: list of hidden state vectors at each time step\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = [h_prev]\n        for x in x_seq:\n            x = x.reshape(-1, 1)\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x)\n            last_hiddens.append(h)\n            h_prev = h\n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, x_seq, target_seq, outputs, last_inputs, last_hiddens, learning_rate=0.01):\n        \"\"\"\n        Backpropagation Through Time (BPTT) for the RNN.\n        Args:\n            x_seq: list of input vectors (each shape: input_size x 1)\n            target_seq: list of expected output vectors (each shape: output_size x 1)\n            outputs: list of output vectors from forward pass\n            last_inputs: list of input vectors at each time step\n            last_hiddens: list of hidden state vectors at each time step\n            learning_rate: learning rate for parameter updates\n        Returns:\n            total_loss: scalar, sum of 1/2 * MSE losses over the sequence\n        \"\"\"\n        # Initialize gradients\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        dh_next = np.zeros((self.hidden_size, 1))\n        total_loss = 0.0\n        T = len(x_seq)\n\n        # Backward through time\n        for t in reversed(range(T)):\n            y = outputs[t]\n            target = target_seq[t].reshape(-1, 1)\n            h = last_hiddens[t+1]\n            h_prev = last_hiddens[t]\n            x = last_inputs[t]\n\n            # Loss: 1/2 * MSE\n            dy = y - target\n            total_loss += 0.5 * np.sum((y - target) ** 2)\n\n            # Gradients for output layer\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n\n            # Backprop into h\n            dh = np.dot(self.W_hy.T, dy) + dh_next  # add gradient from next time step\n            dtanh = (1 - h ** 2) * dh  # derivative of tanh\n\n            dW_xh += np.dot(dtanh, x.T)\n            dW_hh += np.dot(dtanh, h_prev.T)\n            db_h += dtanh\n\n            dh_next = np.dot(self.W_hh.T, dtanh)\n\n        # Clip gradients to prevent exploding gradients\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n\n        # Update parameters\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n        return total_loss\n\n    def train(self, x_seqs, y_seqs, epochs=100, learning_rate=0.01, verbose=True):\n        \"\"\"\n        Trains the RNN on a list of input/output sequences.\n        Args:\n            x_seqs: list of input sequences (each is a list of input vectors)\n            y_seqs: list of target sequences (each is a list of output vectors)\n            epochs: number of training epochs\n            learning_rate: learning rate for parameter updates\n            verbose: whether to print loss during training\n        \"\"\"\n        for epoch in range(1, epochs+1):\n            total_loss = 0.0\n            for x_seq, y_seq in zip(x_seqs, y_seqs):\n                outputs, last_inputs, last_hiddens = self.forward(x_seq)\n                loss = self.backward(x_seq, y_seq, outputs, last_inputs, last_hiddens, learning_rate)\n                total_loss += loss\n            if verbose and (epoch % 10 == 0 or epoch == 1):\n                print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n\n    def predict(self, x_seq):\n        \"\"\"\n        Predicts outputs for a given input sequence.\n        Args:\n            x_seq: list of input vectors\n        Returns:\n            outputs: list of output vectors\n        \"\"\"\n        outputs, _, _ = self.forward(x_seq)\n        return outputs\n\n# Example usage: Sequence prediction (e.g., learn to predict next number in a sequence)\nif __name__ == \"__main__\":\n    np.random.seed(42)\n\n    # Toy dataset: learn to predict the next number in a sequence (e.g., [0,1,2,3] -> [1,2,3,4])\n    # We'll use one-hot encoding for simplicity\n    vocab_size = 5  # numbers 0-4\n    input_size = vocab_size\n    hidden_size = 10\n    output_size = vocab_size\n\n    # Generate training data\n    def one_hot(n, size):\n        v = np.zeros((size, 1))\n        v[n] = 1\n        return v\n\n    x_seqs = []\n    y_seqs = []\n    for start in range(0, vocab_size-1):\n        x_seq = [one_hot(i, vocab_size) for i in range(start, start+3)]\n        y_seq = [one_hot(i+1, vocab_size) for i in range(start, start+3)]\n        x_seqs.append(x_seq)\n        y_seqs.append(y_seq)\n\n    # Initialize and train RNN\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n    rnn.train(x_seqs, y_seqs, epochs=200, learning_rate=0.1)\n\n    # Test prediction\n    test_seq = [one_hot(1, vocab_size), one_hot(2, vocab_size), one_hot(3, vocab_size)]\n    outputs = rnn.predict(test_seq)\n    print(\"\\nTest sequence: [1, 2, 3]\")\n    print(\"Predicted next numbers:\")\n    for i, out in enumerate(outputs):\n        pred = np.argmax(out)\n        print(f\"Step {i+1}: {pred} (raw: {out.ravel()})\")\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x0, dtype=float)\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    for i in range(n):\n        Ap = A @ p\n        pAp = np.dot(p, Ap)\n        if pAp == 0:\n            break  # Avoid division by zero\n        alpha = rs_old / pAp\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new) < tol:\n            break\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n\n    x = np.round(x, 8)\n    return x.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: Solve Ax = b\n    A = np.array([[4, 1], [1, 3]], dtype=float)\n    b = np.array([1, 2], dtype=float)\n    n = 25\n    x_sol = conjugate_gradient(A, b, n)\n    print(\"Solution x:\", x_sol)\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0  # No impurity if no samples\n\n    y = np.array(y)\n    _, counts = np.unique(y, return_counts=True)\n    probabilities = counts / counts.sum()\n    gini = 1.0 - np.sum(probabilities ** 2)\n    return round(gini, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y1 = [0, 0, 1, 1, 1]\n    y2 = [1, 1, 1, 1]\n    y3 = []\n    print(gini_impurity(y1))  # Output: 0.48\n    print(gini_impurity(y2))  # Output: 0.0\n    print(gini_impurity(y3))  # Output: 0.0\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_ptr = [0]  # Start with 0\n\n    for row in dense_matrix:\n        count = 0\n        for col_idx, val in enumerate(row):\n            if val != 0:\n                values.append(val)\n                col_indices.append(col_idx)\n                count += 1\n        row_ptr.append(row_ptr[-1] + count)\n\n    return values, col_indices, row_ptr\n\n# Example usage and test\nif __name__ == \"__main__\":\n    dense = [\n        [0, 0, 3, 0],\n        [22, 0, 0, 0],\n        [7, 5, 0, 1],\n        [0, 0, 0, 0]\n    ]\n    values, col_indices, row_ptr = compressed_row_sparse_matrix(dense)\n    print(\"Values array:      \", values)\n    print(\"Column indices:    \", col_indices)\n    print(\"Row pointer array: \", row_ptr)\n    # Expected output:\n    # Values array:       [3, 22, 7, 5, 1]\n    # Column indices:     [2, 0, 0, 1, 3]\n    # Row pointer array:  [0, 1, 2, 5, 5]\n```"}
{"task_id": 66, "completion_id": 0, "completion": "```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of numbers)\n    :param L: The line vector defining the direction of projection (list of numbers)\n    :return: List representing the projection of v onto L, rounded to 3 decimals\n    \"\"\"\n    if len(v) != len(L):\n        raise ValueError(\"Vectors v and L must have the same dimension.\")\n\n    # Compute dot products\n    dot_vL = sum(vi * Li for vi, Li in zip(v, L))\n    dot_LL = sum(Li * Li for Li in L)\n\n    if dot_LL == 0:\n        raise ValueError(\"Line vector L must not be the zero vector.\")\n\n    # Compute the projection scalar\n    scalar = dot_vL / dot_LL\n\n    # Compute the projection vector\n    proj = [round(scalar * Li, 3) for Li in L]\n    return proj\n\n# Example usage:\nif __name__ == \"__main__\":\n    v = [3, 4]\n    L = [1, 2]\n    print(\"Projection:\", orthogonal_projection(v, L))  # Output: [2.2, 4.4]\n\n    v2 = [1, 2, 3]\n    L2 = [4, 5, 6]\n    print(\"Projection:\", orthogonal_projection(v2, L2))  # Output: [1.662, 2.078, 2.494]\n```\n"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        # Handle empty matrix or matrix with zero columns\n        n_cols = 0 if not dense_matrix else len(dense_matrix[0])\n        return ([], [], [0] * (n_cols + 1))\n\n    n_rows = len(dense_matrix)\n    n_cols = len(dense_matrix[0])\n\n    values = []\n    row_indices = []\n    col_ptr = [0]\n\n    for col in range(n_cols):\n        count = 0\n        for row in range(n_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n                count += 1\n        col_ptr.append(col_ptr[-1] + count)\n\n    return (values, row_indices, col_ptr)\n\n# Example usage and test\nif __name__ == \"__main__\":\n    dense = [\n        [0, 0, 3, 0],\n        [22, 0, 0, 0],\n        [7, 5, 0, 1],\n        [0, 0, 0, 0]\n    ]\n    values, row_indices, col_ptr = compressed_col_sparse_matrix(dense)\n    print(\"Values:\", values)\n    print(\"Row indices:\", row_indices)\n    print(\"Column pointer:\", col_ptr)\n    # Expected output:\n    # Values: [22, 7, 5, 3, 1]\n    # Row indices: [1, 2, 2, 0, 2]\n    # Column pointer: [0, 2, 3, 4, 5]\n```"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Returns a list of basis vectors (as columns) that span the column space (image) of matrix A.\n    The basis vectors are columns from the original matrix A corresponding to the pivot columns\n    in its row echelon form.\n    \"\"\"\n    A = np.array(A, dtype=np.float64)\n    m, n = A.shape\n    # Copy of A to perform row operations\n    R = A.copy()\n    pivot_cols = []\n    row = 0\n    for col in range(n):\n        # Find the pivot row for this column\n        pivot_row = None\n        for r in range(row, m):\n            if abs(R[r, col]) > 1e-12:\n                pivot_row = r\n                break\n        if pivot_row is not None:\n            # Swap current row with pivot_row\n            if pivot_row != row:\n                R[[row, pivot_row], :] = R[[pivot_row, row], :]\n            # Normalize the pivot row\n            R[row, :] = R[row, :] / R[row, col]\n            # Eliminate below\n            for r in range(row+1, m):\n                R[r, :] = R[r, :] - R[r, col] * R[row, :]\n            pivot_cols.append(col)\n            row += 1\n        if row >= m:\n            break\n    # Extract the pivot columns from the original matrix\n    basis = A[:, pivot_cols]\n    # Round and convert to list\n    return np.round(basis, 8).tolist()\n```\n\n**Example usage:**\n```python\nA = np.array([[1, 2, 3],\n              [2, 4, 6],\n              [1, 1, 1]])\nprint(matrix_image(A))\n# Output: [[1.0, 3.0], [2.0, 6.0], [1.0, 1.0]]\n```\nThis returns the basis vectors (as columns) of the column space of `A`."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    if ss_tot == 0:\n        # All y_true values are the same; R^2 is not defined, return 0.0 by convention\n        return 0.0\n    r2 = 1 - (ss_res / ss_tot)\n    return round(r2, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    print(r_squared(y_true, y_pred))  # Output: 0.948\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    # Check if img is empty or not a list\n    if not isinstance(img, list) or len(img) == 0:\n        return -1\n\n    row_length = None\n    total = 0\n    count = 0\n\n    for row in img:\n        # Each row must be a list\n        if not isinstance(row, list):\n            return -1\n        # Set row_length on first row\n        if row_length is None:\n            row_length = len(row)\n        # Check for inconsistent row lengths\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            # Pixel must be int and in [0,255]\n            if not isinstance(pixel, int) or not (0 <= pixel <= 255):\n                return -1\n            total += pixel\n            count += 1\n\n    # If there are no pixels (e.g., [[]]), return -1\n    if count == 0:\n        return -1\n\n    avg = total / count\n    return round(avg, 2)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    # Valid image\n    img1 = [\n        [0, 128, 255],\n        [64, 192, 32]\n    ]\n    print(calculate_brightness(img1))  # Should print 111.83\n\n    # Empty image\n    img2 = []\n    print(calculate_brightness(img2))  # Should print -1\n\n    # Inconsistent row lengths\n    img3 = [\n        [0, 128],\n        [255]\n    ]\n    print(calculate_brightness(img3))  # Should print -1\n\n    # Pixel out of range\n    img4 = [\n        [0, 128, 256]\n    ]\n    print(calculate_brightness(img4))  # Should print -1\n\n    # Non-integer pixel\n    img5 = [\n        [0, 128, 100.5]\n    ]\n    print(calculate_brightness(img5))  # Should print -1\n\n    # Empty rows\n    img6 = [[]]\n    print(calculate_brightness(img6))  # Should print -1\n\n    # Single pixel\n    img7 = [[200]]\n    print(calculate_brightness(img7))  # Should print 200.0\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between two arrays.\n\n    Parameters:\n        y_true (array-like): Actual values.\n        y_pred (array-like): Predicted values.\n\n    Returns:\n        float: RMSE value rounded to three decimal places.\n\n    Raises:\n        ValueError: If input arrays are empty, have mismatched shapes, or are not array-like.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    try:\n        y_true_arr = np.asarray(y_true, dtype=np.float64)\n        y_pred_arr = np.asarray(y_pred, dtype=np.float64)\n    except Exception as e:\n        raise ValueError(\"Input values must be array-like and convertible to float.\") from e\n\n    # Check for empty arrays\n    if y_true_arr.size == 0 or y_pred_arr.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n\n    # Check for shape mismatch\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(f\"Shape mismatch: y_true has shape {y_true_arr.shape}, y_pred has shape {y_pred_arr.shape}.\")\n\n    # Compute RMSE\n    mse = np.mean((y_true_arr - y_pred_arr) ** 2)\n    rmse_value = np.sqrt(mse)\n\n    return round(rmse_value, 3)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    # Valid input\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    print(\"RMSE:\", rmse(y_true, y_pred))  # Output: RMSE: 0.612\n\n    # Edge case: mismatched shapes\n    try:\n        print(rmse([1, 2, 3], [1, 2]))\n    except ValueError as e:\n        print(\"Error:\", e)\n\n    # Edge case: empty arrays\n    try:\n        print(rmse([], []))\n    except ValueError as e:\n        print(\"Error:\", e)\n\n    # Edge case: invalid input types\n    try:\n        print(rmse(\"abc\", [1, 2, 3]))\n    except ValueError as e:\n        print(\"Error:\", e)\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth binary labels.\n    y_pred (array-like): Predicted binary labels.\n\n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n\n    # Handle edge case: both arrays contain only zeros\n    if union == 0:\n        return 1.0\n\n    jaccard = intersection / union\n    return round(jaccard, 3)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    # Perfect overlap\n    print(jaccard_index([1, 0, 1, 1], [1, 0, 1, 1]))  # Output: 1.0\n\n    # No overlap\n    print(jaccard_index([1, 1, 0, 0], [0, 0, 1, 1]))  # Output: 0.0\n\n    # Partial overlap\n    print(jaccard_index([1, 0, 1, 0], [1, 1, 0, 0]))  # Output: 0.333\n\n    # Both arrays all zeros (edge case)\n    print(jaccard_index([0, 0, 0], [0, 0, 0]))        # Output: 1.0\n\n    # One array all zeros, one not\n    print(jaccard_index([0, 0, 0], [1, 0, 0]))        # Output: 0.0\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen\u2013Dice coefficient) for binary classification.\n\n    Parameters:\n        y_true (array-like): Ground truth binary labels (0 or 1).\n        y_pred (array-like): Predicted binary labels (0 or 1).\n\n    Returns:\n        float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays and flatten\n    y_true = np.asarray(y_true).astype(np.bool_).flatten()\n    y_pred = np.asarray(y_pred).astype(np.bool_).flatten()\n\n    # Intersection: number of true positives\n    intersection = np.logical_and(y_true, y_pred).sum()\n    # Sums: number of positives in each\n    sum_true = y_true.sum()\n    sum_pred = y_pred.sum()\n\n    # Handle edge case: both sums are zero (no positives in both)\n    if sum_true + sum_pred == 0:\n        return 1.0\n\n    dice = 2 * intersection / (sum_true + sum_pred)\n    return round(float(dice), 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Perfect overlap\n    print(dice_score([1, 0, 1, 1], [1, 0, 1, 1]))  # Output: 1.0\n\n    # Partial overlap\n    print(dice_score([1, 0, 1, 0], [0, 1, 1, 0]))  # Output: 0.5\n\n    # No positives in both\n    print(dice_score([0, 0, 0], [0, 0, 0]))        # Output: 1.0\n\n    # No overlap\n    print(dice_score([1, 1, 1], [0, 0, 0]))        # Output: 0.0\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef generate_random_hv(dim, seed):\n    \"\"\"\n    Generate a random bipolar hypervector (+1/-1) of given dimension using the provided seed.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    hv = rng.choice([-1, 1], size=dim)\n    return hv\n\ndef bind(hv1, hv2):\n    \"\"\"\n    Bind two hypervectors using element-wise multiplication (XOR for bipolar).\n    \"\"\"\n    return hv1 * hv2\n\ndef bundle(hv_list):\n    \"\"\"\n    Bundle a list of hypervectors by summing and then taking the sign.\n    If sum is zero, assign +1.\n    \"\"\"\n    if not hv_list:\n        raise ValueError(\"hv_list is empty\")\n    sum_hv = np.sum(hv_list, axis=0)\n    # sign(0) = 0, so we set zeros to +1\n    bundled = np.sign(sum_hv)\n    bundled[bundled == 0] = 1\n    return bundled\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a dataset row using HDC principles.\n    \"\"\"\n    feature_hvs = []\n    for feature, value in row.items():\n        # Get the seed for this feature\n        if feature not in random_seeds:\n            raise ValueError(f\"Missing random seed for feature '{feature}'\")\n        seed = random_seeds[feature]\n        \n        # Generate feature name hypervector (fixed for the feature)\n        feature_hv = generate_random_hv(dim, seed)\n        \n        # Generate value hypervector (using the same seed + hash of value for reproducibility)\n        # For categorical values, we can hash the value to get a unique offset\n        # For numeric values, we can treat as string and hash as well\n        value_str = str(value)\n        value_hash = abs(hash(value_str)) % (2**32)\n        value_seed = seed + value_hash\n        value_hv = generate_random_hv(dim, value_seed)\n        \n        # Bind feature and value hypervectors\n        bound_hv = bind(feature_hv, value_hv)\n        feature_hvs.append(bound_hv)\n    \n    # Bundle all bound hypervectors to get the row hypervector\n    row_hv = bundle(feature_hvs)\n    return row_hv.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example row and seeds\n    row = {'color': 'red', 'shape': 'circle', 'size': 5}\n    dim = 10000\n    random_seeds = {'color': 42, 'shape': 99, 'size': 123}\n    hv = create_row_hv(row, dim, random_seeds)\n    print(hv[:20])  # Print first 20 elements for brevity\n```\n"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a 2x2 confusion matrix for binary classification.\n\n    Args:\n        data: List of [y_true, y_pred] pairs.\n\n    Returns:\n        2x2 confusion matrix as a list of lists:\n        [[TP, FP],\n         [FN, TN]]\n    \"\"\"\n    # Initialize counts\n    TP = FP = FN = TN = 0\n\n    for y_true, y_pred in data:\n        if y_true == 1 and y_pred == 1:\n            TP += 1\n        elif y_true == 0 and y_pred == 1:\n            FP += 1\n        elif y_true == 1 and y_pred == 0:\n            FN += 1\n        elif y_true == 0 and y_pred == 0:\n            TN += 1\n        else:\n            # In case of unexpected labels, you may raise an error or skip\n            raise ValueError(f\"Labels must be 0 or 1, got y_true={y_true}, y_pred={y_pred}\")\n\n    return [[TP, FP],\n            [FN, TN]]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data: [y_true, y_pred]\n    data = [\n        [1, 1],  # TP\n        [0, 1],  # FP\n        [1, 0],  # FN\n        [0, 0],  # TN\n        [1, 1],  # TP\n        [0, 0],  # TN\n    ]\n    cm = confusion_matrix(data)\n    print(\"Confusion Matrix:\")\n    print(f\"[[TP, FP],\\n [FN, TN]] = {cm}\")\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Ensure inputs are numpy arrays\n    v1 = np.asarray(v1)\n    v2 = np.asarray(v2)\n    \n    # Check if shapes match\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    \n    # Check for empty vectors\n    if v1.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n    \n    # Compute norms\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    \n    # Check for zero magnitude\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    # Compute cosine similarity\n    cos_sim = np.dot(v1, v2) / (norm_v1 * norm_v2)\n    \n    # Round to three decimal places\n    return round(float(cos_sim), 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    a = np.array([1, 2, 3])\n    b = np.array([4, 5, 6])\n    print(\"Cosine similarity:\", cosine_similarity(a, b))  # Output: 0.974\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"Length of actual and predicted lists must be the same.\")\n    if not all(x in (0, 1) for x in actual + predicted):\n        raise ValueError(\"All elements in actual and predicted must be 0 or 1.\")\n\n    # Initialize confusion matrix components\n    TP = FP = TN = FN = 0\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 1 and p == 0:\n            FN += 1\n\n    # Confusion matrix: [[TN, FP], [FN, TP]]\n    confusion_matrix = [[TN, FP], [FN, TP]]\n\n    # Accuracy\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total > 0 else 0.0\n\n    # F1 Score\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n    if precision + recall > 0:\n        f1_score = 2 * precision * recall / (precision + recall)\n    else:\n        f1_score = 0.0\n\n    # Specificity: TN / (TN + FP)\n    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n\n    # Negative Predictive Value: TN / (TN + FN)\n    negative_predictive_value = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n\n    # Round metrics to three decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n\n# Example usage:\nif __name__ == \"__main__\":\n    actual = [1, 0, 1, 1, 0, 0, 1, 0]\n    predicted = [1, 0, 1, 0, 0, 1, 0, 0]\n    result = performance_metrics(actual, predicted)\n    print(\"Confusion Matrix:\", result[0])\n    print(\"Accuracy:\", result[1])\n    print(\"F1 Score:\", result[2])\n    print(\"Specificity:\", result[3])\n    print(\"Negative Predictive Value:\", result[4])\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a given dataset.\n\n    Parameters:\n        data (list or np.ndarray): List or array of numerical values.\n\n    Returns:\n        dict: Dictionary containing mean, median, mode, variance, standard deviation,\n              25th, 50th, 75th percentiles, and interquartile range (IQR).\n    \"\"\"\n    # Convert input to numpy array for consistency\n    arr = np.array(data)\n    if arr.size == 0:\n        raise ValueError(\"Input data must not be empty.\")\n\n    # Mean\n    mean = np.mean(arr)\n    # Median\n    median = np.median(arr)\n    # Mode (handle multimodal data: return the smallest mode)\n    vals, counts = np.unique(arr, return_counts=True)\n    max_count = np.max(counts)\n    modes = vals[counts == max_count]\n    mode = modes.min()  # If multiple modes, pick the smallest\n\n    # Variance (sample variance, ddof=1)\n    variance = np.var(arr, ddof=1)\n    # Standard deviation (sample std, ddof=1)\n    std_dev = np.std(arr, ddof=1)\n    # Percentiles\n    p25 = np.percentile(arr, 25)\n    p50 = np.percentile(arr, 50)\n    p75 = np.percentile(arr, 75)\n    # Interquartile range\n    iqr = p75 - p25\n\n    # Prepare output dictionary, rounding as specified\n    result = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,  # Mode is from the data, do not round\n        'variance': round(variance, 4),\n        'standard_deviation': round(std_dev, 4),\n        '25th_percentile': round(p25, 4),\n        '50th_percentile': round(p50, 4),\n        '75th_percentile': round(p75, 4),\n        'interquartile_range': round(iqr, 4)\n    }\n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]\n    stats = descriptive_statistics(data)\n    for k, v in stats.items():\n        print(f\"{k}: {v}\")\n```\n"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    if not (0 <= p <= 1):\n        raise ValueError(\"Probability p must be between 0 and 1.\")\n    if not (0 <= k <= n):\n        return 0.0  # Probability is zero if k is not in [0, n]\n    # Calculate binomial coefficient: C(n, k)\n    comb = math.comb(n, k)\n    # Calculate probability\n    prob = comb * (p ** k) * ((1 - p) ** (n - k))\n    return round(prob, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    n = 10\n    k = 3\n    p = 0.5\n    result = binomial_probability(n, k, p)\n    print(f\"P({k} successes in {n} trials, p={p}) = {result}\")\n```\n"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be positive.\")\n    coeff = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    pdf = coeff * math.exp(exponent)\n    return round(pdf, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Standard normal distribution at x=0\n    print(normal_pdf(0, 0, 1))  # Output: 0.39894\n\n    # Normal distribution with mean=2, std_dev=3 at x=1\n    print(normal_pdf(1, 2, 3))  # Output: 0.12579\n\n    # Normal distribution with mean=5, std_dev=2 at x=7\n    print(normal_pdf(7, 5, 2))  # Output: 0.12099\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if not (isinstance(k, int) and k >= 0):\n        raise ValueError(\"k must be a non-negative integer.\")\n    if lam < 0:\n        raise ValueError(\"lam (mean rate) must be non-negative.\")\n\n    # Poisson probability formula: P(k; \u03bb) = (\u03bb^k * e^(-\u03bb)) / k!\n    prob = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    return round(prob, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: Probability of observing exactly 3 events when the mean rate is 2.5\n    k = 3\n    lam = 2.5\n    probability = poisson_probability(k, lam)\n    print(f\"P({k}; {lam}) = {probability}\")\n```\n"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast value (max pixel value - min pixel value).\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray.\")\n    if img.ndim != 2:\n        raise ValueError(\"Input image must be a 2D array (grayscale image).\")\n    if img.size == 0:\n        raise ValueError(\"Input image is empty.\")\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = float(max_val - min_val)\n    return contrast\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample 5x5 grayscale image\n    img = np.array([\n        [  0,  50, 100, 150, 200],\n        [ 10,  60, 110, 160, 210],\n        [ 20,  70, 120, 170, 220],\n        [ 30,  80, 130, 180, 230],\n        [ 40,  90, 140, 190, 240]\n    ], dtype=np.uint8)\n    contrast = calculate_contrast(img)\n    print(f\"Contrast of the image: {contrast}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float or int: The dot product of the two vectors.\n    Raises:\n        ValueError: If the input arrays are not 1D or their lengths do not match.\n    \"\"\"\n    # Ensure inputs are 1D numpy arrays\n    if not (isinstance(vec1, np.ndarray) and isinstance(vec2, np.ndarray)):\n        raise TypeError(\"Both inputs must be numpy.ndarray.\")\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both input arrays must be 1-dimensional.\")\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError(\"Vectors must be of the same length.\")\n    \n    return np.dot(vec1, vec2)\n\n# Example usage:\nif __name__ == \"__main__\":\n    v1 = np.array([1, 2, 3])\n    v2 = np.array([4, 5, 6])\n    result = calculate_dot_product(v1, v2)\n    print(f\"Dot product of {v1} and {v2} is: {result}\")\n    # Output should be: Dot product of [1 2 3] and [4 5 6] is: 32\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: Nested list where each inner list contains the polynomial features of the corresponding data point,\n                           rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    # If data is empty, return empty list\n    if not data:\n        return []\n    # For each data point x, generate [x^0, x^1, ..., x^degree]\n    transformed = []\n    for x in data:\n        features = [round(float(np.power(x, d)), 8) for d in range(degree + 1)]\n        transformed.append(features)\n    return transformed\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [2.0, -1.5, 0.0]\n    degree = 3\n    result = phi_transform(data, degree)\n    print(result)\n    # Output: [[1.0, 2.0, 4.0, 8.0], [1.0, -1.5, 2.25, -3.375], [1.0, 0.0, 0.0, 0.0]]\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Create position indices (shape: [position, 1])\n    pos = np.arange(position)[:, np.newaxis]\n    # Create dimension indices (shape: [1, d_model])\n    i = np.arange(d_model)[np.newaxis, :]\n\n    # Compute the angle rates\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    angle_rads = pos * angle_rates  # shape: [position, d_model]\n\n    # Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...)\n    pos_encoding = np.zeros_like(angle_rads)\n    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    # Convert to float16 and to list\n    return pos_encoding.astype(np.float16).tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1' (Overfitting), '-1' (Underfitting), or '0' (Good fit).\n    \"\"\"\n    # Overfitting: training accuracy much higher than test accuracy\n    if (training_accuracy - test_accuracy) > 0.2:\n        return 1\n    # Underfitting: both accuracies are low\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # Good fit: neither overfitting nor underfitting\n    else:\n        return 0\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    test_cases = [\n        (0.95, 0.65),  # Overfitting\n        (0.65, 0.60),  # Underfitting\n        (0.85, 0.80),  # Good fit\n        (0.75, 0.74),  # Good fit\n        (0.69, 0.69),  # Underfitting\n        (0.90, 0.69),  # Overfitting\n        (0.70, 0.70),  # Good fit\n    ]\n    for train_acc, test_acc in test_cases:\n        result = model_fit_quality(train_acc, test_acc)\n        if result == 1:\n            status = \"Overfitting\"\n        elif result == -1:\n            status = \"Underfitting\"\n        else:\n            status = \"Good fit\"\n        print(f\"Training: {train_acc:.2f}, Test: {test_acc:.2f} => {status}\")\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Convert inputs to numpy arrays for unified handling\n    parameter = np.array(parameter, dtype=np.float64)\n    grad = np.array(grad, dtype=np.float64)\n    m = np.array(m, dtype=np.float64)\n    v = np.array(v, dtype=np.float64)\n\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n\n    # Update parameter\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Round results to 5 decimal places and convert to list\n    parameter_rounded = np.round(parameter, 5).tolist()\n    m_rounded = np.round(m, 5).tolist()\n    v_rounded = np.round(v, 5).tolist()\n\n    return parameter_rounded, m_rounded, v_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Scalar example\n    param, grad, m, v, t = 1.0, 0.1, 0.0, 0.0, 1\n    updated_param, updated_m, updated_v = adam_optimizer(param, grad, m, v, t)\n    print(\"Scalar example:\")\n    print(\"Updated parameter:\", updated_param)\n    print(\"Updated m:\", updated_m)\n    print(\"Updated v:\", updated_v)\n\n    # Array example\n    param = [1.0, 2.0]\n    grad = [0.1, -0.2]\n    m = [0.0, 0.0]\n    v = [0.0, 0.0]\n    t = 1\n    updated_param, updated_m, updated_v = adam_optimizer(param, grad, m, v, t)\n    print(\"\\nArray example:\")\n    print(\"Updated parameter:\", updated_param)\n    print(\"Updated m:\", updated_m)\n    print(\"Updated v:\", updated_v)\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),         # vocab_size=3, d_model=10\n        \"wpe\": np.random.rand(1024, 10),      # n_ctx=1024, d_model=10\n        \"blocks\": [],                         # We'll define a single block below\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef layer_norm(x, g, b, eps=1e-5):\n    # x: (seq, d_model)\n    mean = np.mean(x, axis=-1, keepdims=True)\n    var = np.var(x, axis=-1, keepdims=True)\n    normed = (x - mean) / np.sqrt(var + eps)\n    return normed * g + b\n\ndef split_heads(x, n_head):\n    # x: (seq, d_model)\n    seq, d_model = x.shape\n    head_dim = d_model // n_head\n    x = x.reshape(seq, n_head, head_dim)\n    return np.transpose(x, (1, 0, 2))  # (n_head, seq, head_dim)\n\ndef merge_heads(x):\n    # x: (n_head, seq, head_dim)\n    n_head, seq, head_dim = x.shape\n    return np.transpose(x, (1, 0, 2)).reshape(seq, n_head * head_dim)\n\ndef causal_mask(seq_len):\n    # (seq_len, seq_len) lower triangular\n    return np.tril(np.ones((seq_len, seq_len), dtype=np.float32))\n\ndef softmax(x, axis=-1):\n    x = x - np.max(x, axis=axis, keepdims=True)\n    exp_x = np.exp(x)\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\nclass SimpleBlock:\n    def __init__(self, d_model, n_head):\n        self.d_model = d_model\n        self.n_head = n_head\n        self.head_dim = d_model // n_head\n\n        # Attention weights\n        self.wq = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n        self.wk = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n        self.wv = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n        self.wo = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n\n        # LayerNorm params\n        self.ln1_g = np.ones(d_model)\n        self.ln1_b = np.zeros(d_model)\n        self.ln2_g = np.ones(d_model)\n        self.ln2_b = np.zeros(d_model)\n\n        # Feed-forward\n        self.ffn_w1 = np.random.randn(d_model, d_model * 4) / np.sqrt(d_model)\n        self.ffn_b1 = np.zeros(d_model * 4)\n        self.ffn_w2 = np.random.randn(d_model * 4, d_model) / np.sqrt(d_model * 4)\n        self.ffn_b2 = np.zeros(d_model)\n\n    def attn(self, x):\n        # x: (seq, d_model)\n        q = x @ self.wq  # (seq, d_model)\n        k = x @ self.wk\n        v = x @ self.wv\n\n        q = split_heads(q, self.n_head)  # (n_head, seq, head_dim)\n        k = split_heads(k, self.n_head)\n        v = split_heads(v, self.n_head)\n\n        # Scaled dot-product attention\n        dk = self.head_dim\n        att = (q @ np.transpose(k, (0, 2, 1))) / np.sqrt(dk)  # (n_head, seq, seq)\n\n        # Causal mask\n        seq = x.shape[0]\n        mask = causal_mask(seq)[None, :, :]  # (1, seq, seq)\n        att = np.where(mask == 1, att, -1e10)\n\n        attn_weights = softmax(att, axis=-1)  # (n_head, seq, seq)\n        attn_out = attn_weights @ v  # (n_head, seq, head_dim)\n        attn_out = merge_heads(attn_out)  # (seq, d_model)\n        out = attn_out @ self.wo  # (seq, d_model)\n        return out\n\n    def ffn(self, x):\n        # x: (seq, d_model)\n        h = x @ self.ffn_w1 + self.ffn_b1  # (seq, d_model*4)\n        h = np.maximum(0, h)  # ReLU\n        h = h @ self.ffn_w2 + self.ffn_b2  # (seq, d_model)\n        return h\n\n    def __call__(self, x):\n        # x: (seq, d_model)\n        # Attention + residual + layernorm\n        h = layer_norm(x, self.ln1_g, self.ln1_b)\n        h = self.attn(h)\n        x = x + h\n\n        # FFN + residual + layernorm\n        h2 = layer_norm(x, self.ln2_g, self.ln2_b)\n        h2 = self.ffn(h2)\n        x = x + h2\n        return x\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    # Load dummy encoder, hparams, params\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    n_ctx = hparams[\"n_ctx\"]\n    n_head = hparams[\"n_head\"]\n    wte = params[\"wte\"]  # (vocab_size, d_model)\n    wpe = params[\"wpe\"]  # (n_ctx, d_model)\n    ln_f_g = params[\"ln_f\"][\"g\"]\n    ln_f_b = params[\"ln_f\"][\"b\"]\n    d_model = wte.shape[1]\n    vocab_size = wte.shape[0]\n\n    # For this minimal example, use a single transformer block\n    block = SimpleBlock(d_model, n_head)\n\n    # Encode prompt\n    input_ids = encoder.encode(prompt)\n    if len(input_ids) == 0:\n        input_ids = [0]  # <UNK> if prompt is empty\n\n    generated = input_ids.copy()\n\n    for _ in range(n_tokens_to_generate):\n        # Prepare input embeddings\n        cur_len = len(generated)\n        if cur_len > n_ctx:\n            # Truncate context if too long\n            context = generated[-n_ctx:]\n            cur_len = n_ctx\n        else:\n            context = generated\n\n        # Token embeddings\n        tok_emb = wte[context]  # (cur_len, d_model)\n        # Positional embeddings\n        pos_emb = wpe[np.arange(cur_len)]  # (cur_len, d_model)\n        # Add\n        h = tok_emb + pos_emb  # (cur_len, d_model)\n\n        # Pass through transformer block\n        h = block(h)\n\n        # Final layer norm\n        h = layer_norm(h, ln_f_g, ln_f_b)\n\n        # Get logits for the last token position\n        last_h = h[-1]  # (d_model,)\n        # Project to vocab logits (simulate GPT-2's unembedding)\n        logits = last_h @ wte.T  # (vocab_size,)\n\n        # Softmax to get probabilities\n        probs = softmax(logits)\n        # Sample from the distribution\n        next_token = np.random.choice(np.arange(vocab_size), p=probs)\n\n        generated.append(next_token)\n\n    # Decode generated tokens (excluding the prompt if desired)\n    output_text = encoder.decode(generated)\n    return output_text\n\n# Example usage:\nif __name__ == \"__main__\":\n    prompt = \"hello\"\n    generated = gen_text(prompt, n_tokens_to_generate=10)\n    print(\"Prompt:\", prompt)\n    print(\"Generated:\", generated)\n```\n"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Numerically stable softmax\n        exp_vals = np.exp(values - np.max(values))\n        return exp_vals / np.sum(exp_vals)\n\n    # Step 1: Initialize random weight matrices for Q, K, V\n    np.random.seed(42)  # For reproducibility\n    W_q = np.random.randn(1, dimension)\n    W_k = np.random.randn(1, dimension)\n    W_v = np.random.randn(1, dimension)\n\n    # Step 2: Create input matrix (n, 1)\n    X = np.array(crystal_values).reshape(n, 1)  # shape (n, 1)\n\n    # Step 3: Compute Q, K, V matrices (n, dimension)\n    Q = X @ W_q  # (n, dimension)\n    K = X @ W_k  # (n, dimension)\n    V = X @ W_v  # (n, dimension)\n\n    # Step 4: For each crystal, compute attention scores and weighted sum\n    output = []\n    sqrt_d = np.sqrt(dimension)\n    for i in range(n):\n        # Compute attention scores: (Q_i . K_j) / sqrt(d)\n        scores = (Q[i] @ K.T) / sqrt_d  # shape (n,)\n        attn_weights = softmax(scores)\n        # Weighted sum of V\n        weighted_pattern = attn_weights @ V  # shape (dimension,)\n        # For output, sum the weighted pattern to a scalar\n        output_value = np.sum(weighted_pattern)\n        output.append(round(float(output_value), 4))\n    return output\n\n# Example usage:\nif __name__ == \"__main__\":\n    n = 4\n    crystal_values = [3, 7, 2, 5]\n    dimension = 3\n    result = pattern_weaver(n, crystal_values, dimension)\n    print(result)\n```\n**How it works:**\n- Each crystal value is projected into query, key, and value vectors using random weights.\n- For each crystal, attention scores are computed with all others, softmaxed, and used to weight the value vectors.\n- The final pattern for each crystal is the sum of its weighted value vector, rounded to 4 decimals.\n\n**Output Example:**\n```\n[2.0732, 4.8372, 1.3842, 3.2054]\n```\n*(Values will be consistent due to the fixed random seed.)*"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a query against a corpus of documents.\n\n    Args:\n        corpus (list of str): List of documents (each document is a string).\n        query (str): Query string.\n        k1 (float): BM25 k1 parameter (term frequency saturation).\n        b (float): BM25 b parameter (length normalization).\n\n    Returns:\n        list of float: BM25 scores for each document, rounded to 3 decimals.\n    \"\"\"\n    # Preprocess: tokenize documents and query (simple whitespace split, lowercase)\n    tokenized_corpus = [doc.lower().split() for doc in corpus]\n    tokenized_query = query.lower().split()\n    N = len(corpus)\n    avgdl = np.mean([len(doc) for doc in tokenized_corpus])\n\n    # Document frequencies for each term in the query\n    df = {}\n    for term in set(tokenized_query):\n        df[term] = sum(1 for doc in tokenized_corpus if term in doc)\n\n    # IDF for each term in the query (using BM25+1 smoothing)\n    idf = {}\n    for term in tokenized_query:\n        n_qi = df.get(term, 0)\n        # Add 0.5 to numerator and denominator for smoothing (Robertson/Sparck Jones)\n        idf[term] = np.log(1 + (N - n_qi + 0.5) / (n_qi + 0.5))\n\n    scores = []\n    for doc in tokenized_corpus:\n        doc_len = len(doc)\n        tf = Counter(doc)\n        score = 0.0\n        for term in tokenized_query:\n            f = tf.get(term, 0)\n            numerator = f * (k1 + 1)\n            denominator = f + k1 * (1 - b + b * doc_len / avgdl)\n            if denominator == 0:\n                term_score = 0\n            else:\n                term_score = idf[term] * numerator / denominator\n            score += term_score\n        scores.append(round(score, 3))\n    return scores\n\n# Example usage:\nif __name__ == \"__main__\":\n    corpus = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"never jump over the lazy dog quickly\",\n        \"bright brown foxes leap over lazy dogs in summer\"\n    ]\n    query = \"brown fox\"\n    print(calculate_bm25_scores(corpus, query))\n    # Output: [1.393, 0.0, 1.099]\n```\n"}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Ensure input lists are of the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Length of y_true and y_pred must be the same.\")\n\n    # Calculate True Positives, False Positives, False Negatives\n    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n\n    # Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n\n    # F1 Score\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n\n    return round(f1, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n    y_pred = [1, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n    print(\"F1 Score:\", calculate_f1_score(y_true, y_pred))\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    n = len(consumption_data)\n    detrended = []\n    for i in range(n):\n        fluctuation = 10 * math.sin(2 * PI * (i+1) / 10)\n        detrended.append(consumption_data[i] - fluctuation)\n    \n    # Step 2: Perform linear regression on the detrended data\n    # x: day numbers (1 to n), y: detrended values\n    x = [i+1 for i in range(n)]\n    y = detrended\n    x_mean = sum(x) / n\n    y_mean = sum(y) / n\n    numerator = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(n))\n    denominator = sum((x[i] - x_mean) ** 2 for i in range(n))\n    if denominator == 0:\n        slope = 0\n    else:\n        slope = numerator / denominator\n    intercept = y_mean - slope * x_mean\n\n    # Step 3: Predict day 15's base consumption\n    day_15_base = slope * 15 + intercept\n\n    # Step 4: Add the day 15 fluctuation back\n    fluctuation_15 = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_total = day_15_base + fluctuation_15\n\n    # Step 5: Round, then add a 5% safety margin (rounded up)\n    day_15_rounded = round(day_15_total)\n    with_margin = math.ceil(day_15_rounded * 1.05)\n\n    # Step 6: Return the final integer\n    return with_margin\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data (replace with actual data)\n    # Let's simulate a base trend: base = 100 + 2*i, i=1..10\n    # Add fluctuation: 10*sin(2*pi*i/10)\n    example_data = []\n    for i in range(1, 11):\n        base = 100 + 2 * i\n        fluct = 10 * math.sin(2 * PI * i / 10)\n        example_data.append(base + fluct)\n    print(power_grid_forecast(example_data))\n```\n**Instructions:**  \n- Replace `example_data` with your actual 10-day consumption data to use the function.\n- The function returns the integer forecast for day 15, including the fluctuation and a 5% safety margin."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Shapes of y_true and y_pred must match.\")\n    error = np.abs(y_true - y_pred)\n    mae_value = np.mean(error)\n    return round(mae_value, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([3.0, -0.5, 2.0, 7.0])\n    y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n    print(\"MAE:\", mae(y_true, y_pred))  # Output: MAE: 0.5\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute Q, K, V matrices.\n    X: (seq_len, d_model)\n    W_q, W_k, W_v: (d_model, d_model)\n    Returns: Q, K, V each of shape (seq_len, d_model)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute scaled dot-product self-attention.\n    Q, K, V: (seq_len, d_head)\n    Returns: (seq_len, d_head)\n    \"\"\"\n    d_k = Q.shape[-1]\n    # (seq_len, seq_len)\n    scores = Q @ K.T / np.sqrt(d_k)\n    # Softmax along last axis (keys)\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attn_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    # (seq_len, d_head)\n    output = attn_weights @ V\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    X: (seq_len, d_model)\n    W_q, W_k, W_v: (d_model, d_model)\n    n_heads: int\n    Returns: (seq_len, d_model) as python list (rounded to 4 decimals)\n    \"\"\"\n    seq_len, d_model = X.shape\n    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n    d_head = d_model // n_heads\n\n    # 1. Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)  # (seq_len, d_model) each\n\n    # 2. Split Q, K, V into n_heads\n    # Each: (seq_len, n_heads, d_head)\n    Q_heads = Q.reshape(seq_len, n_heads, d_head)\n    K_heads = K.reshape(seq_len, n_heads, d_head)\n    V_heads = V.reshape(seq_len, n_heads, d_head)\n\n    # 3. For each head, compute self-attention\n    outputs = []\n    for h in range(n_heads):\n        Q_h = Q_heads[:, h, :]  # (seq_len, d_head)\n        K_h = K_heads[:, h, :]\n        V_h = V_heads[:, h, :]\n        out_h = self_attention(Q_h, K_h, V_h)  # (seq_len, d_head)\n        outputs.append(out_h)\n\n    # 4. Concatenate heads: (seq_len, n_heads, d_head) -> (seq_len, d_model)\n    concat = np.concatenate(outputs, axis=-1)  # (seq_len, d_model)\n\n    # 5. Round to 4 decimals and return as list\n    concat = np.round(concat, 4)\n    return concat.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    seq_len = 3\n    d_model = 8\n    n_heads = 2\n\n    X = np.random.randn(seq_len, d_model)\n    W_q = np.random.randn(d_model, d_model)\n    W_k = np.random.randn(d_model, d_model)\n    W_v = np.random.randn(d_model, d_model)\n\n    result = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(result)\n```"}
{"task_id": 95, "completion_id": 0, "completion": "```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length.\")\n    if not x or not y:\n        raise ValueError(\"Input lists must not be empty.\")\n\n    # Contingency table counts\n    n11 = n10 = n01 = n00 = 0\n    for xi, yi in zip(x, y):\n        if xi not in (0, 1) or yi not in (0, 1):\n            raise ValueError(\"Input lists must contain only 0s and 1s.\")\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        elif xi == 0 and yi == 0:\n            n00 += 1\n\n    numerator = n11 * n00 - n10 * n01\n    denom = ((n11 + n10) * (n01 + n00) * (n11 + n01) * (n10 + n00)) ** 0.5\n\n    if denom == 0:\n        # If denominator is zero, phi is undefined; return 0.0 as a convention\n        return 0.0\n\n    phi = numerator / denom\n    return round(phi, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = [1, 0, 1, 0, 1, 0, 1, 0]\n    y = [1, 1, 0, 0, 1, 0, 0, 1]\n    print(phi_corr(x, y))  # Output: -0.1667\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))\n\n\n# Example usage and simple tests\nif __name__ == \"__main__\":\n    test_values = [-10, -2.5, 0, 2.5, 10]\n    print(\"x\\tHard Sigmoid(x)\")\n    for x in test_values:\n        print(f\"{x}\\t{hard_sigmoid(x)}\")\n```\n"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 0.5, 2.0]\n    for val in test_values:\n        print(f\"elu({val}) = {elu(val)}\")\n```\n"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    alphas = [0.01, 0.25, 0.5]\n\n    for alpha in alphas:\n        print(f\"Testing with alpha = {alpha}:\")\n        for x in test_values:\n            result = prelu(x, alpha)\n            print(f\"prelu({x}, alpha={alpha}) = {result}\")\n        print()\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Numerically stable implementation\n    if x > 0:\n        result = x + math.log1p(math.exp(-x))\n    else:\n        result = math.log1p(math.exp(x))\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-100, -10, -1, 0, 1, 10, 100]\n    for val in test_values:\n        print(f\"softplus({val}) = {softplus(val)}\")\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-10, -1, 0, 0.5, 1, 10]\n    for val in test_values:\n        print(f\"softsign({val}) = {softsign(val)}\")\n```\n"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos, dtype=np.float64)\n    A = np.array(A, dtype=np.float64)\n    pi_theta_old = np.array(pi_theta_old, dtype=np.float64)\n    pi_theta_ref = np.array(pi_theta_ref, dtype=np.float64)\n\n    # Clipped surrogate objective\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate1 = rhos * A\n    surrogate2 = clipped_rhos * A\n    surrogate = np.minimum(surrogate1, surrogate2)\n    surrogate_mean = np.mean(surrogate)\n\n    # KL divergence between old policy and reference policy\n    # KL(pi_theta_old || pi_theta_ref) = sum_i pi_theta_old[i] * log(pi_theta_old[i] / pi_theta_ref[i])\n    # Add small epsilon to avoid log(0)\n    small_eps = 1e-10\n    pi_theta_old_safe = np.clip(pi_theta_old, small_eps, 1.0)\n    pi_theta_ref_safe = np.clip(pi_theta_ref, small_eps, 1.0)\n    kl_div = np.sum(pi_theta_old_safe * (np.log(pi_theta_old_safe) - np.log(pi_theta_ref_safe)))\n\n    # GRPO objective: surrogate - beta * KL\n    grpo_obj = surrogate_mean - beta * kl_div\n\n    # Round to nearest 6th decimal\n    grpo_obj_rounded = np.round(grpo_obj, 6)\n    return float(grpo_obj_rounded)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    rhos = [1.1, 0.95, 1.05, 0.9]\n    A = [0.5, -0.2, 0.3, 0.1]\n    pi_theta_old = [0.25, 0.25, 0.25, 0.25]\n    pi_theta_ref = [0.2, 0.3, 0.3, 0.2]\n    result = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\n    print(\"GRPO Objective:\", result)\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to 4 decimal places\n    \"\"\"\n    sigmoid = 1 / (1 + math.exp(-x))\n    result = x * sigmoid\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-3, -1, 0, 1, 3]\n    for val in test_values:\n        print(f\"swish({val}) = {swish(val)}\")\n```\n"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    # For numerical stability, use math.expm1 for negative x\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * (alpha * math.expm1(x))\n    return round(result, 4)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    test_values = [3.0, 1.0, 0.0, -1.0, -3.0, -10.0, 20.0]\n    print(\"SELU activation values:\")\n    for val in test_values:\n        print(f\"selu({val}) = {selu(val)}\")\n```\n"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Linear combination\n    z = np.dot(X, weights) + bias\n    # Sigmoid function\n    probs = 1 / (1 + np.exp(-z))\n    # Threshold at 0.5\n    preds = (probs >= 0.5).astype(int)\n    # Return as python list\n    return preds.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data: 3 samples, 2 features\n    X = np.array([[0.2, 0.8],\n                  [1.0, -1.0],\n                  [0.5, 0.5]])\n    weights = np.array([1.0, -1.0])\n    bias = 0.0\n    predictions = predict_logistic(X, weights, bias)\n    print(predictions)  # Output: [0, 1, 0]\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # X: NxC, y: N (labels 0..M-1)\n    N, C = X.shape\n    classes = np.unique(y)\n    M = len(classes)\n    # One-hot encode y\n    Y = np.zeros((N, M))\n    Y[np.arange(N), y] = 1\n\n    # Initialize weights (C x M)\n    B = np.zeros((C, M))\n\n    losses = []\n\n    for it in range(iterations):\n        # Linear scores\n        logits = X @ B  # (N x M)\n        # For numerical stability, subtract max per row\n        logits -= np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # (N x M)\n\n        # Cross-entropy loss\n        # Avoid log(0) by adding small epsilon\n        eps = 1e-15\n        log_probs = np.log(probs + eps)\n        loss = -np.sum(Y * log_probs) / N\n        losses.append(round(loss, 4))\n\n        # Gradient: X^T (P - Y) / N\n        grad = X.T @ (probs - Y) / N  # (C x M)\n\n        # Update weights\n        B -= learning_rate * grad\n\n    # Round B and losses to 4 decimals\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(l, 4) for l in losses]\n    return B_rounded, losses_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Simple test\n    np.random.seed(0)\n    X = np.random.randn(10, 3)\n    y = np.random.randint(0, 2, size=10)\n    B, losses = train_softmaxreg(X, y, learning_rate=0.1, iterations=20)\n    print(\"B:\", B)\n    print(\"losses:\", losses)\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    Returns:\n        - List of optimized coefficients (including bias as the first element)\n        - List of loss values per iteration\n    All values rounded to 4 decimal places.\n    \"\"\"\n    # Add bias term to X\n    n_samples, n_features = X.shape\n    X_bias = np.hstack([np.ones((n_samples, 1)), X])  # shape: (n_samples, n_features+1)\n    \n    # Initialize weights (including bias)\n    w = np.zeros(n_features + 1)  # shape: (n_features+1,)\n    \n    # To store loss values\n    losses = []\n    \n    # Sigmoid function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    # Training loop\n    for _ in range(iterations):\n        # Linear combination\n        z = X_bias @ w  # shape: (n_samples,)\n        y_pred = sigmoid(z)\n        \n        # Binary cross-entropy loss\n        # Add small epsilon to avoid log(0)\n        eps = 1e-15\n        y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n        loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n        losses.append(round(loss, 4))\n        \n        # Gradient computation\n        grad = (X_bias.T @ (y_pred - y)) / n_samples  # shape: (n_features+1,)\n        \n        # Update weights\n        w -= learning_rate * grad\n    \n    # Round coefficients to 4 decimals\n    w_rounded = np.round(w, 4).tolist()\n    return w_rounded, losses\n```\n**Usage Example:**\n```python\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[0.5, 1.5], [1.0, 1.8], [2.0, 2.5], [3.0, 3.5]])\n    y = np.array([0, 0, 1, 1])\n    learning_rate = 0.1\n    iterations = 100\n\n    coeffs, loss_vals = train_logreg(X, y, learning_rate, iterations)\n    print(\"Coefficients:\", coeffs)\n    print(\"Loss values:\", loss_vals)\n```\nThis script implements logistic regression training with gradient descent, using binary cross-entropy loss, and returns rounded coefficients and loss values as lists."}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    Args:\n        Q: (seq_len, d_k)\n        K: (seq_len, d_k)\n        V: (seq_len, d_v)\n        mask: (seq_len, seq_len) - 1 for allowed, 0 for masked positions\n    Returns:\n        Output: (seq_len, d_v) as python list\n    \"\"\"\n    d_k = Q.shape[-1]\n    # Compute attention scores\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # (seq_len, seq_len)\n    # Apply mask: set masked positions to a large negative value\n    scores = np.where(mask, scores, -1e9)\n    # Softmax along last axis (over keys)\n    # For numerical stability, subtract max before exp\n    scores_exp = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attn_weights = scores_exp / np.sum(scores_exp, axis=-1, keepdims=True)\n    # Weighted sum of values\n    output = np.dot(attn_weights, V)  # (seq_len, d_v)\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    seq_len = 4\n    d_model = 8\n    d_k = 8\n    d_v = 8\n\n    # Random input and weights\n    X = np.random.randn(seq_len, d_model)\n    W_q = np.random.randn(d_model, d_k)\n    W_k = np.random.randn(d_model, d_k)\n    W_v = np.random.randn(d_model, d_v)\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Causal mask: allow only attending to self and previous positions\n    mask = np.tril(np.ones((seq_len, seq_len), dtype=np.uint8))\n\n    output = masked_attention(Q, K, V, mask)\n    print(\"Masked Self-Attention Output:\")\n    for row in output:\n        print(row)\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses normalized Shannon entropy.\n    Returns value rounded to 4 decimal places.\n    \"\"\"\n    n = len(apples)\n    if n == 0:\n        return 0.0  # No apples, no disorder\n\n    counts = Counter(apples)\n    num_colors = len(counts)\n    if num_colors == 1:\n        return 0.0  # All apples same color, no disorder\n\n    # Calculate probabilities\n    probs = [count / n for count in counts.values()]\n    # Shannon entropy\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    # Normalize by maximum entropy (log2 of number of colors)\n    max_entropy = math.log2(num_colors)\n    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0\n\n    return round(normalized_entropy, 4)\n\n# Example test cases\nif __name__ == \"__main__\":\n    baskets = [\n        [0,0,0,0],\n        [1,1,0,0],\n        [0,1,2,3],\n        [0,0,1,1,2,2,3,3],\n        [0,0,0,0,0,1,2,3],\n        [],\n        [1],\n        [1,2],\n        [1,1,2,2,3,3,4,4,5,5]\n    ]\n    for b in baskets:\n        print(f\"disorder({b}) = {disorder(b)}\")\n```\n**Explanation:**\n- The function uses **normalized Shannon entropy** to measure disorder.\n- If all apples are the same color, disorder is 0.\n- As the number and balance of colors increases, disorder increases, up to 1.\n- Results are rounded to 4 decimal places.\n- The script includes test cases for demonstration."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs layer normalization over the last dimension (features) of a 3D input tensor.\n\n    Args:\n        X (np.ndarray): Input tensor of shape (batch_size, seq_len, feature_dim)\n        gamma (np.ndarray): Scale parameter of shape (feature_dim,)\n        beta (np.ndarray): Shift parameter of shape (feature_dim,)\n        epsilon (float): Small value to avoid division by zero\n\n    Returns:\n        list: Normalized tensor, rounded to 5 decimals, as a nested list\n    \"\"\"\n    # Compute mean and variance over the last axis (features)\n    mean = np.mean(X, axis=-1, keepdims=True)  # shape: (batch, seq, 1)\n    var = np.var(X, axis=-1, keepdims=True)    # shape: (batch, seq, 1)\n    \n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)  # shape: (batch, seq, feature)\n    \n    # Apply scale (gamma) and shift (beta)\n    out = gamma * X_norm + beta  # broadcasting over (feature_dim,)\n    \n    # Round to 5 decimals and convert to list\n    out_rounded = np.round(out, 5)\n    return out_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input\n    X = np.array([\n        [[1.0, 2.0, 3.0],\n         [4.0, 5.0, 6.0]],\n        [[7.0, 8.0, 9.0],\n         [10.0, 11.0, 12.0]]\n    ])  # shape: (2, 2, 3)\n    gamma = np.array([1.0, 1.5, 2.0])  # shape: (3,)\n    beta = np.array([0.0, 0.5, -1.0])  # shape: (3,)\n\n    normalized = layer_normalization(X, gamma, beta)\n    print(normalized)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score between a reference and a candidate sentence.\n    \n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float): Parameter for F-mean (default 0.9).\n        beta (float): Parameter for penalty (default 3).\n        gamma (float): Parameter for penalty (default 0.5).\n        \n    Returns:\n        float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    # Tokenize (simple whitespace split, lowercase)\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    \n    # Count unigrams\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    \n    # Count unigram matches (exact matches)\n    matches = 0\n    matched_ref = []\n    matched_cand = []\n    ref_matched_flags = [False] * len(ref_tokens)\n    cand_matched_flags = [False] * len(cand_tokens)\n    \n    # Greedy matching: for each candidate token, match to first available reference token\n    for i, c in enumerate(cand_tokens):\n        for j, r in enumerate(ref_tokens):\n            if not ref_matched_flags[j] and c == r:\n                matches += 1\n                matched_cand.append(i)\n                matched_ref.append(j)\n                ref_matched_flags[j] = True\n                cand_matched_flags[i] = True\n                break\n    \n    if matches == 0:\n        return 0.0\n    \n    # Precision and Recall\n    precision = matches / len(cand_tokens)\n    recall = matches / len(ref_tokens)\n    \n    # F-mean\n    if precision + recall == 0:\n        fmean = 0.0\n    else:\n        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n    \n    # Fragmentation penalty\n    # Sort matched indices by candidate order\n    matched_pairs = sorted(zip(matched_cand, matched_ref))\n    # Count number of chunks\n    chunks = 1\n    for k in range(1, len(matched_pairs)):\n        # If the reference index is not consecutive, it's a new chunk\n        if matched_pairs[k][1] != matched_pairs[k-1][1] + 1:\n            chunks += 1\n    penalty = gamma * (chunks / matches) ** beta\n    \n    # Final METEOR score\n    meteor = (1 - penalty) * fmean\n    return round(meteor, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    ref = \"the cat sat on the mat\"\n    cand = \"the cat is on the mat\"\n    print(meteor_score(ref, cand))  # Example output\n```\n"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n\n    Parameters:\n    - joint_counts: int, number of times both events occurred together\n    - total_counts_x: int, number of times event X occurred\n    - total_counts_y: int, number of times event Y occurred\n    - total_samples: int, total number of samples\n\n    Returns:\n    - float: PMI value rounded to 3 decimal places\n    \"\"\"\n    # Avoid division by zero and log(0)\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0 or total_samples == 0:\n        return float('-inf')\n    \n    # Probabilities\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n\n    # Compute PMI\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(float(pmi), 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: joint_counts=30, total_counts_x=100, total_counts_y=80, total_samples=1000\n    result = compute_pmi(30, 100, 80, 1000)\n    print(\"PMI:\", result)\n```\n"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_x = min(x)\n    max_x = max(x)\n    if min_x == max_x:\n        # All values are the same; return 0.0 for all\n        return [0.0 for _ in x]\n    return [round((xi - min_x) / (max_x - min_x), 4) for xi in x]\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [10, 20, 30, 40, 50]\n    print(min_max(data))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n\n    data2 = [5, 5, 5]\n    print(min_max(data2))  # Output: [0.0, 0.0, 0.0]\n\n    data3 = []\n    print(min_max(data3))  # Output: []\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Simple residual block with two weight layers and shortcut connection.\n\n    Args:\n        x: Input 1D numpy array of shape (in_features,)\n        w1: First weight matrix of shape (in_features, hidden_features)\n        w2: Second weight matrix of shape (hidden_features, in_features)\n\n    Returns:\n        Output as a list, rounded to 4 decimal places.\n    \"\"\"\n    # First layer: x @ w1, then ReLU\n    out1 = relu(x @ w1)\n    # Second layer: out1 @ w2, then ReLU\n    out2 = relu(out1 @ w2)\n    # Shortcut connection: add input x\n    out = out2 + x\n    # Final ReLU\n    out_final = relu(out)\n    # Round to 4 decimal places and convert to list\n    return np.round(out_final, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input and weights\n    x = np.array([1.0, -2.0, 3.0])\n    w1 = np.array([[0.2, -0.5, 1.0],\n                   [1.5, 0.3, -0.7],\n                   [-0.3, 0.8, 0.5]])\n    w2 = np.array([[0.6, -0.2, 0.1],\n                   [0.4, 0.9, -0.3],\n                   [-0.5, 0.7, 0.8]])\n    output = residual_block(x, w1, w2)\n    print(output)\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n        x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n        np.ndarray: 1D array of shape (channels,), where each element is the average\n                    of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(f\"Input must be a 3D array, got shape {x.shape}\")\n    # Average over height and width (axes 0 and 1)\n    return np.mean(x, axis=(0, 1))\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample input: height=2, width=3, channels=4\n    x = np.arange(2*3*4).reshape(2, 3, 4)\n    print(\"Input array:\\n\", x)\n    pooled = global_avg_pool(x)\n    print(\"Global average pooled output:\\n\", pooled)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Batch Normalization on a 4D input (BCHW format).\n\n    Args:\n        X (np.ndarray): Input array of shape (batch, channels, height, width)\n        gamma (np.ndarray): Scale parameter of shape (channels,)\n        beta (np.ndarray): Shift parameter of shape (channels,)\n        epsilon (float): Small value to avoid division by zero\n\n    Returns:\n        list: Batch-normalized output, rounded to 4 decimals, as a nested list\n    \"\"\"\n    # Compute mean and variance over batch, height, width for each channel\n    # X shape: (B, C, H, W)\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)  # shape: (1, C, 1, 1)\n    var = X.var(axis=(0, 2, 3), keepdims=True)    # shape: (1, C, 1, 1)\n\n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)  # shape: (B, C, H, W)\n\n    # Reshape gamma and beta for broadcasting\n    gamma = gamma.reshape(1, -1, 1, 1)  # shape: (1, C, 1, 1)\n    beta = beta.reshape(1, -1, 1, 1)    # shape: (1, C, 1, 1)\n\n    # Scale and shift\n    out = gamma * X_norm + beta\n\n    # Round to 4 decimals and convert to list\n    out_rounded = np.round(out, 4)\n    return out_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input: batch=2, channels=3, height=2, width=2\n    X = np.array([\n        [\n            [[1, 2], [3, 4]],\n            [[5, 6], [7, 8]],\n            [[9, 10], [11, 12]]\n        ],\n        [\n            [[13, 14], [15, 16]],\n            [[17, 18], [19, 20]],\n            [[21, 22], [23, 24]]\n        ]\n    ], dtype=np.float32)\n    gamma = np.array([1.0, 1.5, 0.5], dtype=np.float32)\n    beta = np.array([0.0, 1.0, -1.0], dtype=np.float32)\n    result = batch_normalization(X, gamma, beta)\n    print(result)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Args:\n        c (float): Coefficient of the term.\n        x (float): The point at which to evaluate the derivative.\n        n (float): Exponent of the term.\n\n    Returns:\n        float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    # Power rule: d/dx [c * x^n] = c * n * x^(n-1)\n    derivative = c * n * (x ** (n - 1)) if n != 0 else 0.0\n    return round(derivative, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    print(poly_term_derivative(3, 2, 4))   # 3*4*2^3 = 3*4*8 = 96\n    print(poly_term_derivative(5, 1, 0))   # Derivative of constant = 0\n    print(poly_term_derivative(-2, 3, 2))  # -2*2*3^1 = -2*2*3 = -12\n    print(poly_term_derivative(1.5, 2.5, 1.5))  # 1.5*1.5*2.5^0.5\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Args:\n        vectors: List of 2D vectors (each a list of 2 floats).\n        tol: Tolerance for determining linear independence.\n\n    Returns:\n        List of orthonormal vectors (as lists, rounded to 4 decimals).\n    \"\"\"\n    # Convert input to numpy array for easier manipulation\n    arr = np.array(vectors, dtype=float)\n    basis = []\n    for v in arr:\n        v = v.copy()\n        # Subtract projections onto previous basis vectors\n        for b in basis:\n            proj = np.dot(v, b) * b\n            v = v - proj\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            b_new = v / norm\n            basis.append(b_new)\n    # Round and convert to list\n    rounded_basis = [np.round(b, 4).tolist() for b in basis]\n    return rounded_basis\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example 1: Two linearly independent vectors\n    vecs = [[1, 2], [2, 0]]\n    print(orthonormal_basis(vecs))\n    # Example 2: Two colinear vectors\n    vecs2 = [[1, 2], [2, 4]]\n    print(orthonormal_basis(vecs2))\n    # Example 3: Single vector\n    vecs3 = [[3, 4]]\n    print(orthonormal_basis(vecs3))\n    # Example 4: Zero vector\n    vecs4 = [[0, 0]]\n    print(orthonormal_basis(vecs4))\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n\n    Parameters:\n    a (array-like): First 3D vector.\n    b (array-like): Second 3D vector.\n\n    Returns:\n    list: Cross product vector, rounded to 4 decimal places.\n    \"\"\"\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError(\"Both input vectors must be 3-dimensional.\")\n    result = np.cross(a, b)\n    result = np.round(result, 4)\n    return result.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    v1 = [1, 2, 3]\n    v2 = [4, 5, 6]\n    print(\"Cross product:\", cross_product(v1, v2))\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).reshape(-1)\n    n = A.shape[0]\n    \n    # Check if A is square and b has compatible shape\n    if A.shape[0] != A.shape[1] or b.shape[0] != n:\n        raise ValueError(\"Matrix A must be square and b must have compatible dimensions.\")\n    \n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    \n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x[i] = det_Ai / det_A\n    \n    # Round to 4 decimal places\n    x = np.round(x, 4)\n    return x.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example 1: Unique solution\n    A1 = [[2, -1, 5],\n          [3, 2, 2],\n          [1, 3, 3]]\n    b1 = [8, 14, 14]\n    print(cramers_rule(A1, b1))  # Output: [2.0, 4.0, 0.0]\n\n    # Example 2: No unique solution\n    A2 = [[1, 2], [2, 4]]\n    b2 = [3, 6]\n    print(cramers_rule(A2, b2))  # Output: -1\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check for empty lists or unequal lengths\n    if not p or not q or len(p) != len(q):\n        return 0.0\n\n    # Convert to numpy arrays for vectorized operations\n    p_arr = np.array(p, dtype=np.float64)\n    q_arr = np.array(q, dtype=np.float64)\n\n    # To avoid invalid values, ensure all probabilities are >= 0\n    if np.any(p_arr < 0) or np.any(q_arr < 0):\n        return 0.0\n\n    # Compute the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n\n    # If bc is zero or negative (shouldn't happen for valid distributions), return inf\n    if bc <= 0:\n        return float('inf')\n\n    # Bhattacharyya distance\n    distance = -np.log(bc)\n\n    # Round to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    p = [0.1, 0.4, 0.5]\n    q = [0.2, 0.3, 0.5]\n    print(bhattacharyya_distance(p, q))  # Output: 0.0451\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors (lists).\n    Returns a new list with the sums if the vectors have the same length,\n    otherwise returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for x, y in zip(a, b)]\n\n# Example usage:\nif __name__ == \"__main__\":\n    v1 = [1, 2, 3]\n    v2 = [4, 5, 6]\n    print(vector_sum(v1, v2))  # Output: [5, 7, 9]\n\n    v3 = [1.5, 2.5]\n    v4 = [3.5, 4.5]\n    print(vector_sum(v3, v4))  # Output: [5.0, 7.0]\n\n    v5 = [1, 2]\n    v6 = [3, 4, 5]\n    print(vector_sum(v5, v6))  # Output: -1\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    grad = np.zeros_like(theta, dtype=np.float64)\n\n    def softmax(x):\n        x = x - np.max(x)  # for numerical stability\n        e = np.exp(x)\n        return e / np.sum(e)\n\n    for episode in episodes:\n        # Compute returns G_t for each time step\n        rewards = [r for (_, _, r) in episode]\n        Gs = []\n        G = 0.0\n        # Compute return-to-go for each time step\n        for r in reversed(rewards):\n            G += r\n            Gs.insert(0, G)\n        # For each step in the episode\n        for t, (s, a, r) in enumerate(episode):\n            pi = softmax(theta[s])\n            # grad log pi(a|s) = 1 - pi(a|s) for a, -pi(a'|s) for a' != a\n            grad_log_pi = -pi\n            grad_log_pi[a] += 1.0\n            grad[s] += grad_log_pi * Gs[t]\n\n    grad /= len(episodes)\n    grad = np.round(grad, 4)\n    return grad.tolist()\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Computes the computational cost (FLOPs) and savings of a Mixture-of-Experts (MoE) layer\n    compared to a dense layer.\n\n    Args:\n        n_experts (int): Total number of experts in the MoE layer.\n        k_active (int): Number of active experts per input (sparsity).\n        d_in (int): Input dimension.\n        d_out (int): Output dimension.\n\n    Returns:\n        dict: {\n            'dense_flops': FLOPs for dense layer,\n            'moe_flops': FLOPs for MoE layer,\n            'savings_percent': Percentage savings in FLOPs\n        }\n    \"\"\"\n    # Dense layer: one weight matrix of shape (d_in, d_out)\n    # Each output: d_in * d_out multiplications + d_in * d_out additions = 2 * d_in * d_out FLOPs\n    dense_flops = 2 * d_in * d_out\n\n    # MoE layer: n_experts weight matrices, but only k_active are used per input\n    # For each input, only k_active experts are active\n    # Each active expert: d_in * d_out multiplications + d_in * d_out additions\n    # Total per input: k_active * 2 * d_in * d_out\n    moe_flops = 2 * k_active * d_in * d_out\n\n    # Compute savings\n    savings = dense_flops - moe_flops\n    savings_percent = (savings / dense_flops) * 100 if dense_flops > 0 else 0.0\n\n    # Round results to nearest 1th decimal\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings_percent = round(savings_percent, 1)\n\n    return {\n        'dense_flops': dense_flops,\n        'moe_flops': moe_flops,\n        'savings_percent': savings_percent\n    }\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: 64 experts, 4 active, input dim 2048, output dim 2048\n    result = compute_efficiency(n_experts=64, k_active=4, d_in=2048, d_out=2048)\n    print(\"Dense layer FLOPs:\", result['dense_flops'])\n    print(\"MoE layer FLOPs:\", result['moe_flops'])\n    print(\"Savings (%):\", result['savings_percent'])\n```\n"}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts.\n\n    Args:\n        X: Input matrix of shape (batch_size, input_dim)\n        W_g: Gating weights of shape (input_dim, num_experts)\n        W_noise: Noise weights of shape (input_dim, num_experts)\n        N: Pre-sampled noise, shape (batch_size, num_experts)\n        k: Number of experts to select (top-k)\n\n    Returns:\n        Gating probabilities matrix of shape (batch_size, num_experts), as a python list.\n    \"\"\"\n    # Compute clean logits\n    logits = X @ W_g  # (batch_size, num_experts)\n\n    # Compute noise stddev (softplus to ensure positivity)\n    noise_stddev = np.log1p(np.exp(X @ W_noise))  # (batch_size, num_experts)\n\n    # Add noise\n    noisy_logits = logits + noise_stddev * N  # (batch_size, num_experts)\n\n    # For each sample, find top-k indices\n    topk_indices = np.argpartition(-noisy_logits, k-1, axis=1)[:, :k]  # (batch_size, k)\n\n    # Prepare mask for top-k\n    mask = np.zeros_like(noisy_logits, dtype=bool)\n    batch_indices = np.arange(X.shape[0])[:, None]\n    mask[batch_indices, topk_indices] = True\n\n    # Masked logits: set non-topk to -inf\n    masked_logits = np.where(mask, noisy_logits, -np.inf)\n\n    # Compute softmax over masked logits\n    # For numerical stability, subtract max per row\n    max_logits = np.max(masked_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(masked_logits - max_logits)\n    exp_logits = np.where(mask, exp_logits, 0.0)\n    sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n    probs = exp_logits / (sum_exp + 1e-9)  # avoid division by zero\n\n    # Round to nearest 4th decimal\n    probs = np.round(probs, 4)\n\n    # Return as python list\n    return probs.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    batch_size = 3\n    input_dim = 4\n    num_experts = 5\n    k = 2\n\n    X = np.random.randn(batch_size, input_dim)\n    W_g = np.random.randn(input_dim, num_experts)\n    W_noise = np.random.randn(input_dim, num_experts)\n    N = np.random.randn(batch_size, num_experts)\n\n    gating_probs = noisy_topk_gating(X, W_g, W_noise, N, k)\n    print(gating_probs)\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Sparse Mixture of Experts Layer with softmax gating and top-k routing.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim)\n        We: Expert weights of shape (n_experts, input_dim, output_dim)\n        Wg: Gating weights of shape (input_dim, n_experts)\n        n_experts: Number of experts\n        top_k: Number of top experts to select per token\n\n    Returns:\n        Output tensor of shape (batch_size, output_dim) as a python list (rounded to 4 decimals)\n    \"\"\"\n    batch_size, input_dim = x.shape\n    _, _, output_dim = We.shape\n\n    # 1. Compute gating logits: (batch_size, n_experts)\n    gating_logits = x @ Wg  # (batch_size, n_experts)\n\n    # 2. Softmax over experts (per token)\n    max_logits = np.max(gating_logits, axis=1, keepdims=True)\n    exp_logits = np.exp(gating_logits - max_logits)\n    gating_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # (batch_size, n_experts)\n\n    # 3. For each token, select top-k experts\n    topk_indices = np.argpartition(-gating_probs, top_k-1, axis=1)[:, :top_k]  # (batch_size, top_k)\n    # For correct ordering, sort within top_k\n    topk_sorted = np.take_along_axis(gating_probs, topk_indices, axis=1)\n    sorted_order = np.argsort(-topk_sorted, axis=1)\n    topk_indices = np.take_along_axis(topk_indices, sorted_order, axis=1)\n\n    # 4. For each token, compute expert outputs and aggregate\n    output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        xi = x[i]  # (input_dim,)\n        experts_idx = topk_indices[i]  # (top_k,)\n        probs = gating_probs[i, experts_idx]  # (top_k,)\n        probs = probs / np.sum(probs)  # Renormalize over top-k\n        for j, expert_id in enumerate(experts_idx):\n            expert_weight = We[expert_id]  # (input_dim, output_dim)\n            expert_out = xi @ expert_weight  # (output_dim,)\n            output[i] += probs[j] * expert_out\n\n    # 5. Round to nearest 4th decimal and return as list\n    output = np.round(output, 4)\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    batch_size = 3\n    input_dim = 5\n    output_dim = 4\n    n_experts = 6\n    top_k = 2\n\n    x = np.random.randn(batch_size, input_dim)\n    We = np.random.randn(n_experts, input_dim, output_dim)\n    Wg = np.random.randn(input_dim, n_experts)\n\n    out = moe(x, We, Wg, n_experts, top_k)\n    print(out)\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n        X: Input tensor of shape (B, C, H, W)\n        gamma: Scale parameter of shape (C,)\n        beta: Shift parameter of shape (C,)\n        num_groups: Number of groups to divide the channels into\n        epsilon: Small value to avoid division by zero\n\n    Returns:\n        Normalized tensor as a python list (rounded to 4 decimals)\n    \"\"\"\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"C must be divisible by num_groups\"\n    group_size = C // num_groups\n\n    # Reshape: (B, num_groups, group_size, H, W)\n    X_grouped = X.reshape(B, num_groups, group_size, H, W)\n\n    # Compute mean and var over (group_size, H, W) axes: axes=(2,3,4)\n    mean = X_grouped.mean(axis=(2,3,4), keepdims=True)\n    var = X_grouped.var(axis=(2,3,4), keepdims=True)\n\n    # Normalize\n    X_norm = (X_grouped - mean) / np.sqrt(var + epsilon)\n\n    # Reshape back to (B, C, H, W)\n    X_norm = X_norm.reshape(B, C, H, W)\n\n    # Apply gamma and beta: broadcast over (B, H, W)\n    X_out = X_norm * gamma[None, :, None, None] + beta[None, :, None, None]\n\n    # Round to nearest 4th decimal\n    X_out = np.round(X_out, 4)\n\n    # Return as python list\n    return X_out.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input\n    np.random.seed(0)\n    B, C, H, W = 2, 4, 2, 2\n    X = np.random.randn(B, C, H, W)\n    gamma = np.ones(C)\n    beta = np.zeros(C)\n    num_groups = 2\n\n    out = group_normalization(X, gamma, beta, num_groups)\n    print(out)\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f(x):\n    \"\"\"The land's height function.\"\"\"\n    return x**4 - 3*x**3 + 2\n\ndef df(x):\n    \"\"\"Derivative of the land's height function.\"\"\"\n    return 4*x**3 - 9*x**2\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Finds the x where f(x) = x^4 - 3x^3 + 2 reaches its minimum,\n    using gradient descent starting from start_x.\n    Returns x rounded to 4 decimal places.\n    \"\"\"\n    x = start_x\n    for i in range(max_iters):\n        grad = df(x)\n        new_x = x - learning_rate * grad\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return float(np.round(x, 4))\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Try different starting points to avoid local minima\n    for start in [-2.0, 0.0, 2.0, 3.0]:\n        x_min = find_treasure(start)\n        print(f\"Starting at x={start}, found minimum at x={x_min}, f(x)={np.round(f(x_min), 4)}\")\n```\n**How it works:**\n- The script uses gradient descent to find the minimum of the function \\( f(x) = x^4 - 3x^3 + 2 \\).\n- The derivative is \\( f'(x) = 4x^3 - 9x^2 \\).\n- The function `find_treasure` iteratively updates `x` using the negative gradient until convergence.\n- Results are rounded to 4 decimal places as required.\n- The example at the bottom tries several starting points to help avoid getting stuck in local minima."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) activation function.\n    Args:\n        x (np.ndarray): Input array.\n        alpha (float): Learnable scaling parameter.\n        gamma (float): Learnable scaling parameter.\n        beta (float): Learnable bias parameter.\n    Returns:\n        list[float]: Output after applying DyT, rounded to 4 decimals.\n    \"\"\"\n    # Compute mean and std along the last dimension (like LayerNorm, but not used for normalization)\n    mu = np.mean(x, axis=-1, keepdims=True)\n    sigma = np.std(x, axis=-1, keepdims=True) + 1e-5  # avoid division by zero\n\n    # Dynamic scaling\n    s = alpha / (sigma + alpha)\n\n    # Apply dynamic tanh\n    y = gamma * np.tanh(s * (x - mu)) + beta\n\n    # Round to 4 decimals\n    y_rounded = np.round(y, 4)\n\n    # Return as python list\n    return y_rounded.reshape(-1).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n    alpha = 1.0\n    gamma = 1.2\n    beta = 0.5\n    result = dynamic_tanh(x, alpha, gamma, beta)\n    print(result)\n```"}
