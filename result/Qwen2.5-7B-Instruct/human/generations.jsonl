{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the number of columns in the matrix matches the length of the vector\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Compute the dot product of each row in the matrix with the vector\n    result = []\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Valid case\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    vector = [1, 2, 3]\n    print(matrix_dot_vector(matrix, vector))  # Output: [14, 35, 54]\n\n    # Invalid case due to dimension mismatch\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    vector = [1, 2]\n    print(matrix_dot_vector(matrix, vector))  # Output: -1\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Compute the transpose of a given matrix.\n\n    Args:\n        a (list[list[int|float]]): The input matrix as a list of lists.\n\n    Returns:\n        list[list[int|float]]: The transposed matrix.\n    \"\"\"\n    # Using numpy for efficient matrix operations\n    np_a = np.array(a)\n    transposed_a = np_a.T\n    return transposed_a.tolist()\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntransposed_matrix = transpose_matrix(matrix)\nprint(\"Original Matrix:\")\nfor row in matrix:\n    print(row)\nprint(\"\\nTransposed Matrix:\")\nfor row in transposed_matrix:\n    print(row)\n\n# Alternatively, using pure Python without numpy\ndef transpose_matrix_pure_python(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Compute the transpose of a given matrix using pure Python.\n\n    Args:\n        a (list[list[int|float]]): The input matrix as a list of lists.\n\n    Returns:\n        list[list[int|float]]: The transposed matrix.\n    \"\"\"\n    # Transpose the matrix using zip and list conversion\n    return [list(row) for row in zip(*a)]\n\n# Testing the pure Python implementation\nmatrix_pure = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntransposed_matrix_pure = transpose_matrix_pure_python(matrix_pure)\nprint(\"\\nOriginal Matrix (Pure Python):\")\nfor row in matrix_pure:\n    print(row)\nprint(\"\\nTransposed Matrix (Pure Python):\")\nfor row in transposed_matrix_pure:\n    print(row)\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes a given matrix into the specified shape using numpy.\n    \n    Parameters:\n        a (list[list[int|float]]): The original matrix.\n        new_shape (tuple[int, int]): The target shape to reshape the matrix into.\n        \n    Returns:\n        list[list[int|float]]: The reshaped matrix as a 2D list, or an empty list if reshaping is not possible.\n    \"\"\"\n    # Convert the input list to a numpy array\n    np_array = np.array(a)\n    \n    # Check if reshaping is possible\n    if np_array.size != new_shape[0] * new_shape[1]:\n        return []\n    \n    # Reshape the numpy array to the specified shape\n    reshaped_array = np_array.reshape(new_shape)\n    \n    # Convert the reshaped numpy array back to a list of lists\n    reshaped_list = reshaped_array.tolist()\n    \n    return reshaped_list\n\n# Example usage:\noriginal_matrix = [[1, 2], [3, 4], [5, 6]]\nnew_shape = (3, 2)\nreshaped_matrix = reshape_matrix(original_matrix, new_shape)\nprint(reshaped_matrix)  # Output: [[1, 2], [3, 4], [5, 6]]\n\nnew_shape = (2, 3)\nreshaped_matrix = reshape_matrix(original_matrix, new_shape)\nprint(reshaped_matrix)  # Output: [[1, 2, 3], [4, 5, 6]]\n\ninvalid_new_shape = (3, 3)\nreshaped_matrix = reshape_matrix(original_matrix, invalid_new_shape)\nprint(reshaped_matrix)  # Output: []\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists representing the matrix.\n    :param mode: A string specifying the mode, either 'row' or 'column'.\n    :return: A list of means according to the specified mode.\n    \"\"\"\n    # Convert the list of lists into a NumPy array for easier manipulation\n    np_matrix = np.array(matrix)\n    \n    if mode == 'row':\n        # Calculate mean along axis 1 (columns) for each row\n        means = np_matrix.mean(axis=1).tolist()\n    elif mode == 'column':\n        # Calculate mean along axis 0 (rows) for each column\n        means = np_matrix.mean(axis=0).tolist()\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    \n    return means\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Calculate row means\nrow_means = calculate_matrix_mean(matrix, 'row')\nprint(f\"Row Means: {row_means}\")\n\n# Calculate column means\ncolumn_means = calculate_matrix_mean(matrix, 'column')\nprint(f\"Column Means: {column_means}\")\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies a given matrix by a scalar value.\n\n    Args:\n    matrix (list[list[int|float]]): A 2D list representing the matrix.\n    scalar (int|float): The scalar value to multiply the matrix by.\n\n    Returns:\n    list[list[int|float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n    # Create an empty matrix to store the result\n    result = []\n    \n    # Iterate through each row in the input matrix\n    for row in matrix:\n        # Multiply each element in the row by the scalar and create a new row\n        new_row = [element * scalar for element in row]\n        # Add the new row to the result matrix\n        result.append(new_row)\n    \n    return result\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nscalar = 3\nresult_matrix = scalar_multiply(matrix, scalar)\nprint(result_matrix)\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculates the eigenvalues of a 2x2 matrix and returns them sorted in descending order.\n    \n    Parameters:\n        matrix (list[list[float|int]]): A 2x2 matrix represented as a list of lists.\n        \n    Returns:\n        list[float]: A list containing the eigenvalues sorted from highest to lowest.\n    \"\"\"\n    # Convert the input list to a NumPy array for easier manipulation\n    np_matrix = np.array(matrix)\n    \n    # Calculate the eigenvalues using NumPy's eigenvalue calculator\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    \n    # Sort the eigenvalues in descending order\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    \n    return sorted_eigenvalues\n\n# Check function to verify the correctness of the solution\ndef check_eigenvalues():\n    test_matrix1 = [[4, -1], [1, 3]]\n    test_matrix2 = [[2, 0], [0, 2]]\n    test_matrix3 = [[5, 2], [1, 4]]\n    \n    print(\"Eigenvalues of test_matrix1:\", calculate_eigenvalues(test_matrix1))\n    print(\"Eigenvalues of test_matrix2:\", calculate_eigenvalues(test_matrix2))\n    print(\"Eigenvalues of test_matrix3:\", calculate_eigenvalues(test_matrix3))\n\n# Run the check function to test the calculate_eigenvalues function\ncheck_eigenvalues()\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[float]]:\n    \"\"\"\n    Transforms the matrix A using the operation T^(-1) * A * S, where T and S are invertible matrices.\n    \n    Parameters:\n    A (list[list[int|float]]): The original matrix to be transformed.\n    T (list[list[int|float]]): The transformation matrix T.\n    S (list[list[int|float]]): The second transformation matrix S.\n    \n    Returns:\n    list[list[float]]: The transformed matrix, or -1 if T or S are not invertible.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible\n    try:\n        np.linalg.inv(T_np)\n    except np.linalg.LinAlgError:\n        return -1\n    \n    try:\n        np.linalg.inv(S_np)\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Perform the transformation T^(-1) * A * S\n    transformed_A = np.round(np.dot(np.dot(np.linalg.inv(T_np), A_np), S_np)).tolist()\n    \n    return transformed_A\n\n# Example usage\nA = [[2, 3], [4, 5]]\nT = [[1, 0], [0, 1]]\nS = [[-1, 0], [0, 2]]\n\nresult = transform_matrix(A, T, S)\nprint(result)  # Output should be the same as A\n```"}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[list[float]]: The inverse of the matrix if it is invertible, otherwise returns None.\n    \"\"\"\n    # Convert the input matrix to a NumPy array for easier manipulation\n    matrix_np = np.array(matrix)\n    \n    # Check if the matrix is 2x2\n    if matrix_np.shape != (2, 2):\n        return None\n    \n    # Extract the elements of the matrix\n    a, b = matrix_np[0]\n    c, d = matrix_np[1]\n    \n    # Calculate the determinant\n    det = a * d - b * c\n    \n    # If the determinant is zero, the matrix is not invertible\n    if det == 0:\n        return None\n    \n    # Calculate the inverse using the formula for a 2x2 matrix\n    inverse_matrix = np.array([[d, -b], [-c, a]]) / det\n    \n    # Convert the result back to a list of lists\n    return inverse_matrix.tolist()\n\n# Example usage:\nmatrix1 = [[4, 3], [6, 5]]\nmatrix2 = [[1, 2], [3, 4]]\n\nprint(inverse_2x2(matrix1))  # Should print the inverse of matrix1\nprint(inverse_2x2(matrix2))  # Should print None since matrix2 is not invertible\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies two matrices if they are compatible for multiplication.\n    \n    Parameters:\n    a (list[list[int|float]]): The first matrix to be multiplied.\n    b (list[list[int|float]]): The second matrix to be multiplied.\n    \n    Returns:\n    list[list[int|float]]: The resulting matrix after multiplication, or -1 if the matrices cannot be multiplied.\n    \"\"\"\n    # Convert lists to numpy arrays for easy manipulation\n    A = np.array(a)\n    B = np.array(b)\n    \n    # Check if the number of columns in the first matrix is equal to the number of rows in the second matrix\n    if A.shape[1] != B.shape[0]:\n        return -1\n    \n    # Perform matrix multiplication\n    result = np.dot(A, B)\n    \n    # Convert the result back to a list of lists\n    return result.tolist()\n\n# Example usage\na = [[1, 2], [3, 4]]\nb = [[2, 0], [1, 2]]\nprint(matrixmul(a, b))  # Output: [[4, 4], [10, 8]]\n\nc = [[5, 7], [9, 11]]\nd = [[2], [3]]\nprint(matrixmul(c, d))  # Output: [[31], [69]]\n\ne = [[1, 2, 3], [4, 5, 6]]\nf = [[7, 8], [9, 10], [11, 12]]\nprint(matrixmul(e, f))  # Output: [[58, 64], [139, 154]]\n\ng = [[1, 2], [3, 4]]\nh = [[1, 2, 3]]\nprint(matrixmul(g, h))  # Output: -1\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (list[list[float]]): A list of lists where each inner list represents a feature with its observations.\n\n    Returns:\n    list[list[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    # Convert the list of lists into a NumPy array for easier manipulation\n    observations = np.array(vectors)\n    # Calculate the mean of each vector (feature)\n    means = np.mean(observations, axis=0)\n    # Subtract the mean from each observation in the vector\n    centered_observations = observations - means\n    # Calculate the outer product of centered_observations transposed with itself\n    outer_products = np.dot(centered_observations.T, centered_observations)\n    # Divide by the number of observations minus 1 to get the covariance matrix\n    covariance_matrix = outer_products / (len(vectors) - 1)\n    \n    return covariance_matrix.tolist()\n\n# Test cases\ndef check_covariance_matrix():\n    # Test case 1: Simple example with two features\n    vectors1 = [[1, 2, 3], [4, 5, 6]]\n    expected1 = [[3.0, 3.0], [3.0, 3.0]]\n    assert np.allclose(calculate_covariance_matrix(vectors1), expected1), \"Test case 1 failed\"\n\n    # Test case 2: Example with three features\n    vectors2 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    expected2 = [\n        [6.0, 6.0, 6.0],\n        [6.0, 6.0, 6.0],\n        [6.0, 6.0, 6.0]\n    ]\n    assert np.allclose(calculate_covariance_matrix(vectors2), expected2), \"Test case 2 failed\"\n\n    # Test case 3: Example with zero observations\n    vectors3 = []\n    expected3 = []\n    assert calculate_covariance_matrix(vectors3) == expected3, \"Test case 3 failed\"\n\n    # Test case 4: Example with one observation per feature\n    vectors4 = [[1], [2], [3]]\n    expected4 = [[0.0], [0.0], [0.0]]\n    assert np.allclose(calculate_covariance_matrix(vectors4), expected4), \"Test case 4 failed\"\n\n    print(\"All test cases passed!\")\n\n# Run the test cases\ncheck_covariance_matrix()\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix of the system.\n    b (np.ndarray): Constant terms vector.\n    n (int): Number of iterations to perform.\n    \n    Returns:\n    list: Approximate solution vector as a list, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure A is a diagonally dominant matrix for better convergence\n    if not is_diagonally_dominant(A):\n        raise ValueError(\"Matrix A must be diagonally dominant for the Jacobi method to converge.\")\n    \n    D = np.diag(np.diag(A))  # Diagonal part of A\n    R = A - D                 # Remaining part of A\n    \n    x = np.zeros_like(b)     # Initial guess\n    for _ in range(n):\n        x_new = np.round((b - np.dot(R, x)) / D, decimals=4)\n        x = x_new\n    \n    return x.tolist()\n\ndef is_diagonally_dominant(A: np.ndarray) -> bool:\n    \"\"\"\n    Checks if the matrix is diagonally dominant.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix of the system.\n    \n    Returns:\n    bool: True if the matrix is diagonally dominant, False otherwise.\n    \"\"\"\n    for i in range(A.shape[0]):\n        diagonal_element = abs(A[i, i])\n        row_sums = np.sum(abs(A[i]), axis=1) - diagonal_element\n        if diagonal_element <= max(row_sums):\n            return False\n    return True\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[10., -1., 2., 0.],\n                  [-1., 11., -1., 3.],\n                  [2., -1., 10., -1.],\n                  [0.0, 3., -1., 8.]])\n    b = np.array([6., 25., -11., 15.])\n    n = 20  # Number of iterations\n    \n    solution = solve_jacobi(A, b, n)\n    print(f\"Approximate solution after {n} iterations: {solution}\")\n```"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method.\n    \n    Args:\n    A (np.ndarray): A 2x2 input matrix.\n    \n    Returns:\n    tuple: A tuple containing the singular values of the matrix, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Convert matrix to its transpose for convenience\n    A_t = A.T\n    \n    # Compute the matrices U and V\n    U = A / np.linalg.norm(A, ord=2)\n    V = A_t / np.linalg.norm(A_t, ord=2)\n    \n    # Compute the symmetric matrix W\n    W = A_t @ A\n    \n    # Find eigenvalues and eigenvectors of W\n    eigenvalues, eigenvectors = np.linalg.eig(W)\n    \n    # Sort eigenvalues and corresponding eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Compute the singular values from the square roots of the eigenvalues of W\n    singular_values = np.sqrt(sorted_eigenvalues).real\n    \n    # Round singular values to the nearest 4th decimal\n    singular_values = np.round(singular_values, 4)\n    \n    return singular_values\n\n# Example usage\nA = np.array([[1, 2], [3, 4]], dtype=float)\nsingular_values = svd_2x2_singular_values(A)\nprint(f\"Singular Values: {singular_values}\")\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n    \n    Args:\n    matrix (list[list[int | float]]): A 4x4 matrix represented as a list of lists.\n    \n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n    if not all(len(row) == 4 for row in matrix):\n        raise ValueError(\"Input must be a 4x4 matrix.\")\n    \n    # Base case for 2x2 matrix\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    \n    det = 0\n    for col in range(4):\n        sub_matrix = [row[:col] + row[col+1:] for row in matrix[1:]]\n        sign = (-1) ** col\n        det += sign * matrix[0][col] * determinant_4x4(sub_matrix)\n    return det\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    print(f\"Determinant of the matrix: {determinant_4x4(matrix)}\")\n    # Test with another 4x4 matrix\n    matrix2 = [\n        [1, 0, 0, 0],\n        [0, 2, 0, 0],\n        [0, 0, 3, 0],\n        [0, 0, 0, 4]\n    ]\n    print(f\"Determinant of the second matrix: {determinant_4x4(matrix2)}\")\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation method.\n    \n    Parameters:\n    X (list[list[float]]): A matrix of features where each sublist represents a sample and each element in the sublist represents a feature.\n    y (list[float]): A vector of target values.\n    \n    Returns:\n    list[float]: Coefficients of the linear regression model rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X for the bias term\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    \n    # Calculate the coefficients using the normal equation\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = [round(coef, 4) for coef in theta]\n    \n    return theta_rounded\n\n# Example usage:\nX = [[1, 2], [2, 3], [4, 5], [6, 7]]\ny = [3, 4, 8, 11]\n\ncoefficients = linear_regression_normal_equation(X, y)\nprint(coefficients)\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Performs linear regression using gradient descent on input data X and target y.\n    \n    Args:\n    X : np.ndarray - feature matrix with a column of ones for the intercept\n    y : np.ndarray - target vector\n    alpha : float - learning rate\n    iterations : int - number of iterations to run gradient descent\n    \n    Returns:\n    np.ndarray - rounded coefficients of the linear regression model as a list\n    \"\"\"\n    \n    # Initialize coefficients\n    theta = np.zeros(X.shape[1])\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        gradient = X.T.dot(errors) / y.size\n        theta -= alpha * gradient\n        \n    # Return the rounded coefficients as a list\n    return np.round(theta, 4).tolist()\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on a dataset using both standardization and min-max normalization.\n    \n    Parameters:\n    - data: A 2D NumPy array, where each row represents a data sample and each column represents a feature.\n    \n    Returns:\n    - A tuple containing two elements:\n        - Standardized features as a list of lists.\n        - Min-Max normalized features as a list of lists.\n        \n    Both results are rounded to the nearest 4th decimal place.\n    \"\"\"\n    standardized_features = []\n    normalized_features = []\n    \n    # Mean and standard deviation for standardization\n    means = np.mean(data, axis=0)\n    std_devs = np.std(data, axis=0)\n    \n    # Min and max values for min-max normalization\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    \n    for sample in data:\n        standardized_sample = [(x - mean) / std for x, mean, std in zip(sample, means, std_devs)]\n        normalized_sample = [(x - min_val) / (max_val - min_val) for x, min_val, max_val in zip(sample, min_vals, max_vals)]\n        standardized_features.append([round(x, 4) for x in standardized_sample])\n        normalized_features.append([round(x, 4) for x in normalized_sample])\n    \n    return standardized_features, normalized_features\n\n# Example usage:\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\nstandardized, normalized = feature_scaling(data)\n\nprint(\"Standardized Features:\")\nprint(standardized)\nprint(\"Normalized Features:\")\nprint(normalized)\n\n# Example check function to verify correctness\ndef check_feature_scaling():\n    test_data = np.array([[10, 20, 30], [15, 25, 35], [20, 30, 40]])\n    std_exp = [[-1.2247, -1.2247, -1.2247], [-0.4082, -0.4082, -0.4082], [0.4082, 0.4082, 0.4082]]\n    norm_exp = [[0.0, 0.0, 0.0], [0.3333, 0.3333, 0.3333], [0.6667, 0.6667, 0.6667]]\n    \n    std_result, norm_result = feature_scaling(test_data)\n    \n    assert std_result == std_exp, f\"Expected standardized result does not match. Expected {std_exp}, got {std_result}\"\n    assert norm_result == norm_exp, f\"Expected normalized result does not match. Expected {norm_exp}, got {norm_result}\"\n    \n    print(\"Feature scaling implementation is correct.\")\n\ncheck_feature_scaling()\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Implement the k-Means clustering algorithm.\n    \n    Parameters:\n    - points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n    - k: An integer representing the number of clusters to form\n    - initial_centroids: A list of initial centroid points, each a tuple of coordinates\n    - max_iterations: An integer representing the maximum number of iterations to perform\n    \n    Returns:\n    - A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n    \"\"\"\n    \n    # Convert points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    initial_centroids = np.array(initial_centroids)\n    \n    # Initialize cluster labels and centroids\n    labels = np.zeros(len(points))\n    centroids = initial_centroids\n    \n    # Iterate up to max_iterations times\n    for _ in range(max_iterations):\n        # Compute the distance between each point and centroids\n        distances = cdist(points, centroids, 'euclidean')\n        \n        # Assign each point to the closest cluster\n        labels = np.argmin(distances, axis=1)\n        \n        # Update the centroids\n        new_centroids = np.zeros((k, 2))\n        for i in range(k):\n            # Recalculate the mean of the points in the cluster\n            new_centroids[i, :] = points[labels == i].mean(axis=0)\n        \n        # Check if centroids have changed\n        if np.allclose(centroids, new_centroids, atol=1e-04):\n            break\n        \n        # Update centroids\n        centroids = new_centroids\n    \n    # Round the centroids to the nearest fourth decimal\n    final_centroids = [tuple(round(c, 4)) for c in centroids]\n    \n    return final_centroids\n\n# Example usage\nif __name__ == \"__main__\":\n    points = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0), (8.0, 2.0), (10.0, 2.0), (9.0, 3.0)]\n    k = 3\n    initial_centroids = [(1.0, 2.0), (5.0, 8.0), (9.0, 11.0)]\n    max_iterations = 100\n    \n    final_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\n    print(final_centroids)\n```"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n\n    :param X: Input features array.\n    :param y: Target labels array.\n    :param k: Number of folds (default is 5).\n    :param shuffle: Whether to shuffle the data before splitting into batches.\n    :param random_seed: Seed for the random number generator used for shuffling.\n    :return: A list of tuples containing the train and test indices for each fold.\n    \"\"\"\n    kf = KFold(n_splits=k, shuffle=shuffle, random_state=random_seed)\n    splits = []\n    for train_index, test_index in kf.split(X):\n        splits.append((train_index, test_index))\n    return splits\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some sample data\n    X = np.random.rand(100, 10)  # 100 samples, 10 features\n    y = np.random.randint(0, 2, 100)  # Binary target labels\n\n    # Perform 5-fold cross validation\n    splits = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)\n\n    # Print the indices for each fold\n    for i, (train_idx, test_idx) in enumerate(splits, 1):\n        print(f\"Fold {i}:\")\n        print(f\"  Train: length={len(train_idx)}, indices={train_idx}\")\n        print(f\"  Test: length={len(test_idx)}, indices={test_idx}\")\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the dataset\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    \n    # Step 2: Compute the covariance matrix\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Find eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n    \n    # Step 4: Sort the eigenvalues and their corresponding eigenvectors\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Select the top k eigenvectors\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    \n    # Step 6: Return the top k eigenvectors rounded to 4th decimal\n    return [np.round(top_k_eigenvectors[i], 4).tolist() for i in range(k)]\n\n# Example usage:\ndata = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2.0, 1.6], [1.0, 1.1], [1.5, 1.6], [1.1, 0.9]])\nk = 2\nprint(pca(data, k))\n```\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Implement a decision tree learning algorithm using recursive binary splitting based on entropy and information gain.\n    \n    :param examples: A list of examples, where each example is a dictionary with attribute-value pairs.\n    :param attributes: A list of attribute names.\n    :param target_attr: The name of the target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    \n    # Base case: if all examples belong to the same class or no more attributes left\n    if len(Counter([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    \n    if not attributes:\n        return majority_value(examples, target_attr)\n    \n    # Choose the best attribute to split on\n    best_attr = choose_attribute(attributes, examples, target_attr)\n    \n    # Create a new tree node for the best attribute\n    tree = {best_attr: {}}\n    \n    # Remove the best attribute from the list of available attributes\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    \n    # Split examples into subsets based on attribute values\n    for val in get_values(examples, best_attr):\n        ex_subset = [example for example in examples if example[best_attr] == val]\n        \n        # Recurse to create subtrees\n        subtree = learn_decision_tree(ex_subset, remaining_attributes, target_attr)\n        \n        # Add the subtree to our new tree node\n        tree[best_attr][val] = subtree\n    \n    return tree\n\ndef choose_attribute(attributes: list[str], examples: list[dict], target_attr: str) -> str:\n    \"\"\"\n    Select the best attribute to split the data on, based on information gain.\n    \n    :param attributes: List of attributes to consider.\n    :param examples: List of training examples.\n    :param target_attr: The name of the target attribute for classification.\n    :return: The best attribute to split on.\n    \"\"\"\n    best_gain = -1\n    best_attribute = None\n    for attr in attributes:\n        gain = information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attribute = attr\n    return best_attribute\n\ndef information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Calculate the information gain from splitting the data on the given attribute.\n    \n    :param examples: List of training examples.\n    :param attribute: Attribute to calculate the information gain for.\n    :param target_attr: The name of the target attribute for classification.\n    :return: The information gain as a float.\n    \"\"\"\n    parent_entropy = entropy(examples, target_attr)\n    vals = get_values(examples, attribute)\n    weight_avg_entropy = 0\n    \n    for val in vals:\n        subset = [example for example in examples if example[attribute] == val]\n        weight = len(subset) / len(examples)\n        weight_avg_entropy += weight * entropy(subset, target_attr)\n    \n    return parent_entropy - weight_avg_entropy\n\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculate the entropy of the given examples with respect to the target attribute.\n    \n    :param examples: List of training examples.\n    :param target_attr: The name of the target attribute for classification.\n    :return: The entropy as a float.\n    \"\"\"\n    val_freqs = Counter(example[target_attr] for example in examples)\n    impurities = [-freq/len(examples) * math.log2(freq/len(examples)) for freq in val_freqs.values()]\n    return sum(impurities)\n\ndef get_values(examples: list[dict], attribute: str) -> list:\n    \"\"\"\n    Get the list of unique values for the given attribute in the examples.\n    \n    :param examples: List of training examples.\n    :param attribute: Attribute to get the values for.\n    :return: List of unique values for the attribute.\n    \"\"\"\n    return set(example[attribute] for example in examples)\n\ndef majority_value(examples: list[dict], target_attr: str) -> str:\n    \"\"\"\n    Return the majority value of the target attribute in the given examples.\n    \n    :param examples: List of training examples.\n    :param target_attr: The name of the target attribute for classification.\n    :return: Majority value of the target attribute.\n    \"\"\"\n    class_counts = Counter(example[target_attr] for example in examples)\n    return max(class_counts, key=class_counts.get)\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    # Check if data and labels have compatible shapes\n    assert data.shape[0] == len(labels), \"The number of data samples must match the number of labels\"\n    \n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0\n    \n    # Kernel Matrix\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            if kernel == 'linear':\n                K[i, j] = np.dot(data[i], data[j])\n            elif kernel == 'rbf':\n                K[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * (sigma ** 2)))\n            else:\n                raise ValueError(\"Unsupported kernel type\")\n    \n    # Pegasos algorithm\n    for t in range(iterations):\n        for i in range(n_samples):\n            hinge_loss = max(0, 1 - labels[i] * (b + np.dot(K[i, :], alpha * labels)))\n            if hinge_loss > 0:\n                alpha[i] = alpha[i] - lambda_val * alpha[i] + labels[i] * (1 - lambda_val) * K[i, i]\n                alpha -= lambda_val * alpha * np.sum(alpha * labels)  # Regularization term\n                b += labels[i] * (1 - lambda_val) * K[i, i]\n    \n    # Return the alpha coefficients and bias\n    return alpha.tolist(), round(b, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample dataset and labels\n    X = np.array([[1, 2], [3, 4], [-1, -2], [-3, -4]])\n    y = np.array([1, 1, -1, -1])\n    \n    # Train the model\n    alphas, bias = pegasos_kernel_svm(X, y, kernel='linear', lambda_val=0.01, iterations=100)\n    print(f\"Alpha coefficients: {alphas}\")\n    print(f\"Bias: {bias}\")\n```\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    Parameters:\n    - z (float): The input value for which the sigmoid function will be computed.\n    \n    Returns:\n    - float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    # Compute the sigmoid function\n    sig = 1 / (1 + math.exp(-z))\n    # Round the result to four decimal places\n    return round(sig, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-10, -5, 0, 5, 10]\n    for value in test_values:\n        print(f\"Sigmoid({value}) = {sigmoid(value)}\")\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation for a given list of scores.\n    \n    Args:\n    scores (list[float]): A list of raw scores (floats).\n    \n    Returns:\n    list[float]: A list of softmax activations corresponding to the input scores, each rounded to four decimal places.\n    \"\"\"\n    # Subtract the max score for numerical stability\n    exp_scores = [math.exp(score - max(scores)) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values\n\n# Example usage:\nscores = [3.0, 1.0, 0.2]\nprint(softmax(scores))\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulate a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: A list of feature vectors, where each vector represents multiple features for an example\n    :param labels: A list of true binary labels\n    :param weights: A list of weights, one for each feature\n    :param bias: The bias value for the neuron\n    :return: A tuple containing the predicted probabilities and the mean squared error\n    \"\"\"\n    predictions = []\n    mse = 0.0\n    \n    for feature_vector, label in zip(features, labels):\n        # Calculate the weighted sum of the inputs\n        weighted_sum = sum(feature * weight for feature, weight in zip(feature_vector, weights)) + bias\n        # Apply sigmoid activation function\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n        # Calculate the mean squared error\n        mse += (prediction - label) ** 2\n    \n    # Calculate the mean squared error\n    mse /= len(labels)\n    # Round the predictions and mse to 4 decimal places\n    predictions_rounded = [round(pred, 4) for pred in predictions]\n    mse_rounded = round(mse, 4)\n    \n    # Convert predictions to numpy array and then use tolist() method\n    predictions_np = np.array(predictions_rounded).tolist()\n    \n    return predictions_np, mse_rounded\n\n# Example usage\nfeatures = [[1.0, 2.0, 3.0], [2.0, 3.0, 4.0], [3.0, 4.0, 5.0]]\nlabels = [0, 1, 1]\nweights = [0.5, 0.3, 0.8]\nbias = -1.0\n\npredictions, mse = single_neuron_model(features, labels, weights, bias)\nprint(predictions)  # Output: [0.0373, 0.0994, 0.1966]\nprint(f\"MSE: {mse}\")  # Output: MSE: 0.0004\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    # Convert lists to numpy arrays if needed\n    features = np.array(features)\n    labels = np.array(labels)\n    initial_weights = np.array(initial_weights)\n    \n    # Initialize weights and bias\n    weights = initial_weights.copy()\n    bias = initial_bias\n    \n    # Store MSE values for each epoch\n    mse_values = []\n    \n    # Sigmoid function\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    \n    # Derivative of sigmoid function\n    def sigmoid_derivative(x):\n        return x * (1 - x)\n    \n    for epoch in range(epochs):\n        # Forward pass\n        inputs = np.dot(features, weights) + bias\n        predictions = sigmoid(inputs)\n        \n        # Calculate MSE loss\n        loss = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(loss, 4))\n        \n        # Backward pass\n        d_loss_d_pred = 2 * (predictions - labels) / len(labels)\n        d_pred_d_input = sigmoid_derivative(predictions)\n        d_input_d_weight = features\n        d_input_d_bias = 1\n        \n        # Calculate gradients\n        gradient_wrt_weight = np.dot(d_input_d_weight.T, d_loss_d_pred * d_pred_d_input)\n        gradient_wrt_bias = np.sum(d_loss_d_pred * d_pred_d_input, axis=0, keepdims=True)\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_wrt_weight\n        bias -= learning_rate * gradient_wrt_bias\n    \n    return weights.tolist(), bias, mse_values\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 0, 0, 1])\ninitial_weights = np.array([0.5, 0.5])\ninitial_bias = 0.5\nlearning_rate = 0.1\nepochs = 1000\n\nweights, bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\nprint(\"Updated Weights:\", weights)\nprint(\"Updated Bias:\", round(bias, 4))\nprint(\"MSE Values per Epoch:\", mse_values)\n```"}
{"task_id": 26, "completion_id": 0, "completion": "```python\nimport math\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        \n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n    \n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n        \n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        \n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n        \n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        \n        return out\n    \n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n        \n        def _backward():\n            if self.data > 0:\n                self.grad += 1.0 * out.grad\n            else:\n                self.grad += 0.0 * out.grad\n        out._backward = _backward\n        \n        return out\n    \n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        \n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = a * b; c.label = 'c'\nd = a + b; d.label = 'd'\ne = c.relu(); e.label = 'e'\nf = e + d; f.label = 'f'\n\n# Perform the backward pass\nf.backward()\n\n# Print the gradients\nprint(f'a.grad: {a.grad}, b.grad: {b.grad}, c.grad: {c.grad}, d.grad: {d.grad}, e.grad: {e.grad}, f.grad: {f.grad}')\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the basis vectors into numpy arrays\n    B_array = np.array(B)\n    C_array = np.array(C)\n    \n    # Compute the transformation matrix P from basis B to C\n    P = np.linalg.inv(B_array).dot(C_array)\n    \n    # Round the results to the nearest 4th decimal\n    P_rounded = np.round(P, 4).tolist()\n    \n    return P_rounded\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]  # Standard basis in R^3\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]  # Another basis in R^3\nprint(transform_basis(B, C))\n```\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Ensure input is a 2x2 numpy array\n    if A.shape != (2, 2):\n        raise ValueError(\"Input must be a 2x2 matrix.\")\n    \n    # Compute the transpose of A\n    A_T = A.T\n    \n    # Compute A^T * A\n    A.T @ A\n    \n    # Eigendecomposition of A^T * A\n    eigenvalues, eigenvectors = np.linalg.eigh(A_T @ A)\n    \n    # Construct the matrix V from eigenvectors of A^T * A\n    V = eigenvectors.T.tolist()\n    \n    # Singular values are the square roots of the eigenvalues of A^T * A\n    singular_values = np.sqrt(eigenvalues).tolist()\n    \n    # Construct the diagonal matrix S from singular values\n    S = np.zeros((2, 2))\n    np.fill_diagonal(S, singular_values)\n    S = S.round(4).tolist()\n    \n    # Compute U from AV = \u03bbv where \u03bb are the singular values and v are the eigenvectors\n    U = []\n    for i, singular_value in enumerate(singular_values):\n        if singular_value == 0:\n            continue\n        u = (A @ eigenvectors[:, i]) / singular_value\n        U.append(u.tolist())\n    U = np.array(U).round(4).tolist()\n    \n    return U, S, V\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(f\"U: {U}\")\nprint(f\"S: {S}\")\nprint(f\"V: {V}\")\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays X and y randomly while maintaining their correspondence.\n    \n    Parameters:\n    X (np.ndarray): Input features array.\n    y (np.ndarray): Target labels array.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    tuple: A tuple containing the shuffled X and y as lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X_shuffled = X[indices].tolist()\n    y_shuffled = y[indices].tolist()\n    return X_shuffled, y_shuffled\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 3, 4])\n\n# Shuffle without seed (output should be different each time)\nX_shuffled, y_shuffled = shuffle_data(X, y)\nprint(\"Shuffled X:\", X_shuffled)\nprint(\"Shuffled y:\", y_shuffled)\n\n# Shuffle with seed=42 (should produce the same result each time)\nX_shuffled_seed42, y_shuffled_seed42 = shuffle_data(X, y, seed=42)\nprint(\"Shuffled X with seed 42:\", X_shuffled_seed42)\nprint(\"Shuffled y with seed 42:\", y_shuffled_seed42)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yield batches of data from numpy arrays X and optionally y.\n    \n    :param X: Numpy array containing the input features.\n    :param y: Numpy array containing the target labels (optional).\n    :param batch_size: Size of each batch yielded (default is 64).\n    :return: Yields batches of (X, y) pairs or X only as a python list.\n    \"\"\"\n    assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n    if y is not None:\n        assert isinstance(y, np.ndarray), \"y must be a numpy array if provided\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of rows\"\n    \n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        end = min(i + batch_size, n_samples)\n        if y is not None:\n            yield [X[i:end].tolist(), y[i:end].tolist()]\n        else:\n            yield [X[i:end].tolist()]\n\n# Example usage:\nnp.random.seed(0)\nX = np.random.rand(100, 10)\ny = np.random.randint(0, 2, (100, 1))\n\n# Creating batches of (X, y)\nbatches_X_y = list(batch_iterator(X, y, batch_size=10))\n\n# Creating batches of X only\nbatches_X = list(batch_iterator(X, batch_size=10))\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on the specified feature and threshold.\n    \n    Parameters:\n    X (np.array): The input dataset, where each row represents a data point and each column represents a feature.\n    feature_i (int): The index of the feature to divide the dataset on.\n    threshold (float): The threshold value to compare the feature against.\n    \n    Returns:\n    list: A list containing two np.array elements. The first element contains samples where the feature value is >= threshold,\n          and the second element contains samples where the feature value is < threshold.\n    \"\"\"\n    split_func = None\n    if isinstance(threshold, np.number):\n        split_func = lambda sample: sample[feature_i] >= threshold\n    else:\n        # For string threshold\n        split_func = lambda sample: sample[feature_i] == threshold\n    \n    X_left = np.array([sample for sample in X if split_func(sample)])\n    X_right = np.array([sample for sample in X if not split_func(sample)])\n    \n    return [X_left.tolist(), X_right.tolist()]\n\n# Example usage:\nX = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n    [10, 11, 12]\n])\n\nfeature_i = 0\nthreshold = 5\n\ndivided_data = divide_on_feature(X, feature_i, threshold)\nprint(\"Left subset:\", divided_data[0])\nprint(\"Right subset:\", divided_data[1])\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for the input dataset X up to the specified degree.\n    \n    :param X: 2D numpy array of shape (n_samples, n_features)\n    :param degree: integer specifying the degree of the polynomial features\n    :return: 2D list of polynomial features\n    \"\"\"\n    n_samples, n_features = X.shape\n    # All combinations with replacement of features up to the given degree\n    combos = combinations_with_replacement(range(n_features), degree)\n    # Initialize the result matrix with zeros\n    result = np.zeros((n_samples, int((degree*(degree+1))/2)))\n    \n    # Counter for the current column position in the result matrix\n    col_index = 0\n    \n    for combo in combos:\n        if len(combo) > 1:\n            # Generate interaction term\n            interaction_term = np.prod(X[:, combo], axis=1)\n        else:\n            # If degree is 1 or single feature, just take the corresponding feature\n            interaction_term = X[:, combo[0]]\n        \n        # Assign interaction term to the result matrix\n        result[:, col_index] = interaction_term\n        col_index += 1\n    \n    return result.tolist()\n\n# Example usage\nX = np.array([[1, 2], [3, 4], [5, 6]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n```\n\nThis script defines a function `polynomial_features` which takes a 2D numpy array `X` and an integer `degree` as inputs. It generates all possible polynomial features up to the specified degree and returns them as a 2D list. The example usage at the end demonstrates how to call this function with a sample dataset and degree."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of a given dataset.\n\n    Parameters:\n    - X (np.ndarray): 2D numpy array representing features of the dataset.\n    - y (np.ndarray): 1D numpy array representing labels of the dataset.\n    - n_subsets (int): Number of subsets to generate.\n    - replacements (bool): Whether to sample with replacement or not.\n    - seed (int): Seed for the random number generator.\n\n    Returns:\n    - list of tuples: Each tuple contains (X_subset, y_subset) where X_subset and y_subset are subsets of the original dataset.\n    \"\"\"\n    np.random.seed(seed)  # Set seed for reproducibility\n    subsets = []\n    \n    for _ in range(n_subsets):\n        rows_indices = np.random.choice(len(X), size=len(X), replace=replacements)\n        X_subset = X[rows_indices].tolist()\n        y_subset = y[rows_indices].tolist()\n        subsets.append((X_subset, y_subset))\n    \n    return subsets\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    \n    # Set parameters\n    n_subsets = 3\n    replacements = False\n    \n    # Generate random subsets\n    random_subsets = get_random_subsets(X, y, n_subsets, replacements)\n    \n    # Print results\n    for i, (X_subset, y_subset) in enumerate(random_subsets):\n        print(f\"Subset {i+1}:\")\n        print(\"Features:\\n\", np.array(X_subset))\n        print(\"Labels:\\n\", np.array(y_subset))\n        print(\"-\" * 40)\n```\n\nThis script defines a function `get_random_subsets` that generates random subsets of a given dataset, respecting the constraints about sampling with or without replacement. The example usage demonstrates how to call this function with a sample dataset and print the resulting subsets."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    x (np.ndarray): A 1D numpy array containing integer values to be one-hot encoded.\n    n_col (int, optional): The number of columns for the one-hot encoded array. \n                           If not provided, it will be determined automatically.\n    \n    Returns:\n    list: A list representing the one-hot encoded form of the input array.\n    \"\"\"\n    # Determine the number of columns if not provided\n    if n_col is None:\n        n_col = int(np.max(x)) + 1\n    \n    # Initialize the one_hot_encoded array with zeros\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    \n    # Perform one-hot encoding\n    one_hot_encoded[np.arange(x.size), x] = 1\n    \n    # Convert the numpy array to a list and return\n    return one_hot_encoded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    x = np.array([2, 3, 0, 1, 4])\n    n_col = 5  # Optional, can be omitted\n    print(to_categorical(x, n_col))\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.array): A 1D numpy array.\n    \n    Returns:\n    np.array: A 2D numpy array representing the diagonal matrix.\n    \"\"\"\n    # Initialize an identity matrix of size equal to the length of x\n    identity_matrix = np.eye(len(x))\n    \n    # Multiply the identity matrix by the 1D array x to create a diagonal matrix\n    diagonal_matrix = identity_matrix * x[:, None]\n    \n    # Return the diagonal matrix as a list using tolist()\n    return diagonal_matrix.tolist()\n\n# Example usage:\nx = np.array([1, 2, 3])\nresult = make_diagonal(x)\nprint(result)\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom sklearn.metrics import accuracy_score as accuracy_score_sklearn\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (np.array): A 1D numpy array containing the true labels.\n    y_pred (np.array): A 1D numpy array containing the predicted labels.\n    \n    Returns:\n    float: The accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    # Using sklearn's accuracy_score for calculation\n    result = accuracy_score_sklearn(y_true, y_pred)\n    # Rounding the result to the nearest 4th decimal\n    return round(result, 4)\n\n# Example usage:\ny_true = np.array([0, 1, 0, 1, 0])\ny_pred = np.array([0, 1, 0, 0, 1])\n\nprint(accuracy_score(y_true, y_pred))  # Expected output: 0.6\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array\n    - Y: Optional 2D numpy array\n    \n    Returns:\n    - A 2D numpy array representing the correlation matrix, converted to a python list using tolist()\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Ensure X and Y are 2D arrays\n    X = np.atleast_2d(X)\n    Y = np.atleast_2d(Y)\n    \n    # Calculate means and standard deviations\n    mean_X = np.mean(X, axis=1, keepdims=True)\n    mean_Y = np.mean(Y, axis=1, keepdims=True)\n    std_X = np.std(X, axis=1, keepdims=True)\n    std_Y = np.std(Y, axis=1, keepdims=True)\n    \n    # Center the data\n    centered_X = X - mean_X\n    centered_Y = Y - mean_Y\n    \n    # Calculate the covariance matrix\n    covariance_matrix = (centered_X @ centered_Y.T) / (X.shape[1] - 1)\n    \n    # Calculate the standard deviation of X and Y\n    std_dev_X = np.squeeze(std_X)\n    std_dev_Y = np.squeeze(std_Y)\n    \n    # Calculate the correlation matrix\n    corr_matrix = covariance_matrix / (std_dev_X * std_dev_Y)\n    \n    # Round the results to the nearest 4th decimal\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    \n    return corr_matrix_rounded.tolist()\n\n# Example usage:\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nresult = calculate_correlation_matrix(data)\nprint(result)\n\ndata_with_Y = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nresult_with_Y = calculate_correlation_matrix(data_with_Y, Y)\nprint(result_with_Y)\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.base import clone\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the fit method for an AdaBoost classifier.\n    \n    Parameters:\n    X (np.array): 2D numpy array of shape (n_samples, n_features) representing the dataset.\n    y (np.array): 1D numpy array of shape (n_samples,) representing the labels.\n    n_clf (int): Number of classifiers.\n    \n    Returns:\n    list: A list of dictionaries, where each dictionary contains the parameters of a classifier after it has been trained.\n    \"\"\"\n    # Initialize sample weights\n    n_samples = X.shape[0]\n    sample_weights = np.full(n_samples, (1 / n_samples), dtype=np.float64)\n    \n    classifiers = []\n    \n    for _ in range(n_clf):\n        # Clone the dataset to find best threshold for each feature\n        weighted_X = X * sample_weights.reshape(-1, 1)\n        weighted_sum_X = np.sum(weighted_X, axis=0)\n        \n        # Initialize a decision tree classifier with a very small depth\n        clf = DecisionTreeClassifier(max_depth=1)\n        \n        # Find the best threshold for each feature\n        best_clf = None\n        best_error = float('inf')\n        for feature in range(X.shape[1]):\n            thresholds = np.unique(weighted_X[:, feature])\n            for threshold in thresholds:\n                clf_clone = clone(clf)\n                clf_clone.threshold = threshold\n                clf_clone.classes_ = np.array([0, 1])\n                \n                # Predict based on the current threshold\n                y_pred = (weighted_X[:, feature] >= threshold).astype(int)\n                \n                # Calculate the error\n                incorrect = y_pred != y\n                e = np.sum(sample_weights[incorrect])\n                \n                # Update the best classifier if the error is smaller\n                if e < best_error:\n                    best_error = e\n                    best_clf = {'feature': feature, 'threshold': round(threshold, 4), 'error': round(best_error, 4)}\n                    \n        # If no best classifier was found, use a default one\n        if best_clf is None:\n            best_clf = {'feature': 0, 'threshold': 0.0, 'error': 0.0}\n        \n        # Train the best classifier found\n        clf_clone = clone(clf)\n        clf_clone.fit(X, y, sample_weight=sample_weights)\n        classifiers.append({'classifier': clf_clone, 'feature': best_clf['feature'], 'threshold': best_clf['threshold'], 'error': best_clf['error']})\n        \n        # Calculate the alpha value\n        alpha = round(0.5 * math.log((1 - best_error) / best_error), 4)\n        \n        # Update the sample weights\n        incorrect = clf_clone.predict(X) != y\n        sample_weights *= np.exp(alpha * incorrect)\n        sample_weights /= np.sum(sample_weights)\n        \n        # Clip sample weights to avoid numerical issues\n        sample_weights = np.clip(sample_weights, 0, 1)\n        \n    return classifiers\n```\n\nThis function initializes the AdaBoost algorithm by setting equal weights for all samples and then iteratively trains `n_clf` classifiers, each time adjusting the sample weights based on the performance of the classifier. The best classifier at each iteration is stored along with its feature index, threshold, and error rate. All results are rounded to the nearest 4th decimal as per the output constraints."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    This function takes a list of scores (numpy array) and computes the log-softmax of each element.\n    The result is rounded to the nearest 4th decimal place and returned as a list.\n    \n    Parameters:\n    scores (list): A list of scores (real numbers)\n    \n    Returns:\n    list: A list of log-softmax values corresponding to the input scores\n    \"\"\"\n    # Convert the list to a numpy array\n    scores_array = np.array(scores)\n    \n    # Compute the softmax values\n    softmax_values = np.exp(scores_array - np.max(scores_array)) / np.sum(np.exp(scores_array - np.max(scores_array)))\n    \n    # Compute the log of the softmax values\n    log_softmax_values = np.log(softmax_values)\n    \n    # Round the results to the nearest 4th decimal place\n    log_softmax_values_rounded = np.round(log_softmax_values, 4)\n    \n    # Convert the numpy array back to a list\n    return log_softmax_values_rounded.tolist()\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self, input_shape=None):\n        if input_shape is None and self.input_shape is None:\n            raise ValueError(\"Input shape must be provided either during initialization or through set_input_shape method\")\n        if input_shape is not None:\n            self.input_shape = input_shape\n        \n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        \n        # Assuming basic GradientDescent optimizers for simplicity\n        self.optimizer_W = SGD(lr=0.01)\n        self.optimizer_w0 = SGD(lr=0.01)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Calculate gradients\n        grad_wrt_W = np.dot(self.layer_input.T, accum_grad)\n        grad_wrt_w0 = np.sum(accum_grad, axis=0)\n        grad_wrt_prev_layer = np.dot(accum_grad, self.W.T)\n\n        # Update weights and biases if the layer is trainable\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, grad_wrt_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_wrt_w0)\n\n        return grad_wrt_prev_layer\n\n    def output_shape(self):\n        return [self.n_units]\n\nclass SGD:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n    \n    def update(self, layer_params, grad):\n        return layer_params - self.lr * grad\n\n# Example usage:\ndense_layer = Dense(n_units=64, input_shape=(100,))\ndense_layer.initialize()\ninput_data = np.random.rand(32, 100)  # Random input data with shape (32, 100)\noutput = dense_layer.forward_pass(input_data)\nprint(f\"Output shape: {output.shape}\")  # Should print (32, 64)\n\n# For backward pass, we need the gradient from the next layer\naccum_grad = np.random.rand(32, 64)  # Random gradient with shape (32, 64)\ngrad = dense_layer.backward_pass(accum_grad)\nprint(f\"Gradient shape: {grad.shape}. Rounded: {grad.tolist()}\")  # Should print (32, 100)\n\n# Parameters count\nparam_count = dense_layer.parameters()\nprint(f\"Total parameters: {param_count}\")\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution on the input matrix using the given kernel.\n    \n    Parameters:\n    - input_matrix: A 2D numpy array representing the input image.\n    - kernel: A 2D numpy array representing the convolutional kernel.\n    - padding: An integer representing the number of pixels to pad the input image.\n    - stride: An integer representing the stride of the convolution operation.\n    \n    Returns:\n    - The result of the convolution operation as a list of lists, with results rounded to the nearest 4th decimal.\n    \"\"\"\n    # Determine input dimensions\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Add padding to the input matrix\n    padded_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    \n    # Determine output dimensions\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    \n    # Initialize the output matrix\n    output_matrix = np.zeros((output_height, output_width))\n    \n    # Perform convolution\n    for y in range(output_height):\n        for x in range(output_width):\n            output_matrix[y][x] = np.sum(padded_matrix[y*stride:y*stride+kernel_height, x*stride:x*stride+kernel_width] * kernel)\n    \n    # Round the results to the nearest 4th decimal\n    output_matrix = np.round(output_matrix, decimals=4).tolist()\n    \n    return output_matrix\n\n# Example usage\ninput_matrix = np.array([[1, 2, 3, 4],\n                         [2, 3, 4, 5],\n                         [3, 4, 5, 6],\n                         [4, 5, 6, 7]])\nkernel = np.array([[1, 0],\n                    [0, 1]])\npadding = 1\nstride = 1\n\nresult = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(result)\n```"}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function to the input.\n\n    Args:\n        z (float): Input value\n\n    Returns:\n        float: Output of the ReLU function applied to the input\n    \"\"\"\n    return max(0, z)\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    test_values = [-1.5, 0.0, 2.3, -0.7, 5.0, 10.3, -3.6]\n    for value in test_values:\n        print(f\"Input: {value}, ReLU Output: {relu(value)}\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss function.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n    w (np.ndarray): Coefficients vector of shape (n_features,)\n    y_true (np.ndarray): True labels vector of shape (n_samples,)\n    alpha (float): Regularization parameter\n    \n    Returns:\n    float: The calculated Ridge loss, rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + reg_term / len(y_true)\n    return round(ridge_loss_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example feature matrix (X), coefficients (w), and true labels (y_true)\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, -0.2])\n    y_true = np.array([1.2, 2.8, 4.4])\n    alpha = 0.1\n    \n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"The Ridge loss is: {loss}\")\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function to the input value.\n    \n    Parameters:\n        z (float): The input value.\n        alpha (float, optional): The slope of the function when the input is less than 0. Defaults to 0.01.\n        \n    Returns:\n        float | int: The result of applying the Leaky ReLU function to the input value.\n    \"\"\"\n    return z if z > 0 else alpha * z\n\n# Example usage:\nif __name__ == \"__main__\":\n    import numpy as np\n    \n    # Test cases\n    test_values = [-10.0, -1.0, 0.0, 1.0, 10.0]\n    for value in test_values:\n        print(f\"leaky_relu({value}) = {leaky_relu(value)}\")\n    \n    # Using a custom alpha value\n    print(f\"leaky_relu(-2.0, alpha=0.1) = {leaky_relu(-2.0, alpha=0.1)}\")\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    - x1: A numpy array representing the first input vector.\n    - x2: A numpy array representing the second input vector.\n    \n    Returns:\n    - float: The dot product (linear kernel) of x1 and x2.\n    \"\"\"\n    # Ensure both inputs are numpy arrays\n    if not isinstance(x1, np.ndarray) or not isinstance(x2, np.ndarray):\n        raise ValueError(\"Both x1 and x2 must be numpy arrays.\")\n    \n    # Compute the dot product of x1 and x2\n    return np.dot(x1, x2)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define two example vectors\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n    \n    # Calculate the linear kernel\n    result = kernel_function(x1, x2)\n    print(f\"Linear Kernel between {x1} and {x2}: {result}\")\n    \n    # Check function to verify correctness\n    def check_kernel_function():\n        assert np.isclose(kernel_function(np.array([1, 2]), np.array([2, 3])), 11.0), \"Test case 1 failed\"\n        assert np.isclose(kernel_function(np.array([0, 0, 0]), np.array([4, 5, 6])), 0.0), \"Test case 2 failed\"\n        assert np.isclose(kernel_function(np.array([1, 1, 1, 1]), np.array([1, 0, 0, 1])), 3.0), \"Test case 3 failed\"\n        print(\"All test cases passed.\")\n    \n    # Run the check function\n    check_kernel_function()\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (numpy.ndarray): An array containing the true binary labels.\n    y_pred (numpy.ndarray): An array containing the predicted binary labels.\n\n    Returns:\n    float: The precision metric value.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (tp) and false positives (fp)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Avoid division by zero\n    if tp + fp == 0:\n        return 0.0\n    \n    # Calculate precision\n    precision_score = tp / (tp + fp)\n    return precision_score\n\n# Example usage:\n# True labels\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 1])\n# Predicted labels\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0])\n\n# Calculate precision\nprec = precision(y_true, y_pred)\nprint(f\"Precision: {prec}\")\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Implements Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent using Mean Squared Error (MSE) as the loss function.\n    \n    Parameters:\n        X (np.array): Input data matrix of shape (n_samples, n_features)\n        y (np.array): Target values of shape (n_samples,)\n        weights (np.array): Initial weights of shape (n_features,)\n        learning_rate (float): Learning rate for updating weights\n        n_iterations (int): Number of iterations for gradient descent\n        batch_size (int): Size of mini-batch for mini-batch gradient descent, defaults to 1 for SGD and 'X.shape[0]' for batch\n        method (str): Type of gradient descent: 'sgd', 'batch', or 'mini_batch'\n        \n    Returns:\n        list: Updated weights after n_iterations, rounded to 4th decimal place and converted to a python list\n    \"\"\"\n    n_samples, n_features = X.shape\n    if method == 'batch':\n        batch_size = n_samples\n    elif method == 'sgd':\n        batch_size = 1\n    \n    for _ in range(n_iterations):\n        if method == 'batch':\n            prediction = np.dot(X, weights)\n            error = prediction - y\n            gradient = np.dot(X.T, error) / n_samples\n            weights -= learning_rate * gradient\n        elif method == 'sgd':\n            for idx in range(n_samples):\n                sample_X = X[idx:idx+1]\n                sample_y = y[idx:idx+1]\n                prediction = np.dot(sample_X, weights)\n                error = prediction - sample_y\n                gradient = np.dot(sample_X.T, error)\n                weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            sample_X = X[indices]\n            sample_y = y[indices]\n            prediction = np.dot(sample_X, weights)\n            error = prediction - sample_y\n            gradient = np.dot(sample_X.T, error) / batch_size\n            weights -= learning_rate * gradient\n            \n    return round_weights(weights.tolist())\n\ndef round_weights(weights_list):\n    \"\"\"\n    Rounds the weights to the nearest 4th decimal place.\n    \n    Parameters:\n        weights_list (list): List of weights\n    \n    Returns:\n        list: Rounded weights\n    \"\"\"\n    return [round(weight, 4) for weight in weights_list]\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix to its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix (list of lists): A 2D list representing the matrix.\n\n    Returns:\n    list: The Reduced Row Echelon Form of the matrix as a nested list.\n    \"\"\"\n    # Convert the input matrix to a numpy array for easier manipulation\n    matrix = np.array(matrix)\n    \n    # Get the number of rows and columns\n    rows, cols = matrix.shape\n    \n    # Initialize the current row and pivot column\n    r, c = 0, 0\n    \n    while r < rows and c < cols:\n        # Find the first non-zero element in the current column\n        if c < cols and all(matrix[r:, c] == 0):\n            c += 1\n            continue\n        \n        # Find the row with the maximum absolute value in the current column\n        max_row = np.argmax(np.abs(matrix[r:, c])) + r\n        \n        # Swap the current row with the row having the maximum absolute value\n        if matrix[max_row, c] != 0:\n            matrix[[r, max_row]] = matrix[[max_row, r]]\n        \n        # Make the leading coefficient 1\n        if matrix[r, c] != 1:\n            scalar = matrix[r, c]\n            matrix[r] /= scalar\n        \n        # Eliminate other entries in the current column\n        for i in range(rows):\n            if i != r and matrix[i, c] != 0:\n                factor = matrix[i, c] / matrix[r, c]\n                matrix[i] -= factor * matrix[r]\n        \n        # Move to the next row and column\n        r += 1\n        c += 1\n    \n    # Convert the matrix back to a list and return it\n    return matrix.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[1, 2, -1, 4], [3, 5, 0, 6], [-2, -3, 1, -5]]\n    print(rref(matrix))\n```"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    # Initialize first and second moments of gradients\n    m = np.zeros_like(x0)  # First moment estimate\n    v = np.zeros_like(x0)  # Second moment estimate\n    \n    # Initialize iteration counter\n    t = 1\n    \n    # Adam optimization loop\n    for i in range(num_iterations):\n        # Compute gradient\n        g = grad(x0)\n        \n        # Update bias-corrected first moment estimate\n        m_corrected = m / (1 - beta1**t)\n        m = beta1 * m + (1 - beta1) * g\n        \n        # Update bias-corrected second moment estimate\n        v_corrected = v / (1 - beta2**t)\n        v = beta2 * v + (1 - beta2) * (g**2)\n        \n        # Parameter update\n        x0 -= learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n        \n        # Increment iteration counter\n        t += 1\n    \n    return x0.tolist()\n\n# Example usage:\n# Define a simple quadratic function for testing\ndef f(x):\n    return np.sum(x**2)\n\n# Define a function to compute the gradient of the quadratic function\ndef grad_f(x):\n    return 2 * x\n\n# Initial parameter values\nx0 = np.array([1.0, 1.0])\n\n# Run the Adam optimizer on the quadratic function\noptimized_params = adam_optimizer(f, grad_f, x0)\nprint(optimized_params)\n```"}
{"task_id": 50, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Add intercept term to X\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    \n    # Initialize weights and bias\n    weights = np.zeros(X.shape[1])\n    bias = 0.0\n    \n    n_samples, n_features = X.shape\n    prev_cost = float('inf')\n    cost = None\n    \n    for _ in range(max_iter):\n        # Predicted values\n        y_pred = np.dot(X, weights) + bias\n        \n        # Cost function\n        cost = (1 / (2 * n_samples)) * np.sum((y_pred - y) ** 2)\n        \n        # Gradients\n        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y)) + (alpha / n_samples) * np.sign(weights)\n        db = (1 / n_samples) * np.sum(y_pred - y)\n        \n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        \n        # Check for convergence\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    \n    return weights.tolist(), round(bias, 4)\n\n# Example usage:\nnp.random.seed(42)\nX = np.random.rand(100, 3)\ny = np.random.rand(100, 1)\nweights, bias = l1_regularization_gradient_descent(X, y)\nprint(f\"Weights: {weights}\")\nprint(f\"Bias: {bias}\")\n```\n```\n\nThis script defines the `l1_regularization_gradient_descent` function, which implements Lasso Regression using gradient descent. The function takes input features `X`, target values `y`, and optional parameters for the regularization strength `alpha`, learning rate `learning_rate`, maximum number of iterations `max_iter`, and tolerance `tol`. It returns the final weights and bias as a list and a float, respectively, both rounded to the nearest 4th decimal place.\n\nThe example usage at the end demonstrates how to call this function with random data, showing the flexibility of the implementation."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    Allowed operations are insert, delete, substitute, and transpose.\n    Each operation costs 1 unit.\n\n    :param source: The source string to be transformed.\n    :param target: The target string after transformation.\n    :return: Minimum number of edits required to transform source into target.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    \n    # Initialize the distance matrix\n    distance_matrix = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    \n    # Fill in the distance matrix\n    for i in range(len_source + 1):\n        for j in range(len_target + 1):\n            if i == 0:\n                distance_matrix[i][j] = j  # Cost of all insertions\n            elif j == 0:\n                distance_matrix[i][j] = i  # Cost of all deletions\n            elif source[i - 1] == target[j - 1]:\n                distance_matrix[i][j] = distance_matrix[i - 1][j - 1]\n            else:\n                substitution_cost = distance_matrix[i - 1][j - 1] + 1\n                insertion_cost = distance_matrix[i][j - 1] + 1\n                deletion_cost = distance_matrix[i - 1][j] + 1\n                transpose_cost = float('inf')\n                \n                # Check if transpose is possible and less costly\n                if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                    transpose_cost = distance_matrix[i - 2][j - 2] + 1\n                \n                distance_matrix[i][j] = min(substitution_cost, insertion_cost, deletion_cost, transpose_cost)\n    \n    return distance_matrix[len_source][len_target]\n\n# Example usage:\nsource_str = \"caper\"\ntarget_str = \"acer\"\nprint(f\"OSA distance between '{source_str}' and '{target_str}': {OSA(source_str, target_str)}\")\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List of true binary labels (0 or 1).\n    - y_pred: List of predicted binary labels (0 or 1).\n    \n    Returns:\n    - Recall value rounded to three decimal places. Returns 0.0 if the denominator is zero.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Count True Positives (TP) and False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall: TP / (TP + FN)\n    if TP + FN == 0:\n        recall_value = 0.0\n    else:\n        recall_value = TP / (TP + FN)\n    \n    # Return the recall value rounded to three decimal places\n    return round(recall_value, 3)\n\n# Example usage:\ntrue_labels = [1, 0, 1, 1, 0, 1, 0, 1]\npredicted_labels = [1, 0, 0, 1, 0, 0, 1, 1]\n\nprint(recall(true_labels, predicted_labels))  # Output should be the recall value\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism for a given input matrix X.\n    \n    Args:\n    - X (np.array): Input feature matrix of shape (sequence_length, embedding_dim).\n    - W_q (np.array): Query weights matrix of shape (embedding_dim, d_k).\n    - W_k (np.array): Key weights matrix of shape (embedding_dim, d_k).\n    - W_v (np.array): Value weights matrix of shape (embedding_dim, d_v).\n    \n    Returns:\n    - np.array: Self-attention output after applying weights and aggregation.\n    \"\"\"\n    assert X.shape[1] == W_q.shape[0] == W_k.shape[0] == W_v.shape[0], \"Embedding dimensions must match\"\n    sequence_length, embedding_dim = X.shape\n    \n    # Ensure d_k and d_v are the same as the embedding dimension\n    d_k = W_q.shape[1]\n    d_v = W_v.shape[1]\n    \n    # Compute Query, Key, Value matrices\n    Q = np.dot(X, W_q)  # Shape: (sequence_length, d_k)\n    K = np.dot(X, W_k)  # Shape: (sequence_length, d_k)\n    V = np.dot(X, W_v)  # Shape: (sequence_length, d_v)\n    \n    # Compute attention scores\n    attention_scores = np.matmul(Q.T, K) / np.sqrt(d_k)  # Shape: (d_k, sequence_length)\n    attention_probs = npsoftmax(attention_scores)  # Apply softmax to get probabilities\n    \n    # Compute the weighted sum of the values\n    self_att_output = np.matmul(attention_probs, V.T).T  # Shape: (sequence_length, d_v)\n    \n    return np.round(self_att_output, 4).tolist()\n\ndef npsoftmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims=True)\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])  # Example input matrix\nW_q = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])  # Example query weights matrix\nW_k = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])  # Example key weights matrix\nW_v = np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])  # Example value weights matrix\n\noutput = self_attention(X, W_q, W_k, W_v)\nprint(output)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell that processes a sequence of input vectors\n    and produces the final hidden state using the tanh activation function.\n    \n    :param input_sequence: List of input vectors (each vector is a list of floats)\n    :param initial_hidden_state: Initial hidden state (a list of floats)\n    :param Wx: Weight matrix for input-to-hidden connections (2D list or numpy array)\n    :param Wh: Weight matrix for hidden-to-hidden connections (2D list or numpy array)\n    :param b: Bias vector (a list or numpy array)\n    :return: Final hidden state after processing the entire sequence (rounded to 4 decimal places)\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    x = np.array(input_sequence)\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden states list\n    hidden_states = [h_prev]\n    \n    # Process each input vector in the sequence\n    for x_t in x:\n        h_prev = np.tanh(np.dot(x_t, Wx) + np.dot(h_prev, Wh) + b)\n        hidden_states.append(h_prev)\n    \n    # Return the final hidden state, rounded to 4 decimal places\n    final_hidden_state = np.round(hidden_states[-1].tolist(), 4)\n    return final_hidden_state\n\n# Example usage\nif __name__ == \"__main__\":\n    # Input sequence of vectors\n    input_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n    \n    # Initial hidden state\n    initial_hidden_state = [0.0, 0.0]\n    \n    # Weight matrices and bias vector\n    Wx = [[0.2, 0.3], [0.4, 0.5]]  # Input-to-hidden weights\n    Wh = [[0.1, -0.2], [-0.3, 0.4]]  # Hidden-to-hidden weights\n    b = [0.1, 0.2]  # Bias vector\n    \n    # Call the function\n    final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(final_hidden_state)\n```\n\nThis script defines the `rnn_forward` function which processes an input sequence through a simple RNN cell, applying the `tanh` activation function at each step. The example usage demonstrates how to call this function with a sample input sequence, initial hidden state, and weight matrices and bias vector. The final hidden state is printed, rounded to four decimal places."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Parameters:\n    points (list): A list of [x, y] coordinates.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n\n    Returns:\n    list: The translated points as a new list of [x, y] coordinates.\n    \"\"\"\n    # Convert the list of points to a numpy array\n    points_array = np.array(points)\n    \n    # Create the translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    \n    # Stack 1s to the points array for affine transformation\n    homogeneous_points = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n    \n    # Apply the translation matrix to each point\n    transformed_points_homogeneous = np.dot(translation_matrix[:2, :2], homogeneous_points.T) + translation_matrix[:2, 2]\n    \n    # Convert the result back to a list of [x, y] coordinates\n    transformed_points = transformed_points_homogeneous.T.tolist()\n    \n    return transformed_points\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 10\nty = 5\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) Divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n\n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    ratio = sigma_p / sigma_q\n    delta = mu_p - mu_q\n    kl_div = 0.5 * (np.log(sigma_q**2 / sigma_p**2) + (sigma_p**2 + delta**2) / sigma_q**2 - 1)\n    return kl_div\n\n# Example usage:\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 1.5\n\nkl_divergence = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(f\"KL Divergence: {kl_divergence}\")\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A)\n    b = np.array(b)\n    \n    # If x_ini is not provided, initialize it to a vector of zeros\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Ensure A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    # Iteratively solve the system using the Gauss-Seidel method\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i+1:], x[i+1:])) / A[i, i]\n    \n    # Return the result as a python list rounded to the nearest 4th decimal\n    return np.round(x, decimals=4).tolist()\n\n# Example usage:\nA = [[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 4]]\nb = [15, -4, -17, 20]\nn = 100\nsolution = gauss_seidel(A, b, n)\nprint(solution)\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix of the system.\n    b (np.ndarray): Right-hand side vector of the system.\n    \n    Returns:\n    list: Solution vector x, rounded to the nearest 4th decimal.\n    \"\"\"\n    n = len(b)\n    # Augment A with b as the last column\n    Ab = np.hstack((A, b.reshape(n, 1)))\n    \n    # Perform Gaussian elimination with partial pivoting\n    for i in range(n):\n        # Find pivot row with maximum absolute value in current column\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        \n        if Ab[i][i] == 0:\n            raise ValueError(\" Singular matrix \")\n        \n        # Eliminate current column below pivot\n        for j in range(i + 1, n):\n            factor = Ab[j][i] / Ab[i][i]\n            Ab[j] -= factor * Ab[i]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = Ab[i, n] / Ab[i, i]\n        for j in range(i - 1, -1, -1):\n            Ab[j, n] -= Ab[j, i] * x[i]\n    \n    # Return the solution vector x, rounded to the nearest 4th decimal\n    return np.round(Ab[:, n], 4).tolist()\n\n# Example usage:\nA = np.array([[4, 3, -2], [2, 6, 3], [-1, -1, 5]])\nb = np.array([1, -2, -3])\nsolution = gaussian_elimination(A, b)\nprint(solution)\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = x.shape[0]\n        hs = []\n        h = initial_hidden_state\n        c = initial_cell_state\n\n        for t in range(T):\n            xt = x[t]\n            xt_hc = np.concatenate((xt, h), axis=0).reshape(self.input_size + self.hidden_size, 1)\n            \n            ft = self.sigmoid(np.dot(self.Wf, xt_hc) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, xt_hc) + self.bi)\n            cct = self.tanh(np.dot(self.Wc, xt_hc) + self.bc)\n            ot = self.sigmoid(np.dot(self.Wo, xt_hc) + self.bo)\n\n            c = ft * c + it * cct\n            h = ot * self.tanh(c)\n            hs.append(h.T.tolist()[0])  # Append the hidden state for this time step\n\n        hs = np.array(hs).round(4).tolist()\n        return hs, h.round(4).tolist(), c.round(4).tolist()\n\n# Example usage\ninput_size = 3\nhidden_size = 2\nlstm = LSTM(input_size, hidden_size)\n\n# Example input sequence (3 time steps, each with 3 features)\nx = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]).round(4)\ninitial_hidden_state = np.zeros((hidden_size, 1)).round(4)\ninitial_cell_state = np.zeros((hidden_size, 1)).round(4)\n\n# Perform forward pass\nhidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\nprint(\"Hidden States:\", hidden_states)\nprint(\"Final Hidden State:\", final_hidden_state)\nprint(\"Final Cell State:\", final_cell_state)\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a given corpus of documents.\n    \n    :param corpus: A list of documents, where each document is a list of words.\n    :param query: A list of words for which to compute the TF-IDF scores.\n    :return: A list of lists containing the TF-IDF scores for the query words in each document, rounded to five decimal places.\n    \"\"\"\n    if not corpus:\n        raise ValueError(\"Corpus cannot be empty\")\n    \n    # Step 1: Calculate Term Frequencies (TF)\n    tf = []\n    for doc in corpus:\n        doc_tfs = {}\n        for word in query:\n            doc_tfs[word] = doc.count(word) / len(doc)\n        tf.append(doc_tfs)\n    \n    # Step 2: Calculate Document Frequencies (DF)\n    df = {word: 0 for word in query}\n    for doc in corpus:\n        for word in query:\n            if word in doc:\n                df[word] += 1\n    \n    # Step 3: Calculate Inverse Document Frequency (IDF)\n    idf = {word: np.log((1 + len(corpus)) / (1 + df[word])) + 1 for word in query}\n    \n    # Step 4: Compute TF-IDF scores\n    tf_idf_scores = []\n    for doc_tfs in tf:\n        doc_scores = []\n        for word in query:\n            if word in doc_tfs:\n                score = doc_tfs[word] * idf[word]\n            else:\n                score = 0\n            doc_scores.append(round(score, 5))\n        tf_idf_scores.append(doc_scores)\n    \n    return tf_idf_scores\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\"],\n    [\"jumped\", \"over\", \"a\", \"lazy\", \"dog\"],\n    [\"the\", \"fox\", \"is\", \"quick\"]\n]\n\nquery = [\"the\", \"quick\", \"brown\", \"dog\"]\n\ntf_idf_scores = compute_tf_idf(corpus, query)\nprint(tf_idf_scores)\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (tp), false positives (fp) and false negatives (fn)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n    \n    # Calculate F-Score using the provided beta value\n    fscore = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (precision + recall) != 0 else 0\n    \n    # Return the F-Score rounded to three decimal places\n    return round(fscore, 3)\n\n# Example usage:\ntrue_labels = np.array([1, 0, 1, 1, 0, 1, 0, 1])\npredicted_labels = np.array([1, 0, 1, 0, 0, 1, 0, 0])\nbeta_value = 1\nprint(f_score(true_labels, predicted_labels, beta_value))  # Output: F1 Score\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n        \"\"\"\n        # Initialize the hidden state\n        h = np.zeros((hidden_size, 1))\n        output_sequence = []\n        \n        for t in range(len(input_sequence)):\n            x_t = input_sequence[t].reshape((-1, 1))\n            # Compute the hidden state\n            h = np.tanh(np.dot(W_xh, x_t) + np.dot(W_hh, h) + b_h)\n            y = np.dot(W_hy, h) + b_y\n            output_sequence.append(y)\n        \n        return output_sequence, h, input_sequence\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_hidden_states, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        # Initialize gradients\n        dW_xh, dW_hh, dW_hy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n        db_h, db_y = np.zeros_like(b_h), np.zeros_like(b_y)\n        \n        # Calculate loss for each time step\n        losses = [(np.square(o - e)).mean() / 2 for o, e in zip(outputs, expected_output)]\n        total_loss = sum(losses)\n        \n        # Initialize delta for the last time step\n        delta = (outputs[-1] - expected_output[-1]) * (1 - last_hidden_states**2)\n        db_y += delta.sum(axis=1, keepdims=True) * learning_rate\n        dW_hy += np.dot(delta, last_hidden_states.T) * learning_rate\n        \n        # Backpropagate through time\n        for t in reversed(range(len(input_sequence))):\n            x_t = input_sequence[t].reshape((-1, 1))\n            h_t = last_hidden_states if t == len(input_sequence) - 1 else hidden_states[t + 1]\n            \n            # Calculate deltas for hidden states and input weights\n            delta_h = (np.dot(W_hh.T, delta) + db_h) * (1 - h_t**2)\n            db_h += delta_h.sum(axis=1, keepdims=True) * learning_rate\n            dW_hh += np.dot(delta_h, h_t.T) * learning_rate\n            \n            # Calculate deltas for input to hidden weights\n            delta_x = np.dot(W_xh.T, delta_h)\n            db_xh = delta_x.sum(axis=1, keepdims=True) * learning_rate\n            dW_xh += np.dot(delta_x, x_t.T) * learning_rate\n            \n            delta = delta_h\n        \n        return dW_xh, dW_hh, dW_hy, db_h, db_y, total_loss\n```\n```\n\nTo further complete the task, you would need to add methods for training the model using these functions, such as `train`, and possibly a method to test the model's performance. However, the provided code snippet focuses solely on the implementation of the forward and backward passes for the SimpleRNN class."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    # Initialize variables\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = b - A @ x\n    p = r.copy()\n    rsold = np.dot(r, r)\n\n    # Perform CG iterations up to n times\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = b - A @ x\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + (rsnew / rsold) * p\n        rsold = rsnew\n\n    # Return the solution vector x rounded to 8 decimal places\n    return np.round(x, 8).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a symmetric positive-definite matrix A and a vector b\n    A = np.array([[3, 1], [1, 2]], dtype=float)\n    b = np.array([[-1], [-6]], dtype=float)\n\n    # Call the conjugate gradient function with initial guess x0=[0, 0]\n    x = conjugate_gradient(A, b, n=100, x0=np.array([[0], [0]]))\n\n    # Print the result\n    print(\"Solution x:\", x)\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Count the frequency of each class in the list\n    label_freq = np.bincount(y)\n    # Calculate the total number of instances\n    total_instances = len(y)\n    # Calculate the probabilities of each class\n    probabilities = label_freq / total_instances\n    # Calculate the Gini Impurity\n    gini = 1 - np.sum(probabilities ** 2)\n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    class_labels = [0, 1, 0, 0, 1, 1, 0, 1, 2, 2, 2, 1, 0, 1, 0]\n    print(\"Gini Impurity:\", gini_impurity(class_labels))\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    # Convert the input list to a NumPy array for easier manipulation\n    dense_array = np.array(dense_matrix)\n    \n    # Find the non-zero elements and their indices\n    rows, cols = dense_array.nonzero()\n    \n    # Values array will contain all non-zero values in row-major order\n    values = dense_array[rows, cols].tolist()\n    \n    # Column indices array will contain the column index for each value\n    col_indices = cols.tolist()\n    \n    # Row pointer array will contain the cumulative number of non-zero elements per row\n    num_rows = dense_array.shape[0]\n    row_pointer = [sum(rows == i) for i in range(num_rows)]\n    row_pointer = [sum(row_pointer[:i+1]) for i in range(num_rows)]\n    \n    return (values, col_indices, row_pointer)\n\n# Example usage:\ndense_matrix = [\n    [1, 0, 2],\n    [0, 3, 0],\n    [4, 5, 6]\n]\n\n# Expected output: (values array, column indices array, row pointer array)\nvalues, col_indices, row_pointer = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", col_indices)\nprint(\"Row Pointer:\", row_pointer)\n\n# Check function to verify correctness\ndef check_csr_conversion(dense_matrix, expected_values, expected_col_indices, expected_row_pointer):\n    values, col_indices, row_pointer = compressed_row_sparse_matrix(dense_matrix)\n    assert values == expected_values, f\"Values mismatch: {values} != {expected_values}\"\n    assert col_indices == expected_col_indices, f\"Column Indices mismatch: {col_indices} != {expected_col_indices}\"\n    assert row_pointer == expected_row_pointer, f\"Row Pointer mismatch: {row_pointer} != {expected_row_pointer}\"\n    print(\"Conversion is correct!\")\n\n# Test data\ncheck_csr_conversion(\n    dense_matrix=[\n        [1, 0, 2],\n        [0, 3, 0],\n        [4, 5, 6]\n    ],\n    expected_values=[1, 2, 3, 4, 5, 6],\n    expected_col_indices=[0, 2, 1, 0, 1, 2],\n    expected_row_pointer=[0, 2, 3, 6]\n)\n```"}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Normalize the line vector L\n    L = np.array(L)\n    L_norm = L / np.linalg.norm(L)\n    \n    # Calculate the dot product of v and L\n    dot_product = np.dot(v, L_norm)\n    \n    # Project v onto L\n    projection = dot_product * L_norm\n    \n    # Round the result to 3 decimal places\n    return [round(coord, 3) for coord in projection]\n\n# Check function to verify the correctness of the solution\ndef check_orthogonal_projection():\n    v1 = [5, 5]\n    L1 = [1, 2]\n    expected_projection1 = [4.286, 8.571]\n    assert orthogonal_projection(v1, L1) == expected_projection1, f\"Test 1 failed, expected {expected_projection1}, got {orthogonal_projection(v1, L1)}\"\n    \n    v2 = [3, 0]\n    L2 = [0, 1]\n    expected_projection2 = [0.0, 3.0]\n    assert orthogonal_projection(v2, L2) == expected_projection2, f\"Test 2 failed, expected {expected_projection2}, got {orthogonal_projection(v2, L2)}\"\n    \n    v3 = [1, 1]\n    L3 = [1, 1]\n    expected_projection3 = [0.5, 0.5]\n    assert orthogonal_projection(v3, L3) == expected_projection3, f\"Test 3 failed, expected {expected_projection3}, got {orthogonal_projection(v3, L3)}\"\n    \n    print(\"All tests passed!\")\n\n# Run the check function\ncheck_orthogonal_projection()\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    # Convert the dense matrix to a sparse CSR matrix using scipy\n    from scipy.sparse import csc_matrix\n    \n    sparse_matrix = csc_matrix(dense_matrix)\n    \n    # Extract the necessary components for CSC format\n    values = sparse_matrix.data\n    row_indices = sparse_matrix.indices\n    col_pointers = sparse_matrix.indptr\n    \n    return (values, row_indices, col_pointers)\n\n# Example usage\ndense_matrix = [\n    [1, 0, 3],\n    [4, 5, 0],\n    [0, 7, 8]\n]\n\n# Call the function and print the result\ncsc_representation = compressed_col_sparse_matrix(dense_matrix)\nprint(csc_representation)\n\n# Optionally, convert back to dense matrix to verify correctness\nreconstructed_dense_matrix = csc_matrix((csc_representation[0], (csc_representation[1], csc_representation[2])), shape=(3, 3)).toarray()\nprint(reconstructed_dense_matrix)\n```"}
{"task_id": 68, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    This function finds the column space (image) of a matrix A using row echelon form.\n    It returns the basis vectors that span the column space of A, rounded to 8 decimal places.\n\n    :param A: numpy array representing the matrix\n    :return: List of basis vectors that span the column space of A\n    \"\"\"\n    # Convert the input matrix to row echelon form\n    U = np.linalg.matrix_rank(A)\n    rref_A, _ = np.linalg.qr(A.T)\n    rref_A = np.nan_to_num(rref_A)  # Convert NaNs to zeros for further processing\n    \n    # Identify pivot columns in the row echelon form\n    pivots = np.any(rref_A != 0, axis=0)\n    \n    # Extract basis vectors from the original matrix\n    basis_vectors = A[:, pivots]\n    \n    # Round the basis vectors to 8 decimal places\n    basis_vectors_rounded = np.round(basis_vectors, 8).tolist()\n    \n    return basis_vectors_rounded\n\n# Example usage:\nA = np.array([[1, 2, 3], [2, 5, 7], [4, 10, 11]], dtype=float)\nprint(matrix_image(A))\n```\n```\n\nThis script defines a function `matrix_image(A)` that takes a numpy array `A` as input and returns the basis vectors of its column space using the QR decomposition method to find the row echelon form (RREF). The pivot columns in the RREF indicate which columns of the original matrix are linearly independent and span the column space. These columns are then selected from the original matrix and rounded to 8 decimal places before being returned as a list."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a given set of true and predicted values.\n    \n    Args:\n    y_true (np.array): Array of true values.\n    y_pred (np.array): Array of predicted values.\n    \n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    # Calculate the total sum of squares (SST)\n    SST = np.sum((y_true - np.mean(y_true))**2)\n    # Calculate the residual sum of squares (SSR)\n    SSR = np.sum((y_true - y_pred)**2)\n    \n    # Calculate R-squared\n    r_squared_value = 1 - (SSR / SST)\n    \n    # Return the R-squared value rounded to three decimal places\n    return round(r_squared_value, 3)\n\n# Check function with provided data points\ndef check_r_squared():\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.1, 1.9, 3, 4.1, 4.9])\n    rsq = r_squared(y_true, y_pred)\n    print(f'R-squared: {rsq}')\n    \n    # Additional checks\n    assert np.isclose(r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 6])), 0.924, \"Test case 1 failed\"\n    assert np.isclose(r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])), 0.0, \"Test case 2 failed\"\n    assert np.isclose(r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])), 0.147, \"Test case 3 failed\"\n\ncheck_r_squared()\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image represented as a 2D matrix.\n    \n    Parameters:\n    img (list of list of int): Grayscale image represented as a 2D matrix.\n    \n    Returns:\n    float: Average brightness of the image rounded to two decimal places. Returns -1 for invalid inputs.\n    \"\"\"\n    # Check if the image is empty\n    if not img or not img[0]:\n        return -1\n    \n    # Check for inconsistent row lengths\n    row_lengths = [len(row) for row in img]\n    if len(set(row_lengths)) > 1:\n        return -1\n    \n    # Check for pixel values out of range\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    \n    # Calculate average brightness\n    total_brightness = sum(pixel for row in img for pixel in row)\n    num_pixels = sum(len(row) for row in img)\n    avg_brightness = round(total_brightness / num_pixels, 2)\n    \n    return avg_brightness\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    import doctest\n\n    # Valid image\n    img1 = [\n        [0, 128, 255],\n        [128, 128, 128],\n        [255, 0, 128]\n    ]\n    print(f\"Average brightness of img1: {calculate_brightness(img1)}\")  # Expected output: 127.78\n    \n    # Empty image\n    img2 = []\n    print(f\"Average brightness of img2: {calculate_brightness(img2)}\")  # Expected output: -1\n    \n    # Inconsistent row lengths\n    img3 = [\n        [0, 128, 255],\n        [128, 128],\n        [255, 0, 128]\n    ]\n    print(f\"Average brightness of img3: {calculate_brightness(img3)}\")  # Expected output: -1\n    \n    # Out-of-range pixel values\n    img4 = [\n        [0, 128, 255],\n        [128, 256, 128],  # Invalid value\n        [255, 0, 128]\n    ]\n    print(f\"Average brightness of img4: {calculate_brightness(img4)}\")  # Expected output: -1\n\n    # Valid image with single row\n    img5 = [\n        [128, 128, 128]\n    ]\n    print(f\"Average brightness of img5: {calculate_brightness(img5)}\")  # Expected output: 128.00\n\n    # Valid image with single pixel\n    img6 = [\n        [128]\n    ]\n    print(f\"Average brightness of img6: {calculate_brightness(img6)}\")  # Expected output: 128.00\n\n    # Run doctests to verify correctness\n    doctest.testmod()\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual and predicted values.\n\n    Parameters:\n    y_true (np.array): An array of actual values.\n    y_pred (np.array): An array of predicted values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If input arrays have mismatched shapes, are empty, or contain invalid inputs.\n    \"\"\"\n    # Check if inputs are numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Both y_true and y_pred must be numpy arrays.\")\n\n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n\n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    # Calculate the RMSE\n    rmse_value = np.sqrt(np.mean((y_true - y_pred) ** 2))\n\n    # Return the RMSE value rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n\n    try:\n        result = rmse(y_true, y_pred)\n        print(f\"Root Mean Square Error: {result}\")\n    except ValueError as e:\n        print(e)\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays y_true and y_pred.\n\n    :param y_true: numpy array of true binary labels\n    :param y_pred: numpy array of predicted binary labels\n    :return: Jaccard Index as a float value rounded to three decimal places\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    \n    # Handle the edge case where both arrays are all zeros\n    if union == 0:\n        return 0.0\n    \n    jaccard = intersection / union\n    return round(jaccard, 3)\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0])\ny_pred = np.array([1, 1, 1, 0, 0, 1, 0, 0])\n\nprint(jaccard_index(y_true, y_pred))  # Expected output: 0.500\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays y_true and y_pred.\n    \n    Parameters:\n    - y_true: np.array, ground truth binary array\n    - y_pred: np.array, predicted binary array\n    \n    Returns:\n    - score: float, Dice Score rounded to 3 decimal places\n    \"\"\"\n    # Ensure both inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the number of true positives, false positives, and false negatives\n    intersection = np.sum(y_true * y_pred)\n    true_positives = intersection\n    false_positives = np.sum(y_pred) - intersection\n    false_negatives = np.sum(y_true) - intersection\n    \n    # Calculate the Dice Score\n    if (true_positives + false_positives + false_negatives) == 0:\n        # Avoid division by zero in case of empty intersection and no true or predicted positives\n        return round(0.0, 3)\n    else:\n        score = (2.0 * true_positives) / (true_positives + false_positives + false_negatives)\n        return round(score, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0])\n    y_pred = np.array([1, 0, 1, 1, 0, 1, 1, 0])\n    print(f\"Dice Score: {dice_score(y_true, y_pred)}\")\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    - list: A composite hypervector representing the entire row.\n    \"\"\"\n    def create_hypervector(value, seed, dim):\n        np.random.seed(seed)\n        return np.random.randint(2, size=dim).tolist()\n    \n    # Initialize an empty list for the composite hypervector\n    composite_hv = []\n    \n    # Loop through each feature in the row\n    for feature, value in row.items():\n        # Get the seed for the current feature from the random_seeds dictionary\n        feature_seed = random_seeds.get(feature, np.random.randint(1e5))\n        \n        # Create a hypervector for the current feature and its value\n        hv = create_hypervector(value, feature_seed, dim)\n        \n        # Bind the hypervector with the composite hypervector\n        if not composite_hv:\n            composite_hv = hv\n        else:\n            composite_hv = np.bitwise_xor(composite_hv, hv).tolist()\n    \n    return composite_hv\n\n# Example usage\nif __name__ == \"__main__\":\n    row = {\"feature1\": 5, \"feature2\": \"apple\", \"feature3\": 3.14}\n    dim = 100\n    random_seeds = {\"feature1\": 42, \"feature2\": 1001, \"feature3\": 314}\n    composite_hv = create_row_hv(row, dim, random_seeds)\n    print(composite_hv)\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification based on the true and predicted labels provided in the dataset.\n\n    Parameters:\n    data (list of lists): A list where each element is a sublist [y_true, y_pred] representing the true and predicted labels for each observation.\n\n    Returns:\n    list: A 2x2 confusion matrix represented as a list of lists, where the order of elements is [[true_negative, false_positive], [false_negative, true_positive]].\n    \"\"\"\n    # Count occurrences of each combination of true and predicted labels\n    label_counts = Counter(tuple(item) for item in data)\n    \n    # Initialize the confusion matrix with zeros\n    cm = [[0, 0], [0, 0]]\n    \n    # Fill in the confusion matrix based on the counts\n    for (y_true, y_pred), count in label_counts.items():\n        if y_true == 0 and y_pred == 0:\n            cm[0][0] += count\n        elif y_true == 0 and y_pred == 1:\n            cm[1][0] += count\n        elif y_true == 1 and y_pred == 0:\n            cm[0][1] += count\n        elif y_true == 1 and y_pred == 1:\n            cm[1][1] += count\n    \n    return cm\n\n# Example usage\ndata = [\n    [0, 0], [0, 0], [0, 1], [0, 1],\n    [1, 0], [1, 0], [1, 1], [1, 1], [1, 1]\n]\n\ncm = confusion_matrix(data)\nprint(\"Confusion Matrix:\", cm)\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Parameters:\n    - v1: Numpy array representing the first vector.\n    - v2: Numpy array representing the second vector.\n\n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n    \n    Constraints:\n    - Both input vectors must have the same shape.\n    - Input vectors cannot be empty or have zero magnitude.\n    \"\"\"\n    # Check if vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Vectors must have the same shape\")\n\n    # Check if vectors are not empty and not zero magnitude\n    if np.any(v1 == 0) or np.any(v2 == 0) or len(v1) == 0 or len(v2) == 0:\n        raise ValueError(\"Vectors cannot be empty or have zero magnitude\")\n\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    v1 = np.array([1, 2, 3, 4, 5])\n    v2 = np.array([2, 4, 6, 8, 10])\n    print(cosine_similarity(v1, v2))  # Output the cosine similarity\n\n    v3 = np.array([0, 1, 0, 1])\n    v4 = np.array([1, 0, 1, 0])\n    print(cosine_similarity(v3, v4))  # Output the cosine similarity\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_recall_fscore_support\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Check if both lists have the same length\n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted lists must have the same length.\")\n    \n    # Check if all elements are either 0 or 1\n    if not all(label in [0, 1] for label in actual + predicted):\n        raise ValueError(\"All elements in actual and predicted lists must be either 0 or 1.\")\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(actual, predicted)\n    \n    # Unpack confusion matrix counts\n    tn, fp, fn, tp = cm.ravel()\n    \n    # Compute accuracy\n    accuracy = round(accuracy_score(actual, predicted), 3)\n    \n    # Compute F1 score\n    f1 = round(f1_score(actual, predicted), 3)\n    \n    # Compute specificity\n    specificity = round(tn / (tn + fp), 3)\n    \n    # Compute negative predictive value\n    npv = round(tn / (tn + fn), 3)\n    \n    return (cm, accuracy, f1, specificity, npv)\n\n# Example usage\nactual_labels = [1, 0, 1, 1, 0, 1, 0, 1, 0, 1]\npredicted_labels = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]\n\nresult = performance_metrics(actual_labels, predicted_labels)\nprint(result)\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n    - data: A list or NumPy array of numerical values\n    \n    Returns:\n    A dictionary containing the following statistics:\n    - mean\n    - median\n    - mode\n    - variance\n    - standard deviation\n    - 25th percentile\n    - 50th percentile (median)\n    - 75th percentile\n    - interquartile range (IQR)\n    \"\"\"\n    # Convert data to numpy array if it's not already one\n    data = np.array(data)\n    \n    # Calculate mean\n    mean_value = np.mean(data)\n    \n    # Calculate median\n    median_value = np.median(data)\n    \n    # Calculate mode\n    mode_value = stats.mode(data)[0][0] if len(stats.mode(data)[1][0]) > 0 else np.nan\n    \n    # Calculate variance\n    variance_value = np.var(data)\n    \n    # Calculate standard deviation\n    std_dev_value = np.std(data)\n    \n    # Calculate percentiles\n    twenty_five_percentile = np.percentile(data, 25)\n    fifty_percentile = np.percentile(data, 50)\n    seventy_five_percentile = np.percentile(data, 75)\n    \n    # Calculate interquartile range\n    iqr_value = seventy_five_percentile - twenty_five_percentile\n    \n    # Round all values to four decimal places\n    result = {\n        'mean': round(mean_value, 4),\n        'median': round(median_value, 4),\n        'mode': round(mode_value, 4) if not np.isnan(mode_value) else np.nan,\n        'variance': round(variance_value, 4),\n        'standard_deviation': round(std_dev_value, 4),\n        '25th_percentile': round(twenty_five_percentile, 4),\n        '50th_percentile': round(fifty_percentile, 4),\n        '75th_percentile': round(seventy_five_percentile, 4),\n        'interquartile_range': round(iqr_value, 4)\n    }\n    \n    return result\n\n# Example usage\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]\nstats = descriptive_statistics(data)\nprint(stats)\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    # Binomial coefficient (n choose k)\n    def binom_coefficient(n, k):\n        return math.comb(n, k)\n\n    # Binomial probability formula\n    q = 1 - p\n    binom_prob = binom_coefficient(n, k) * (p ** k) * (q ** (n - k))\n    return round(binom_prob, 5)\n\n# Example usage\nn = 10\nk = 3\np = 0.5\nprint(f\"The probability of getting exactly {k} successes in {n} trials is: {binomial_probability(n, k, p)}\")\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    # PDF formula for the normal distribution: (1 / (\u03c3 * sqrt(2\u03c0))) * e^(-(x - \u03bc)^2 / (2\u03c3^2))\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    \n    # Round the result to 5 decimal places\n    return round(pdf_value, 5)\n\n# Example usage\nif __name__ == \"__main__\":\n    x = 0\n    mean = 0\n    std_dev = 1\n    print(f\"PDF value at x={x} with mean={mean} and std_dev={std_dev}: {normal_pdf(x, mean, std_dev)}\")\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    # Poisson distribution formula: P(x) = (e^(-\u03bb) * \u03bb^x) / x!\n    probability = (math.exp(-lam) * pow(lam, k)) / math.factorial(k)\n    return round(probability, 5)\n\n# Example usage:\nk = 3\nlam = 2\nprint(f\"Probability of observing exactly {k} events when the mean rate is {lam}: {poisson_probability(k, lam)}\")\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast value of the image.\n    \"\"\"\n    # Check if the input is a 2D numpy array\n    if not isinstance(img, np.ndarray) or len(img.shape) != 2:\n        raise ValueError(\"Input must be a 2D numpy array representing a grayscale image.\")\n    \n    # Calculate the maximum and minimum pixel values\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    \n    # Calculate contrast as the difference between max and min pixel values\n    contrast = max_pixel_value - min_pixel_value\n    \n    return contrast\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample grayscale image (2D numpy array)\n    sample_img = np.array([[0, 10, 20],\n                           [30, 40, 50],\n                           [60, 70, 80]])\n    \n    # Calculate and print the contrast of the sample image\n    contrast_value = calculate_contrast(sample_img)\n    print(f\"Contrast of the sample image: {contrast_value}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        int or float: The dot product of the two input vectors.\n    \"\"\"\n    # Check if both inputs are 1D numpy arrays\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise ValueError(\"Both inputs must be numpy arrays.\")\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Inputs must be 1D arrays.\")\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Inputs must have the same shape.\")\n\n    # Calculate the dot product\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\n\n# Example usage\nif __name__ == \"__main__\":\n    vec1 = np.array([1, 2, 3])\n    vec2 = np.array([4, 5, 6])\n    result = calculate_dot_product(vec1, vec2)\n    print(f\"Dot product of vec1 and vec2: {result}\")\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    # Convert the input list to a numpy array for easier manipulation\n    data_array = np.array(data)\n\n    # Initialize the transformed data list\n    transformed_data = []\n\n    # Generate polynomial features\n    for item in data_array:\n        transformed_features = [item**i for i in range(1, degree + 1)]\n        transformed_data.append(transformed_features)\n\n    # Round the results to 8 decimal places\n    transformed_data_rounded = [[round(feature, 8)] for feature in np.array(transformed_data).T]\n\n    return transformed_data_rounded\n\n# Example usage\ndata_point = [2.0]\ndegree = 3\nprint(phi_transform(data_point, degree))\n\ndata_points = [1.0, 2.0, 3.0]\ndegree = 2\nprint(phi_transform(data_points, degree))\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encoding for a given position and model dimensionality.\n\n    Args:\n        position (int): The position in the sequence.\n        d_model (int): The dimensionality of the model.\n\n    Returns:\n        list: The positional encoding as a float16 numpy array converted to a list.\n\n    Note:\n        If position is 0 or d_model is less than or equal to 0, return -1.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n\n    pe = np.zeros(d_model, dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[i] += np.sin(pos / (10000 ** (i / d_model)))\n            pe[i + 1] += np.cos(pos / (10000 ** ((i + 1) / d_model)))\n\n    return pe.tolist()\n\n# Example usage\nposition = 50\nd_model = 24\npositional_encoding = pos_encoding(position, d_model)\nprint(positional_encoding)\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Define thresholds for overfitting and underfitting\n    overfitting_threshold = 0.2\n    underfitting_threshold = 0.7\n\n    # Check for overfitting\n    if training_accuracy - test_accuracy > overfitting_threshold:\n        return 1  # Overfitting\n\n    # Check for underfitting\n    elif training_accuracy < underfitting_threshold and test_accuracy < underfitting_threshold:\n        return -1  # Underfitting\n\n    # Otherwise, it's a good fit\n    else:\n        return 0  # Good fit\n\n# Example check function to test the model_fit_quality function\ndef check_model_fit_quality():\n    # Test cases\n    test_cases = [\n        (0.95, 0.73),  # Overfitting\n        (0.65, 0.65),  # Underfitting\n        (0.85, 0.84),  # Good fit\n        (0.78, 0.92)   # Not overfitting; not underfitting; good fit\n    ]\n\n    # Expected results\n    expected_results = [1, -1, 0, 0]\n\n    # Run tests\n    for i, (training_acc, test_acc) in enumerate(test_cases):\n        result = model_fit_quality(training_acc, test_acc)\n        if result == expected_results[i]:\n            print(f\"Test {i+1} passed.\")\n        else:\n            print(f\"Test {i+1} failed: Expected {expected_results[i]}, got {result}\")\n\n# Run the check function\ncheck_model_fit_quality()\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Bias correction terms\n    m_t = beta1 * m + (1 - beta1) * grad\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    m_corrected = m_t / (1 - beta1 ** (t + 1))\n    v_corrected = v_t / (1 - beta2 ** (t + 1))\n    \n    # Parameter update\n    parameter_updated = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    \n    return [np.round(parameter_updated.tolist(), 5), np.round(m_t.tolist(), 5), np.round(v_t.tolist(), 5)]\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    # Load the dummy encoder, hyperparameters, and model parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Tokenize the prompt\n    token_ids = encoder.encode(prompt)\n    \n    # Initialize the context with the prompt's token IDs\n    context = np.array(token_ids, dtype=np.int32).reshape((1, len(token_ids)))\n\n    # Generate the text\n    generated_text_ids = []\n    for _ in range(n_tokens_to_generate):\n        # Retrieve the current context token\n        curr_ctx = context[:, -hparams[\"n_ctx\"]:]\n        \n        # Simulate attention (for simplicity, just use the last token)\n        attended_token = curr_ctx[0][-1]\n        \n        # Retrieve the embedding for the attended token\n        attended_emb = params[\"wte\"][attended_token]\n        \n        # Simulate feed-forward network (for simplicity, just add a random vector)\n        ff_output = attended_emb + np.random.rand(1, 10)\n        \n        # Normalize the output\n        normalized_output = (ff_output - params[\"ln_f\"][\"b\"]) / params[\"ln_f\"][\"g\"]\n        \n        # Predict the next token ID using a simple softmax over the vocabulary\n        next_token_prob = np.exp(normalized_output) / np.sum(np.exp(normalized_output))\n        next_token_id = np.random.choice(len(encoder.encoder_dict), p=next_token_prob)\n        \n        # Append the predicted token ID to the generated text\n        generated_text_ids.append(next_token_id)\n\n    # Decode the generated token IDs back to text\n    generated_text = encoder.decode(generated_text_ids)\n\n    # Return the generated text\n    return generated_text\n\n# Example usage\ngenerated_text = gen_text(\"hello\", n_tokens_to_generate=5)\nprint(generated_text)\n```"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        \"\"\"Compute the softmax of a vector.\"\"\"\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n\n    # Ensure input is valid\n    if not all(isinstance(x, (int, float)) for x in crystal_values) or len(crystal_values) != n:\n        raise ValueError(\"Invalid input: crystal_values must be a list of numeric values with length n\")\n\n    # Initialize the attention matrix\n    attention_matrix = np.zeros((n, n))\n\n    # Calculate the attention scores between each pair of crystals\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                attention_matrix[i][j] = np.exp(crystal_values[i] * crystal_values[j])\n\n    # Apply softmax to transform the attention scores\n    normalized_attention_matrix = softmax(attention_matrix.sum(axis=1))\n\n    # Derive the final weighted pattern for each crystal\n    weighted_patterns = []\n    for i in range(n):\n        pattern_sum = sum(attention_matrix[i] * crystal_values[j] for j in range(n) if j != i)\n        weighted_patterns.append(pattern_sum * normalized_attention_matrix[i])\n\n    # Return the final weighted patterns rounded to 4 decimal places\n    return [round(value, 4) for value in weighted_patterns]\n\n# Example usage:\ncrystals = [1.0, 2.0, 3.0]\nn_crystals = len(crystals)\ndimensionality = 50  # This can be ignored in this simplified version as we don't use dimensions directly\n\nenhanced_patterns = pattern_weaver(n_crystals, crystals, dimensionality)\nprint(enhanced_patterns)\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_tf(term, doc):\n    return doc.count(term) / len(doc)\n\ndef calculate_idf(term, corpus):\n    df = sum([1 for doc in corpus if term in doc])\n    N = len(corpus)\n    return np.log((N - df + 0.5) / (df + 0.5))\n\ndef calculate_avg_doc_length(corpus):\n    lengths = [len(doc) for doc in corpus]\n    return np.mean(lengths), np.std(lengths)\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    N = len(corpus)\n    avg_len, std_dev = calculate_avg_doc_length(corpus)\n    \n    # Splitting query into terms\n    query_terms = set(query.split())\n    \n    # Calculating IDF values for all terms in the query\n    idfs = {term: calculate_idf(term, corpus) for term in query_terms}\n    \n    # Calculating BM25 scores for each document\n    bm25_scores = []\n    for doc in corpus:\n        score = 0\n        doc_terms = set(doc.split())\n        \n        # Calculating TF for each term in the query\n        for term in query_terms:\n            tf = calculate_tf(term, doc)\n            idf = idfs[term]\n            \n            # Applying BM25 formula\n            score += (idf * tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len(doc) / avg_len))\n        \n        bm25_scores.append(round(score, 3))\n    \n    return bm25_scores\n\n# Example usage\ncorpus = [\n    \"the quick brown fox jumps over the lazy dog\",\n    \"the dog barked at the cat\",\n    \"the fox was seen near the dog\",\n    \"jumps over the dog again\"\n]\n\nquery = \"the fox\"\n\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n```"}
{"task_id": 91, "completion_id": 0, "completion": "```markdown\n```python\nfrom sklearn.metrics import f1_score\n\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Calculate the F1 score using sklearn's f1_score function\n    score = f1_score(y_true, y_pred, average='binary')\n    \n    # Round the score to three decimal places\n    return round(score, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n    y_pred = [0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n    print(calculate_f1_score(y_true, y_pred))  # Output should be the F1 score rounded to 3 decimal places\n```\n```\n\nThe provided code defines a function `calculate_f1_score` that calculates the F1 score using scikit-learn's `f1_score` function. It takes two lists as input: `y_true` for the true labels and `y_pred` for the predicted labels. The function then returns the F1 score rounded to three decimal places. An example usage is also included to demonstrate how the function can be called."}
{"task_id": 92, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef power_grid_forecast(consumption_data):\n    PI = 3.14159\n    \n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    fluctuation = 10 * np.sin(2 * PI * np.arange(1, len(consumption_data) + 1) / 10)\n    detrended_data = consumption_data - fluctuation\n    \n    # 2) Perform linear regression on the detrended data.\n    X = np.arange(1, len(detrended_data) + 1).reshape(-1, 1)\n    y = detrended_data\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # 3) Predict day 15's base consumption.\n    day_15_detrended = model.predict(np.array([15]).reshape(-1, 1))[0]\n    \n    # 4) Add the day 15 fluctuation back.\n    day_15_fluctuation = 10 * np.sin(2 * PI * 15 / 10)\n    day_15_consumption = round(day_15_detrended) + int(day_15_fluctuation * 1.05)\n    \n    return day_15_consumption\n\n# Example consumption data for 10 days\nconsumption_data = [10, 12, 15, 18, 21, 24, 27, 30, 33, 36]\n\nfinal_consumption_prediction = power_grid_forecast(consumption_data)\nprint(final_consumption_prediction)\n```\n```\n\nThis script defines a function `power_grid_forecast` which takes a list of daily power usage measurements as input. It then subtracts the known daily fluctuation from each data point, fits a linear regression model to the detrended data, predicts the base consumption for day 15, adds back the fluctuation for day 15, applies a 5% safety margin, and returns the final integer value. The example consumption data represents a hypothetical scenario for 10 days, and the function will be applied to this data to make the prediction for day 15."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate absolute differences between true and predicted values\n    abs_diff = np.abs(y_true - y_pred)\n    \n    # Calculate the mean of the absolute differences\n    mae_value = np.mean(abs_diff)\n    \n    # Return the MAE rounded to 3 decimal places\n    return round(mae_value, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([1.0, 2.0, 3.0, 4.0])\n    y_pred = np.array([1.0, 2.5, 2.5, 4.0])\n    print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the query, key, and value vectors from the input matrix X using the weight matrices W_q, W_k, and W_v.\n    \n    Args:\n    - X (np.ndarray): Input embeddings of shape (batch_size, seq_length, embedding_dim).\n    - W_q (np.ndarray): Weight matrix for queries of shape (embedding_dim, q_k_dim).\n    - W_k (np.ndarray): Weight matrix for keys of shape (embedding_dim, q_k_dim).\n    - W_v (np.ndarray): Weight matrix for values of shape (embedding_dim, v_dim).\n    \n    Returns:\n    - tuple: A tuple containing query (Q), key (K), and value (V) matrices, each of shape (batch_size, seq_length, head_dim).\n    \"\"\"\n    q_k_dim = W_q.shape[1]\n    v_dim = W_v.shape[1]\n    \n    # Project the input embeddings into query, key, and value vectors\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    \n    # Split the projection matrices into multiple heads\n    batch_size, seq_length, embedding_dim = X.shape\n    head_dim = q_k_dim // n_heads\n    \n    Q = np.reshape(Q, (batch_size, seq_length, n_heads, head_dim)).transpose(0, 2, 1, 3).reshape(batch_size * n_heads, seq_length, head_dim)\n    K = np.reshape(K, (batch_size, seq_length, n_heads, head_dim)).transpose(0, 2, 1, 3).reshape(batch_size * n_heads, seq_length, head_dim)\n    V = np.reshape(V, (batch_size, seq_length, n_heads, v_dim)).transpose(0, 2, 1, 3).reshape(batch_size * n_heads, seq_length, head_dim)\n    \n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention scores and apply the attention mechanism.\n    \n    Args:\n    - Q (np.ndarray): Query matrix of shape (batch_size * n_heads, seq_length, head_dim).\n    - K (np.ndarray): Key matrix of shape (batch_size * n_heads, seq_length, head_dim).\n    - V (np.ndarray): Value matrix of shape (batch_size * n_heads, seq_length, head_dim).\n    \n    Returns:\n    - np.ndarray: The output of the self-attention mechanism of shape (batch_size * n_heads, seq_length, head_dim).\n    \"\"\"\n    # Compute the scaled dot-product attention\n    attention_scores = np.matmul(Q, K.transpose(0, 1)) / np.sqrt(K.shape[-1])\n    attention_probs = np.softmax(attention_scores, axis=-1)\n    \n    # Apply the attention mechanism to the values\n    output = np.matmul(attention_probs, V)\n    output = np.reshape(output, (batch_size, n_heads, seq_length, head_dim)).transpose(0, 2, 1, 3).reshape(batch_size, seq_length, n_heads * head_dim)\n    \n    return np.round(output, decimals=4)\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head self-attention mechanism for a given input matrix X and weight matrices W_q, W_k, W_v.\n    \n    Args:\n    - X (np.ndarray): Input embeddings of shape (batch_size, seq_length, embedding_dim).\n    - W_q (np.ndarray): Weight matrix for queries of shape (embedding_dim, q_k_dim).\n    - W_k (np.ndarray): Weight matrix for keys of shape (embedding_dim, q_k_dim).\n    - W_v (np.ndarray): Weight matrix for values of shape (embedding_dim, v_dim).\n    - n_heads (int): Number of attention heads.\n    \n    Returns:\n    - list: A list containing the output of the multi-head self-attention mechanism for each head.\n    \"\"\"\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    head_outputs = [self_attention(Q[:, i*head_dim:(i+1)*head_dim], K[:, i*head_dim:(i+1)*head_dim], V[:, i*head_dim:(i+1)*head_dim]) for i in range(n_heads)]\n    \n    # Return the outputs as a list for each head\n    return [np.round(head_output.tolist(), decimals=4) for head_output in head_outputs]\n\n# Example usage\nbatch_size = 2\nseq_length = 4\nembedding_dim = 8\nn_heads = 2\nq_k_dim = 4\nv_dim = 4\n\nX = np.random.rand(batch_size, seq_length, embedding_dim)\nW_q = np.random.rand(embedding_dim, q_k_dim)\nW_k = np.random.rand(embedding_dim, q_k_dim)\nW_v = np.random.rand(embedding_dim, v_dim)\n\noutput_heads = multi_head_attention(X, W_q, W_k, W_v, n_heads)\nfor head_output in output_heads:\n    print(head_output)\n```"}
{"task_id": 95, "completion_id": 0, "completion": "```python\nfrom math import sqrt\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n\n    # Count the occurrences of each combination of binary values\n    true_true = sum(1 for i in range(len(x)) if x[i] == 1 and y[i] == 1)\n    true_false = sum(1 for i in range(len(x)) if x[i] == 1 and y[i] == 0)\n    false_true = sum(1 for i in range(len(x)) if x[i] == 0 and y[i] == 1)\n    false_false = sum(1 for i in range(len(x)) if x[i] == 0 and y[i] == 0)\n\n    # Calculate the totals for rows and columns\n    row_total_ones = true_true + true_false\n    row_total_zeros = false_true + false_false\n    col_total_ones = true_true + false_true\n    col_total_zeros = true_false + false_false\n    grand_total = row_total_ones + row_total_zeros\n\n    # Compute the Phi coefficient\n    phi_squared = (true_true * false_false - true_false * false_true) ** 2 / (row_total_ones * row_total_zeros * col_total_ones * col_total_zeros)\n\n    # Return the square root of phi_squared to get the Phi coefficient\n    return round(sqrt(phi_squared), 4)\n\n# Example usage\nx = [0, 1, 1, 0, 1, 1, 0, 0, 1, 1]\ny = [1, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n\nphi_coefficient = phi_corr(x, y)\nprint(f\"Phi Coefficient: {phi_coefficient}\")\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0\n    elif x > 2.5:\n        return 1\n    else:\n        return 0.2 * x + 0.5\n\n# Test the hard_sigmoid function\nimport numpy as np\n\n# Define test cases\ntest_values = [-3, -2.6, -1, 0, 1, 2.4, 3]\nexpected_results = [0, 0, 0.3, 0.5, 0.7, 0.98, 1]\n\n# Run tests\nfor value, expected in zip(test_values, expected_results):\n    result = hard_sigmoid(value)\n    print(f\"Input: {value}, Expected Output: {expected}, Actual Output: {result}\")\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n\n# Example usage:\ninput_value = -2.0\nelu_output = elu(input_value)\nprint(f\"ELU activation of {input_value} is {round(elu_output, 4)}\")\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```markdown\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(alpha * x, x)\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    import numpy as np\n\n    # Define a range of input values\n    x_values = np.linspace(-10, 10, 100)\n    \n    # Compute PReLU for each value with default alpha\n    y_values_default_alpha = [prelu(x) for x in x_values]\n    \n    # Compute PReLU for each value with a different alpha\n    y_values_custom_alpha = [prelu(x, alpha=0.5) for x in x_values]\n\n    # Print the results or plot them to visualize the activation function\n    print(\"PReLU with default alpha:\", y_values_default_alpha[:10], \"...\", y_values_default_alpha[-10:])\n    print(\"PReLU with custom alpha:\", y_values_custom_alpha[:10], \"...\", y_values_custom_alpha[-10:])\n\n    # Optionally, you can plot the results using matplotlib\n    try:\n        import matplotlib.pyplot as plt\n\n        plt.plot(x_values, y_values_default_alpha, label='Default alpha (0.25)')\n        plt.plot(x_values, y_values_custom_alpha, label='Custom alpha (0.5)')\n        plt.xlabel('Input Value')\n        plt.ylabel('PReLU Activation Value')\n        plt.title('PReLU Activation Function')\n        plt.legend()\n        plt.show()\n    except ImportError:\n        print(\"matplotlib is not installed. Please install it to visualize the activation function.\")\n```\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x > 20:\n        # Prevents numerical overflow for large positive values\n        return math.log(1 + math.exp(x - 20)) + 20\n    elif x < -20:\n        # Prevents numerical underflow for large negative values\n        return math.log(1 + math.exp(x + 20)) - 20\n    else:\n        # Normal computation of softplus\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Example usage\ninput_value = 1.5\noutput_value = softplus(input_value)\nprint(f\"Softplus({input_value}) = {output_value}\")\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    # The Softsign function is defined as x / (1 + abs(x))\n    result = x / (1 + np.abs(x))\n    return round(result, 4)\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    test_values = [-100.0, -0.5, 0.0, 0.5, 100.0]\n    expected_results = [-0.999, -0.3333, 0.0, 0.3333, 0.999]\n    \n    for value in test_values:\n        computed_result = softsign(value)\n        assert computed_result == expected_results[test_values.index(value)], f\"Failed for input {value}. Expected {expected_results[test_values.index(value)]}, got {computed_result}\"\n        print(f\"Softsign({value}) = {computed_result}\")\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence(p, q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two probability distributions p and q.\n    Args:\n        p: List or array of probability distribution for p.\n        q: List or array of probability distribution for q.\n    Returns:\n        The computed KL divergence.\n    \"\"\"\n    return np.sum(np.where(p != 0, p * np.log(p / q), 0)).round(6)\n\ndef clip_value(x, min_val, max_val):\n    \"\"\"\n    Clip x to be within the interval [min_val, max_val].\n    Args:\n        x: Input value to be clipped.\n        min_val: Minimum value of the interval.\n        max_val: Maximum value of the interval.\n    Returns:\n        Clipped value.\n    \"\"\"\n    return np.clip(x, min_val, max_val).round(6)\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Calculate the surrogate loss using the GRPO objective formula\n    surr = np.where(rhos >= 1 + epsilon, (rhos - 1 - beta * np.array(pi_theta_ref) / np.array(pi_theta_old)).dot(A),\n                    np.where(rhos <= 1 - epsilon, (rhos - 1 + beta * np.array(pi_theta_ref) / np.array(pi_theta_old)).dot(A),\n                             rhos.dot(A)\n                            )\n    \n    # Return the mean of the surrogate loss\n    return surr.mean().round(6)\n\n# Example usage\nrhos = np.array([1.2, 0.8, 1.5, 0.9, 1.1])\nA = np.array([0.5, -0.3, 0.7, -0.6, 0.4])\npi_theta_old = np.array([0.1, 0.2, 0.3, 0.4, 0.0])\npi_theta_ref = np.array([0.05, 0.15, 0.35, 0.45, 0.05])\n\n# Calculate the GRPO objective\ngrpo_obj_value = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\nprint(f\"GRPO Objective Value: {grpo_obj_value}\")\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```markdown\n```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * sigmoid(x)\n\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the sigmoid activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The sigmoid activation value\n    \"\"\"\n    return 1 / (1 + math.exp(-x))\n\n# Test the function with a sample input\ninput_value = 2.0\noutput_value = swish(input_value)\nrounded_output_value = round(output_value, 4)\nprint(f\"Swish({input_value}) = {rounded_output_value}\")\n```\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```markdown\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    if x >= 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n# Test cases to ensure numerical stability and correctness\ntest_values = [0.0, 1.0, -1.0, 2.0, -2.0, 0.5, -0.5, 1e-10, -1e-10]\nrounded_results = {val: round(selu(val), 4) for val in test_values}\nprint(rounded_results)\n```\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Calculate the weighted sum of inputs + bias\n    z = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    \n    # Convert probabilities to binary predictions based on 0.5 threshold\n    predictions = [1 if prob > 0.5 else 0 for prob in probabilities]\n    \n    # Return the predictions as a list\n    return predictions\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Input feature matrix (3 samples, 2 features each)\n    X = np.array([[1.0, 2.0], [2.0, 1.0], [0.5, 0.5]])\n    \n    # Model weights (2 features)\n    weights = np.array([0.4, -0.6])\n    \n    # Bias term\n    bias = 0.0\n    \n    # Predict the binary labels\n    predicted_labels = predict_logistic(X, weights, bias)\n    \n    print(predicted_labels)  # Expected output might be [1, 1, 0] depending on the input values\n```\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Add bias term to X\n    X = np.insert(X, 0, 1, axis=1)\n    \n    # Initialize weights with random values\n    num_features = X.shape[1]\n    num_classes = len(np.unique(y))\n    B = np.random.rand(num_classes, num_features)\n    \n    # One-hot encoding for labels\n    Y_one_hot = np.zeros((y.size, num_classes))\n    Y_one_hot[np.arange(y.size), y] = 1\n    \n    # Collect losses\n    losses = []\n    \n    # Softmax function\n    def softmax(Z):\n        A = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n        return A\n    \n    # Cross-Entropy Loss\n    def cross_entropy_loss(A, Y):\n        loss = -np.mean(np.sum(Y * np.log(A), axis=1))\n        return round(loss, 4)\n    \n    # Forward pass\n    def forward_pass(X, B):\n        Z = X @ B.T\n        A = softmax(Z)\n        return A\n    \n    # Backward pass (gradient computation)\n    def backward_pass(X, B, A, Y):\n        m = Y.size\n        dZ = A - Y\n        dB = (1/m) * dZ @ X.T\n        return dB\n    \n    # Training loop\n    for i in range(iterations):\n        A = forward_pass(X, B)\n        loss = cross_entropy_loss(A, Y_one_hot)\n        losses.append(loss)\n        \n        # Update weights\n        dB = backward_pass(X, B, A, Y_one_hot)\n        B -= learning_rate * dB\n    \n    return [B.round(4).tolist(), losses]\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function to map any real value to a value between 0 and 1.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef binary_cross_entropy_loss(y_true, y_pred):\n    \"\"\"\n    Binary Cross Entropy Loss function.\n    \"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n    return np.mean(loss)\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add bias term to input features\n    X = np.insert(X, 0, 1, axis=1)\n    m, n = X.shape\n    # Initialize weights\n    theta = np.zeros(n).reshape(-1, 1)\n    \n    # Lists to store loss values\n    losses = []\n    \n    # Gradient descent\n    for _ in range(iterations):\n        # Compute the hypothesis\n        hypothesis = sigmoid(np.dot(X, theta))\n        # Compute the gradient\n        gradient = np.dot(X.T, (hypothesis - y)) / m\n        # Update the weights\n        theta -= learning_rate * gradient\n        \n        # Compute the loss and append it to the losses list\n        loss = binary_cross_entropy_loss(y, hypothesis)\n        losses.append(round(loss, 4))\n    \n    return theta.flatten().tolist(), losses\n```\n\nThis script defines a logistic regression trainer using gradient descent. The `sigmoid` function maps predictions to probabilities, and the `binary_cross_entropy_loss` function calculates the loss. The `train_logreg` function takes input features `X`, target labels `y`, learning rate, and number of iterations to optimize the model parameters. It returns the optimized coefficients and a list of losses over iterations, rounded to the 4th decimal place."}
{"task_id": 107, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Ensure mask is broadcastable\n    if mask.ndim < K.ndim:\n        mask = mask[np.newaxis, np.newaxis, :, :]\n    \n    # Compute the dot product of Q and K, then apply the mask\n    score = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    score = np.where(mask == 0, -1e9, score)  # Mask out invalid values\n    \n    # Apply softmax to obtain attention weights\n    attention_weights = np.exp(score) / np.sum(np.exp(score), axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of V using the attention weights\n    output = np.dot(attention_weights, V)\n    return output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.random.randn(10, 32)  # Batch size 10, sequence length 32, embedding dimension 32\n    W_q = np.random.randn(32, 64)  # Query weight matrix\n    W_k = np.random.randn(32, 64)  # Key weight matrix\n    W_v = np.random.randn(32, 64)  # Value weight matrix\n    \n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    mask = np.tril(np.ones((1, 1, 32, 32)))  # Lower triangular mask for causal masking\n    \n    output = masked_attention(Q, K, V, mask)\n    print(output)\n```\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    # Count the frequency of each apple color\n    color_counts = Counter(apples)\n    \n    # Calculate the total number of apples\n    total_apples = len(apples)\n    \n    # Calculate the probability of each color\n    probabilities = [count / total_apples for count in color_counts.values()]\n    \n    # Calculate disorder using the Shannon entropy formula\n    disorder_score = -sum(p * (p ** -1) if p != 0 else 0 for p in probabilities)\n    \n    # Normalize the score to ensure it's between 0 and 1\n    normalized_disorder = 1 - (disorder_score / max(entropy(total_apples), 1e-9))\n    \n    # Round the result to the nearest 4th decimal\n    return round(normalized_disorder, 4)\n\ndef entropy(n: int) -> float:\n    \"\"\"\n    Helper function to calculate the maximum possible entropy given n items.\n    \"\"\"\n    return sum(-p * (p ** -1) if p != 0 else 0 for p in [1/n] * n)\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: Input tensor of shape (batch_size, sequence_length, feature_dim)\n    - gamma: Scaling parameter of shape (1, 1, feature_dim)\n    - beta: Shifting parameter of shape (1, 1, feature_dim)\n    - epsilon: Small value to stabilize division\n    \n    Returns:\n    - Normalized and scaled tensor, with values rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    batch_size, sequence_length, feature_dim = X.shape\n    assert gamma.shape == (1, 1, feature_dim), \"Gamma must have the same feature dimension as X\"\n    assert beta.shape == (1, 1, feature_dim), \"Beta must have the same feature dimension as X\"\n    \n    # Calculate mean and variance across the feature dimension for each sequence\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    \n    # Perform normalization\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scaling and shifting\n    X_scaled_and_shifted = gamma * X_normalized + beta\n    \n    # Round to 5 decimal places and convert to list\n    return np.round(X_scaled_and_shifted, 5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample input tensor\n    X = np.random.rand(10, 20, 30)\n    \n    # Sample gamma and beta\n    gamma = np.random.rand(1, 1, 30)\n    beta = np.random.rand(1, 1, 30)\n    \n    # Perform layer normalization\n    result = layer_normalization(X, gamma, beta)\n    print(result[:2])  # Print the first two sequences for verification\n```\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR score for evaluating machine translation quality.\n    \n    Parameters:\n    reference (str): The reference translation string.\n    candidate (str): The candidate translation string.\n    alpha (float): The alpha parameter for penalizing wrong matches.\n    beta (float): The beta parameter for penalizing word order errors.\n    gamma (float): The gamma parameter for penalizing length ratio.\n    \n    Returns:\n    float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    \n    # Tokenize the reference and candidate strings\n    ref_words = reference.lower().split()\n    cand_words = candidate.lower().split()\n    \n    # Matched words in both reference and candidate\n    matches = 0\n    ref_counts = Counter(ref_words)\n    cand_counts = Counter(cand_words)\n    \n    for word, count in ref_counts.items():\n        min_count = min(count, cand_counts[word])\n        matches += min_count\n        cand_counts[word] -= min_count\n    \n    # Incorrect matches are those where the candidate has more occurrences of the word than the reference\n    wrong_matches = sum(count for count in cand_counts.values() if count > 0)\n    \n    # Precision: Number of correct matches divided by number of words in candidate\n    prec = float(matches) / len(cand_words)\n    \n    # Penalty for wrong matches\n    pen_w = wrong_matches * (1 - alpha)\n    \n    # Recall: Number of correct matches divided by number of words in reference\n    rec = float(matches) / len(ref_words)\n    \n    # F-Measure\n    fmes = (1 + beta**2) * prec * rec / (beta**2 * prec + rec)\n    \n    # Length penalty\n    ref_len = len(ref_words)\n    cand_len = len(cand_words)\n    pen_l = (cand_len - ref_len) / ref_len if ref_len else (cand_len - ref_len)\n    \n    # Final METEOR score\n    meteor = (1 - gamma) * fmes - gamma * abs(pen_l)\n    \n    return round(meteor, 3)\n\n# Example usage\nreference_translation = \"the cat sat on the mat\"\ncandidate_translation = \"a cat was sitting on the mat\"\n\nprint(meteor_score(reference_translation, candidate_translation))\n```\n\n### Explanation:\n- **Reference and Candidate Tokenization**: Both the reference and candidate translations are converted to lower case and split into individual words.\n- **Match Calculation**: Words that appear in both the reference and candidate translations are counted. The `matches` variable keeps track of the correctly matched words.\n- **Precision Calculation**: Precision is the ratio of the number of correct matches to the total number of words in the candidate translation.\n- **Penalty for Wrong Matches**: A penalty is calculated for words that are incorrectly matched (more in the candidate than in the reference).\n- **Recall Calculation**: Recall is the ratio of the number of correct matches to the total number of words in the reference translation.\n- **F-Measure**: This combines precision and recall using the harmonic mean.\n- **Length Penalty**: A penalty is applied for different lengths between the reference and candidate translations.\n- **Final METEOR Score**: The final score is computed as a weighted combination of the F-measure and the length penalty.\n\n### Notes:\n- The parameters `alpha`, `beta`, and `gamma` control the penalties for wrong matches, word order errors, and length differences respectively.\n- The score is rounded to 3 decimal places for consistency and readability.\n```\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) for a pair of events.\n    \n    Parameters:\n    - joint_counts (int): The count of the joint occurrence of the two events.\n    - total_counts_x (int): The count of event X occurring.\n    - total_counts_y (int): The count of event Y occurring.\n    - total_samples (int): The total number of samples.\n    \n    Returns:\n    - float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Calculate the probability of the joint occurrence\n    p_xy = joint_counts / total_samples\n    \n    # Calculate the probability of each event occurring independently\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Calculate the PMI using the formula: PMI = log(p_xy / (p_x * p_y))\n    if p_x * p_y == 0:\n        pmi = 0\n    else:\n        pmi = np.log2(p_xy / (p_x * p_y))\n    \n    # Round the result to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\njoint_counts = 150\ntotal_counts_x = 300\ntotal_counts_y = 400\ntotal_samples = 1000\n\npmi_value = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)\nprint(f\"PMI Value: {pmi_value}\")\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max normalization on a list of integers.\n    \n    Parameters:\n    x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    # Avoid division by zero if all values are the same\n    if min_val == max_val:\n        return [round(0.0, 4) for _ in x]\n    \n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]\n\n# Example usage\nif __name__ == \"__main__\":\n    data = [10, 20, 30, 40, 50]\n    normalized_data = min_max(data)\n    print(\"Normalized Data:\", normalized_data)\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    This function implements a simple residual block using NumPy.\n    \n    Parameters:\n    x (np.ndarray): Input 1D array.\n    w1 (np.ndarray): Weight matrix for the first linear transformation.\n    w2 (np.ndarray): Weight matrix for the second linear transformation.\n    \n    Returns:\n    np.ndarray: Output of the residual block as a list with values rounded to 4 decimal places.\n    \"\"\"\n    # First linear transformation\n    x1 = np.dot(x, w1)\n    x1 = np.maximum(0, x1)  # Apply ReLU activation\n    \n    # Second linear transformation\n    x2 = np.dot(x1, w2)\n    x2 = np.maximum(0, x2 + x)  # Add shortcut connection and apply ReLU activation\n    \n    return np.round(x2, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input array and weights\n    x = np.array([1, 2, 3])\n    w1 = np.array([[0.1, -0.2, 0.3], [-0.4, 0.5, -0.6], [0.7, -0.8, 0.9]])\n    w2 = np.array([[0.9, 0.8, -0.7], [-0.6, 0.5, 0.4], [0.3, -0.2, 0.1]])\n    \n    # Create and print the residual block output\n    output = residual_block(x, w1, w2)\n    print(output)\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    if len(x.shape) != 3:\n        raise ValueError(\"Input array must be 3D\")\n\n    height, width, channels = x.shape\n    # Compute the average for each channel over the entire feature map\n    pooled = np.zeros(channels)\n    for i in range(channels):\n        pooled[i] = np.mean(x[:, :, i])\n\n    return pooled\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random 3D array for testing\n    x = np.random.rand(5, 5, 3)\n    print(\"Original Input Array (shape: {}, dtype {}):\\n{}\".format(x.shape, x.dtype, x))\n    \n    # Perform Global Average Pooling\n    result = global_avg_pool(x)\n    print(\"\\nGlobal Average Pooled Output Array (shape: {}, dtype {}):\\n{}\".format(result.shape, result.dtype, result))\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array (BCHW format).\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, channels, height, width).\n    gamma (np.ndarray): Scale parameter of shape (channels,).\n    beta (np.ndarray): Shift parameter of shape (channels,).\n    epsilon (float): Small value for numerical stability.\n\n    Returns:\n    np.ndarray: Normalized tensor with the same shape as X, rounded to 4 decimal places.\n    \"\"\"\n    # Compute mean and variance per channel over (batch, height, width)\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n\n    # Normalize the input\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n\n    # Apply scale and shift parameters\n    X_normalized = X_norm * gamma + beta\n\n    # Round the result to 4 decimal places\n    X_normalized_rounded = np.round(X_normalized, 4)\n\n    return X_normalized_rounded.tolist()\n\n# Example usage and check function\ndef check_batch_normalization():\n    np.random.seed(42)\n    X = np.random.rand(8, 16, 24, 24)  # Example input tensor\n    gamma = np.random.rand(16)         # Random gamma values for each channel\n    beta = np.random.rand(16)          # Random beta values for each channel\n\n    normalized_tensor = batch_normalization(X, gamma, beta)\n    \n    # Since the inputs are random, we can't predict exact output values,\n    # but we can check the shape of the output\n    assert normalized_tensor.shape == X.shape, \"The output tensor should have the same shape as input tensor.\"\n    print(\"Output tensor shape is correct:\", normalized_tensor.shape == X.shape)\n\n    # Print a sample output for verification\n    print(\"Sample output:\", normalized_tensor[:,:,:,:1])  # Print only the first slice of the first channel for brevity\n\ncheck_batch_normalization()\n```\n\nThis script defines a `batch_normalization` function that normalizes the input tensor `X` of shape (batch_size, channels, height, width) across the batch and spatial dimensions for each channel. It then applies the `gamma` and `beta` parameters to scale and shift the normalized values respectively. The function uses `epsilon` to avoid division by zero and ensures numerical stability. The output tensor is rounded to 4 decimal places and converted to a list before being returned. The `check_batch_normalization` function generates random data and parameters to verify the correctness of the implementation."}
{"task_id": 116, "completion_id": 0, "completion": "```python\nimport math\n\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the term.\n    \n    Returns:\n    float: The value of the derivative, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        # The derivative of a constant term (x^0) is 0\n        return 0.0000\n    else:\n        # Applying the power rule of differentiation: n * c * x^(n-1)\n        derivative_value = n * c * math.pow(x, n - 1)\n        return round(derivative_value, 4)\n\n# Example usage:\n# Given a polynomial term 3 * x^4, find its derivative at x = 2\nexample_c = 3.0\nexample_x = 2.0\nexample_n = 4.0\nprint(f\"The derivative of {example_c} * x^{example_n} at x = {example_x} is: {poly_term_derivative(example_c, example_x, example_n)}\")\n\n# Check function to verify the correctness\ndef check_derivative():\n    test_cases = [\n        (2.0, 3.0, 0.0, 0.0000),  # Constant term\n        (5.0, 2.0, 3.0, 30.0000), # 5 * x^3\n        (7.0, 1.0, 4.0, 28.0000), # 7 * x^4\n        (1.0, 5.0, 1.0, 5.0000)   # Linear term\n    ]\n    \n    for c, x, n, expected in test_cases:\n        result = poly_term_derivative(c, x, n)\n        assert math.isclose(result, expected, rel_tol=1e-4), f\"Failed on c={c}, x={x}, n={n}. Expected {expected}, got {result}\"\n        print(f\"Passed: c={c}, x={x}, n={n} -> Derivative = {result}\")\n\ncheck_derivative()\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    - vectors: A list of 2D vectors as lists of floats.\n    - tol: Tolerance value for determining linear independence.\n    \n    Returns:\n    - A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    # Convert the list of vectors into a numpy array\n    V = np.array(vectors)\n    \n    # Perform Gram-Schmidt process to orthogonalize the vectors\n    Q = np.zeros_like(V)\n    R = np.zeros((len(vectors), len(vectors)))\n    \n    for i in range(len(vectors)):\n        v = V[i]\n        for j in range(i):\n            R[j, i] = np.dot(Q[j], V[i])\n            v = v - R[j, i] * Q[j]\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            R[i, i] = norm\n            Q[i] = v / norm\n    \n    # Select the linearly independent vectors from the orthogonal set\n    Q = Q[:np.linalg.matrix_rank(R)]\n    \n    # Round the result to 4 decimal places\n    Q_rounded = np.round(Q, 4).tolist()\n    \n    return Q_rounded\n\n# Example usage\nvectors = [[1.0, 1.0], [2.0, 2.0]]\nresult = orthonormal_basis(vectors)\nprint(result)\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (np.array): First 3D vector.\n    b (np.array): Second 3D vector.\n    \n    Returns:\n    np.array: Resultant vector from the cross product, rounded to 4 decimal places.\n    \"\"\"\n    if len(a) != 3 or len(b) != 3:\n        raise ValueError(\"Both vectors must be 3-dimensional.\")\n    \n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()\n\n# Example usage\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nprint(cross_product(a, b))  # Expected output: [-3.0, 6.0, -3.0]\n\n# Check function to verify the correctness of the solution\ndef check_cross_product():\n    test_cases = [\n        (np.array([1, 2, 3]), np.array([4, 5, 6]), [-3.0, 6.0, -3.0]),\n        (np.array([2, 7, -6]), np.array([-3, 1, 2]), [16.0, -16.0, 23.0]),\n        (np.array([0, 0, 0]), np.array([1, 2, 3]), [0.0, 0.0, 0.0])\n    ]\n    \n    for i, (a, b, expected) in enumerate(test_cases):\n        result = cross_product(a, b)\n        if not np.allclose(result, expected):\n            print(f\"Test case {i+1} failed: cross_product({a}, {b}) returned {result}, expected {expected}\")\n        else:\n            print(f\"Test case {i+1} passed.\")\n\ncheck_cross_product()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): Square coefficient matrix of the system of equations.\n    b (np.array): Constant vector of the system of equations.\n    \n    Returns:\n    list: Solution vector x if the system has a unique solution, otherwise returns -1.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Check if the matrix is square\n    if A.shape[0] != A.shape[1]:\n        return -1\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, return -1 since there's no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Calculate the determinants for each column substitution\n    for i in range(A.shape[0]):\n        # Create the matrix Ai by replacing the ith column of A with b\n        Ai = A.copy()\n        Ai[:, i] = b\n        \n        # Append the solution for xi which is det(Ai) / det(A)\n        x.append(np.round(np.linalg.det(Ai) / det_A, 4))\n    \n    # Return the solution vector as a list\n    return x.tolist()\n\n# Example usage:\nA = [[1, 2], [3, 4]]\nb = [5, 6]\nsolution = cramers_rule(A, b)\nprint(solution)  # Output: [-1.0, 2.0]\n\nA = [[2, 3], [4, 6]]\nb = [10, 18]\nsolution = cramers_rule(A, b)\nprint(solution)  # Output: -1.0\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the input lists are empty or have different lengths\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(np.array(p) * np.array(q)))\n    \n    # Calculate Bhattacharyya distance\n    bdist = -np.log(bc)\n    \n    # Round the result to 4 decimal places\n    return round(bdist, 4)\n\n# Example usage\np = [0.5, 0.5]\nq = [0.6, 0.4]\nprint(bhattacharyya_distance(p, q))  # Expected output: 0.2231\n\np = [0.1, 0.9]\nq = [0.8, 0.2]\nprint(bhattacharyya_distance(p, q))  # Expected output: 0.1733\n\nempty_list = []\np = [0.1, 0.2, 0.7]\nprint(bhattacharyya_distance(empty_list, p))  # Expected output: 0.0\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\nfrom typing import List\n\ndef vector_sum(a: List[int | float], b: List[int | float]) -> List[int | float]:\n    # Check if both vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Initialize an empty list to store the result\n    result = []\n    \n    # Compute the element-wise sum\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n        \n    return result\n\n# Example usage and test cases\nif __name__ == \"__main__\":\n    # Test case 1: Vectors of the same length\n    vector1 = [1.0, 2.5, 3.5]\n    vector2 = [4.0, 5.5, 6.5]\n    print(\"Vector sum:\", vector_sum(vector1, vector2))  # Output: [5.0, 8.0, 10.0]\n\n    # Test case 2: Vectors of different lengths\n    vector3 = [1.0, 2.5]\n    vector4 = [4.0, 5.5, 6.5]\n    print(\"Vector sum:\", vector_sum(vector3, vector4))  # Output: -1\n\n    # Test case 3: Another pair of vectors of the same length\n    vector5 = [2, 3, 4]\n    vector6 = [5, 6, 7]\n    print(\"Vector sum:\", vector_sum(vector5, vector6))  # Output: [7, 9, 11]\n\n    # Test case 4: One vector is empty\n    vector7 = []\n    vector8 = [1, 2, 3]\n    print(\"Vector sum:\", vector_sum(vector7, vector8))  # Output: -1\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```markdown\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    # Function to compute the policy for a given state\n    def policy(state):\n        action_values = theta[state]\n        exp_values = np.exp(action_values - np.max(action_values))  # Numerically stable softmax\n        return exp_values / np.sum(exp_values)\n    \n    gradients = []\n    for episode in episodes:\n        G_t = 0  # Initialize return G_t\n        for transition in reversed(episode):\n            state, action, reward = transition\n            # Compute the return G_t for the current transition\n            G_t += reward\n            # Compute probability of taking action under current policy\n            prob = policy(state)[action]\n            # Compute the log-probability\n            log_prob = np.log(prob)\n            # Compute the gradient of log-probability\n            grad_log_prob = log_prob * (1 - prob * np.ones_like(policy(state)))\n            # Accumulate the gradient\n            if state not in [trans[0] for trans in gradients]:\n                gradients.append((state, grad_log_prob))\n            else:\n                index = [trans[0] for trans in gradients].index(state)\n                gradients[index][1] += grad_log_prob\n    \n    # Normalize the gradients by the number of times each state was visited\n    normalized_gradients = [(state, grad / len([trans for trans in episode if trans[0] == state])) \n                             for state, grad in gradients]\n    \n    # Compute the average gradient across states\n    avg_gradient = np.mean([grad for _, grad in normalized_gradients], axis=0)\n    return avg_gradient.tolist()\n\n# Example usage\ntheta = np.random.rand(5, 3)  # Randomly initialized theta matrix\nepisodes = [\n    [(0, 1, 10), (1, 0, 5), (2, 2, 20)],\n    [(0, 2, 8), (1, 1, 7), (2, 1, 9)],\n    [(0, 0, 6), (1, 2, 15), (2, 0, 4)]\n]\n\ncompute_policy_gradient(theta, episodes)\n```\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\nimport math\n\ndef compute_flops(d_in, d_out, n_experts, k_active):\n    # FLOPs for dense layer\n    flops_dense = d_in * d_out * 2  # Multiplication and addition for each element\n    \n    # FLOPs for MoE layer\n    flops_moe = k_active * d_in * d_out * 2  # Multiplication and addition for each active expert\n    \n    return flops_dense, flops_moe\n\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense, flops_moe = compute_flops(d_in, d_out, n_experts, k_active)\n    \n    # Compute the efficiency savings\n    efficiency_savings = ((flops_dense - flops_moe) / flops_dense) * 100\n    \n    # Round the result to the nearest 1th decimal\n    efficiency_savings_rounded = round(efficiency_savings, 1)\n    \n    return efficiency_savings_rounded\n\n# Example usage\nn_experts = 16\nk_active = 4\nd_in = 1024\nd_out = 512\n\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Computational efficiency savings: {savings_percentage}%\")\n```"}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Ensure inputs are numpy arrays for ease of computation\n    X = np.array(X)\n    W_g = np.array(W_g)\n    W_noise = np.array(W_noise)\n    N = np.array(N)\n\n    # Compute the logits from the weight matrices\n    logits = X @ W_g + N @ W_noise\n\n    # Apply softmax to convert logits into gating probabilities\n    gating_probs = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)\n\n    # Compute the top-k probabilities\n    topk_indices = np.argsort(logits, axis=1)[:, -k:]\n    topk_probs = np.zeros_like(gating_probs)\n\n    for i in range(gating_probs.shape[0]):\n        topk_probs[i, topk_indices[i]] = 1.0 / k\n\n    # Add noise to the top-k probabilities\n    epsilon = 1e-5\n    topk_probs += (np.random.rand(*topk_probs.shape) < topk_probs) * epsilon\n    topk_probs /= np.sum(topk_probs, axis=1, keepdims=True)\n\n    # Reshape and return the result as a list\n    return topk_probs.tolist()\n\n# Example usage\nX = np.random.rand(3, 8)\nW_g = np.random.rand(8, 4)\nW_noise = np.random.rand(3, 4)\nN = np.random.rand(4)\nk = 2\n\nresult = noisy_topk_gating(X, W_g, W_noise, N, k)\nprint(result)\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Number of tokens in the input tensor\n    num_tokens = x.shape[0]\n    \n    # Compute the gating scores\n    gate_scores = x @ Wg\n    \n    # Apply softmax to get gating probabilities\n    gate_probs = np.exp(gate_scores - np.max(gate_scores, axis=1, keepdims=True))\n    gate_probs /= np.sum(gate_probs, axis=1, keepdims=True)\n    \n    # Initialize the output tensor\n    output = np.zeros((num_tokens, We.shape[1]))\n    \n    # Loop over each token to select top-k experts\n    for i in range(num_tokens):\n        # Get the top-k expert indices based on the gating probabilities\n        top_k_indices = np.argsort(gate_probs[i])[-top_k:]\n        \n        # Select the top-k experts and compute their outputs\n        selected_experts = We[top_k_indices]\n        selected_x = x[i].reshape(1, -1)\n        expert_outputs = selected_experts @ selected_x.T\n        \n        # Aggregate the results\n        output[i] = np.sum(expert_outputs, axis=0).T\n    \n    # Round the results to the nearest 4th decimal\n    output_rounded = np.round(output, decimals=4).tolist()\n    return output_rounded\n```\n\nThis function `moe` implements a Sparse Mixture of Experts layer with softmax gating and top-k routing. It takes an input tensor `x`, a set of expert weight matrices `We`, a gating weight matrix `Wg`, the number of experts `n_experts`, and the value of `k` for top-k routing. The function returns the final output tensor after applying the specified operations and rounding the results to the nearest 4th decimal."}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor X with shape (B, C, H, W).\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (B, C, H, W)\n    gamma (np.ndarray): Scale parameter of shape (C,)\n    beta (np.ndarray): Shift parameter of shape (C,)\n    num_groups (int): Number of groups to divide the channels into\n    epsilon (float): Small value added for numerical stability\n    \n    Returns:\n    np.ndarray: Normalized tensor reshaped to (B, C, H, W) and converted to a list using tolist()\n    \"\"\"\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"Number of groups should divide the number of channels evenly\"\n    \n    # Reshape X to (B, G, C/G, H, W)\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n    \n    # Calculate mean and variance along the channel axis\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize\n    X = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to original shape (B, C, H, W)\n    X = X.reshape(B, C, H, W)\n    \n    # Apply scale and shift\n    X = X * gamma + beta\n    \n    return X.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    B, C, H, W = 2, 8, 4, 4\n    X = np.random.rand(B, C, H, W)\n    gamma = np.random.rand(C)\n    beta = np.random.rand(C)\n    num_groups = 2\n    \n    normalized_tensor = group_normalization(X, gamma, beta, num_groups)\n    print(normalized_tensor)\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Finds the value of x where the function f(x) = x^4 - 3x^3 + 2 reaches its minimum using gradient descent.\n    \n    :param start_x: Initial guess for the location of the minimum.\n    :param learning_rate: Step size for each iteration of gradient descent.\n    :param tolerance: Tolerance level for stopping criteria based on change in x between iterations.\n    :param max_iters: Maximum number of iterations to perform.\n    :return: The x value where the function reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    def df(x):\n        return 4*x**3 - 9*x**2\n    \n    x = start_x\n    for i in range(max_iters):\n        previous_x = x\n        x -= learning_rate * df(previous_x)\n        if abs(f(x) - f(previous_x)) < tolerance:\n            break\n    \n    return round(x, 4)\n\n# Example usage:\nstart_position = 2.5  # Random initial position\nminimum_x = find_treasure(start_position)\nprint(f\"The value of x where the treasure is hidden: {minimum_x}\")\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, which is a normalization-free transformation\n    inspired by the Tanh function. The function is applied element-wise on the input array x.\n    \n    Args:\n    x (np.ndarray): Input array of arbitrary shape.\n    alpha (float): Scaling factor for the linear part of the activation.\n    gamma (float): Scales the output of the tanh function.\n    beta (float): Shifts the output of the tanh function.\n    \n    Returns:\n    list[float]: List containing the transformed values of the input array, rounded to 4th decimal.\n    \"\"\"\n    # Element-wise application of the DyT function\n    dyt_values = gamma * np.tanh(alpha * x + beta)\n    # Reshape the array to a flat list and round to 4th decimal place\n    return np.round(dyt_values.flatten().tolist(), 4)\n\n# Example usage:\nx = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\nalpha = 0.5\ngamma = 2.0\nbeta = 1.0\nresult = dynamic_tanh(x, alpha, gamma, beta)\nprint(result)\n```"}
