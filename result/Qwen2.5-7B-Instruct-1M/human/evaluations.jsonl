{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [list(row) for row in zip(*a)]\n    return transposed\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a given matrix into a specified shape.\n    \n    Parameters:\n    a (list[list[int|float]]): The original matrix to be reshaped.\n    new_shape (tuple[int, int]): The target shape to reshape the matrix into.\n    \n    Returns:\n    list[list[int|float]]: The reshaped matrix as a list of lists, or [] if reshaping is not possible.\n    \"\"\"\n    np_array = np.array(a)\n    if np_array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = np_array.reshape(new_shape).tolist()\n    return reshaped_array\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists where each inner list represents a row of the matrix.\n    :param mode: A string indicating the mode of calculation ('row' or 'column').\n    :return: A list of means according to the specified mode.\n    \"\"\"\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        means = [sum(col) / num_rows for col in zip(*matrix)]\n    else:\n        raise ValueError(\"Mode should be either 'row' or 'column'\")\n    return means\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import numpy as np\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return sorted_eigenvalues\nmatrix = [[4, -2], [1, 1]]\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^-1 * A * S, where T and S are invertible matrices.\n    \n    Parameters:\n    A (list[list[int|float]]): The original matrix to be transformed.\n    T (list[list[int|float]]): The invertible matrix T used for transformation.\n    S (list[list[int|float]]): The invertible matrix S used for transformation.\n    \n    Returns:\n    list[list[int|float]]: The transformed matrix rounded to the nearest 4th decimal.\n                           Returns -1 if there is no solution or matrices are not invertible.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n            return -1\n        transformed_matrix = np.round(np.dot(np.linalg.inv(T_np), np.dot(A_np, S_np)), 4)\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[list[float]]: The inverse of the matrix if it exists, otherwise None.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse_matrix = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse_matrix\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    vectors_array = np.array(vectors)\n    means = vectors_array.mean(axis=0)\n    centered_vectors = vectors_array - means\n    covariance_matrix = np.cov(centered_vectors, rowvar=False)\n    return covariance_matrix.tolist()\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: Matrix A must be diagonal dominant or diagonally dominant.", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if not np.all(np.abs(np.diag(A)) >= np.sum(np.abs(A), axis=1)):\n        raise ValueError('Matrix A must be diagonal dominant or diagonally dominant.')\n    D = np.diag(np.diag(A))\n    R = A - D\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.dot(D, b) - np.dot(R, x)\n        x = x_new.round(4)\n    return x.reshape(-1).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([5, 4])\nn = 25\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: invalid index to scalar variable.", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n\n    def jacobian(A):\n        (m, n) = A.shape\n        J = np.zeros((n, m))\n        for i in range(n):\n            for j in range(m):\n                C = A.copy()\n                C[j, i] += 1e-05\n                yield (C - A)\n\n    def svd_update(u, v, jacobian_mat):\n        alpha = np.dot(np.dot(v.T, jacobian_mat), u)\n        denom = 1 + alpha * v[i][i]\n        s = np.sqrt(np.abs(1 / denom))\n        u_new = u * s\n        v_new = v * s\n        s_new = 1 / denom\n        return (u_new, v_new, s_new)\n    u = np.eye(2)\n    v = np.eye(2)\n    s = np.zeros(2)\n    for iteration in range(10):\n        for jacobian_mat in jacobian(A):\n            for i in range(2):\n                (u, v, s[i]) = svd_update(u[:, i].copy(), v[i, :].copy(), jacobian_mat)\n    return tuple((round(x, 4) for x in s))\nA = np.array([[2, 0], [1, 2]])\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    if len(matrix) == 1:\n        return matrix[0][0]\n    det = 0\n    for col in range(len(matrix)):\n        sign = (-1) ** col\n        minor_matrix = []\n        for row in range(1, len(matrix)):\n            minor_row = []\n            for m_col in range(len(matrix)):\n                if m_col != col:\n                    minor_row.append(matrix[row][m_col])\n            minor_matrix.append(minor_row)\n        det += sign * matrix[0][col] * determinant_4x4(minor_matrix)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    return np.round(theta, 4).tolist()\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (3,) (2,) ", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.zeros(X_b.shape[1])\n    for _ in range(iterations):\n        gradients = 2 * X_b.T.dot(X_b.dot(theta) - y) / len(y)\n        theta -= alpha * gradients\n    return np.round(theta.reshape(-1).tolist(), 4)\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales features of the given dataset using standardization and min-max normalization.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array with samples in the rows and features in the columns.\n    \n    Returns:\n    (list[list[float]], list[list[float]]): A tuple containing two lists of lists:\n                                            - The first list contains the standardized features.\n                                            - The second list contains the min-max normalized features.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data_rounded = [np.round(row, 4).tolist() for row in standardized_data]\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_scaled_data = (data - min_val) / (max_val - min_val)\n    min_max_scaled_rounded = [np.round(row, 4).tolist() for row in min_max_scaled_data]\n    return (standardized_data_rounded, min_max_scaled_rounded)\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (9,1,2) (2,3) ", "solution": "import numpy as np\nimport numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    initial_centroids_array = np.array(initial_centroids)\n    centroids = initial_centroids_array\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids.T, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = []\n        for i in range(k):\n            if np.any(labels == i):\n                new_centroid = np.mean(points_array[labels == i], axis=0)\n                new_centroids.append(new_centroid)\n        new_centroids_array = np.array(new_centroids)\n        if np.allclose(centroids, new_centroids_array, atol=0.0001):\n            break\n        centroids = new_centroids_array\n    final_centroids = [tuple(round(c, 4)) for c in centroids]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0), (8.0, 2.0), (10.0, 2.0), (9.0, 3.0)]\nk = 3\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0), (8.0, 8.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train-test indices for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: Features dataset (np.ndarray).\n    - y: Labels dataset (np.ndarray).\n    - k: Number of folds (default is 5).\n    - shuffle: Boolean to indicate whether to shuffle data before splitting into batches (default is True).\n    - random_seed: Integer value for reproducibility of results (optional).\n\n    Returns:\n    - A list of tuples containing train and test indices for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    fold_indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(fold_indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    train_test_indices = []\n    start_idx = 0\n    for fold in range(k):\n        test_size = fold_size + (1 if fold < remainder else 0)\n        test_indices = fold_indices[start_idx:start_idx + test_size]\n        train_indices = np.array(list(fold_indices[:start_idx]) + list(fold_indices[start_idx + test_size:]))\n        train_test_indices.append((train_indices, test_indices))\n        start_idx += test_size\n    return train_test_indices\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: name 'eigenvector' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = data.mean(axis=0)\n    std_dev = data.std(axis=0)\n    standardized_data = (data - mean) / std_dev\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    return ([round(eigenvalue, 4) for eigenvalue in eigenvalues[-k:]], [list(round(value, 4)) for value in eigenvector.tolist() for eigenvector in eigenvectors[:, -k:]])\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": false, "result": "failed: object of type 'int' has no len()", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    This function implements a decision tree learning algorithm using recursive binary splitting.\n    It selects the attribute that best splits the data based on information gain and constructs\n    a decision tree represented as a nested dictionary.\n\n    Parameters:\n    - examples: List of dictionaries where each dictionary represents an example with attribute-value pairs.\n    - attributes: List of strings representing the available attributes for splitting.\n    - target_attr: String representing the name of the target attribute to predict.\n\n    Returns:\n    - A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(examples):\n        \"\"\"Calculate the entropy of a given dataset.\"\"\"\n        if not examples:\n            return 0\n        class_counts = Counter((example[target_attr] for example in examples))\n        p = [count / len(examples) for count in class_counts.values()]\n        return -sum((p_i * math.log2(p_i) for p_i in p if p_i != 0))\n\n    def info_gain(left, right, current_entropy):\n        \"\"\"Calculate the information gain between the parent node and two child nodes.\"\"\"\n        return current_entropy - sum((len(leaf) / len(examples) * entropy(leaf) for leaf in [left, right]))\n\n    def find_best_split(examples, attributes):\n        \"\"\"Find the attribute that provides the highest information gain.\"\"\"\n        base_entropy = entropy(examples)\n        (best_attr, best_gain) = (None, 0)\n        for attr in attributes:\n            value_counts = Counter((example[attr] for example in examples))\n            weighted_entropy = sum((len(value_ex) / len(examples) * entropy(value_ex) for (value, value_ex) in value_counts.items()))\n            gain = base_entropy - weighted_entropy\n            if gain > best_gain:\n                (best_gain, best_attr) = (gain, attr)\n        return best_attr\n\n    def build_tree(examples, attributes):\n        \"\"\"Recursively build the decision tree.\"\"\"\n        if not examples:\n            return Counter({target_attr: 'none'}).most_common(1)[0][0]\n        if all((example[target_attr] == examples[0][target_attr] for example in examples)):\n            return examples[0][target_attr]\n        if not attributes:\n            return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n        best_attr = find_best_split(examples, attributes)\n        attributes.remove(best_attr)\n        (left_examples, right_examples) = ([], [])\n        for example in examples:\n            if example[best_attr] == example[best_attr]:\n                left_examples.append(example)\n            else:\n                right_examples.append(example)\n        tree = {best_attr: {}}\n        tree[best_attr]['yes'] = build_tree(left_examples, attributes[:])\n        tree[best_attr]['no'] = build_tree(right_examples, attributes[:])\n        return tree\n    return build_tree(examples, list(attributes))\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'true', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': 'true', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'windy']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: too many indices for array: array is 1-dimensional, but 2 were indexed", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm for training a kernel SVM classifier.\n    \n    Parameters:\n    - data: 2D numpy array of shape (n_samples, n_features)\n    - labels: 1D numpy array of labels of length n_samples\n    - kernel: type of kernel to use ('linear' or 'rbf')\n    - lambda_val: regularization parameter\n    - iterations: number of iterations to run the algorithm\n    - sigma: standard deviation for RBF kernel\n    \n    Returns:\n    - alpha: numpy array of alpha coefficients\n    - bias: bias term\n    \n    All results are rounded to the nearest 4th decimal and returned as a list.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    labels = labels.reshape(-1, 1) * 1.0\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'rbf':\n        fullGram = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                fullGram[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n        K = fullGram\n    else:\n        raise ValueError('Unsupported kernel type')\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        predictions = K @ alpha + bias\n        conditions = np.where(labels * predictions <= 1, True, False)\n        alpha[conditions] += 1.0 / (t * lambda_val)\n        alpha[np.invert(conditions)] = np.maximum(alpha[np.invert(conditions)] - 1.0 / (t * lambda_val), 0.0)\n        bias = np.mean(labels - K @ alpha)\n    alpha = np.round(alpha, 4).tolist()\n    bias = np.round(bias, 4)\n    return (alpha, [bias])\ndata = np.array([[1, 2], [2, 2], [3, 3], [5, 7], [6, 5]])\nlabels = np.array([1, 1, 1, -1, -1])\nkernel = 'linear'\nlambda_val = 0.01\niterations = 100\nsigma = 1.0\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid function for the input value z.\n    \n    Args:\n    z (float): Input value for the sigmoid function.\n    \n    Returns:\n    float: Output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: List of feature vectors (2D list).\n    :param labels: True binary labels (list of integers).\n    :param weights: Weights for each feature (list of floats).\n    :param bias: Bias term (float).\n    :return: Predicted probabilities and mean squared error, both rounded to 4 decimal places.\n    \"\"\"\n    predicted_probabilities = []\n    total_error = 0\n    for (feature_vector, label) in zip(features, labels):\n        weighted_sum = bias\n        for (feature, weight) in zip(feature_vector, weights):\n            weighted_sum += feature * weight\n        probability = 1 / (1 + math.exp(-weighted_sum))\n        predicted_probabilities.append(round(probability, 4))\n        total_error += (probability - label) ** 2\n    mse = round(total_error / len(labels), 4)\n    return ([predicted_probabilities], mse)\nfeatures = [[1.0, 2.0], [2.0, 3.0]]\nlabels = [0, 1]\nweights = [0.5, 0.5]\nbias = 0.5\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        predictions = 1 / (1 + np.exp(-np.dot(features, weights) - bias))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        d_error_d_pred = predictions - labels\n        d_pred_d_z = predictions * (1 - predictions)\n        d_z_d_w = features\n        d_z_d_b = 1\n        grad_w = np.dot(d_error_d_pred * d_pred_d_z, d_z_d_w.T) / len(features)\n        grad_b = np.sum(d_error_d_pred * d_pred_d_z * d_z_d_b) / len(features)\n        weights -= learning_rate * grad_w\n        bias -= learning_rate * grad_b\n    return (weights, bias, mse_values)\nfeatures = np.array([[0.5, 0.1], [0.8, 0.2], [0.1, 0.9], [0.6, 0.3]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.1, 0.1])\ninitial_bias = 0.1\nlearning_rate = 0.1\nepochs = 50\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def backward(self):\n        topo_order = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo_order.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo_order):\n            v._backward()\n            v.grad = v._backward()\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C in R^3.\n    \n    Parameters:\n    B (list[list[int]]): List of lists representing the basis B.\n    C (list[list[int]]): List of lists representing the basis C.\n    \n    Returns:\n    list[list[float]]: The transformation matrix P from basis B to C, rounded to 4 decimal places.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    P = np.linalg.solve(B_array, C_array)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [1, 0, 1], [0, 1, 1]]\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Matrix A must be 2x2')\n    A_T = A.T\n    A_T_A = A_T @ A\n    (eigen_values_A_T_A, Q) = np.linalg.eig(A_T_A)\n    singular_values = np.sqrt(np.real(eigen_values_A_T_A))\n    S = np.diag(singular_values)\n    V = Q\n    U = [np.linalg.norm(A @ V[:, i]) * V[:, i] for i in range(V.shape[1])]\n    U = np.array(U).T\n    U = np.round(U, 4).tolist()\n    S = np.round(S, 4).tolist()\n    V = np.round(V, 4).tolist()\n    return (U, S, V)\nA = np.array([[2, 4], [1, 3]])\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays X and y in place while maintaining their correspondence.\n    \n    Parameters:\n    - X (np.ndarray): Features of the dataset.\n    - y (np.ndarray): Labels of the dataset.\n    - seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    - list: A list of tuples containing the shuffled elements of X and y.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    return list(zip(X[indices], y[indices]))\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from numpy arrays X and optionally y.\n    \n    Parameters:\n    - X (np.ndarray): The feature array.\n    - y (np.ndarray, optional): The label array.\n    - batch_size (int): The size of each batch to yield.\n    \n    Yields:\n    - A tuple (np.ndarray, np.ndarray) of (features, labels) for each batch if y is not None.\n    - np.ndarray of features otherwise.\n    \"\"\"\n    assert len(X) > 0\n    assert batch_size > 0\n    assert batch_size <= len(X), 'Batch size is larger than the dataset size.'\n    if y is not None:\n        for i in range(0, len(X), batch_size):\n            yield (X[i:i + batch_size], y[i:i + batch_size])\n    else:\n        for i in range(0, len(X), batch_size):\n            yield X[i:i + batch_size]\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Splits the dataset X into two subsets based on whether the values of a specific feature\n    are greater than or equal to a given threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset, a 2D array where each row is a sample and columns are features.\n    feature_i (int): The index of the feature to split the dataset on.\n    threshold (float): The threshold value to compare each feature's value against.\n\n    Returns:\n    list: A list containing two lists of arrays:\n          - The first list contains samples meeting the threshold condition.\n          - The second list contains samples not meeting the threshold condition.\n          All lists are converted to numpy arrays and then to lists before returning.\n    \"\"\"\n    above_threshold = X[X[:, feature_i] >= threshold]\n    below_threshold = X[X[:, feature_i] < threshold]\n    return [above_threshold.tolist(), below_threshold.tolist()]\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for the given dataset up to the specified degree.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features).\n    degree (int): The degree of the polynomial features.\n    \n    Returns:\n    List[List[float]]: A list of lists where each inner list contains the polynomial features as floats.\n    \"\"\"\n    if degree <= 1:\n        return X.tolist()\n    column_combinations = combinations_with_replacement(range(X.shape[1]), degree)\n    result_rows = []\n    for combo in column_combinations:\n        combined_feature = X[:, combo[0]]\n        for idx in range(1, len(combo)):\n            combined_feature *= X[:, combo[idx]]\n        result_rows.append(combined_feature)\n    poly_features = np.vstack(result_rows)\n    return poly_features.T.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 3\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (5,2) (2,2) ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates n_subsets of (X,y) pairs through random sampling with or without replacement.\n    \n    Parameters:\n    - X (np.array): The feature set, a 2D numpy array.\n    - y (np.array): The target set, a 1D numpy array.\n    - n_subsets (int): Number of random subsets to generate.\n    - replacements (bool): Sampling with or without replacement.\n    - seed (int): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    - List of tuples: Each containing a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    random_indices = [np.random.choice(len(y), size=len(y), replace=replacements) for _ in range(n_subsets)]\n    return [(X[indices], y[indices]) for indices in random_indices]\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    - x: A 1D numpy array of integers to be one-hot encoded.\n    - n_col: The number of columns for the one-hot encoded result. \n             If not provided, it will be set to the maximum value in x plus one.\n    \n    Returns:\n    A list representation of the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = int(x.max()) + 1\n    encoded = (x.reshape(-1, 1) == np.arange(n_col)).astype(int)\n    return encoded.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (np.array): A 1D numpy array.\n\n    Returns:\n    np.array: A 2D numpy array representing the diagonal matrix.\n    \"\"\"\n    return np.diag(x)\nx = np.array([1, 2, 3, 4])\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import numpy as np\nimport numpy as np\nfrom sklearn.metrics import accuracy_score as sk_accuracy_score\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of model predictions.\n    \n    Parameters:\n    - y_true: A 1D numpy array of true labels.\n    - y_pred: A 1D numpy array of predicted labels.\n    \n    Returns:\n    - accuracy: The accuracy score as a float, rounded to 4 decimal places.\n    \"\"\"\n    accuracy = sk_accuracy_score(y_true, y_pred)\n    return round(accuracy, 4)\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix from the given 2D numpy arrays X and Y.\n    If Y is not provided, calculate the correlation matrix of X with itself.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): Another 2D numpy array. Defaults to None.\n    \n    Returns:\n    list: A 2D list representing the correlation matrix, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        correlation_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        correlation_matrix = np.corrcoef(np.column_stack((X, Y)), rowvar=False)\n    return np.round(correlation_matrix, 4).tolist()\nX = np.array([[1, 2, 3, 4, 5], [10, 9, 8, 7, 6]])\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: math domain error", "solution": "import numpy as np\nimport math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    m = X.shape[0]\n    D = np.full(m, 1 / m)\n    classifiers = []\n    for _ in range(n_clf):\n        best_classifiers = []\n        for feature_index in range(X.shape[1]):\n            sorted_indices = np.argsort(X[:, feature_index])\n            sorted_X = X[sorted_indices, feature_index]\n            sorted_y = y[sorted_indices]\n            pos_weight_cumsum = np.cumsum(D[sorted_y == 1])\n            neg_weight_cumsum = -np.cumsum(D[sorted_y == -1][::-1])[::-1]\n            errors = pos_weight_cumsum + neg_weight_cumsum\n            best_error = np.min(errors)\n            best_threshold_index = np.argmin(errors)\n            best_threshold = (sorted_X[best_threshold_index] + sorted_X[best_threshold_index + 1]) / 2\n            best_classifiers.append((feature_index, best_threshold, best_error))\n        best_classifier_info = min(best_classifiers, key=lambda x: x[2])\n        (feature_index, best_threshold, best_error) = best_classifier_info\n        classifier = {'feature_index': feature_index, 'threshold': best_threshold, 'classification_error': best_error}\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        predictions = X[:, feature_index] > best_threshold\n        D *= np.exp(-alpha * y * predictions)\n        D /= np.sum(D)\n        classifiers.append(classifier)\n    return classifiers\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 1, -1, -1])\nn_clf = 2\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores, dtype=np.float64)\n    z_max = np.max(scores_array)\n    log_softmax_result = np.log(np.exp(scores_array - z_max).sum()) + z_max\n    return np.round(log_softmax_result, 4).tolist()\nscores = [1000, 1002, 1001]\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: Dense.initialize() takes 1 positional argument but 2 were given", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        if self.input_shape is not None:\n            limit = 1 / math.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n            self.w0 = np.zeros(self.n_units)\n        else:\n            self.W = None\n            self.w0 = None\n        self.optimizer_W = lambda x: x\n        self.optimizer_w0 = lambda x: x\n\n    def parameters(self):\n        if self.W is not None and self.w0 is not None:\n            return self.W.size + self.w0.size\n        return 0\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        input_grad = np.dot(accum_grad, self.W.T)\n        self.W_grad = np.dot(self.layer_input.T, accum_grad)\n        self.w0_grad = np.sum(accum_grad, axis=0)\n        self.W = self.optimizer_W(self.W - self.W_grad)\n        self.w0 = self.optimizer_w0(self.w0 - self.w0_grad)\n        return input_grad\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_width = input_matrix.shape[1] + 2 * padding\n    padded_height = input_matrix.shape[0] + 2 * padding\n    padded_matrix = np.zeros((padded_height, padded_width))\n    padded_matrix[padding:padded_height - padding, padding:padded_width - padding] = input_matrix\n    output_height = (padded_matrix.shape[0] - kernel.shape[0]) // stride + 1\n    output_width = (padded_matrix.shape[1] - kernel.shape[1]) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            output_matrix[i, j] = np.sum(padded_matrix[i * stride:i * stride + kernel.shape[0], j * stride:j * stride + kernel.shape[1]] * kernel)\n    return output_matrix.round(4).tolist()\ninput_matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nkernel = np.array([[1, 0], [0, -1]])\npadding = 0\nstride = 1\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function to the input float.\n    \n    Parameters:\n    z (float): A floating point number to apply the ReLU function on.\n    \n    Returns:\n    float: The result of applying ReLU to z.\n    \"\"\"\n    return max(0.0, z)\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss for a given feature matrix X, coefficients w,\n    true labels y_true, and regularization parameter alpha.\n    \n    Parameters:\n    - X: A 2D numpy array representing the feature matrix.\n    - w: A 1D numpy array representing the coefficients.\n    - y_true: A 1D numpy array representing the true labels.\n    - alpha: A float representing the regularization parameter.\n    \n    Returns:\n    - The Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_pred - y_true) ** 2)\n    L2_norm = np.sum(w ** 2)\n    loss = mse + alpha * L2_norm\n    return round(loss, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport math\ndef leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"Apply the Leaky ReLU activation function to the input z with the given alpha value.\"\"\"\n    return max(0, z) + min(0, z, alpha * z)\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two vectors x1 and x2.\n    \n    The linear kernel is defined as the dot product of x1 and x2.\n    \n    Parameters:\n    - x1: numpy array, the first vector\n    - x2: numpy array, the second vector\n    \n    Returns:\n    - float, the dot product of x1 and x2\n    \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n    \n    Parameters:\n    - y_true: np.ndarray, an array of true binary labels.\n    - y_pred: np.ndarray, an array of predicted binary labels.\n    \n    Returns:\n    float, the precision metric.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 0\n    precision = true_positives / predicted_positives\n    return precision\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef compute_gradient(X, y, weights):\n    \"\"\"Compute the gradient of the loss function with respect to the weights.\"\"\"\n    error = y - X @ weights\n    gradient = -(2 / len(X)) * X.T @ error\n    return gradient\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent with different variants: batch, stochastic, and mini-batch.\n    \n    Parameters:\n    - X: Feature matrix (numpy array)\n    - y: Target vector (numpy array)\n    - weights: Initial weights (numpy array)\n    - learning_rate: Learning rate for weight updates\n    - n_iterations: Number of iterations to perform\n    - batch_size: Size of batches for mini-batch gradient descent\n    - method: Type of gradient descent ('batch', 'stochastic', or 'mini_batch')\n    \n    Returns:\n    - List of weights after each iteration, rounded to 4 decimal places.\n    \"\"\"\n    weight_history = [weights.copy()]\n    if method == 'stochastic':\n        batch_size = 1\n    for _ in range(n_iterations):\n        if method == 'batch':\n            gradient = compute_gradient(X, y, weights)\n        elif method == 'stochastic':\n            shuffled_indices = np.random.permutation(len(X))\n            (X_shuffled, y_shuffled) = (X[shuffled_indices], y[shuffled_indices])\n            for start_idx in range(0, len(X), batch_size):\n                end_idx = start_idx + batch_size\n                gradient = compute_gradient(X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx], weights)\n        elif method == 'mini_batch':\n            shuffled_indices = np.random.permutation(len(X))\n            (X_shuffled, y_shuffled) = (X[shuffled_indices], y[shuffled_indices])\n            for start_idx in range(0, len(X), batch_size):\n                end_idx = start_idx + batch_size\n                gradient = compute_gradient(X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx], weights)\n        else:\n            raise ValueError('Invalid method specified for gradient descent.')\n        updated_weights = weights - learning_rate * gradient\n        weight_history.append(updated_weights.copy())\n    return [weights.round(4).tolist() for weights in weight_history]\nX = np.array([[1, 1], [1, 2], [1, 3]])\ny = np.array([1.1, 1.9, 2.8])\nweights = np.array([0, 0])\nlearning_rate = 0.1\nn_iterations = 10\nbatch_size = 2\nmethod = 'mini_batch'\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D list representing the matrix to be converted.\n    \n    Returns:\n    list: A 2D list representing the matrix in RREF.\n    \"\"\"\n    mat = np.array(matrix)\n\n    def swap_rows(mat, row1, row2):\n        \"\"\"Swap two rows in the matrix.\"\"\"\n        mat[[row1, row2]] = mat[[row2, row1]]\n\n    def eliminate_column(mat, pivot_row, pivot_col):\n        \"\"\"Make all entries below the pivot zero.\"\"\"\n        for row in range(pivot_row + 1, mat.shape[0]):\n            factor = mat[row, pivot_col] / mat[pivot_row, pivot_col]\n            mat[row] -= factor * mat[pivot_row]\n    (row_count, col_count) = mat.shape\n    for col in range(min(col_count, row_count)):\n        pivot_row = col\n        while pivot_row < row_count and abs(mat[pivot_row, col]) < 1e-10:\n            pivot_row += 1\n        if pivot_row == row_count:\n            continue\n        if pivot_row != col:\n            swap_rows(mat, pivot_row, col)\n        if abs(mat[col, col]) != 1:\n            mat[col] /= mat[col, col]\n        eliminate_column(mat, col, col)\n    mat = np.where(np.isclose(mat, 1) | np.isclose(mat, 0), mat, 0)\n    return mat.tolist()\nmatrix = [[1, 2, -1, 4], [-2, -3, 2, -7], [4, 3, -4, 8]]\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Perform optimization of a function using the Adam optimization algorithm.\n\n    Parameters:\n    f : function\n        The objective function to be optimized.\n    grad : function\n        A function that computes the gradient of f.\n    x0 : array_like\n        Initial parameter values.\n    learning_rate : float, optional\n        The step size (default: 0.001).\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates (default: 0.9).\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates (default: 0.999).\n    epsilon : float, optional\n        A small constant for numerical stability (default: 1e-8).\n    num_iterations : int, optional\n        Number of iterations to run the optimizer (default: 1000).\n\n    Returns:\n    list\n        Optimized parameters rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x0)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for i in range(num_iterations):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * np.power(g, 2)\n        m_hat = m / (1 - np.power(beta1, i + 1))\n        v_hat = v / (1 - np.power(beta2, i + 1))\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x.tolist(), 4)\nx0 = [1.0, 1.0]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent with L1 Regularization.\n\n    Parameters:\n    X : np.array\n        Feature matrix of shape (n_samples, n_features).\n    y : np.array\n        Target vector of length n_samples.\n    alpha : float, optional\n        Regularization parameter. Default is 0.1.\n    learning_rate : float, optional\n        Learning rate for gradient descent. Default is 0.01.\n    max_iter : int, optional\n        Maximum number of iterations. Default is 1000.\n    tol : float, optional\n        Tolerance for convergence. Default is 1e-4.\n\n    Returns:\n    tuple: A tuple containing the learned weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    converged = False\n    iteration = 0\n    while not converged and iteration < max_iter:\n        y_pred = np.dot(X, weights) + bias\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y) + alpha * np.sign(weights)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        if np.allclose(dw, 0, atol=tol):\n            converged = True\n        iteration += 1\n    return (np.round(weights.tolist(), 4), np.round(bias, 4))\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            insertions = dp[i][j - 1] + 1\n            deletions = dp[i - 1][j] + 1\n            substitutions = dp[i - 1][j - 1] + cost\n            transpositions = dp[i - 2][j - 2] + cost if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]) else float('inf')\n            dp[i][j] = min(insertions, deletions, substitutions, transpositions)\n    return dp[m][n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List of true binary labels (0 or 1).\n    - y_pred: List of predicted binary labels (0 or 1).\n    \n    Returns:\n    - Recall value rounded to 3 decimal places. Returns 0.0 if the denominator is zero.\n    \"\"\"\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    recall_value = np.where(TP + FN == 0, 0.0, TP / (TP + FN))\n    return np.round(recall_value, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    X = np.array(X)\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T)\n    attention_scores = attention_scores - np.max(attention_scores, axis=-1, keepdims=True)\n    attention_scores = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    attention_output = np.dot(attention_scores, V)\n    attention_output = np.round(attention_output, 4)\n    return attention_output.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\nW_q = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nW_k = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nW_v = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell. Processes a sequence of input vectors and produces the final hidden state.\n    \n    Parameters:\n    - input_sequence: List of input vectors (sequences).\n    - initial_hidden_state: Initial hidden state.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n    \n    Returns:\n    - Final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(x, Wx) + np.dot(h, Wh) + b)\n    final_hidden_state = np.round(h, 4).tolist()\n    return final_hidden_state\ninput_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\ninitial_hidden_state = [0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.2]\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.hstack((points_array, np.ones((len(points), 1)))).astype(float)\n    transformed_homogeneous_points = np.dot(homogeneous_points, translation_matrix.T)\n    translated_points = transformed_homogeneous_points[:, :-1]\n    return translated_points.tolist()\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions P and Q.\n    \n    Parameters:\n    mu_p (float): Mean of distribution P.\n    sigma_p (float): Standard deviation of distribution P.\n    mu_q (float): Mean of distribution Q.\n    sigma_q (float): Standard deviation of distribution Q.\n    \n    Returns:\n    float: The KL divergence from distribution P to distribution Q.\n    \"\"\"\n    sigma_p_sq = sigma_p ** 2\n    sigma_q_sq = sigma_q ** 2\n    return 0.5 * (sigma_q_sq / sigma_p_sq + (mu_p - mu_q) ** 2 / sigma_p_sq - 1 + np.log(sigma_q_sq / sigma_p_sq))\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    - A: Coefficient matrix (square matrix)\n    - b: Right-hand side vector\n    - n: Number of iterations\n    - x_ini: Initial guess for the solution (optional, defaults to a vector of zeros)\n    \n    Returns:\n    - Approximate solution vector x as a python list\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return x_new.tolist()\nA = [[4, 1, 0], [1, 3, 1], [0, 1, 5]]\nb = [2, 3, 5]\nn = 25\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix of shape (n, n)\n    b (np.array): Dependent variable vector of shape (n,)\n    \n    Returns:\n    list: Solution vector x as a python list.\n    \"\"\"\n    n = len(b)\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    for i in range(n):\n        max_line = np.argmax(np.abs(Ab[i:, i])) + i\n        if Ab[max_line, i] == 0:\n            raise ValueError('Matrix is singular.')\n        Ab[[i, max_line], :] = Ab[[max_line, i], :]\n        for j in range(i + 1, n):\n            ratio = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= ratio * Ab[i, i:]\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = Ab[i, n] / Ab[i, i]\n        Ab[i, i + 1:] -= x[i] * Ab[i, i + 1:]\n    return Ab[:, :-1].reshape(-1).tolist()\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = x.shape[0]\n        hs = []\n        h = initial_hidden_state\n        c = initial_cell_state\n        for t in range(T):\n            xt = x[t]\n            input_gate = self.sigmoid(np.dot(self.Wi, np.concatenate((xt, h), axis=0)) + self.bi)\n            forget_gate = self.sigmoid(np.dot(self.Wf, np.concatenate((xt, h), axis=0)) + self.bf)\n            output_gate = self.sigmoid(np.dot(self.Wo, np.concatenate((xt, h), axis=0)) + self.bo)\n            candidate_cell_state = self.tanh(np.dot(self.Wc, np.concatenate((xt, h), axis=0)) + self.bc)\n            c = forget_gate * c + input_gate * candidate_cell_state\n            h = output_gate * self.tanh(c)\n            hs.append(h)\n        return (hs, h, c)\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom collections import defaultdict\nfrom math import log\ndef compute_tf_idf(corpus, query):\n\n    def term_frequency(word, document):\n        return document.count(word) / len(document)\n\n    def inverse_document_frequency(word, corpus):\n        df = sum((1 for doc in corpus if word in doc))\n        return log(len(corpus) / (1 + df))\n    tfidf_scores = []\n    if not corpus:\n        raise ValueError('Corpus cannot be empty.')\n    doc_freqs = defaultdict(int)\n    for doc in corpus:\n        for word in set(doc):\n            doc_freqs[word] += 1\n    for doc in corpus:\n        doc_tfidf = []\n        for word in query:\n            tf = term_frequency(word, doc)\n            idf = inverse_document_frequency(word, corpus)\n            tfidf = tf * idf if tfidf_scores else 0\n            doc_tfidf.append(round(tfidf, 5))\n        tfidf_scores.append(doc_tfidf)\n    return np.array(tfidf_scores).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox'], ['jumps', 'over', 'the', 'lazy', 'dog']]\nquery = ['the', 'fox']\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import numpy as np\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    try:\n        fscore = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    except ZeroDivisionError:\n        fscore = 0.0\n    return round(fscore, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: 'SimpleRNN' object has no attribute 'forward'", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    @staticmethod\n    def initialize_weights(input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN parameters (weights and biases).\n        \"\"\"\n        return {'W_xh': np.random.randn(hidden_size, input_size) * 0.01, 'W_hh': np.random.randn(hidden_size, hidden_size) * 0.01, 'W_hy': np.random.randn(output_size, hidden_size) * 0.01, 'b_h': np.zeros((hidden_size, 1)), 'b_y': np.zeros((output_size, 1))}\n\n    @staticmethod\n    def sigmoid(x):\n        \"\"\"\n        Sigmoid activation function.\n        \"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    @staticmethod\n    def softmax(x):\n        \"\"\"\n        Softmax activation function for the output layer.\n        \"\"\"\n        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n\n    @staticmethod\n    def mse_loss(y_true, y_pred):\n        \"\"\"\n        Mean Squared Error loss function.\n        \"\"\"\n        return np.mean(np.square(y_true - y_pred))\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs,\n        and the hidden states.\n        \"\"\"\n        h = np.zeros((hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for t in range(len(input_sequence)):\n            x_t = np.array([input_sequence[t]]).T\n            h = self.sigmoid(np.dot(W_xh, x_t) + np.dot(W_hh, h) + b_h)\n            o = self.softmax(np.dot(W_hy, h) + b_y)\n            outputs.append(o)\n            last_inputs.append(x_t)\n            last_hiddens.append(h)\n        return (outputs, last_inputs, last_hiddens)\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights.\n        \"\"\"\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        cumulative_loss = 0\n        dprev_h = np.zeros((hidden_size, 1))\n        for t in reversed(range(len(input_sequence))):\n            x_t = last_inputs[t]\n            h_t = last_hiddens[t]\n            o_t = outputs[t]\n            delta_y = o_t - expected_output[t]\n            dW_hy += np.dot(delta_y, h_t.T)\n            db_y += delta_y\n            delta_h = np.dot(W_hy.T, delta_y) + dprev_h\n            dW_xh += np.dot(delta_h, x_t.T)\n            dW_hh += np.dot(delta_h, h_t.T)\n            db_h += delta_h\n            cumulative_loss += self.mse_loss(expected_output[t], o_t)\n            dprev_h = np.dot(W_hh.T, delta_h)\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n        return (cumulative_loss / len(input_sequence), W_xh, W_hh, W_hy, b_h, b_y)\ninput_sequence = [1, 2, 3, 4, 5]\nexpected_output = [[0.6], [0.8], [0.9], [0.95], [0.98]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x0, dtype=float)\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    for i in range(n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new) < tol:\n            break\n        p = r + rs_new / rs_old * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()\nA = np.array([[3, 1], [1, 2]], dtype=float)\nb = np.array([2, -1], dtype=float)\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    counts = np.bincount(y)\n    n_samples = len(y)\n    gini = 1.0 - sum(((count / n_samples) ** 2 for count in counts if count > 0))\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    from collections import defaultdict\n    values = []\n    cols = []\n    rows = [0]\n    for (i, row) in enumerate(dense_matrix):\n        row_values = [(j, value) for (j, value) in enumerate(row) if value != 0]\n        for (col_idx, value) in row_values:\n            values.append(value)\n            cols.append(col_idx)\n        rows.append(len(values))\n    return (values, cols, rows)\ndense_matrix = [[2, 0, 3], [0, 0, 0], [4, 5, 0]]\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    L_norm = [x / sum([x ** 2 for x in L]) ** 0.5 for x in L]\n    projection = [v[i] * L_norm[i] for i in range(len(v))]\n    return [round(num, 3) for num in projection]\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    from collections import defaultdict\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_pointer = [0]\n    for col in range(num_cols):\n        row_index = []\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                row_index.append(row)\n        values.extend((dense_matrix[row][col] for row in row_index))\n        row_indices.extend(row_index)\n        col_pointer.append(len(values))\n    return (values, row_indices, col_pointer)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: module 'numpy.linalg' has no attribute 'lu'", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image/span) of a given matrix A.\n    \n    Parameters:\n    A (np.array): A numpy array for which the column space is to be found.\n    \n    Returns:\n    list: A list of lists representing the basis vectors of the column space,\n          rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    A_rref = np.linalg.matrix_rank(A) == np.linalg.lu(A)[1].diagonal().nonzero()[0].size\n    pivot_cols = np.linalg.matrix_rank(A)\n    basis = [list(A[:, i]) for i in range(pivot_cols)]\n    basis_rounded = [list(map(round, col, [1e-09] * len(col))) for col in basis]\n    return basis_rounded\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculates the R-squared value for a regression model given the true and predicted values.\n\n    Parameters:\n    - y_true: Array-like, the true values of the target variable.\n    - y_pred: Array-like, the predicted values of the target variable by the model.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    TSS = np.sum((y_true - np.mean(y_true)) ** 2)\n    RSS = np.sum((y_true - y_pred) ** 2)\n    R_squared = 1 - RSS / TSS\n    return round(R_squared, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    if num_pixels == 0:\n        return -1\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between two arrays of true and predicted values.\n\n    Parameters:\n    y_true (np.array): An array of true values.\n    y_pred (np.array): An array of predicted values corresponding to the true values.\n\n    Returns:\n    float: The RMSE value rounded to 3 decimal places.\n\n    Raises:\n    ValueError: If the input arrays are mismatched in shape, empty, or if inputs are of invalid types.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be of type numpy.ndarray.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    rmse_value = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    return round(rmse_value, 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = [1, 0, 1, 1]\ny_pred = [1, 1, 0, 1]\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient) for binary classification.\n    \n    Parameters:\n    - y_true: numpy array of true labels (binary)\n    - y_pred: numpy array of predicted labels (binary)\n    \n    Returns:\n    - Dice Score as a float rounded to 3 decimal places.\n    \"\"\"\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(y_true) + np.sum(y_pred)\n    if union == 0:\n        return 1.0 if np.all(y_true) or np.all(y_pred) else 0.0\n    dice_score = 2.0 * intersection / union\n    return round(dice_score, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: Unknown format code 'b' for object of type 'float'", "solution": "import numpy as np\nimport numpy as np\ndef hash_to_hv(hash_val, dim):\n    \"\"\"Convert a hash value to a binary hypervector.\"\"\"\n    return np.array([1 if bit == '1' else 0 for bit in format(hash_val, f'0{dim}b')])\ndef create_value_hv(value, dim, seed):\n    \"\"\"Create a hypervector for a value using a given seed.\"\"\"\n    np.random.seed(seed)\n    return hash_to_hv(np.abs(np.random.randn(1)[0]), dim)\ndef create_feature_hv(feature_name, feature_value, dim, random_seeds):\n    \"\"\"Create a composite hypervector for a feature name and its value.\"\"\"\n    feature_seed = random_seeds.get(feature_name)\n    if feature_seed is None:\n        raise ValueError(f'Seed not found for feature: {feature_name}')\n    value_hv = create_value_hv(feature_value, dim, feature_seed)\n    return value_hv\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using hyperdimensional computing.\n    \n    Parameters:\n    - row: dict, representing a dataset row with feature names as keys and values as values.\n    - dim: int, dimensionality of the hypervectors.\n    - random_seeds: dict, seeds for reproducibility of hypervectors for each feature.\n    \n    Returns:\n    - list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = None\n    for (feature_name, feature_value) in row.items():\n        feature_hv = create_feature_hv(feature_name, feature_value, dim, random_seeds)\n        if composite_hv is None:\n            composite_hv = feature_hv\n        else:\n            composite_hv = composite_hv * feature_hv\n    return composite_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for binary classification results.\n    \n    Parameters:\n    - data: List of lists, where each inner list contains [y_true, y_pred] for one observation.\n    \n    Returns:\n    - A 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    counts = Counter(((actual, predicted) for (actual, predicted) in data))\n    (tn, fp, fn, tp) = (counts[0, 0], counts[0, 1], counts[1, 0], counts[1, 1])\n    return [[tp, fn], [fp, tn]]\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two non-empty vectors v1 and v2.\n    \n    Parameters:\n    - v1: numpy array, first vector\n    - v2: numpy array, second vector\n    \n    Returns:\n    - float: cosine similarity between vectors, rounded to 3 decimal places.\n    \"\"\"\n    if v1.shape != v2.shape or len(v1) == 0 or np.linalg.norm(v1) == 0 or (np.linalg.norm(v2) == 0):\n        raise ValueError('Vectors must not be empty and must have the same shape and non-zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must have the same length.')\n    confusion_matrix = [0] * 4\n    tp = sum((a == 1 and p == 1 for (a, p) in zip(actual, predicted)))\n    fn = sum((a == 1 and p == 0 for (a, p) in zip(actual, predicted)))\n    fp = sum((a == 0 and p == 1 for (a, p) in zip(actual, predicted)))\n    tn = sum((a == 0 and p == 0 for (a, p) in zip(actual, predicted)))\n    confusion_matrix[0] = tn\n    confusion_matrix[1] = fp\n    confusion_matrix[2] = fn\n    confusion_matrix[3] = tp\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0.0\n    f1_score = 2 * tp / (2 * tp + fp + fn) if 2 * tp + fp + fn > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 0, 1, 1, 0, 1]\npredicted = [1, 1, 1, 0, 0, 0]\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n    - data: List or NumPy array of numerical values\n    \n    Returns:\n    A dictionary containing mean, median, mode, variance, standard deviation,\n    25th, 50th, and 75th percentiles, and the interquartile range (IQR).\n    All values except mode should be rounded to four decimal places.\n    \"\"\"\n    data_array = np.array(data)\n    mean_val = np.mean(data_array)\n    median_val = np.median(data_array)\n    try:\n        mode_val = stats.mode(data_array)[0][0]\n    except IndexError:\n        mode_val = None\n    variance_val = np.var(data_array)\n    std_dev_val = np.std(data_array)\n    percentiles = np.percentile(data_array, [25, 50, 75])\n    twenty_five_percentile = percentiles[0]\n    fifty_percentile = percentiles[1]\n    seventy_five_percentile = percentiles[2]\n    iqr_val = seventy_five_percentile - twenty_five_percentile\n    result = {'mean': round(mean_val, 4), 'median': round(median_val, 4), 'mode': mode_val, 'variance': round(variance_val, 4), 'standard_deviation': round(std_dev_val, 4), '25th_percentile': round(twenty_five_percentile, 4), '50th_percentile': round(fifty_percentile, 4), '75th_percentile': round(seventy_five_percentile, 4), 'interquartile_range': round(iqr_val, 4)}\n    return result\ndata = [1, 2, 2, 3, 4]\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials rounded to five decimal places\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = round(math.exp(exponent) / (math.sqrt(2 * math.pi) * std_dev), 5)\n    return pdf_value\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": false, "result": "failed: Pixel values must be uint8 integers between 0 and 255.", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n        \n    Returns:\n        float: The contrast value of the image.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise ValueError('Input must be a numpy ndarray.')\n    if img.dtype != np.uint8:\n        raise ValueError('Pixel values must be uint8 integers between 0 and 255.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise ValueError('Both inputs must be NumPy arrays.')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both vectors must be one-dimensional.')\n    return np.dot(vec1, vec2)\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": false, "result": "failed: object of type 'float' has no len()", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    def polynomial_features(point, deg):\n        features = []\n        for product in range(deg + 1):\n            for exponents in np.ndindex((deg,) * len(point)):\n                if sum(exponents) == product:\n                    features.append(np.prod([point[i] ** exponents[i] for i in range(len(point))]))\n        return [round(feature, 8) for feature in features]\n    return [polynomial_features(point, degree) for point in data]\ndegree = 2\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate sinusoidal positional encodings for a Transformer model.\n    \n    Args:\n    position : int\n        The position along the sequence axis.\n    d_model : int\n        The model dimensionality (number of features).\n\n    Returns:\n    list\n        A list containing the positional encoding for the given position and model dimensionality.\n        \n    Constraints:\n    - Returns -1 if position is 0 or d_model <= 0.\n    - The output array should be of dtype np.float16.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    positions = np.array(range(position))[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n    encoding_even = np.sin(positions * div_term)\n    encoding_odd = np.cos(positions * div_term)\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    pos_enc[:, 0::2] = encoding_even\n    pos_enc[:, 1::2] = encoding_odd\n    return pos_enc.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m_hat = m / (1 - np.power(beta1, t))\n    v_hat = v / (1 - np.power(beta2, t))\n    update_step = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter - update_step\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    return [np.round(x, 5).tolist() for x in [updated_parameter, m, v]]\nparameter = np.array([0.1, 0.2])\ngrad = np.array([0.01, 0.02])\nm = np.zeros_like(parameter)\nv = np.zeros_like(parameter)\nt = 1\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: list index out of range", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    n_ctx = hparams['n_ctx']\n    (n_vocab, d_model) = params['wte'].shape\n    hparams['n_ctx'] = min(n_ctx, len(token_ids))\n    pos = np.arange(hparams['n_ctx'])\n    pos_emb = params['wpe'][pos]\n    token_emb = params['wte'][token_ids]\n    h = token_emb + pos_emb\n    generated_tokens = []\n    for _ in range(n_tokens_to_generate):\n        linear_output = np.dot(h, params['blocks'][0]['c1']['w']) + params['blocks'][0]['c1']['b']\n        relu_output = np.maximum(linear_output, 0)\n        linear_output = np.dot(relu_output, params['blocks'][0]['c2']['w']) + params['blocks'][0]['c2']['b']\n        mean = np.mean(linear_output, axis=1, keepdims=True)\n        var = np.var(linear_output, axis=1, keepdims=True)\n        normalized = (linear_output - mean) / np.sqrt(var + params['ln_f']['b'])\n        h = params['ln_f']['g'] * normalized + params['ln_f']['b']\n        next_token_scores = h.dot(params['blocks'][0]['attn']['c_proj']['w']) + params['blocks'][0]['attn']['c_proj']['b']\n        next_token_id = np.argmax(next_token_scores, axis=1)[0]\n        generated_tokens.append(next_token_id)\n        h = np.row_stack((h, params['wte'][next_token_id]))\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nprompt = 'hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=5)\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'T'", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=-1, keepdims=True)\n    if not isinstance(crystal_values, list) or len(crystal_values) != n:\n        raise ValueError('Invalid crystal values provided.')\n    attention_matrix = np.zeros((n, dimension))\n    for i in range(n):\n        current_crystal = np.array(crystal_values[i])\n        attention_scores = np.dot(crystal_values, current_crystal)\n        attention_weights = softmax(attention_scores)\n        weighted_pattern = np.sum(np.multiply(crystal_values.T, attention_weights), axis=1)\n        attention_matrix[i] = weighted_pattern\n    result = [round(value, 4) for value in attention_matrix.flatten()]\n    return result\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: name 'k1' is not defined", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_term_frequency(term, document):\n    \"\"\"\n    Calculate the term frequency of a term in a document.\n    \n    :param term: String, the term to calculate frequency for.\n    :param document: List of strings, the document terms.\n    :return: Float, the term frequency of the term in the document.\n    \"\"\"\n    if not document:\n        return 0.0\n    term_count = document.count(term)\n    return term_count / len(document)\ndef calculate_idf(term, corpus):\n    \"\"\"\n    Calculate the inverse document frequency of a term.\n    \n    :param term: String, the term to calculate IDF for.\n    :param corpus: List of lists of strings, the corpus.\n    :return: Float, the IDF value for the term.\n    \"\"\"\n    if not corpus:\n        return 0.0\n    containing_docs = [doc for doc in corpus if term in doc]\n    idf = np.log((len(corpus) + 1) / (len(containing_docs) + 1))\n    return idf\ndef calculate_doc_length_norm(document, avg_doc_len):\n    \"\"\"\n    Calculate the document length normalization factor for a document.\n    \n    :param document: List of strings, the terms in the document.\n    :param avg_doc_len: Int, the average document length in the corpus.\n    :return: Float, the document length normalization factor.\n    \"\"\"\n    if not document:\n        return 1.0\n    return 1 - b + b * (len(document) / avg_doc_len)\ndef calculate_term_saturation(tf, avg_term_freq):\n    \"\"\"\n    Calculate the term saturation factor for a term in a document.\n    \n    :param tf: Float, the term frequency of the term in the document.\n    :param avg_term_freq: Float, the average term frequency in the corpus.\n    :return: Float, the term saturation factor.\n    \"\"\"\n    return tf / (k1 * (tf + (1 - b + b * len(document) / avg_doc_len)))\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for documents in a corpus given a query.\n    \n    :param corpus: List of lists of strings, the corpus of documents.\n    :param query: List of strings, the terms in the query.\n    :param k1: Float, the k1 parameter for BM25 calculation.\n    :param b: Float, the b parameter for BM25 calculation.\n    :return: List of floats, the BM25 scores for each document in the corpus.\n    \"\"\"\n    scores = []\n    total_terms_in_corpus = sum((sum((1 for t in doc)) for doc in corpus))\n    avg_term_freq = total_terms_in_corpus / sum((len(doc) for doc in corpus))\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n    for document in corpus:\n        doc_score = 0.0\n        for term in set(query):\n            tf = calculate_term_frequency(term, document)\n            idf = calculate_idf(term, corpus)\n            ts = calculate_term_saturation(tf, avg_term_freq)\n            doc_score += ts * ((k1 + 1) * tf) / (k1 * ts + tf)\n        doc_score *= idf * calculate_doc_length_norm(document, avg_doc_len)\n        scores.append(round(doc_score, 3))\n    return scores\ncorpus = [['query', 'retrieval', 'model'], ['information', 'retrieval', 'system'], ['bm25', 'algorithm', 'ranking']]\nquery = ['query', 'retrieval']\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "\nfrom sklearn.metrics import f1_score as score_func\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    f1 = score_func(y_true, y_pred, average='weighted')\n    return round(f1, 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "import math\nPI = 3.14159\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    fluctuation = [10 * np.sin(2 * PI * i / 10) for i in range(1, 11)]\n    detrended_data = [consumption - fluctuation[i - 1] for (i, consumption) in enumerate(consumption_data)]\n    X = np.array([[i] for i in range(1, 11)])\n    y = np.array(detrended_data)\n    model = LinearRegression().fit(X, y)\n    day_15_base_consumption = model.predict([[15]])[0]\n    day_15_fluctuation = 10 * np.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = day_15_base_consumption + day_15_fluctuation\n    final_consumption = np.ceil(predicted_consumption_day_15 * 1.05)\n    return int(final_consumption)\nconsumption_data = [120, 122, 125, 128, 130, 135, 140, 142, 145, 150]\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    abs_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(abs_errors)\n    return round(mae_value, 3)\ny_true = np.array([3.0, -0.5, 2.0, 7.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0])\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: not enough values to unpack (expected 3, got 2)", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(q: np.ndarray, k: np.ndarray, v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform self-attention using the query, key, and value matrices.\n    \n    :param q: Queries of shape (batch_size, sequence_length, qk_dim)\n    :param k: Keys of shape (batch_size, sequence_length, qk_dim)\n    :param v: Values of shape (batch_size, sequence_length, v_dim)\n    \n    :return: Resultant matrix of shape (batch_size, sequence_length, v_dim)\n    \"\"\"\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(k.shape[-1])\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    out = np.matmul(attention_weights, v)\n    return out\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement multi-head attention mechanism.\n    \n    :param X: Input data of shape (batch_size, sequence_length, hidden_dim)\n    :param W_q: Query weights for each head of shape (hidden_dim, qk_dim/n_heads)\n    :param W_k: Key weights for each head of shape (hidden_dim, qk_dim/n_heads)\n    :param W_v: Value weights for each head of shape (hidden_dim, v_dim/n_heads)\n    :param n_heads: Number of attention heads\n    \n    :return: List of outputs from each head of shape [(batch_size, sequence_length, v_dim/n_heads)] * n_heads\n    \"\"\"\n    q = W_q / np.sqrt(W_q.shape[-1])\n    k = W_k / np.sqrt(W_k.shape[-1])\n    v = W_v / np.sqrt(W_v.shape[-1])\n    (batch_size, sequence_length, hidden_dim) = X.shape\n    q_split = np.array_split(q, n_heads, axis=-1)\n    k_split = np.array_split(k, n_heads, axis=-1)\n    v_split = np.array_split(v, n_heads, axis=-1)\n    q_heads = [np.matmul(X, q) for q in q_split]\n    k_heads = [np.matmul(X, k) for k in k_split]\n    v_heads = [np.matmul(X, v) for v in v_split]\n    outs = [self_attention(q_head, k_head, v_head) for (q_head, k_head, v_head) in zip(q_heads, k_heads, v_heads)]\n    out = np.concatenate(outs, axis=2)\n    return [round(num, 4) for num in out.flatten().tolist()]\nX = np.random.rand(2, 3, 64)\nW_q = np.random.rand(64, 8)\nW_k = np.random.rand(64, 8)\nW_v = np.random.rand(64, 8)\nn_heads = 8\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": false, "result": "failed: cannot import name 'contingency_tables' from 'scipy.stats' (/home/ma-user/work/code_dev/liurb/miniconda3/envs/vllm/lib/python3.10/site-packages/scipy/stats/__init__.py)", "solution": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    from scipy.stats import contingency_tables\n    contingency = contingency_tables(x, bins=[0, 1])\n    (a, b, c, d) = (contingency.table[0, 0], contingency.table[0, 1], contingency.table[1, 0], contingency.table[1, 1])\n    phi = (a * d - b * c) ** 2 / (a + b) / (c + d) / (a + c) / (b + d)\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places\n    \"\"\"\n    if x < 0:\n        return alpha * (math.exp(x) - 1)\n    else:\n        return x\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x >= 0:\n        return round(math.log1p(math.exp(x)), 4)\n    else:\n        return round(math.log1p(math.exp(x - 20)) + 20, 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_obj = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_div = np.mean(np.abs(pi_theta_old - pi_theta_ref))\n    final_obj = surrogate_obj - beta * kl_div\n    return round(final_obj, 6)\nrhos = [0.8, 1.2, 1.0, 0.9]\nA = [1.5, 0.8, -0.3, 2.0]\npi_theta_old = [0.1, 0.2, 0.7, 0.0]\npi_theta_ref = [0.2, 0.3, 0.5, 0.0]\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": false, "result": "failed: test case failed: swish(1)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * 1 / (1 + math.exp(-x))\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": false, "result": "failed: test case failed: selu(1.0)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x < 0:\n        return scale * alpha * math.exp(x)\n    else:\n        return scale * x\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    Z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-Z))\n    predictions = (probabilities >= 0.5).astype(int).tolist()\n    return predictions\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (4,5) (5,4) (4,5) ", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    num_classes = np.unique(y).size\n    num_features = X.shape[1]\n    B = np.random.rand(num_classes, num_features)\n\n    def softmax(Z):\n        e_Z = np.exp(Z - np.max(Z, axis=0))\n        return e_Z / e_Z.sum(axis=0)\n\n    def cross_entropy_loss(probs, true_class):\n        m = true_class.shape[0]\n        return -np.sum(np.log(probs[np.arange(m), true_class])) / m\n    losses = []\n    for _ in range(iterations):\n        Z = np.dot(X, B.T)\n        probs = softmax(Z)\n        true_class_one_hot = np.zeros((X.shape[0], num_classes))\n        true_class_one_hot[np.arange(X.shape[0]), y] = 1\n        current_loss = cross_entropy_loss(probs, y)\n        losses.append(round(current_loss, 4))\n        error = probs.copy()\n        error[np.arange(X.shape[0]), y] -= 1\n        B -= learning_rate * (X.T @ error) / X.shape[0]\n    B = B.round(4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return (B, losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: non-broadcastable output operand with shape (4,1) doesn't match the broadcast shape (4,10)", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Sigmoid function applied element-wise.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n).reshape(-1, 1)\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, theta)\n        h = sigmoid(z)\n        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n        losses.append(loss)\n        dw = 1 / m * np.dot(X.T, h - y)\n        theta -= learning_rate * dw\n    losses = [round(loss, 4) for loss in losses]\n    theta = [round(theta[i][0], 4) for i in range(len(theta))]\n    return (theta, losses)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: name 'masked_attention' is not defined", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is 0 if all apples are the same color. Otherwise, it increases with the variety of colors.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    disorder_score = -sum((p * p ** (-0.5) * (1 - p ** (-0.5)) for p in probabilities))\n    return round(disorder_score, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (batch_size, seq_len, features).\n    - gamma (np.ndarray): Scaling parameter of shape (features,).\n    - beta (np.ndarray): Shifting parameter of shape (features,).\n    - epsilon (float): Small float added for numerical stability during division.\n\n    Returns:\n    - normalized_X (list): List representation of the normalized tensor rounded to 5 decimal places.\n    \"\"\"\n    (batch_size, seq_len, features) = X.shape\n    normalized_X = np.zeros_like(X)\n    for i in range(batch_size):\n        for j in range(seq_len):\n            mean = np.mean(X[i, j])\n            var = np.var(X[i, j], ddof=1)\n            normalized_X[i, j] = (X[i, j] - mean) / np.sqrt(var + epsilon)\n    normalized_X = normalized_X * gamma + beta\n    normalized_X = np.round(normalized_X, 5).tolist()\n    return normalized_X\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'lower'", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def word_match(reference, candidate):\n        ref_words = Counter(reference.lower().split())\n        can_words = Counter(candidate.lower().split())\n        match = sum((ref_words & can_words).values())\n        total = sum(ref_words.values()) + sum(can_words.values())\n        return (match, total - match)\n    (ref_words, can_words) = word_match(reference.split(), candidate.split())\n    prec = ref_words / can_words if can_words > 0 else 0\n    rec = ref_words / len(reference.split()) if len(reference.split()) > 0 else 0\n    prec = prec + 1e-10\n    fmean = (1 + alpha ** 2) * prec * rec / (alpha ** 2 * prec + rec + 1e-10)\n    order_penalty = np.exp(-beta * (len(candidate.split()) - len(reference.split())) ** 2)\n    meteor = fmean * order_penalty\n    return round(meteor, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n    \n    Parameters:\n    - joint_counts: int, the number of times both events occurred together.\n    - total_counts_x: int, the total occurrences of event X.\n    - total_counts_y: int, the total occurrences of event Y.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float: the PMI value rounded to 3 decimal places.\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x * p_y == 0:\n        return 0.0\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\njoint_counts = 50\ntotal_counts_x = 150\ntotal_counts_y = 100\ntotal_samples = 500\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(value - min_val) / (max_val - min_val) for value in x]\n    return [round(value, 4) for value in normalized]\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: 'numpy.ndarray' object has no attribute 'relu'", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Applies a residual block operation on the input x using the weight matrices w1 and w2.\n    \n    Parameters:\n    - x (np.ndarray): A 1D input numpy array.\n    - w1 (np.ndarray): Weight matrix for the first linear layer.\n    - w2 (np.ndarray): Weight matrix for the second linear layer.\n    \n    Returns:\n    - List: The computed output as a list, rounded to 4 decimal places.\n    \"\"\"\n    a1 = x.dot(w1)\n    a1.relu = lambda v: np.maximum(v, 0)\n    a2 = a1.relu(a1).dot(w2)\n    a2.relu = a1.relu\n    out = a2.relu(a2) + x\n    return round_array(out.relu(out))\ndef round_array(arr):\n    \"\"\"Rounds the array elements to 4 decimal places and converts to list.\"\"\"\n    return arr.round(4).tolist()\nx = np.array([1, 2, 3, 4])\nw1 = np.array([[1, 2], [3, 4], [5, 6], [7, 0]])\nw2 = np.array([[0, 1], [1, 0]])\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,) where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if len(x.shape) != 3:\n        raise ValueError('Input array must have exactly 3 dimensions.')\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: name 'batch_normalization' is not defined", "solution": "import numpy as np\nimport numpy as np\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    vectors (list[list[float]]): A list of 2D vectors represented as lists.\n    tol (float): Tolerance value for determining linear independence.\n    \n    Returns:\n    list[np.ndarray]: A list of orthonormal vectors spanning the same subspace as the input vectors, rounded to 4 decimal places.\n    \"\"\"\n    if not vectors:\n        return []\n    v = np.array(vectors, dtype=float)\n    q = np.zeros_like(v)\n    norms = np.zeros(v.shape[0])\n    for (i, vector) in enumerate(v):\n        current_vector = vector.copy()\n        for existing_vector in q[:i]:\n            projection = np.dot(current_vector, existing_vector) / np.linalg.norm(existing_vector) ** 2\n            current_vector -= projection * existing_vector\n        norm = np.linalg.norm(current_vector)\n        if np.isclose(norm, 0, atol=tol):\n            continue\n        q[i] = current_vector / norm\n        norms[i] = norm\n    return [np.round(basis.tolist(), 4) for basis in q]\nvectors = [[1, 1], [1, -1]]\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'size'", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Calculate the cross product of two 3-dimensional vectors a and b.\n    \n    Parameters:\n    a (np.array): A 3-dimensional numpy array representing the first vector.\n    b (np.array): A 3-dimensional numpy array representing the second vector.\n    \n    Returns:\n    list: A list representing the cross product of a and b, rounded to 4 decimal places.\n    \"\"\"\n    if a.size != 3 or b.size != 3:\n        raise ValueError('Vectors must be 3-dimensional.')\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): Coefficient matrix, assumed to be square and non-singular.\n    b (np.array): Constant vector.\n    \n    Returns:\n    list: Solution vector as a list of floats rounded to 4 decimal places, or -1 if no unique solution exists.\n    \"\"\"\n    try:\n        det_A = np.linalg.det(A)\n        if abs(det_A) < 1e-10:\n            return -1\n        n = len(b)\n        x = np.array([cramer_coeff(A, col, b) for col in range(n)])\n        return [round(val, 4) for val in x]\n    except Exception as e:\n        return -1\ndef cramer_coeff(A, col, b):\n    \"\"\"\n    Calculates the coefficient for Cramer's rule by replacing the specified column with b.\n    \"\"\"\n    return np.linalg.det(A[:, :col] + b[:, np.newaxis] * A[:, col:])\nA = np.array([[3, 2], [3, 1]], dtype=float)\nb = np.array([9, 5], dtype=float)\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    bc_sum = sum((np.sqrt(p[i] * q[i]) for i in range(len(p))))\n    distance = -np.log(bc_sum)\n    return round(distance, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Parameters:\n    a (list[int | float]): The first vector.\n    b (list[int | float]): The second vector.\n\n    Returns:\n    list[int | float]: A new vector representing the element-wise sum of the input vectors if they have the same length.\n                       Otherwise, returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list[float]:\n\n    def softmax(x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=1, keepdims=True)\n    log_policy_grads = []\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        states = np.array(states)\n        actions = np.array(actions)\n        discounts = np.array([0.95 ** i for i in range(len(rewards) + 1)][:-1])\n        theta_s = theta[states, :]\n        policy_s = softmax(theta_s)\n        log_policy_s = np.log(policy_s[np.arange(len(actions)), actions])\n        returns = rewards[::-1] * discounts[::-1]\n        grad_log_policy = (policy_s[np.arange(len(actions)), actions] * (np.eye(len(actions)) - policy_s)).sum(axis=1)\n        episode_gradient = grad_log_policy * returns\n        log_policy_grads.append(episode_gradient)\n    avg_gradient = np.mean(log_policy_grads, axis=0)\n    return np.round(avg_gradient.tolist(), 4)\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out + d_out * n_experts\n    flops_moe = k_active * d_in * d_out + d_out * n_experts\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings, 1)\nn_experts = 4\nk_active = 2\nd_in = 512\nd_out = 256\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    num_experts = W_g.shape[0]\n    k = min(k, num_experts)\n    gated_input = np.matmul(X, W_g.T)\n    noisy_gated_input = gated_input + W_noise * N\n    initial_gating_probs = np.exp(noisy_gated_input - np.max(noisy_gated_input, axis=1, keepdims=True))\n    initial_gating_probs /= np.sum(initial_gating_probs, axis=1, keepdims=True)\n    sorted_indices = np.argsort(-initial_gating_probs, axis=1)\n    topk_indices = sorted_indices[:, :k]\n    final_gating_probs = np.zeros_like(initial_gating_probs)\n    for i in range(X.shape[0]):\n        for j in range(k):\n            final_gating_probs[i, topk_indices[i, j]] = initial_gating_probs[i, sorted_indices[i, j]] / k\n    final_gating_probs = np.round(final_gating_probs, 4).tolist()\n    return final_gating_probs\nX = np.array([[1, 2], [3, 4]])\nW_noise = np.array([[1, 1], [1, 1]])\nN = np.array([[0.5, 0.5], [0.5, 0.5]])\nk = 2\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: Input dimension does not match expert weight dimensions.", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    if x.shape[1] != We.shape[1]:\n        raise ValueError('Input dimension does not match expert weight dimensions.')\n    expert_outputs = [np.dot(x, We[:, i].T) for i in range(n_experts)]\n    expert_outputs = np.stack(expert_outputs, axis=1)\n    gating_logits = np.dot(x, Wg.T)\n    gating_probs = np.exp(gating_logits - np.max(gating_logits, axis=1, keepdims=True))\n    gating_probs /= gating_probs.sum(axis=1, keepdims=True)\n    (batch_size, _) = x.shape\n    final_output = np.zeros((batch_size, top_k * expert_outputs.shape[-1]))\n    top_k_indices = np.argsort(-gating_probs, axis=1)[:, :top_k]\n    expert_topk_outputs = []\n    for i in range(batch_size):\n        expert_topk_outputs.append(expert_outputs[i, top_k_indices[i]].flatten())\n    aggregated_outputs = []\n    for i in range(batch_size):\n        aggregated_output = np.sum(expert_topk_outputs[i] * gating_probs[i, top_k_indices[i]].flatten(), axis=0)\n        aggregated_outputs.append(aggregated_output)\n    final_output = [np.round(arr, 4).tolist() for arr in aggregated_outputs]\n    return final_output\nx = np.array([[1, 2], [3, 4]])\nn_experts = 5\ntop_k = 2\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": false, "result": "failed: group_normalization.<locals>.safe_mean() got an unexpected keyword argument 'keepdims'", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n\n    def safe_mean(arr, axis=None):\n        return np.mean(arr, axis=axis)\n\n    def safe_var(arr, axis=None):\n        return np.var(arr, axis=axis)\n    num_channels = X.shape[1]\n    channels_per_group = num_channels // num_groups\n    result = []\n    for i in range(0, num_channels, channels_per_group):\n        group = X[:, i:i + channels_per_group, :, :]\n        flattened = group.reshape(group.shape[0], -1)\n        mean = safe_mean(flattened, axis=1, keepdims=True)\n        var = safe_var(flattened, axis=1, keepdims=True) + epsilon\n        std = np.sqrt(var)\n        normalized_group = (flattened - mean) / std\n        normalized_group = normalized_group.reshape(group.shape)\n        normalized_group = normalized_group * gamma[i:i + channels_per_group] + beta[i:i + channels_per_group]\n        result.append(normalized_group)\n    result = np.concatenate(result, axis=1)\n    return result.tolist()\nX = np.random.rand(2, 8, 32, 32)\ngamma = np.random.rand(8)\nbeta = np.random.rand(8)\nnum_groups = 2\nresult = group_normalization(X, gamma, beta, num_groups)\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the value of x where the function f(x) = x^4 - 3x^3 + 2 reaches its minimum,\n    starting from the given initial position 'start_x'.\n    \n    :param start_x: Initial guess for the position of the minimum.\n    :param learning_rate: Step size for each iteration.\n    :param tolerance: The precision goal for the value of f(x).\n    :param max_iters: Maximum number of iterations to perform.\n    :return: The x value where the minimum is reached, rounded to 4 decimal places.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        derivative = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * derivative\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) transformation to the input array `x`.\n\n    :param x: Input numpy array of any shape.\n    :param alpha: Scaling parameter for the DyT function.\n    :param gamma: Shifting parameter for the DyT function.\n    :param beta: Clipping parameter for the DyT function.\n    :return: A list containing the transformed values, rounded to 4 decimal places.\n    \"\"\"\n    dyt = np.tanh(alpha * x + gamma) + beta\n    return dyt.astype(np.float64).tolist()\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
