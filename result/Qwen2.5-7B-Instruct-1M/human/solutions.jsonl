{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [list(row) for row in zip(*a)]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a given matrix into a specified shape.\n    \n    Parameters:\n    a (list[list[int|float]]): The original matrix to be reshaped.\n    new_shape (tuple[int, int]): The target shape to reshape the matrix into.\n    \n    Returns:\n    list[list[int|float]]: The reshaped matrix as a list of lists, or [] if reshaping is not possible.\n    \"\"\"\n    np_array = np.array(a)\n    if np_array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = np_array.reshape(new_shape).tolist()\n    return reshaped_array"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists where each inner list represents a row of the matrix.\n    :param mode: A string indicating the mode of calculation ('row' or 'column').\n    :return: A list of means according to the specified mode.\n    \"\"\"\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        means = [sum(col) / num_rows for col in zip(*matrix)]\n    else:\n        raise ValueError(\"Mode should be either 'row' or 'column'\")\n    return means"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import numpy as np\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return sorted_eigenvalues\nmatrix = [[4, -2], [1, 1]]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^-1 * A * S, where T and S are invertible matrices.\n    \n    Parameters:\n    A (list[list[int|float]]): The original matrix to be transformed.\n    T (list[list[int|float]]): The invertible matrix T used for transformation.\n    S (list[list[int|float]]): The invertible matrix S used for transformation.\n    \n    Returns:\n    list[list[int|float]]: The transformed matrix rounded to the nearest 4th decimal.\n                           Returns -1 if there is no solution or matrices are not invertible.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n            return -1\n        transformed_matrix = np.round(np.dot(np.linalg.inv(T_np), np.dot(A_np, S_np)), 4)\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[list[float]]: The inverse of the matrix if it exists, otherwise None.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse_matrix = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    vectors_array = np.array(vectors)\n    means = vectors_array.mean(axis=0)\n    centered_vectors = vectors_array - means\n    covariance_matrix = np.cov(centered_vectors, rowvar=False)\n    return covariance_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if not np.all(np.abs(np.diag(A)) >= np.sum(np.abs(A), axis=1)):\n        raise ValueError('Matrix A must be diagonal dominant or diagonally dominant.')\n    D = np.diag(np.diag(A))\n    R = A - D\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.dot(D, b) - np.dot(R, x)\n        x = x_new.round(4)\n    return x.reshape(-1).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([5, 4])\nn = 25"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n\n    def jacobian(A):\n        (m, n) = A.shape\n        J = np.zeros((n, m))\n        for i in range(n):\n            for j in range(m):\n                C = A.copy()\n                C[j, i] += 1e-05\n                yield (C - A)\n\n    def svd_update(u, v, jacobian_mat):\n        alpha = np.dot(np.dot(v.T, jacobian_mat), u)\n        denom = 1 + alpha * v[i][i]\n        s = np.sqrt(np.abs(1 / denom))\n        u_new = u * s\n        v_new = v * s\n        s_new = 1 / denom\n        return (u_new, v_new, s_new)\n    u = np.eye(2)\n    v = np.eye(2)\n    s = np.zeros(2)\n    for iteration in range(10):\n        for jacobian_mat in jacobian(A):\n            for i in range(2):\n                (u, v, s[i]) = svd_update(u[:, i].copy(), v[i, :].copy(), jacobian_mat)\n    return tuple((round(x, 4) for x in s))\nA = np.array([[2, 0], [1, 2]])"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    if len(matrix) == 1:\n        return matrix[0][0]\n    det = 0\n    for col in range(len(matrix)):\n        sign = (-1) ** col\n        minor_matrix = []\n        for row in range(1, len(matrix)):\n            minor_row = []\n            for m_col in range(len(matrix)):\n                if m_col != col:\n                    minor_row.append(matrix[row][m_col])\n            minor_matrix.append(minor_row)\n        det += sign * matrix[0][col] * determinant_4x4(minor_matrix)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.zeros(X_b.shape[1])\n    for _ in range(iterations):\n        gradients = 2 * X_b.T.dot(X_b.dot(theta) - y) / len(y)\n        theta -= alpha * gradients\n    return np.round(theta.reshape(-1).tolist(), 4)"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales features of the given dataset using standardization and min-max normalization.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array with samples in the rows and features in the columns.\n    \n    Returns:\n    (list[list[float]], list[list[float]]): A tuple containing two lists of lists:\n                                            - The first list contains the standardized features.\n                                            - The second list contains the min-max normalized features.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data_rounded = [np.round(row, 4).tolist() for row in standardized_data]\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_scaled_data = (data - min_val) / (max_val - min_val)\n    min_max_scaled_rounded = [np.round(row, 4).tolist() for row in min_max_scaled_data]\n    return (standardized_data_rounded, min_max_scaled_rounded)\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    initial_centroids_array = np.array(initial_centroids)\n    centroids = initial_centroids_array\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids.T, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = []\n        for i in range(k):\n            if np.any(labels == i):\n                new_centroid = np.mean(points_array[labels == i], axis=0)\n                new_centroids.append(new_centroid)\n        new_centroids_array = np.array(new_centroids)\n        if np.allclose(centroids, new_centroids_array, atol=0.0001):\n            break\n        centroids = new_centroids_array\n    final_centroids = [tuple(round(c, 4)) for c in centroids]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0), (8.0, 2.0), (10.0, 2.0), (9.0, 3.0)]\nk = 3\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0), (8.0, 8.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train-test indices for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: Features dataset (np.ndarray).\n    - y: Labels dataset (np.ndarray).\n    - k: Number of folds (default is 5).\n    - shuffle: Boolean to indicate whether to shuffle data before splitting into batches (default is True).\n    - random_seed: Integer value for reproducibility of results (optional).\n\n    Returns:\n    - A list of tuples containing train and test indices for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    fold_indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(fold_indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    train_test_indices = []\n    start_idx = 0\n    for fold in range(k):\n        test_size = fold_size + (1 if fold < remainder else 0)\n        test_indices = fold_indices[start_idx:start_idx + test_size]\n        train_indices = np.array(list(fold_indices[:start_idx]) + list(fold_indices[start_idx + test_size:]))\n        train_test_indices.append((train_indices, test_indices))\n        start_idx += test_size\n    return train_test_indices"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = data.mean(axis=0)\n    std_dev = data.std(axis=0)\n    standardized_data = (data - mean) / std_dev\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    return ([round(eigenvalue, 4) for eigenvalue in eigenvalues[-k:]], [list(round(value, 4)) for value in eigenvector.tolist() for eigenvector in eigenvectors[:, -k:]])"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    This function implements a decision tree learning algorithm using recursive binary splitting.\n    It selects the attribute that best splits the data based on information gain and constructs\n    a decision tree represented as a nested dictionary.\n\n    Parameters:\n    - examples: List of dictionaries where each dictionary represents an example with attribute-value pairs.\n    - attributes: List of strings representing the available attributes for splitting.\n    - target_attr: String representing the name of the target attribute to predict.\n\n    Returns:\n    - A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(examples):\n        \"\"\"Calculate the entropy of a given dataset.\"\"\"\n        if not examples:\n            return 0\n        class_counts = Counter((example[target_attr] for example in examples))\n        p = [count / len(examples) for count in class_counts.values()]\n        return -sum((p_i * math.log2(p_i) for p_i in p if p_i != 0))\n\n    def info_gain(left, right, current_entropy):\n        \"\"\"Calculate the information gain between the parent node and two child nodes.\"\"\"\n        return current_entropy - sum((len(leaf) / len(examples) * entropy(leaf) for leaf in [left, right]))\n\n    def find_best_split(examples, attributes):\n        \"\"\"Find the attribute that provides the highest information gain.\"\"\"\n        base_entropy = entropy(examples)\n        (best_attr, best_gain) = (None, 0)\n        for attr in attributes:\n            value_counts = Counter((example[attr] for example in examples))\n            weighted_entropy = sum((len(value_ex) / len(examples) * entropy(value_ex) for (value, value_ex) in value_counts.items()))\n            gain = base_entropy - weighted_entropy\n            if gain > best_gain:\n                (best_gain, best_attr) = (gain, attr)\n        return best_attr\n\n    def build_tree(examples, attributes):\n        \"\"\"Recursively build the decision tree.\"\"\"\n        if not examples:\n            return Counter({target_attr: 'none'}).most_common(1)[0][0]\n        if all((example[target_attr] == examples[0][target_attr] for example in examples)):\n            return examples[0][target_attr]\n        if not attributes:\n            return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n        best_attr = find_best_split(examples, attributes)\n        attributes.remove(best_attr)\n        (left_examples, right_examples) = ([], [])\n        for example in examples:\n            if example[best_attr] == example[best_attr]:\n                left_examples.append(example)\n            else:\n                right_examples.append(example)\n        tree = {best_attr: {}}\n        tree[best_attr]['yes'] = build_tree(left_examples, attributes[:])\n        tree[best_attr]['no'] = build_tree(right_examples, attributes[:])\n        return tree\n    return build_tree(examples, list(attributes))\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'true', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': 'true', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'windy']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm for training a kernel SVM classifier.\n    \n    Parameters:\n    - data: 2D numpy array of shape (n_samples, n_features)\n    - labels: 1D numpy array of labels of length n_samples\n    - kernel: type of kernel to use ('linear' or 'rbf')\n    - lambda_val: regularization parameter\n    - iterations: number of iterations to run the algorithm\n    - sigma: standard deviation for RBF kernel\n    \n    Returns:\n    - alpha: numpy array of alpha coefficients\n    - bias: bias term\n    \n    All results are rounded to the nearest 4th decimal and returned as a list.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    labels = labels.reshape(-1, 1) * 1.0\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'rbf':\n        fullGram = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                fullGram[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n        K = fullGram\n    else:\n        raise ValueError('Unsupported kernel type')\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        predictions = K @ alpha + bias\n        conditions = np.where(labels * predictions <= 1, True, False)\n        alpha[conditions] += 1.0 / (t * lambda_val)\n        alpha[np.invert(conditions)] = np.maximum(alpha[np.invert(conditions)] - 1.0 / (t * lambda_val), 0.0)\n        bias = np.mean(labels - K @ alpha)\n    alpha = np.round(alpha, 4).tolist()\n    bias = np.round(bias, 4)\n    return (alpha, [bias])\ndata = np.array([[1, 2], [2, 2], [3, 3], [5, 7], [6, 5]])\nlabels = np.array([1, 1, 1, -1, -1])\nkernel = 'linear'\nlambda_val = 0.01\niterations = 100\nsigma = 1.0"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid function for the input value z.\n    \n    Args:\n    z (float): Input value for the sigmoid function.\n    \n    Returns:\n    float: Output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: List of feature vectors (2D list).\n    :param labels: True binary labels (list of integers).\n    :param weights: Weights for each feature (list of floats).\n    :param bias: Bias term (float).\n    :return: Predicted probabilities and mean squared error, both rounded to 4 decimal places.\n    \"\"\"\n    predicted_probabilities = []\n    total_error = 0\n    for (feature_vector, label) in zip(features, labels):\n        weighted_sum = bias\n        for (feature, weight) in zip(feature_vector, weights):\n            weighted_sum += feature * weight\n        probability = 1 / (1 + math.exp(-weighted_sum))\n        predicted_probabilities.append(round(probability, 4))\n        total_error += (probability - label) ** 2\n    mse = round(total_error / len(labels), 4)\n    return ([predicted_probabilities], mse)\nfeatures = [[1.0, 2.0], [2.0, 3.0]]\nlabels = [0, 1]\nweights = [0.5, 0.5]\nbias = 0.5"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        predictions = 1 / (1 + np.exp(-np.dot(features, weights) - bias))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        d_error_d_pred = predictions - labels\n        d_pred_d_z = predictions * (1 - predictions)\n        d_z_d_w = features\n        d_z_d_b = 1\n        grad_w = np.dot(d_error_d_pred * d_pred_d_z, d_z_d_w.T) / len(features)\n        grad_b = np.sum(d_error_d_pred * d_pred_d_z * d_z_d_b) / len(features)\n        weights -= learning_rate * grad_w\n        bias -= learning_rate * grad_b\n    return (weights, bias, mse_values)\nfeatures = np.array([[0.5, 0.1], [0.8, 0.2], [0.1, 0.9], [0.6, 0.3]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.1, 0.1])\ninitial_bias = 0.1\nlearning_rate = 0.1\nepochs = 50"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def backward(self):\n        topo_order = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo_order.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo_order):\n            v._backward()\n            v.grad = v._backward()\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C in R^3.\n    \n    Parameters:\n    B (list[list[int]]): List of lists representing the basis B.\n    C (list[list[int]]): List of lists representing the basis C.\n    \n    Returns:\n    list[list[float]]: The transformation matrix P from basis B to C, rounded to 4 decimal places.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    P = np.linalg.solve(B_array, C_array)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [1, 0, 1], [0, 1, 1]]"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Matrix A must be 2x2')\n    A_T = A.T\n    A_T_A = A_T @ A\n    (eigen_values_A_T_A, Q) = np.linalg.eig(A_T_A)\n    singular_values = np.sqrt(np.real(eigen_values_A_T_A))\n    S = np.diag(singular_values)\n    V = Q\n    U = [np.linalg.norm(A @ V[:, i]) * V[:, i] for i in range(V.shape[1])]\n    U = np.array(U).T\n    U = np.round(U, 4).tolist()\n    S = np.round(S, 4).tolist()\n    V = np.round(V, 4).tolist()\n    return (U, S, V)\nA = np.array([[2, 4], [1, 3]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays X and y in place while maintaining their correspondence.\n    \n    Parameters:\n    - X (np.ndarray): Features of the dataset.\n    - y (np.ndarray): Labels of the dataset.\n    - seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    - list: A list of tuples containing the shuffled elements of X and y.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    return list(zip(X[indices], y[indices]))"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from numpy arrays X and optionally y.\n    \n    Parameters:\n    - X (np.ndarray): The feature array.\n    - y (np.ndarray, optional): The label array.\n    - batch_size (int): The size of each batch to yield.\n    \n    Yields:\n    - A tuple (np.ndarray, np.ndarray) of (features, labels) for each batch if y is not None.\n    - np.ndarray of features otherwise.\n    \"\"\"\n    assert len(X) > 0\n    assert batch_size > 0\n    assert batch_size <= len(X), 'Batch size is larger than the dataset size.'\n    if y is not None:\n        for i in range(0, len(X), batch_size):\n            yield (X[i:i + batch_size], y[i:i + batch_size])\n    else:\n        for i in range(0, len(X), batch_size):\n            yield X[i:i + batch_size]"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Splits the dataset X into two subsets based on whether the values of a specific feature\n    are greater than or equal to a given threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset, a 2D array where each row is a sample and columns are features.\n    feature_i (int): The index of the feature to split the dataset on.\n    threshold (float): The threshold value to compare each feature's value against.\n\n    Returns:\n    list: A list containing two lists of arrays:\n          - The first list contains samples meeting the threshold condition.\n          - The second list contains samples not meeting the threshold condition.\n          All lists are converted to numpy arrays and then to lists before returning.\n    \"\"\"\n    above_threshold = X[X[:, feature_i] >= threshold]\n    below_threshold = X[X[:, feature_i] < threshold]\n    return [above_threshold.tolist(), below_threshold.tolist()]"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for the given dataset up to the specified degree.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features).\n    degree (int): The degree of the polynomial features.\n    \n    Returns:\n    List[List[float]]: A list of lists where each inner list contains the polynomial features as floats.\n    \"\"\"\n    if degree <= 1:\n        return X.tolist()\n    column_combinations = combinations_with_replacement(range(X.shape[1]), degree)\n    result_rows = []\n    for combo in column_combinations:\n        combined_feature = X[:, combo[0]]\n        for idx in range(1, len(combo)):\n            combined_feature *= X[:, combo[idx]]\n        result_rows.append(combined_feature)\n    poly_features = np.vstack(result_rows)\n    return poly_features.T.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 3"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates n_subsets of (X,y) pairs through random sampling with or without replacement.\n    \n    Parameters:\n    - X (np.array): The feature set, a 2D numpy array.\n    - y (np.array): The target set, a 1D numpy array.\n    - n_subsets (int): Number of random subsets to generate.\n    - replacements (bool): Sampling with or without replacement.\n    - seed (int): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    - List of tuples: Each containing a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    random_indices = [np.random.choice(len(y), size=len(y), replace=replacements) for _ in range(n_subsets)]\n    return [(X[indices], y[indices]) for indices in random_indices]"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    - x: A 1D numpy array of integers to be one-hot encoded.\n    - n_col: The number of columns for the one-hot encoded result. \n             If not provided, it will be set to the maximum value in x plus one.\n    \n    Returns:\n    A list representation of the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = int(x.max()) + 1\n    encoded = (x.reshape(-1, 1) == np.arange(n_col)).astype(int)\n    return encoded.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (np.array): A 1D numpy array.\n\n    Returns:\n    np.array: A 2D numpy array representing the diagonal matrix.\n    \"\"\"\n    return np.diag(x)\nx = np.array([1, 2, 3, 4])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.metrics import accuracy_score as sk_accuracy_score\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of model predictions.\n    \n    Parameters:\n    - y_true: A 1D numpy array of true labels.\n    - y_pred: A 1D numpy array of predicted labels.\n    \n    Returns:\n    - accuracy: The accuracy score as a float, rounded to 4 decimal places.\n    \"\"\"\n    accuracy = sk_accuracy_score(y_true, y_pred)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix from the given 2D numpy arrays X and Y.\n    If Y is not provided, calculate the correlation matrix of X with itself.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): Another 2D numpy array. Defaults to None.\n    \n    Returns:\n    list: A 2D list representing the correlation matrix, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        correlation_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        correlation_matrix = np.corrcoef(np.column_stack((X, Y)), rowvar=False)\n    return np.round(correlation_matrix, 4).tolist()\nX = np.array([[1, 2, 3, 4, 5], [10, 9, 8, 7, 6]])"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    m = X.shape[0]\n    D = np.full(m, 1 / m)\n    classifiers = []\n    for _ in range(n_clf):\n        best_classifiers = []\n        for feature_index in range(X.shape[1]):\n            sorted_indices = np.argsort(X[:, feature_index])\n            sorted_X = X[sorted_indices, feature_index]\n            sorted_y = y[sorted_indices]\n            pos_weight_cumsum = np.cumsum(D[sorted_y == 1])\n            neg_weight_cumsum = -np.cumsum(D[sorted_y == -1][::-1])[::-1]\n            errors = pos_weight_cumsum + neg_weight_cumsum\n            best_error = np.min(errors)\n            best_threshold_index = np.argmin(errors)\n            best_threshold = (sorted_X[best_threshold_index] + sorted_X[best_threshold_index + 1]) / 2\n            best_classifiers.append((feature_index, best_threshold, best_error))\n        best_classifier_info = min(best_classifiers, key=lambda x: x[2])\n        (feature_index, best_threshold, best_error) = best_classifier_info\n        classifier = {'feature_index': feature_index, 'threshold': best_threshold, 'classification_error': best_error}\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        predictions = X[:, feature_index] > best_threshold\n        D *= np.exp(-alpha * y * predictions)\n        D /= np.sum(D)\n        classifiers.append(classifier)\n    return classifiers\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 1, -1, -1])\nn_clf = 2"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores, dtype=np.float64)\n    z_max = np.max(scores_array)\n    log_softmax_result = np.log(np.exp(scores_array - z_max).sum()) + z_max\n    return np.round(log_softmax_result, 4).tolist()\nscores = [1000, 1002, 1001]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self):\n        if self.input_shape is not None:\n            limit = 1 / math.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n            self.w0 = np.zeros(self.n_units)\n        else:\n            self.W = None\n            self.w0 = None\n        self.optimizer_W = lambda x: x\n        self.optimizer_w0 = lambda x: x\n\n    def parameters(self):\n        if self.W is not None and self.w0 is not None:\n            return self.W.size + self.w0.size\n        return 0\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        input_grad = np.dot(accum_grad, self.W.T)\n        self.W_grad = np.dot(self.layer_input.T, accum_grad)\n        self.w0_grad = np.sum(accum_grad, axis=0)\n        self.W = self.optimizer_W(self.W - self.W_grad)\n        self.w0 = self.optimizer_w0(self.w0 - self.w0_grad)\n        return input_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_width = input_matrix.shape[1] + 2 * padding\n    padded_height = input_matrix.shape[0] + 2 * padding\n    padded_matrix = np.zeros((padded_height, padded_width))\n    padded_matrix[padding:padded_height - padding, padding:padded_width - padding] = input_matrix\n    output_height = (padded_matrix.shape[0] - kernel.shape[0]) // stride + 1\n    output_width = (padded_matrix.shape[1] - kernel.shape[1]) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            output_matrix[i, j] = np.sum(padded_matrix[i * stride:i * stride + kernel.shape[0], j * stride:j * stride + kernel.shape[1]] * kernel)\n    return output_matrix.round(4).tolist()\ninput_matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nkernel = np.array([[1, 0], [0, -1]])\npadding = 0\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function to the input float.\n    \n    Parameters:\n    z (float): A floating point number to apply the ReLU function on.\n    \n    Returns:\n    float: The result of applying ReLU to z.\n    \"\"\"\n    return max(0.0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss for a given feature matrix X, coefficients w,\n    true labels y_true, and regularization parameter alpha.\n    \n    Parameters:\n    - X: A 2D numpy array representing the feature matrix.\n    - w: A 1D numpy array representing the coefficients.\n    - y_true: A 1D numpy array representing the true labels.\n    - alpha: A float representing the regularization parameter.\n    \n    Returns:\n    - The Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_pred - y_true) ** 2)\n    L2_norm = np.sum(w ** 2)\n    loss = mse + alpha * L2_norm\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "import math\ndef leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"Apply the Leaky ReLU activation function to the input z with the given alpha value.\"\"\"\n    return max(0, z) + min(0, z, alpha * z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two vectors x1 and x2.\n    \n    The linear kernel is defined as the dot product of x1 and x2.\n    \n    Parameters:\n    - x1: numpy array, the first vector\n    - x2: numpy array, the second vector\n    \n    Returns:\n    - float, the dot product of x1 and x2\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n    \n    Parameters:\n    - y_true: np.ndarray, an array of true binary labels.\n    - y_pred: np.ndarray, an array of predicted binary labels.\n    \n    Returns:\n    float, the precision metric.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 0\n    precision = true_positives / predicted_positives\n    return precision"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef compute_gradient(X, y, weights):\n    \"\"\"Compute the gradient of the loss function with respect to the weights.\"\"\"\n    error = y - X @ weights\n    gradient = -(2 / len(X)) * X.T @ error\n    return gradient\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent with different variants: batch, stochastic, and mini-batch.\n    \n    Parameters:\n    - X: Feature matrix (numpy array)\n    - y: Target vector (numpy array)\n    - weights: Initial weights (numpy array)\n    - learning_rate: Learning rate for weight updates\n    - n_iterations: Number of iterations to perform\n    - batch_size: Size of batches for mini-batch gradient descent\n    - method: Type of gradient descent ('batch', 'stochastic', or 'mini_batch')\n    \n    Returns:\n    - List of weights after each iteration, rounded to 4 decimal places.\n    \"\"\"\n    weight_history = [weights.copy()]\n    if method == 'stochastic':\n        batch_size = 1\n    for _ in range(n_iterations):\n        if method == 'batch':\n            gradient = compute_gradient(X, y, weights)\n        elif method == 'stochastic':\n            shuffled_indices = np.random.permutation(len(X))\n            (X_shuffled, y_shuffled) = (X[shuffled_indices], y[shuffled_indices])\n            for start_idx in range(0, len(X), batch_size):\n                end_idx = start_idx + batch_size\n                gradient = compute_gradient(X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx], weights)\n        elif method == 'mini_batch':\n            shuffled_indices = np.random.permutation(len(X))\n            (X_shuffled, y_shuffled) = (X[shuffled_indices], y[shuffled_indices])\n            for start_idx in range(0, len(X), batch_size):\n                end_idx = start_idx + batch_size\n                gradient = compute_gradient(X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx], weights)\n        else:\n            raise ValueError('Invalid method specified for gradient descent.')\n        updated_weights = weights - learning_rate * gradient\n        weight_history.append(updated_weights.copy())\n    return [weights.round(4).tolist() for weights in weight_history]\nX = np.array([[1, 1], [1, 2], [1, 3]])\ny = np.array([1.1, 1.9, 2.8])\nweights = np.array([0, 0])\nlearning_rate = 0.1\nn_iterations = 10\nbatch_size = 2\nmethod = 'mini_batch'"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D list representing the matrix to be converted.\n    \n    Returns:\n    list: A 2D list representing the matrix in RREF.\n    \"\"\"\n    mat = np.array(matrix)\n\n    def swap_rows(mat, row1, row2):\n        \"\"\"Swap two rows in the matrix.\"\"\"\n        mat[[row1, row2]] = mat[[row2, row1]]\n\n    def eliminate_column(mat, pivot_row, pivot_col):\n        \"\"\"Make all entries below the pivot zero.\"\"\"\n        for row in range(pivot_row + 1, mat.shape[0]):\n            factor = mat[row, pivot_col] / mat[pivot_row, pivot_col]\n            mat[row] -= factor * mat[pivot_row]\n    (row_count, col_count) = mat.shape\n    for col in range(min(col_count, row_count)):\n        pivot_row = col\n        while pivot_row < row_count and abs(mat[pivot_row, col]) < 1e-10:\n            pivot_row += 1\n        if pivot_row == row_count:\n            continue\n        if pivot_row != col:\n            swap_rows(mat, pivot_row, col)\n        if abs(mat[col, col]) != 1:\n            mat[col] /= mat[col, col]\n        eliminate_column(mat, col, col)\n    mat = np.where(np.isclose(mat, 1) | np.isclose(mat, 0), mat, 0)\n    return mat.tolist()\nmatrix = [[1, 2, -1, 4], [-2, -3, 2, -7], [4, 3, -4, 8]]"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Perform optimization of a function using the Adam optimization algorithm.\n\n    Parameters:\n    f : function\n        The objective function to be optimized.\n    grad : function\n        A function that computes the gradient of f.\n    x0 : array_like\n        Initial parameter values.\n    learning_rate : float, optional\n        The step size (default: 0.001).\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates (default: 0.9).\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates (default: 0.999).\n    epsilon : float, optional\n        A small constant for numerical stability (default: 1e-8).\n    num_iterations : int, optional\n        Number of iterations to run the optimizer (default: 1000).\n\n    Returns:\n    list\n        Optimized parameters rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x0)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for i in range(num_iterations):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * np.power(g, 2)\n        m_hat = m / (1 - np.power(beta1, i + 1))\n        v_hat = v / (1 - np.power(beta2, i + 1))\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x.tolist(), 4)\nx0 = [1.0, 1.0]"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent with L1 Regularization.\n\n    Parameters:\n    X : np.array\n        Feature matrix of shape (n_samples, n_features).\n    y : np.array\n        Target vector of length n_samples.\n    alpha : float, optional\n        Regularization parameter. Default is 0.1.\n    learning_rate : float, optional\n        Learning rate for gradient descent. Default is 0.01.\n    max_iter : int, optional\n        Maximum number of iterations. Default is 1000.\n    tol : float, optional\n        Tolerance for convergence. Default is 1e-4.\n\n    Returns:\n    tuple: A tuple containing the learned weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    converged = False\n    iteration = 0\n    while not converged and iteration < max_iter:\n        y_pred = np.dot(X, weights) + bias\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y) + alpha * np.sign(weights)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        if np.allclose(dw, 0, atol=tol):\n            converged = True\n        iteration += 1\n    return (np.round(weights.tolist(), 4), np.round(bias, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            insertions = dp[i][j - 1] + 1\n            deletions = dp[i - 1][j] + 1\n            substitutions = dp[i - 1][j - 1] + cost\n            transpositions = dp[i - 2][j - 2] + cost if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]) else float('inf')\n            dp[i][j] = min(insertions, deletions, substitutions, transpositions)\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List of true binary labels (0 or 1).\n    - y_pred: List of predicted binary labels (0 or 1).\n    \n    Returns:\n    - Recall value rounded to 3 decimal places. Returns 0.0 if the denominator is zero.\n    \"\"\"\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    recall_value = np.where(TP + FN == 0, 0.0, TP / (TP + FN))\n    return np.round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    X = np.array(X)\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T)\n    attention_scores = attention_scores - np.max(attention_scores, axis=-1, keepdims=True)\n    attention_scores = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    attention_output = np.dot(attention_scores, V)\n    attention_output = np.round(attention_output, 4)\n    return attention_output.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\nW_q = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nW_k = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nW_v = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell. Processes a sequence of input vectors and produces the final hidden state.\n    \n    Parameters:\n    - input_sequence: List of input vectors (sequences).\n    - initial_hidden_state: Initial hidden state.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n    \n    Returns:\n    - Final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(x, Wx) + np.dot(h, Wh) + b)\n    final_hidden_state = np.round(h, 4).tolist()\n    return final_hidden_state\ninput_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\ninitial_hidden_state = [0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.2]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.hstack((points_array, np.ones((len(points), 1)))).astype(float)\n    transformed_homogeneous_points = np.dot(homogeneous_points, translation_matrix.T)\n    translated_points = transformed_homogeneous_points[:, :-1]\n    return translated_points.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions P and Q.\n    \n    Parameters:\n    mu_p (float): Mean of distribution P.\n    sigma_p (float): Standard deviation of distribution P.\n    mu_q (float): Mean of distribution Q.\n    sigma_q (float): Standard deviation of distribution Q.\n    \n    Returns:\n    float: The KL divergence from distribution P to distribution Q.\n    \"\"\"\n    sigma_p_sq = sigma_p ** 2\n    sigma_q_sq = sigma_q ** 2\n    return 0.5 * (sigma_q_sq / sigma_p_sq + (mu_p - mu_q) ** 2 / sigma_p_sq - 1 + np.log(sigma_q_sq / sigma_p_sq))"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    - A: Coefficient matrix (square matrix)\n    - b: Right-hand side vector\n    - n: Number of iterations\n    - x_ini: Initial guess for the solution (optional, defaults to a vector of zeros)\n    \n    Returns:\n    - Approximate solution vector x as a python list\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return x_new.tolist()\nA = [[4, 1, 0], [1, 3, 1], [0, 1, 5]]\nb = [2, 3, 5]\nn = 25"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix of shape (n, n)\n    b (np.array): Dependent variable vector of shape (n,)\n    \n    Returns:\n    list: Solution vector x as a python list.\n    \"\"\"\n    n = len(b)\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    for i in range(n):\n        max_line = np.argmax(np.abs(Ab[i:, i])) + i\n        if Ab[max_line, i] == 0:\n            raise ValueError('Matrix is singular.')\n        Ab[[i, max_line], :] = Ab[[max_line, i], :]\n        for j in range(i + 1, n):\n            ratio = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= ratio * Ab[i, i:]\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = Ab[i, n] / Ab[i, i]\n        Ab[i, i + 1:] -= x[i] * Ab[i, i + 1:]\n    return Ab[:, :-1].reshape(-1).tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = x.shape[0]\n        hs = []\n        h = initial_hidden_state\n        c = initial_cell_state\n        for t in range(T):\n            xt = x[t]\n            input_gate = self.sigmoid(np.dot(self.Wi, np.concatenate((xt, h), axis=0)) + self.bi)\n            forget_gate = self.sigmoid(np.dot(self.Wf, np.concatenate((xt, h), axis=0)) + self.bf)\n            output_gate = self.sigmoid(np.dot(self.Wo, np.concatenate((xt, h), axis=0)) + self.bo)\n            candidate_cell_state = self.tanh(np.dot(self.Wc, np.concatenate((xt, h), axis=0)) + self.bc)\n            c = forget_gate * c + input_gate * candidate_cell_state\n            h = output_gate * self.tanh(c)\n            hs.append(h)\n        return (hs, h, c)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\nfrom math import log\ndef compute_tf_idf(corpus, query):\n\n    def term_frequency(word, document):\n        return document.count(word) / len(document)\n\n    def inverse_document_frequency(word, corpus):\n        df = sum((1 for doc in corpus if word in doc))\n        return log(len(corpus) / (1 + df))\n    tfidf_scores = []\n    if not corpus:\n        raise ValueError('Corpus cannot be empty.')\n    doc_freqs = defaultdict(int)\n    for doc in corpus:\n        for word in set(doc):\n            doc_freqs[word] += 1\n    for doc in corpus:\n        doc_tfidf = []\n        for word in query:\n            tf = term_frequency(word, doc)\n            idf = inverse_document_frequency(word, corpus)\n            tfidf = tf * idf if tfidf_scores else 0\n            doc_tfidf.append(round(tfidf, 5))\n        tfidf_scores.append(doc_tfidf)\n    return np.array(tfidf_scores).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox'], ['jumps', 'over', 'the', 'lazy', 'dog']]\nquery = ['the', 'fox']"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.metrics import precision_score, recall_score\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    try:\n        fscore = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    except ZeroDivisionError:\n        fscore = 0.0\n    return round(fscore, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    @staticmethod\n    def initialize_weights(input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN parameters (weights and biases).\n        \"\"\"\n        return {'W_xh': np.random.randn(hidden_size, input_size) * 0.01, 'W_hh': np.random.randn(hidden_size, hidden_size) * 0.01, 'W_hy': np.random.randn(output_size, hidden_size) * 0.01, 'b_h': np.zeros((hidden_size, 1)), 'b_y': np.zeros((output_size, 1))}\n\n    @staticmethod\n    def sigmoid(x):\n        \"\"\"\n        Sigmoid activation function.\n        \"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    @staticmethod\n    def softmax(x):\n        \"\"\"\n        Softmax activation function for the output layer.\n        \"\"\"\n        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n\n    @staticmethod\n    def mse_loss(y_true, y_pred):\n        \"\"\"\n        Mean Squared Error loss function.\n        \"\"\"\n        return np.mean(np.square(y_true - y_pred))\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs,\n        and the hidden states.\n        \"\"\"\n        h = np.zeros((hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for t in range(len(input_sequence)):\n            x_t = np.array([input_sequence[t]]).T\n            h = self.sigmoid(np.dot(W_xh, x_t) + np.dot(W_hh, h) + b_h)\n            o = self.softmax(np.dot(W_hy, h) + b_y)\n            outputs.append(o)\n            last_inputs.append(x_t)\n            last_hiddens.append(h)\n        return (outputs, last_inputs, last_hiddens)\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights.\n        \"\"\"\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        cumulative_loss = 0\n        dprev_h = np.zeros((hidden_size, 1))\n        for t in reversed(range(len(input_sequence))):\n            x_t = last_inputs[t]\n            h_t = last_hiddens[t]\n            o_t = outputs[t]\n            delta_y = o_t - expected_output[t]\n            dW_hy += np.dot(delta_y, h_t.T)\n            db_y += delta_y\n            delta_h = np.dot(W_hy.T, delta_y) + dprev_h\n            dW_xh += np.dot(delta_h, x_t.T)\n            dW_hh += np.dot(delta_h, h_t.T)\n            db_h += delta_h\n            cumulative_loss += self.mse_loss(expected_output[t], o_t)\n            dprev_h = np.dot(W_hh.T, delta_h)\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n        return (cumulative_loss / len(input_sequence), W_xh, W_hh, W_hy, b_h, b_y)\ninput_sequence = [1, 2, 3, 4, 5]\nexpected_output = [[0.6], [0.8], [0.9], [0.95], [0.98]]"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x0, dtype=float)\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    for i in range(n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new) < tol:\n            break\n        p = r + rs_new / rs_old * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()\nA = np.array([[3, 1], [1, 2]], dtype=float)\nb = np.array([2, -1], dtype=float)"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    counts = np.bincount(y)\n    n_samples = len(y)\n    gini = 1.0 - sum(((count / n_samples) ** 2 for count in counts if count > 0))\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    from collections import defaultdict\n    values = []\n    cols = []\n    rows = [0]\n    for (i, row) in enumerate(dense_matrix):\n        row_values = [(j, value) for (j, value) in enumerate(row) if value != 0]\n        for (col_idx, value) in row_values:\n            values.append(value)\n            cols.append(col_idx)\n        rows.append(len(values))\n    return (values, cols, rows)\ndense_matrix = [[2, 0, 3], [0, 0, 0], [4, 5, 0]]"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    L_norm = [x / sum([x ** 2 for x in L]) ** 0.5 for x in L]\n    projection = [v[i] * L_norm[i] for i in range(len(v))]\n    return [round(num, 3) for num in projection]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    from collections import defaultdict\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_pointer = [0]\n    for col in range(num_cols):\n        row_index = []\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                row_index.append(row)\n        values.extend((dense_matrix[row][col] for row in row_index))\n        row_indices.extend(row_index)\n        col_pointer.append(len(values))\n    return (values, row_indices, col_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image/span) of a given matrix A.\n    \n    Parameters:\n    A (np.array): A numpy array for which the column space is to be found.\n    \n    Returns:\n    list: A list of lists representing the basis vectors of the column space,\n          rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    A_rref = np.linalg.matrix_rank(A) == np.linalg.lu(A)[1].diagonal().nonzero()[0].size\n    pivot_cols = np.linalg.matrix_rank(A)\n    basis = [list(A[:, i]) for i in range(pivot_cols)]\n    basis_rounded = [list(map(round, col, [1e-09] * len(col))) for col in basis]\n    return basis_rounded"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculates the R-squared value for a regression model given the true and predicted values.\n\n    Parameters:\n    - y_true: Array-like, the true values of the target variable.\n    - y_pred: Array-like, the predicted values of the target variable by the model.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    TSS = np.sum((y_true - np.mean(y_true)) ** 2)\n    RSS = np.sum((y_true - y_pred) ** 2)\n    R_squared = 1 - RSS / TSS\n    return round(R_squared, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    if num_pixels == 0:\n        return -1\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between two arrays of true and predicted values.\n\n    Parameters:\n    y_true (np.array): An array of true values.\n    y_pred (np.array): An array of predicted values corresponding to the true values.\n\n    Returns:\n    float: The RMSE value rounded to 3 decimal places.\n\n    Raises:\n    ValueError: If the input arrays are mismatched in shape, empty, or if inputs are of invalid types.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be of type numpy.ndarray.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    rmse_value = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = [1, 0, 1, 1]\ny_pred = [1, 1, 0, 1]"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient) for binary classification.\n    \n    Parameters:\n    - y_true: numpy array of true labels (binary)\n    - y_pred: numpy array of predicted labels (binary)\n    \n    Returns:\n    - Dice Score as a float rounded to 3 decimal places.\n    \"\"\"\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(y_true) + np.sum(y_pred)\n    if union == 0:\n        return 1.0 if np.all(y_true) or np.all(y_pred) else 0.0\n    dice_score = 2.0 * intersection / union\n    return round(dice_score, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef hash_to_hv(hash_val, dim):\n    \"\"\"Convert a hash value to a binary hypervector.\"\"\"\n    return np.array([1 if bit == '1' else 0 for bit in format(hash_val, f'0{dim}b')])\ndef create_value_hv(value, dim, seed):\n    \"\"\"Create a hypervector for a value using a given seed.\"\"\"\n    np.random.seed(seed)\n    return hash_to_hv(np.abs(np.random.randn(1)[0]), dim)\ndef create_feature_hv(feature_name, feature_value, dim, random_seeds):\n    \"\"\"Create a composite hypervector for a feature name and its value.\"\"\"\n    feature_seed = random_seeds.get(feature_name)\n    if feature_seed is None:\n        raise ValueError(f'Seed not found for feature: {feature_name}')\n    value_hv = create_value_hv(feature_value, dim, feature_seed)\n    return value_hv\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using hyperdimensional computing.\n    \n    Parameters:\n    - row: dict, representing a dataset row with feature names as keys and values as values.\n    - dim: int, dimensionality of the hypervectors.\n    - random_seeds: dict, seeds for reproducibility of hypervectors for each feature.\n    \n    Returns:\n    - list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = None\n    for (feature_name, feature_value) in row.items():\n        feature_hv = create_feature_hv(feature_name, feature_value, dim, random_seeds)\n        if composite_hv is None:\n            composite_hv = feature_hv\n        else:\n            composite_hv = composite_hv * feature_hv\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for binary classification results.\n    \n    Parameters:\n    - data: List of lists, where each inner list contains [y_true, y_pred] for one observation.\n    \n    Returns:\n    - A 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    counts = Counter(((actual, predicted) for (actual, predicted) in data))\n    (tn, fp, fn, tp) = (counts[0, 0], counts[0, 1], counts[1, 0], counts[1, 1])\n    return [[tp, fn], [fp, tn]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two non-empty vectors v1 and v2.\n    \n    Parameters:\n    - v1: numpy array, first vector\n    - v2: numpy array, second vector\n    \n    Returns:\n    - float: cosine similarity between vectors, rounded to 3 decimal places.\n    \"\"\"\n    if v1.shape != v2.shape or len(v1) == 0 or np.linalg.norm(v1) == 0 or (np.linalg.norm(v2) == 0):\n        raise ValueError('Vectors must not be empty and must have the same shape and non-zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must have the same length.')\n    confusion_matrix = [0] * 4\n    tp = sum((a == 1 and p == 1 for (a, p) in zip(actual, predicted)))\n    fn = sum((a == 1 and p == 0 for (a, p) in zip(actual, predicted)))\n    fp = sum((a == 0 and p == 1 for (a, p) in zip(actual, predicted)))\n    tn = sum((a == 0 and p == 0 for (a, p) in zip(actual, predicted)))\n    confusion_matrix[0] = tn\n    confusion_matrix[1] = fp\n    confusion_matrix[2] = fn\n    confusion_matrix[3] = tp\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0.0\n    f1_score = 2 * tp / (2 * tp + fp + fn) if 2 * tp + fp + fn > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 0, 1, 1, 0, 1]\npredicted = [1, 1, 1, 0, 0, 0]"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n    - data: List or NumPy array of numerical values\n    \n    Returns:\n    A dictionary containing mean, median, mode, variance, standard deviation,\n    25th, 50th, and 75th percentiles, and the interquartile range (IQR).\n    All values except mode should be rounded to four decimal places.\n    \"\"\"\n    data_array = np.array(data)\n    mean_val = np.mean(data_array)\n    median_val = np.median(data_array)\n    try:\n        mode_val = stats.mode(data_array)[0][0]\n    except IndexError:\n        mode_val = None\n    variance_val = np.var(data_array)\n    std_dev_val = np.std(data_array)\n    percentiles = np.percentile(data_array, [25, 50, 75])\n    twenty_five_percentile = percentiles[0]\n    fifty_percentile = percentiles[1]\n    seventy_five_percentile = percentiles[2]\n    iqr_val = seventy_five_percentile - twenty_five_percentile\n    result = {'mean': round(mean_val, 4), 'median': round(median_val, 4), 'mode': mode_val, 'variance': round(variance_val, 4), 'standard_deviation': round(std_dev_val, 4), '25th_percentile': round(twenty_five_percentile, 4), '50th_percentile': round(fifty_percentile, 4), '75th_percentile': round(seventy_five_percentile, 4), 'interquartile_range': round(iqr_val, 4)}\n    return result\ndata = [1, 2, 2, 3, 4]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials rounded to five decimal places\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = round(math.exp(exponent) / (math.sqrt(2 * math.pi) * std_dev), 5)\n    return pdf_value"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n        \n    Returns:\n        float: The contrast value of the image.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise ValueError('Input must be a numpy ndarray.')\n    if img.dtype != np.uint8:\n        raise ValueError('Pixel values must be uint8 integers between 0 and 255.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise ValueError('Both inputs must be NumPy arrays.')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both vectors must be one-dimensional.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    def polynomial_features(point, deg):\n        features = []\n        for product in range(deg + 1):\n            for exponents in np.ndindex((deg,) * len(point)):\n                if sum(exponents) == product:\n                    features.append(np.prod([point[i] ** exponents[i] for i in range(len(point))]))\n        return [round(feature, 8) for feature in features]\n    return [polynomial_features(point, degree) for point in data]\ndegree = 2"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate sinusoidal positional encodings for a Transformer model.\n    \n    Args:\n    position : int\n        The position along the sequence axis.\n    d_model : int\n        The model dimensionality (number of features).\n\n    Returns:\n    list\n        A list containing the positional encoding for the given position and model dimensionality.\n        \n    Constraints:\n    - Returns -1 if position is 0 or d_model <= 0.\n    - The output array should be of dtype np.float16.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    positions = np.array(range(position))[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n    encoding_even = np.sin(positions * div_term)\n    encoding_odd = np.cos(positions * div_term)\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    pos_enc[:, 0::2] = encoding_even\n    pos_enc[:, 1::2] = encoding_odd\n    return pos_enc.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m_hat = m / (1 - np.power(beta1, t))\n    v_hat = v / (1 - np.power(beta2, t))\n    update_step = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter - update_step\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    return [np.round(x, 5).tolist() for x in [updated_parameter, m, v]]\nparameter = np.array([0.1, 0.2])\ngrad = np.array([0.01, 0.02])\nm = np.zeros_like(parameter)\nv = np.zeros_like(parameter)\nt = 1"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    n_ctx = hparams['n_ctx']\n    (n_vocab, d_model) = params['wte'].shape\n    hparams['n_ctx'] = min(n_ctx, len(token_ids))\n    pos = np.arange(hparams['n_ctx'])\n    pos_emb = params['wpe'][pos]\n    token_emb = params['wte'][token_ids]\n    h = token_emb + pos_emb\n    generated_tokens = []\n    for _ in range(n_tokens_to_generate):\n        linear_output = np.dot(h, params['blocks'][0]['c1']['w']) + params['blocks'][0]['c1']['b']\n        relu_output = np.maximum(linear_output, 0)\n        linear_output = np.dot(relu_output, params['blocks'][0]['c2']['w']) + params['blocks'][0]['c2']['b']\n        mean = np.mean(linear_output, axis=1, keepdims=True)\n        var = np.var(linear_output, axis=1, keepdims=True)\n        normalized = (linear_output - mean) / np.sqrt(var + params['ln_f']['b'])\n        h = params['ln_f']['g'] * normalized + params['ln_f']['b']\n        next_token_scores = h.dot(params['blocks'][0]['attn']['c_proj']['w']) + params['blocks'][0]['attn']['c_proj']['b']\n        next_token_id = np.argmax(next_token_scores, axis=1)[0]\n        generated_tokens.append(next_token_id)\n        h = np.row_stack((h, params['wte'][next_token_id]))\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nprompt = 'hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=5)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=-1, keepdims=True)\n    if not isinstance(crystal_values, list) or len(crystal_values) != n:\n        raise ValueError('Invalid crystal values provided.')\n    attention_matrix = np.zeros((n, dimension))\n    for i in range(n):\n        current_crystal = np.array(crystal_values[i])\n        attention_scores = np.dot(crystal_values, current_crystal)\n        attention_weights = softmax(attention_scores)\n        weighted_pattern = np.sum(np.multiply(crystal_values.T, attention_weights), axis=1)\n        attention_matrix[i] = weighted_pattern\n    result = [round(value, 4) for value in attention_matrix.flatten()]\n    return result"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_term_frequency(term, document):\n    \"\"\"\n    Calculate the term frequency of a term in a document.\n    \n    :param term: String, the term to calculate frequency for.\n    :param document: List of strings, the document terms.\n    :return: Float, the term frequency of the term in the document.\n    \"\"\"\n    if not document:\n        return 0.0\n    term_count = document.count(term)\n    return term_count / len(document)\ndef calculate_idf(term, corpus):\n    \"\"\"\n    Calculate the inverse document frequency of a term.\n    \n    :param term: String, the term to calculate IDF for.\n    :param corpus: List of lists of strings, the corpus.\n    :return: Float, the IDF value for the term.\n    \"\"\"\n    if not corpus:\n        return 0.0\n    containing_docs = [doc for doc in corpus if term in doc]\n    idf = np.log((len(corpus) + 1) / (len(containing_docs) + 1))\n    return idf\ndef calculate_doc_length_norm(document, avg_doc_len):\n    \"\"\"\n    Calculate the document length normalization factor for a document.\n    \n    :param document: List of strings, the terms in the document.\n    :param avg_doc_len: Int, the average document length in the corpus.\n    :return: Float, the document length normalization factor.\n    \"\"\"\n    if not document:\n        return 1.0\n    return 1 - b + b * (len(document) / avg_doc_len)\ndef calculate_term_saturation(tf, avg_term_freq):\n    \"\"\"\n    Calculate the term saturation factor for a term in a document.\n    \n    :param tf: Float, the term frequency of the term in the document.\n    :param avg_term_freq: Float, the average term frequency in the corpus.\n    :return: Float, the term saturation factor.\n    \"\"\"\n    return tf / (k1 * (tf + (1 - b + b * len(document) / avg_doc_len)))\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for documents in a corpus given a query.\n    \n    :param corpus: List of lists of strings, the corpus of documents.\n    :param query: List of strings, the terms in the query.\n    :param k1: Float, the k1 parameter for BM25 calculation.\n    :param b: Float, the b parameter for BM25 calculation.\n    :return: List of floats, the BM25 scores for each document in the corpus.\n    \"\"\"\n    scores = []\n    total_terms_in_corpus = sum((sum((1 for t in doc)) for doc in corpus))\n    avg_term_freq = total_terms_in_corpus / sum((len(doc) for doc in corpus))\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n    for document in corpus:\n        doc_score = 0.0\n        for term in set(query):\n            tf = calculate_term_frequency(term, document)\n            idf = calculate_idf(term, corpus)\n            ts = calculate_term_saturation(tf, avg_term_freq)\n            doc_score += ts * ((k1 + 1) * tf) / (k1 * ts + tf)\n        doc_score *= idf * calculate_doc_length_norm(document, avg_doc_len)\n        scores.append(round(doc_score, 3))\n    return scores\ncorpus = [['query', 'retrieval', 'model'], ['information', 'retrieval', 'system'], ['bm25', 'algorithm', 'ranking']]\nquery = ['query', 'retrieval']"}
{"task_id": 91, "completion_id": 0, "solution": "from sklearn.metrics import f1_score as score_func\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    f1 = score_func(y_true, y_pred, average='weighted')\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    fluctuation = [10 * np.sin(2 * PI * i / 10) for i in range(1, 11)]\n    detrended_data = [consumption - fluctuation[i - 1] for (i, consumption) in enumerate(consumption_data)]\n    X = np.array([[i] for i in range(1, 11)])\n    y = np.array(detrended_data)\n    model = LinearRegression().fit(X, y)\n    day_15_base_consumption = model.predict([[15]])[0]\n    day_15_fluctuation = 10 * np.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = day_15_base_consumption + day_15_fluctuation\n    final_consumption = np.ceil(predicted_consumption_day_15 * 1.05)\n    return int(final_consumption)\nconsumption_data = [120, 122, 125, 128, 130, 135, 140, 142, 145, 150]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    abs_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(abs_errors)\n    return round(mae_value, 3)\ny_true = np.array([3.0, -0.5, 2.0, 7.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0])"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(q: np.ndarray, k: np.ndarray, v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform self-attention using the query, key, and value matrices.\n    \n    :param q: Queries of shape (batch_size, sequence_length, qk_dim)\n    :param k: Keys of shape (batch_size, sequence_length, qk_dim)\n    :param v: Values of shape (batch_size, sequence_length, v_dim)\n    \n    :return: Resultant matrix of shape (batch_size, sequence_length, v_dim)\n    \"\"\"\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(k.shape[-1])\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    out = np.matmul(attention_weights, v)\n    return out\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement multi-head attention mechanism.\n    \n    :param X: Input data of shape (batch_size, sequence_length, hidden_dim)\n    :param W_q: Query weights for each head of shape (hidden_dim, qk_dim/n_heads)\n    :param W_k: Key weights for each head of shape (hidden_dim, qk_dim/n_heads)\n    :param W_v: Value weights for each head of shape (hidden_dim, v_dim/n_heads)\n    :param n_heads: Number of attention heads\n    \n    :return: List of outputs from each head of shape [(batch_size, sequence_length, v_dim/n_heads)] * n_heads\n    \"\"\"\n    q = W_q / np.sqrt(W_q.shape[-1])\n    k = W_k / np.sqrt(W_k.shape[-1])\n    v = W_v / np.sqrt(W_v.shape[-1])\n    (batch_size, sequence_length, hidden_dim) = X.shape\n    q_split = np.array_split(q, n_heads, axis=-1)\n    k_split = np.array_split(k, n_heads, axis=-1)\n    v_split = np.array_split(v, n_heads, axis=-1)\n    q_heads = [np.matmul(X, q) for q in q_split]\n    k_heads = [np.matmul(X, k) for k in k_split]\n    v_heads = [np.matmul(X, v) for v in v_split]\n    outs = [self_attention(q_head, k_head, v_head) for (q_head, k_head, v_head) in zip(q_heads, k_heads, v_heads)]\n    out = np.concatenate(outs, axis=2)\n    return [round(num, 4) for num in out.flatten().tolist()]\nX = np.random.rand(2, 3, 64)\nW_q = np.random.rand(64, 8)\nW_k = np.random.rand(64, 8)\nW_v = np.random.rand(64, 8)\nn_heads = 8"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    from scipy.stats import contingency_tables\n    contingency = contingency_tables(x, bins=[0, 1])\n    (a, b, c, d) = (contingency.table[0, 0], contingency.table[0, 1], contingency.table[1, 0], contingency.table[1, 1])\n    phi = (a * d - b * c) ** 2 / (a + b) / (c + d) / (a + c) / (b + d)\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places\n    \"\"\"\n    if x < 0:\n        return alpha * (math.exp(x) - 1)\n    else:\n        return x"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x >= 0:\n        return round(math.log1p(math.exp(x)), 4)\n    else:\n        return round(math.log1p(math.exp(x - 20)) + 20, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_obj = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_div = np.mean(np.abs(pi_theta_old - pi_theta_ref))\n    final_obj = surrogate_obj - beta * kl_div\n    return round(final_obj, 6)\nrhos = [0.8, 1.2, 1.0, 0.9]\nA = [1.5, 0.8, -0.3, 2.0]\npi_theta_old = [0.1, 0.2, 0.7, 0.0]\npi_theta_ref = [0.2, 0.3, 0.5, 0.0]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * 1 / (1 + math.exp(-x))"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x < 0:\n        return scale * alpha * math.exp(x)\n    else:\n        return scale * x"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    Z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-Z))\n    predictions = (probabilities >= 0.5).astype(int).tolist()\n    return predictions"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    num_classes = np.unique(y).size\n    num_features = X.shape[1]\n    B = np.random.rand(num_classes, num_features)\n\n    def softmax(Z):\n        e_Z = np.exp(Z - np.max(Z, axis=0))\n        return e_Z / e_Z.sum(axis=0)\n\n    def cross_entropy_loss(probs, true_class):\n        m = true_class.shape[0]\n        return -np.sum(np.log(probs[np.arange(m), true_class])) / m\n    losses = []\n    for _ in range(iterations):\n        Z = np.dot(X, B.T)\n        probs = softmax(Z)\n        true_class_one_hot = np.zeros((X.shape[0], num_classes))\n        true_class_one_hot[np.arange(X.shape[0]), y] = 1\n        current_loss = cross_entropy_loss(probs, y)\n        losses.append(round(current_loss, 4))\n        error = probs.copy()\n        error[np.arange(X.shape[0]), y] -= 1\n        B -= learning_rate * (X.T @ error) / X.shape[0]\n    B = B.round(4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return (B, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Sigmoid function applied element-wise.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n).reshape(-1, 1)\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, theta)\n        h = sigmoid(z)\n        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n        losses.append(loss)\n        dw = 1 / m * np.dot(X.T, h - y)\n        theta -= learning_rate * dw\n    losses = [round(loss, 4) for loss in losses]\n    theta = [round(theta[i][0], 4) for i in range(len(theta))]\n    return (theta, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is 0 if all apples are the same color. Otherwise, it increases with the variety of colors.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    disorder_score = -sum((p * p ** (-0.5) * (1 - p ** (-0.5)) for p in probabilities))\n    return round(disorder_score, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (batch_size, seq_len, features).\n    - gamma (np.ndarray): Scaling parameter of shape (features,).\n    - beta (np.ndarray): Shifting parameter of shape (features,).\n    - epsilon (float): Small float added for numerical stability during division.\n\n    Returns:\n    - normalized_X (list): List representation of the normalized tensor rounded to 5 decimal places.\n    \"\"\"\n    (batch_size, seq_len, features) = X.shape\n    normalized_X = np.zeros_like(X)\n    for i in range(batch_size):\n        for j in range(seq_len):\n            mean = np.mean(X[i, j])\n            var = np.var(X[i, j], ddof=1)\n            normalized_X[i, j] = (X[i, j] - mean) / np.sqrt(var + epsilon)\n    normalized_X = normalized_X * gamma + beta\n    normalized_X = np.round(normalized_X, 5).tolist()\n    return normalized_X"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def word_match(reference, candidate):\n        ref_words = Counter(reference.lower().split())\n        can_words = Counter(candidate.lower().split())\n        match = sum((ref_words & can_words).values())\n        total = sum(ref_words.values()) + sum(can_words.values())\n        return (match, total - match)\n    (ref_words, can_words) = word_match(reference.split(), candidate.split())\n    prec = ref_words / can_words if can_words > 0 else 0\n    rec = ref_words / len(reference.split()) if len(reference.split()) > 0 else 0\n    prec = prec + 1e-10\n    fmean = (1 + alpha ** 2) * prec * rec / (alpha ** 2 * prec + rec + 1e-10)\n    order_penalty = np.exp(-beta * (len(candidate.split()) - len(reference.split())) ** 2)\n    meteor = fmean * order_penalty\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n    \n    Parameters:\n    - joint_counts: int, the number of times both events occurred together.\n    - total_counts_x: int, the total occurrences of event X.\n    - total_counts_y: int, the total occurrences of event Y.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float: the PMI value rounded to 3 decimal places.\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x * p_y == 0:\n        return 0.0\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\njoint_counts = 50\ntotal_counts_x = 150\ntotal_counts_y = 100\ntotal_samples = 500"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(value - min_val) / (max_val - min_val) for value in x]\n    return [round(value, 4) for value in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Applies a residual block operation on the input x using the weight matrices w1 and w2.\n    \n    Parameters:\n    - x (np.ndarray): A 1D input numpy array.\n    - w1 (np.ndarray): Weight matrix for the first linear layer.\n    - w2 (np.ndarray): Weight matrix for the second linear layer.\n    \n    Returns:\n    - List: The computed output as a list, rounded to 4 decimal places.\n    \"\"\"\n    a1 = x.dot(w1)\n    a1.relu = lambda v: np.maximum(v, 0)\n    a2 = a1.relu(a1).dot(w2)\n    a2.relu = a1.relu\n    out = a2.relu(a2) + x\n    return round_array(out.relu(out))\ndef round_array(arr):\n    \"\"\"Rounds the array elements to 4 decimal places and converts to list.\"\"\"\n    return arr.round(4).tolist()\nx = np.array([1, 2, 3, 4])\nw1 = np.array([[1, 2], [3, 4], [5, 6], [7, 0]])\nw2 = np.array([[0, 1], [1, 0]])"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,) where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if len(x.shape) != 3:\n        raise ValueError('Input array must have exactly 3 dimensions.')\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    vectors (list[list[float]]): A list of 2D vectors represented as lists.\n    tol (float): Tolerance value for determining linear independence.\n    \n    Returns:\n    list[np.ndarray]: A list of orthonormal vectors spanning the same subspace as the input vectors, rounded to 4 decimal places.\n    \"\"\"\n    if not vectors:\n        return []\n    v = np.array(vectors, dtype=float)\n    q = np.zeros_like(v)\n    norms = np.zeros(v.shape[0])\n    for (i, vector) in enumerate(v):\n        current_vector = vector.copy()\n        for existing_vector in q[:i]:\n            projection = np.dot(current_vector, existing_vector) / np.linalg.norm(existing_vector) ** 2\n            current_vector -= projection * existing_vector\n        norm = np.linalg.norm(current_vector)\n        if np.isclose(norm, 0, atol=tol):\n            continue\n        q[i] = current_vector / norm\n        norms[i] = norm\n    return [np.round(basis.tolist(), 4) for basis in q]\nvectors = [[1, 1], [1, -1]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Calculate the cross product of two 3-dimensional vectors a and b.\n    \n    Parameters:\n    a (np.array): A 3-dimensional numpy array representing the first vector.\n    b (np.array): A 3-dimensional numpy array representing the second vector.\n    \n    Returns:\n    list: A list representing the cross product of a and b, rounded to 4 decimal places.\n    \"\"\"\n    if a.size != 3 or b.size != 3:\n        raise ValueError('Vectors must be 3-dimensional.')\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): Coefficient matrix, assumed to be square and non-singular.\n    b (np.array): Constant vector.\n    \n    Returns:\n    list: Solution vector as a list of floats rounded to 4 decimal places, or -1 if no unique solution exists.\n    \"\"\"\n    try:\n        det_A = np.linalg.det(A)\n        if abs(det_A) < 1e-10:\n            return -1\n        n = len(b)\n        x = np.array([cramer_coeff(A, col, b) for col in range(n)])\n        return [round(val, 4) for val in x]\n    except Exception as e:\n        return -1\ndef cramer_coeff(A, col, b):\n    \"\"\"\n    Calculates the coefficient for Cramer's rule by replacing the specified column with b.\n    \"\"\"\n    return np.linalg.det(A[:, :col] + b[:, np.newaxis] * A[:, col:])\nA = np.array([[3, 2], [3, 1]], dtype=float)\nb = np.array([9, 5], dtype=float)"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    bc_sum = sum((np.sqrt(p[i] * q[i]) for i in range(len(p))))\n    distance = -np.log(bc_sum)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Parameters:\n    a (list[int | float]): The first vector.\n    b (list[int | float]): The second vector.\n\n    Returns:\n    list[int | float]: A new vector representing the element-wise sum of the input vectors if they have the same length.\n                       Otherwise, returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list[float]:\n\n    def softmax(x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=1, keepdims=True)\n    log_policy_grads = []\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        states = np.array(states)\n        actions = np.array(actions)\n        discounts = np.array([0.95 ** i for i in range(len(rewards) + 1)][:-1])\n        theta_s = theta[states, :]\n        policy_s = softmax(theta_s)\n        log_policy_s = np.log(policy_s[np.arange(len(actions)), actions])\n        returns = rewards[::-1] * discounts[::-1]\n        grad_log_policy = (policy_s[np.arange(len(actions)), actions] * (np.eye(len(actions)) - policy_s)).sum(axis=1)\n        episode_gradient = grad_log_policy * returns\n        log_policy_grads.append(episode_gradient)\n    avg_gradient = np.mean(log_policy_grads, axis=0)\n    return np.round(avg_gradient.tolist(), 4)"}
{"task_id": 123, "completion_id": 0, "solution": "import math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out + d_out * n_experts\n    flops_moe = k_active * d_in * d_out + d_out * n_experts\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings, 1)\nn_experts = 4\nk_active = 2\nd_in = 512\nd_out = 256"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    num_experts = W_g.shape[0]\n    k = min(k, num_experts)\n    gated_input = np.matmul(X, W_g.T)\n    noisy_gated_input = gated_input + W_noise * N\n    initial_gating_probs = np.exp(noisy_gated_input - np.max(noisy_gated_input, axis=1, keepdims=True))\n    initial_gating_probs /= np.sum(initial_gating_probs, axis=1, keepdims=True)\n    sorted_indices = np.argsort(-initial_gating_probs, axis=1)\n    topk_indices = sorted_indices[:, :k]\n    final_gating_probs = np.zeros_like(initial_gating_probs)\n    for i in range(X.shape[0]):\n        for j in range(k):\n            final_gating_probs[i, topk_indices[i, j]] = initial_gating_probs[i, sorted_indices[i, j]] / k\n    final_gating_probs = np.round(final_gating_probs, 4).tolist()\n    return final_gating_probs\nX = np.array([[1, 2], [3, 4]])\nW_noise = np.array([[1, 1], [1, 1]])\nN = np.array([[0.5, 0.5], [0.5, 0.5]])\nk = 2"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    if x.shape[1] != We.shape[1]:\n        raise ValueError('Input dimension does not match expert weight dimensions.')\n    expert_outputs = [np.dot(x, We[:, i].T) for i in range(n_experts)]\n    expert_outputs = np.stack(expert_outputs, axis=1)\n    gating_logits = np.dot(x, Wg.T)\n    gating_probs = np.exp(gating_logits - np.max(gating_logits, axis=1, keepdims=True))\n    gating_probs /= gating_probs.sum(axis=1, keepdims=True)\n    (batch_size, _) = x.shape\n    final_output = np.zeros((batch_size, top_k * expert_outputs.shape[-1]))\n    top_k_indices = np.argsort(-gating_probs, axis=1)[:, :top_k]\n    expert_topk_outputs = []\n    for i in range(batch_size):\n        expert_topk_outputs.append(expert_outputs[i, top_k_indices[i]].flatten())\n    aggregated_outputs = []\n    for i in range(batch_size):\n        aggregated_output = np.sum(expert_topk_outputs[i] * gating_probs[i, top_k_indices[i]].flatten(), axis=0)\n        aggregated_outputs.append(aggregated_output)\n    final_output = [np.round(arr, 4).tolist() for arr in aggregated_outputs]\n    return final_output\nx = np.array([[1, 2], [3, 4]])\nn_experts = 5\ntop_k = 2"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n\n    def safe_mean(arr, axis=None):\n        return np.mean(arr, axis=axis)\n\n    def safe_var(arr, axis=None):\n        return np.var(arr, axis=axis)\n    num_channels = X.shape[1]\n    channels_per_group = num_channels // num_groups\n    result = []\n    for i in range(0, num_channels, channels_per_group):\n        group = X[:, i:i + channels_per_group, :, :]\n        flattened = group.reshape(group.shape[0], -1)\n        mean = safe_mean(flattened, axis=1, keepdims=True)\n        var = safe_var(flattened, axis=1, keepdims=True) + epsilon\n        std = np.sqrt(var)\n        normalized_group = (flattened - mean) / std\n        normalized_group = normalized_group.reshape(group.shape)\n        normalized_group = normalized_group * gamma[i:i + channels_per_group] + beta[i:i + channels_per_group]\n        result.append(normalized_group)\n    result = np.concatenate(result, axis=1)\n    return result.tolist()\nX = np.random.rand(2, 8, 32, 32)\ngamma = np.random.rand(8)\nbeta = np.random.rand(8)\nnum_groups = 2\nresult = group_normalization(X, gamma, beta, num_groups)"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the value of x where the function f(x) = x^4 - 3x^3 + 2 reaches its minimum,\n    starting from the given initial position 'start_x'.\n    \n    :param start_x: Initial guess for the position of the minimum.\n    :param learning_rate: Step size for each iteration.\n    :param tolerance: The precision goal for the value of f(x).\n    :param max_iters: Maximum number of iterations to perform.\n    :return: The x value where the minimum is reached, rounded to 4 decimal places.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        derivative = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * derivative\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) transformation to the input array `x`.\n\n    :param x: Input numpy array of any shape.\n    :param alpha: Scaling parameter for the DyT function.\n    :param gamma: Shifting parameter for the DyT function.\n    :param beta: Clipping parameter for the DyT function.\n    :return: A list containing the transformed values, rounded to 4 decimal places.\n    \"\"\"\n    dyt = np.tanh(alpha * x + gamma) + beta\n    return dyt.astype(np.float64).tolist()"}
