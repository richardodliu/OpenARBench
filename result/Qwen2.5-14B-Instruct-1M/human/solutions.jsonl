{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float | int]:\n    if len(b) != len(a[0]):\n        return -1\n    result = [sum((x * y for (x, y) in zip(row, b))) for row in a]\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "from typing import List\ndef transpose_matrix(a: List[List[int | float]]) -> List[List[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n\n    Args:\n        a (List[List[int | float]]): A matrix represented as a list of lists where each sublist represents a row.\n\n    Returns:\n        List[List[int | float]]: The transpose of the given matrix.\n    \"\"\"\n    if not a or not a[0]:\n        return []\n    transposed = [list(row) for row in zip(*a)]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given 2D matrix 'a' into a new shape specified by 'new_shape'.\n    If reshaping is not possible, returns an empty list [].\n    \n    Parameters:\n    a (list of list of int/float): The original matrix.\n    new_shape (tuple of two ints): The desired shape (rows, columns).\n    \n    Returns:\n    list of list of int/float: The reshaped matrix as a list or an empty list if reshaping fails.\n    \"\"\"\n    arr = np.array(a)\n    if arr.size != new_shape[0] * new_shape[1]:\n        return []\n    try:\n        reshaped_arr = arr.reshape(new_shape).tolist()\n    except ValueError:\n        return []\n    return reshaped_arr"}
{"task_id": 4, "completion_id": 0, "solution": "from typing import List\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column based on the given mode.\n    \n    :param matrix: A list of lists of floats representing the matrix.\n    :param mode: A string, either 'row' or 'column', indicating the mode of calculation.\n    :return: A list of floats representing the calculated means.\n    \"\"\"\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        if not matrix:\n            return []\n        return [sum(column) / len(matrix) for column in zip(*matrix)]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the specified scalar value.\n\n    :param matrix: A list of lists representing the matrix to be multiplied.\n    :param scalar: The scalar value to multiply the matrix by.\n    :return: A new list of lists representing the resulting matrix after multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n\n    :param matrix: A list of lists representing the 2x2 matrix.\n    :return: A list of floats representing the eigenvalues of the matrix, sorted from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix, dtype=float)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = np.sort(eigenvalues)[::-1]\n    return sorted_eigenvalues.tolist()"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms the matrix A using the operation T^-1 * A * S, provided T and S are invertible.\n    If either T or S is not invertible returns -1.\n    \"\"\"\n    try:\n        A = np.array(A)\n        T = np.array(T)\n        S = np.array(S)\n        if A.shape[0] != A.shape[1] or T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n            raise ValueError('All matrices must be square matrices.')\n        if T.shape[1] != A.shape[0] or S.shape[0] != A.shape[1]:\n            raise ValueError('Incompatible dimensions for matrix multiplication.')\n        T_inv = np.linalg.inv(T)\n        S_inv = np.linalg.inv(S)\n        result = T_inv @ A @ S\n        result_rounded = np.round(result, 4).tolist()\n        return result_rounded\n    except (np.linalg.LinAlgError, ValueError) as e:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[5, 6], [7, 8]]\nS = [[9, 10], [11, 12]]"}
{"task_id": 8, "completion_id": 0, "solution": "from typing import List, Optional\ndef inverse_2x2(matrix: List[List[float]]) -> Optional[List[List[float]]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n\n    Parameters:\n    matrix (List[List[float]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    Optional[List[List[float]]]: The inverse of the matrix or None if the matrix is not invertible.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse_matrix = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    :param vectors: A list of lists, where each inner list represents a feature with its observations.\n                    Each vector should have the same length.\n    :return: A covariance matrix as a list of lists.\n    \"\"\"\n    numpy_array = np.array(vectors).T\n    covariance_matrix = np.cov(numpy_array)\n    return covariance_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the equation Ax=b using the Jacobi method for n iterations.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix of the linear system.\n        b (np.ndarray): Right-hand side vector.\n        n (int): Number of iterations to perform.\n        \n    Returns:\n        list: Approximate solution vector x rounded to 4 decimals.\n    \"\"\"\n    D = np.diag(np.diag(A))\n    LU = A - D\n    D_inv = np.linalg.inv(D)\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x = np.dot(-D_inv, np.dot(LU, x)) + np.dot(D_inv, b)\n        x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition of a 2x2 matrix A\n    using the Jacobian method without using numpy's svd function.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 matrix for which to compute the SVD.\n    \n    Returns:\n    tuple: (U, s, Vt) where U and Vt are orthogonal matrices and s are singular values.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2x2 matrix.')\n    U = np.eye(2)\n    Vt = np.eye(2)\n    AtA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(AtA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues).real.round(4)\n    V = eigenvectors.real.round(4)\n    sigma = np.diag(singular_values).round(4)\n    sigma_inv = np.linalg.inv(sigma)\n    U = (A @ V @ sigma_inv).round(4)\n    if not np.allclose(U @ U.T, np.eye(2)):\n        U = U / np.linalg.norm(U, axis=1)[:, np.newaxis]\n    return (U, singular_values, V.T)\nA = np.array([[4, -2], [2, 0]])"}
{"task_id": 13, "completion_id": 0, "solution": "from typing import List\ndef determinant_4x4(matrix: List[List[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's expansion.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix.\n    \"\"\"\n\n    def det_3x3(submatrix):\n        return submatrix[0][0] * (submatrix[1][1] * submatrix[2][2] - submatrix[1][2] * submatrix[2][1]) - submatrix[0][1] * (submatrix[1][0] * submatrix[2][2] - submatrix[1][2] * submatrix[2][0]) + submatrix[0][2] * (submatrix[1][0] * submatrix[2][1] - submatrix[1][1] * submatrix[2][0])\n    if len(matrix) == 4:\n        det = 0\n        for c in range(4):\n            submatrix = [row[:c] + row[c + 1:] for row in matrix[1:]]\n            cofactor = (-1) ** c * matrix[0][c] * det_3x3(submatrix)\n            det += cofactor\n        return det\n    else:\n        raise ValueError('The input must be a 4x4 matrix.')"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation method.\n    \n    Parameters:\n    X (list of lists of floats): The feature matrix.\n    y (list of floats): The target values.\n    \n    Returns:\n    list of floats: The coefficients of the linear regression model, rounded to 4 decimal places.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    return [round(val, 4) for val in theta]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix with a column of ones for the intercept.\n    y (np.ndarray): Target vector.\n    alpha (float): Learning rate.\n    iterations (int): Number of iterations to perform gradient descent.\n    \n    Returns:\n    np.ndarray: Coefficients of the linear regression model rounded to four decimal places.\n    \"\"\"\n    theta = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        theta -= alpha / m * np.dot(X.T, errors)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input data using Standardization and Min-Max Normalization.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a data point and each column is a feature.\n    \n    Returns:\n    - std_data: A 2D list of the same shape as `data`, scaled using Standardization.\n    - mm_data: A 2D list of the same shape as `data`, scaled using Min-Max Normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    std_data = ((data - mean) / std_dev).round(4).tolist()\n    X_min = np.min(data, axis=0)\n    X_max = np.max(data, axis=0)\n    mm_data = ((data - X_min) / (X_max - X_min)).round(4).tolist()\n    return (std_data, mm_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n\n    def euclidean_distance(point1, point2):\n        return np.sqrt(np.sum((point1 - point2) ** 2))\n    for _ in range(max_iterations):\n        clusters = [[] for _ in range(k)]\n        for point in points:\n            distances = [euclidean_distance(point, centroid) for centroid in centroids]\n            closest_cluster = np.argmin(distances)\n            clusters[closest_cluster].append(point)\n        new_centroids = []\n        for cluster in clusters:\n            if cluster:\n                new_centroid = np.mean(cluster, axis=0)\n                new_centroids.append(new_centroid)\n            else:\n                new_centroids.append(centroids[len(new_centroids)])\n        if np.allclose(centroids, new_centroids, atol=0.0001):\n            break\n        centroids = np.array(new_centroids)\n    final_centroids = [(round(x, 4), round(y, 4)) for (x, y) in centroids]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train/test indices for k-fold cross-validation.\n    \n    Parameters:\n    - X: np.ndarray, feature matrix.\n    - y: np.ndarray, target vector.\n    - k: int, number of folds.\n    - shuffle: boolean, whether to shuffle the data before splitting.\n    - random_seed: int or None, seed for random shuffling.\n    \n    Returns:\n    - A list of tuples (train_indices, test_indices) for each fold.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.asarray(X)\n    if not isinstance(y, np.ndarray):\n        y = np.asarray(y)\n    assert X.shape[0] == y.shape[0], 'X and y must have the same first dimension'\n    sample_count = X.shape[0]\n    indices = np.arange(sample_count)\n    if shuffle:\n        rng = np.random.RandomState(random_seed)\n        rng.shuffle(indices)\n    fold_sizes = np.full(k, sample_count // k, dtype=np.int_)\n    fold_sizes[:sample_count % k] += 1\n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        (start, end) = (current, current + fold_size)\n        folds.append(indices[start:end])\n        current = end\n    result = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.hstack(folds[:i] + folds[i + 1:])\n        result.append((train_indices, test_indices))\n    return result"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given dataset.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row represents a data sample and each column represents a feature.\n    - k: An integer representing the number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the principal components (eigenvectors corresponding to the largest eigenvalues),\n      rounded to the nearest 4th decimal.\n    \"\"\"\n    means = np.mean(data, axis=0)\n    std_deviations = np.std(data, axis=0)\n    standardized_data = (data - means) / std_deviations\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_index = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_index]\n    sorted_eigenvectors = eigenvectors[:, sorted_index]\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    principal_components = np.round(top_k_eigenvectors, decimals=4).tolist()\n    return principal_components"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef entropy(class_counts):\n    \"\"\"Calculate the entropy from class counts.\"\"\"\n    total = sum(class_counts)\n    probs = [count / total for count in class_counts]\n    return -sum((p * math.log2(p) for p in probs if p > 0))\ndef partition_by_attribute(examples, attribute):\n    \"\"\"Partition the examples by the given attribute.\"\"\"\n    partitions = {}\n    for example in examples:\n        value = example[attribute]\n        if value not in partitions:\n            partitions[value] = []\n        partitions[value].append(example)\n    return partitions\ndef majority_value(examples):\n    \"\"\"Return the most common target value in the examples.\"\"\"\n    class_counts = Counter((example['class'] for example in examples))\n    return class_counts.most_common(1)[0][0]\ndef info_gain(examples, attribute, target_attr='class'):\n    \"\"\"Calculate the information gain for the given attribute.\"\"\"\n    parent_entropy = entropy(Counter((example[target_attr] for example in examples)).values())\n    partitions = partition_by_attribute(examples, attribute)\n    weighted_entropy = sum((len(partition) / len(examples) * entropy(Counter((example[target_attr] for example in partition)).values()) for partition in partitions.values()))\n    return parent_entropy - weighted_entropy\ndef choose_best_attribute(examples, attributes, target_attr='class'):\n    \"\"\"Choose the best attribute to split on based on information gain.\"\"\"\n    return max(attributes, key=lambda attr: info_gain(examples, attr, target_attr))\ndef create_decision_tree(examples, attributes, target_attr='class'):\n    \"\"\"Recursively create a decision tree.\"\"\"\n    class_counts = Counter((example[target_attr] for example in examples))\n    if len(class_counts) == 1:\n        return next(iter(class_counts))\n    if not attributes:\n        return majority_value(examples)\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    decision_tree = {best_attr: {}}\n    attributes.remove(best_attr)\n    partitions = partition_by_attribute(examples, best_attr)\n    for (value, partition) in partitions.items():\n        sub_tree = create_decision_tree(partition, attributes.copy(), target_attr)\n        decision_tree[best_attr][value] = sub_tree\n    return decision_tree\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    return create_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2.T)\ndef rbf_kernel(x1, x2, sigma=1.0):\n    x1_size = x1.shape[0]\n    x2_size = x2.shape[0]\n    x1_squared = np.sum(x1 ** 2, axis=1, keepdims=True)\n    x2_squared = np.sum(x2 ** 2, axis=1, keepdims=True)\n    k = np.tile(x2_squared.T, x1_size) + np.tile(x1_squared, (x2_size, 1)) - 2 * np.dot(x1, x2.T)\n    return np.exp(-k / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    for t in range(iterations):\n        if kernel == 'linear':\n            K = linear_kernel(data, data)\n        elif kernel == 'rbf':\n            K = rbf_kernel(data, data, sigma)\n        else:\n            raise ValueError('Unsupported kernel type')\n        predictions = np.sign(np.dot(alphas * labels, K))\n        incorrectly_classified_idx = np.where(predictions != labels)[0]\n        eta = 1.0 / (lambda_val * (t + 1))\n        incorrectly_classified_vector = np.zeros(n_samples)\n        incorrectly_classified_vector[incorrectly_classified_idx] = labels[incorrectly_classified_idx]\n        alphas += eta / n_samples * (incorrectly_classified_vector - alphas * labels)\n        alphas = np.minimum(alphas, 1.0 / (lambda_val * (t + 1)))\n    bias = np.mean(labels - np.dot(alphas * labels, K))\n    return [np.round(alphas.tolist(), 4), np.round(bias, 4)]\ndata = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\nlabels = np.array([1, 1, -1, -1])"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function for a given input z.\n    \n    Args:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The result of the sigmoid function, rounded to 4 decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    exp_sum = sum(exp_scores)\n    softmax_values = [round(exp / exp_sum, 4) for exp in exp_scores]\n    return softmax_values\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulate a single neuron with sigmoid activation for binary classification.\n    \n    Args:\n        features: A list of feature vectors.\n        labels: True binary labels for the given features.\n        weights: Weights for each feature.\n        bias: Bias term for the neuron.\n    \n    Returns:\n        A tuple containing the list of predicted probabilities and the mean squared error.\n    \"\"\"\n    predictions = []\n    for feature_vector in features:\n        linear\u7ec4\u5408 = sum((weight * feature for (weight, feature) in zip(weights, feature_vector))) + bias\n        prediction = sigmoid(linear\u7ec4\u5408)\n        predictions.append(prediction)\n    predictions_np = np.array(predictions)\n    mse = np.mean((predictions_np - labels) ** 2)\n    rounded_predictions = np.round(predictions_np, 4).tolist()\n    rounded_mse = round(mse, 4)\n    return (rounded_predictions, rounded_mse)\nfeatures = [[0.2, 0.4], [0.3, 0.5], [0.6, 0.7]]\nlabels = [0, 1, 1]\nweights = [0.5, -0.3]\nbias = 0.2"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n    s = sigmoid(x)\n    return s * (1 - s)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = np.copy(initial_weights)\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        prediction = sigmoid(z)\n        error = prediction - labels\n        mse = np.mean(error ** 2)\n        mse_values.append(np.round(mse, 4))\n        derror_dpred = 2 * error / len(labels)\n        dpred_dz = sigmoid_derivative(z)\n        dz_dw = features\n        dz_db = 1\n        derror_dz = derror_dpred * dpred_dz\n        derror_dw = np.dot(dz_dw.T, derror_dz)\n        derror_db = np.dot(dz_db, derror_dz)\n        weights -= learning_rate * derror_dw\n        bias -= learning_rate * np.sum(derror_db)\n    return (weights.tolist(), np.round(bias, 4), mse_values)\nfeatures = np.array([[0.2, 0.5], [0.6, 0.9], [0.5, 0.8]])\nlabels = np.array([0, 1, 1])\ninitial_weights = np.array([0.1, -0.1])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 10"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op='', _backward=lambda : None):\n        self.data = data\n        self.grad = 0.0\n        self._backward = _backward\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to basis C in R^3.\n    \n    Parameters:\n    - B: A 3x3 matrix representing the basis B.\n    - C: A 3x3 matrix representing the basis C.\n    \n    Returns:\n    - A 3x3 list of lists representing the transformation matrix from B to C, rounded to the 4th decimal.\n    \"\"\"\n    B_arr = np.array(B)\n    C_arr = np.array(C)\n    B_inv = np.linalg.inv(B_arr)\n    P = C_arr.dot(B_inv)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n    \n    Args:\n    A (np.ndarray): A 2x2 numpy array representing the input matrix.\n\n    Returns:\n    tuple: A tuple containing three lists: the matrices U, S, and V such that A = U * S * V.\n          All results are rounded to the nearest 4th decimal.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigen_vals, eigen_vecs) = np.linalg.eigh(ATA)\n    idx = eigen_vals.argsort()[::-1]\n    eigen_vals = eigen_vals[idx]\n    eigen_vecs = eigen_vecs[:, idx]\n    S = np.diag(np.sqrt(eigen_vals))\n    S_rounded = np.round(S, decimals=4).tolist()\n    V = eigen_vecs\n    V_rounded = np.round(V, decimals=4).tolist()\n    U = np.zeros((2, 2), dtype=float)\n    for i in range(2):\n        if eigen_vals[i] != 0:\n            U[:, i] = np.round(np.dot(A, V[:, i]) / S[0, i], decimals=4)\n    U_rounded = U.tolist()\n    return (U_rounded, S_rounded, V_rounded)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in the numpy arrays `X` and `y` while maintaining the corresponding order between them.\n    \n    Parameters:\n    - X: Numpy array of features.\n    - y: Numpy array of target values, corresponding to `X`.\n    - seed: An optional parameter for reproducing the output.\n    \n    Returns:\n    A list containing two elements, the first being the shuffled `X` and the second being the shuffled `y`, both converted to lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    This generator function yields batches of (X, y) pairs or just X if y is None.\n    Each yielded batch is reshaped and converted to a list before being returned.\n    \n    :param X: numpy array of features\n    :param y: optional numpy array of labels\n    :param batch_size: integer size of the batch\n    :yield: list of (X, y) pairs or list of X if y is None\n    \"\"\"\n    if y is not None and len(X) != len(y):\n        raise ValueError('The number of samples in X and y must be the same.')\n    n_samples = X.shape[0]\n    n_batches = n_samples // batch_size + (n_samples % batch_size != 0)\n    for batch_index in range(n_batches):\n        batch_X = X[batch_index * batch_size:(batch_index + 1) * batch_size]\n        if y is not None:\n            batch_y = y[batch_index * batch_size:(batch_index + 1) * batch_size]\n            yield [np.array(batch_X).tolist(), np.array(batch_y).tolist()]\n        else:\n            yield np.array(batch_X).tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset X based on whether the value of a specified feature is \n    greater than or equal to a given threshold.\n    \n    Parameters:\n    -----------\n    X : array-like {type: numpy.ndarray}\n        The dataset to be divided.\n    feature_i : int\n        The index of the feature on which to divide the dataset.\n    threshold : int or float\n        The threshold value for the division.\n        \n    Returns:\n    --------\n    tuple : (list, list)\n        Two lists containing numpy arrays. The first list contains samples \n        where the feature value is greater than or equal to the threshold,\n        and the second list contains the remaining samples.\n    \"\"\"\n    split_func = lambda row: row[feature_i] >= threshold\n    (true_rows, false_rows) = (list(), list())\n    for row in X:\n        if split_func(row):\n            true_rows.append(row)\n        else:\n            false_rows.append(row)\n    return (np.array(true_rows).tolist(), np.array(false_rows).tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates a new feature matrix consisting of all polynomial combinations \n    of the features with degree less than or equal to the specified degree.\n    \n    :param X: 2D numpy array of shape (samples, features)\n    :param degree: the maximum degree of the polynomial terms\n    :return: 2D numpy array of shape (samples, polynomial_features) converted to list\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    n_features = X.shape[1]\n    interaction_idx = list(combinations_with_replacement(range(n_features), degree))\n    X_poly = np.ones(len(X))\n    for idx in interaction_idx:\n        X_poly = np.column_stack((X_poly, np.prod(X[:, idx], axis=1)))\n    return X_poly.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets from the given dataset.\n    \n    Parameters:\n    - X: 2D numpy array of features.\n    - y: 1D numpy array of labels.\n    - n_subsets: Number of subsets to generate.\n    - replacements: Boolean value indicating whether to sample with replacement.\n    - seed: Seed for reproducibility.\n    \n    Returns:\n    A list of n_subsets random subsets, each subset being a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        (X_subset, y_subset) = (X[indices], y[indices])\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Converts an array of integers to a one-hot encoded numpy array and returns it as a list.\n    \n    Parameters:\n    - x: 1D numpy array of integers\n    - n_col: Optional parameter specifying the number of columns for the one-hot encoded array\n    \n    Returns:\n    - A list representation of the one-hot encoded numpy array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.array): A 1D numpy array.\n    \n    Returns:\n    list: A list of lists representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.zeros((len(x), len(x)))\n    np.fill_diagonal(diagonal_matrix, x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n    \n    Parameters:\n    - y_true: 1D numpy array, true labels.\n    - y_pred: 1D numpy array, predicted labels.\n    \n    Returns:\n    - float, accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must have same number of elements.')\n    correct_count = np.sum(y_true == y_pred)\n    total_count = len(y_true)\n    accuracy = correct_count / total_count\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for the given datasets X and Y.\n    If Y is None, calculates the correlation matrix of X with itself.\n    \n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the first dataset.\n    Y (np.ndarray, optional): A 2D numpy array representing the second dataset.\n    \n    Returns:\n    list: The correlation matrix as a list of lists, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        corr_matrix = np.corrcoef(X, rowvar=True)\n    else:\n        corr_matrix = np.corrcoef(np.vstack((X.T, Y.T)))\n        corr_matrix = corr_matrix[:X.shape[1], X.shape[1]:]\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    return corr_matrix_rounded.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6]])\nY = np.array([[7, 8, 9], [10, 11, 12]])"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given 1D numpy array of scores.\n    \n    Parameters:\n    scores (list): A 1D list of scores.\n    \n    Returns:\n    list: A list containing the log-softmax values, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores_array = np.array(scores, dtype=np.float64)\n    scores_max = np.max(scores_array)\n    scores_stable = scores_array - scores_max\n    exp_scores = np.exp(scores_stable)\n    exp_sum = np.sum(exp_scores)\n    log_exp_sum = np.log(exp_sum)\n    log_softmax_values = scores_stable - log_exp_sum\n    return np.round(log_softmax_values, 4).tolist()\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optim = None\n        self.w0_optim = None\n        if input_shape is not None:\n            self.initialize()\n\n    def initialize(self, optimizer='SGD', init_type=' XavierUniform '):\n        init_func = self.__get_initialization(init_type)\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = init_func(self.n_units, self.input_shape[0], limit)\n        self.w0 = np.zeros((self.n_units, 1))\n        if self.W_optim is None:\n            self.W_optim = copy.copy(optimizer)\n            self.w0_optim = copy.copy(optimizer)\n\n    def __get_initialization(self, init_type):\n        if ' Xavier ' in init_type:\n            if ' Uniform ' in init_type:\n\n                def initializer(out_size, in_size, limit):\n                    return np.random.uniform(low=-limit, high=limit, size=(out_size, in_size))\n                return initializer\n            elif ' Normal ' in init_type:\n\n                def initializer(out_size, in_size, limit):\n                    std_dev = limit * math.sqrt(3)\n                    return np.random.normal(loc=0.0, scale=std_dev, size=(out_size, in_size))\n                return initializer\n        else:\n            return self.__get_initialization(' XavierUniform ')\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(self.W, self.layer_input) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            self.W_optim.update(self.W, accum_grad.dot(self.layer_input.T).reshape(*self.W.shape))\n            self.w0_optim.update(self.w0, accum_grad.sum(axis=1, keepdims=True))\n        return W.T.dot(accum_grad)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    \"\"\"\n    Perform a simple 2D convolution on the input matrix with the given kernel,\n    padding, and stride parameters. The result is rounded to the nearest 4th decimal\n    and returned as a list.\n    \n    :param input_matrix: numpy.ndarray, the input matrix to apply convolution on.\n    :param kernel: numpy.ndarray, the kernel (filter) to be convolved with the input.\n    :param padding: int, the number of zero pixels to add as a border.\n    :param stride: int, the step size used when moving the kernel over the input.\n    :return: list, the convoluted matrix as a list of lists.\n    \"\"\"\n    (kernel_height, kernel_width) = kernel.shape\n    (input_height, input_width) = input_matrix.shape\n    padded_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_height = int((input_height - kernel_height + 2 * padding) / stride + 1)\n    output_width = int((input_width - kernel_width + 2 * padding) / stride + 1)\n    output_matrix = np.zeros((output_height, output_width))\n    for map_height in range(output_height):\n        for map_width in range(output_width):\n            roi = padded_matrix[map_height * stride:map_height * stride + kernel_height, map_width * stride:map_width * stride + kernel_width]\n            output_matrix[map_height, map_width] = np.sum(roi * kernel)\n    return np.round(output_matrix, 4).tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A scalar value.\n    \n    Returns:\n    float: The result of applying the ReLU function to the input.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression Loss.\n    \n    :param X: 2D numpy array of feature matrix\n    :param w: 1D numpy array of coefficients\n    :param y_true: 1D numpy array of true labels\n    :param alpha: float representing the regularization parameter\n    :return: Ridge Regression loss as a float rounded to 4 decimals\n    \"\"\"\n    mse_term = np.mean((np.dot(X, w) - y_true) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse_term + reg_term / 2.0\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input z.\n\n    Parameters:\n    - z: float. The input value to the activation function.\n    - alpha: float. The slope to use for negative inputs. Defaults to 0.01.\n\n    Returns:\n    - float. The result after applying the Leaky ReLU activation function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two vectors x1 and x2.\n\n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n\n    Returns:\n    float: The result of the linear kernel computation.\n    \"\"\"\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric from two arrays: true labels and predicted labels.\n\n    Precision is computed as the ratio of true positive predictions to the total number of positive predictions (true positives + false positives).\n\n    Parameters:\n    - y_true: numpy array of true binary labels\n    - y_pred: numpy array of predicted binary labels\n\n    Returns:\n    - precision: float value representing the precision score\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n    total_positives = true_positive + false_positive\n    if total_positives == 0:\n        return 0.0\n    precision = true_positive / total_positives\n    return precision"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent to minimize the mean squared error.\n    \n    Parameters:\n    - X: numpy array, feature matrix.\n    - y: numpy array, target values.\n    - weights: numpy array, initial weights.\n    - learning_rate: float, step size for the gradient descent.\n    - n_iterations: int, number of iterations to perform.\n    - batch_size: int, size of each batch for mini-batch and stochastic gradient descent.\n    - method: str, type of gradient descent ('stochastic', 'batch', 'mini-batch').\n    \n    Returns:\n    A list of the learned weights after n_iterations, rounded to the nearest 4th decimal.\n    \"\"\"\n    m = len(y)\n    for epoch in range(n_iterations):\n        if method == 'batch':\n            predictions = X.dot(weights)\n            errors = predictions - y\n            gradient = X.T.dot(errors) / m\n        elif method == 'stochastic':\n            for i in range(m):\n                random_index = np.random.randint(m)\n                xi = X[random_index:random_index + 1]\n                yi = y[random_index:random_index + 1]\n                prediction = xi.dot(weights)\n                errors = prediction - yi\n                gradient = xi.T.dot(errors)\n        elif method == 'mini-batch':\n            for i in range(0, m, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                predictions = xi.dot(weights)\n                errors = predictions - yi\n                gradient = xi.T.dot(errors) / batch_size\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'\")\n        weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    - matrix: A 2D list representing the input matrix.\n    \n    Returns:\n    - The RREF of the matrix as a 2D list.\n    \"\"\"\n    arr = np.array(matrix, dtype=float)\n    (rows, cols) = arr.shape\n    lead = 0\n    r = 0\n    for r in range(rows):\n        if lead >= cols:\n            return arr.tolist()\n        i = r\n        while arr[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    return arr.tolist()\n        arr[[i, r]] = arr[[r, i]]\n        lv = arr[r, lead]\n        arr[r] = arr[r] / lv\n        for i in range(rows):\n            if i != r:\n                lv = arr[i, lead]\n                arr[i] = arr[i] - lv * arr[r]\n        lead += 1\n    return arr.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Implements the Adam optimization algorithm.\n    \n    Args:\n        f: Objective function to be optimized.\n        grad: Function that computes the gradient of `f`.\n        x0: Initial parameter values.\n        learning_rate: Step size.\n        beta1: Exponential decay rate for the first moment estimates.\n        beta2: Exponential decay rate for the second moment estimates.\n        epsilon: A small constant for numerical stability.\n        num_iterations: Number of iterations to run the optimizer.\n        \n    Returns:\n        Optimized parameters as a list rounded to the nearest 4th decimal.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    x = np.array(x0)\n    for t in range(num_iterations):\n        gradients = np.array(grad(x))\n        m = beta1 * m + (1 - beta1) * gradients\n        v = beta2 * v + (1 - beta2) * gradients ** 2\n        m_hat = m / (1 - beta1 ** (t + 1))\n        v_hat = v / (1 - beta2 ** (t + 1))\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x.tolist(), 4)\nx0 = [0.5]"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Fits a Lasso Regression model using Gradient Descent.\n    \n    Args:\n        X (np.array): Feature matrix.\n        y (np.array): Target vector.\n        alpha (float): Regularization strength.\n        learning_rate (float): Learning rate for gradient descent.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Tolerance for stopping criteria.\n        \n    Returns:\n        tuple: A tuple containing the final weights and bias as lists.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    sample_count = n_samples\n    for iteration in range(max_iter):\n        y_predicted = np.dot(X, weights) + bias\n        error = y_predicted - y\n        grad_weight = 1 / sample_count * X.T.dot(error) + alpha * np.sign(weights)\n        grad_bias = 1 / sample_count * np.sum(error)\n        prev_weights = weights.copy()\n        weights -= learning_rate * grad_weight\n        bias -= learning_rate * grad_bias\n        if np.all(np.abs(weights - prev_weights) < tol):\n            break\n    return ([round(weight, 4) for weight in weights.tolist()], round(bias, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    \n    :param source: First input string.\n    :param target: Second input string.\n    :return: Minimum edit distance to transform source into target.\n    \"\"\"\n    (len_source, len_target) = (len(source), len(target))\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(1, len_source + 1):\n        dp[i][0] = i\n    for j in range(1, len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            insertion = dp[i][j - 1] + 1\n            deletion = dp[i - 1][j] + 1\n            substitution = dp[i - 1][j - 1] + cost\n            if i > 1 and j > 1 and (source[i - 2] == target[j - 1]) and (source[i - 1] == target[j - 2]):\n                transposition = dp[i - 2][j - 2] + cost\n            else:\n                transposition = np.inf\n            dp[i][j] = min(insertion, deletion, substitution, transposition)\n    return dp[len_source][len_target]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Recall is defined as the ratio of True Positive (TP) to the sum of True Positive (TP)\n    and False Negative (FN). This function returns the calculated recall value rounded to\n    three decimal places. If the denominator (TP + FN) is zero, it returns 0.0 to avoid\n    division by zero.\n\n    Parameters:\n    - y_true: List[int], the true binary labels (0 or 1).\n    - y_pred: List[int], the predicted binary labels (0 or 1).\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_pred == 1) & (y_true == 1))\n    FN = np.sum((y_pred == 0) & (y_true == 1))\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.spatial.distance import cdist\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Args:\n    - X (np.array): Input sequence of shape (seq_length, d_model)\n    - W_q, W_k, W_v (np.array): Linear transformation weights for Queries, Keys, and Values respectively,\n                                each of shape (d_model, d_model)\n    \n    Returns:\n    - V (list): Self-attention output as a list, rounded to 4 decimals.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_model = np.sqrt(X.shape[1])\n    scores = Q @ K.T / d_model\n    attention_weights = softmax(scores)\n    context = attention_weights @ V\n    result = np.round(context, 4).tolist()\n    return result\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims=True)"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements the forward pass of a simple RNN cell over a sequence of inputs.\n    \n    :param input_sequence: A list of lists, where each inner list represents an input vector at a time step.\n    :param initial_hidden_state: A list representing the initial hidden state of the RNN.\n    :param Wx: A 2D list (matrix) representing weights from inputs to the hidden state.\n    :param Wh: A 2D list (matrix) representing weights from the previous hidden state to the next.\n    :param b: A list representing the biases.\n    :return: The final hidden state after processing the sequence, rounded to 4 decimal places.\n    \"\"\"\n    inputs_np = np.array(input_sequence)\n    h0_np = np.array(initial_hidden_state)\n    Wx_np = np.array(Wx)\n    Wh_np = np.array(Wh)\n    b_np = np.array(b)\n    hs = [h0_np.copy()]\n    for i in range(inputs_np.shape[0]):\n        ht = np.tanh(np.dot(Wx_np, inputs_np[i]) + np.dot(Wh_np, hs[-1]) + b_np)\n        hs.append(ht)\n    final_hidden_state = hs[-1]\n    final_hidden_state_rounded = np.round(final_hidden_state, 4).tolist()\n    return final_hidden_state_rounded"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of points.\n\n    Args:\n    - points: A list of [x, y] lists representing points.\n    - tx: The translation distance in the x direction.\n    - ty: The translation distance in the y direction.\n\n    Returns:\n    - A list of translated points as [x, y] lists.\n    \"\"\"\n    points_array = np.array(points)\n    points_homo = np.c_[points_array, np.ones(len(points))]\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = points_homo @ translation_matrix.T\n    translated_points_reshaped = translated_points[:, :2]\n    result = translated_points_reshaped.tolist()\n    return result"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\nimport scipy.stats as stats\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence from P to Q for two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: The KL divergence from P to Q.\n    \"\"\"\n    var_p = np.square(sigma_p)\n    var_q = np.square(sigma_q)\n    divergence = var_p / (2 * var_q) + (mu_q - mu_p) ** 2 / (2 * var_q) - 0.5 + 0.5 * np.log(var_q / var_p)\n    return divergence"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the linear system Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n        A (np.array): Coefficient matrix, must be square.\n        b (np.array): Right hand side vector.\n        n (int): Number of iterations.\n        x_ini (np.array, optional): Initial guess for the solution vector.\n                                    Defaults to a zero vector of appropriate size if not provided.\n                                    \n    Returns:\n        list: Approximate solution vector after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    (A, b) = (np.array(A), np.array(b))\n    N = A.shape[0]\n    x = np.zeros(N) if x_ini is None else np.array(x_ini)\n    for k in range(n):\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n    A : numpy array\n        Coefficient matrix.\n    b : numpy array\n        Dependent variable vector.\n    \n    Returns:\n    x : list\n        Solution vector rounded to the 4th decimal.\n    \"\"\"\n    Ab = np.column_stack((A, b))\n    n = len(b)\n    for i in range(n):\n        maxel = abs(Ab[i, i])\n        maxrow = i\n        for k in range(i + 1, n):\n            if abs(Ab[k, i]) > maxel:\n                maxel = abs(Ab[k, i])\n                maxrow = k\n        Ab[[i, maxrow]] = Ab[[maxrow, i]]\n        for k in range(i + 1, n):\n            factor = Ab[k, i] / Ab[i, i]\n            Ab[k] -= factor * Ab[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, n] - np.dot(Ab[i, i:n], x[i:n])) / Ab[i, i]\n    return x.round(4).tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        T = len(x)\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        for t in range(T):\n            x_t = x[t].reshape(-1, 1)\n            input_t = np.vstack((x_t, h_t))\n            f_t = self._sigmoid(self.Wf @ input_t + self.bf)\n            i_t = self._sigmoid(self.Wi @ input_t + self.bi)\n            c_tilde_t = np.tanh(self.Wc @ input_t + self.bc)\n            c_t = f_t * c_t + i_t * c_tilde_t\n            o_t = self._sigmoid(self.Wo @ input_t + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(h_t.reshape(-1))\n        return [np.round(np.array(hidden_states), 4).tolist(), np.round(h_t, 4).tolist(), np.round(c_t, 4).tolist()]\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF scores for a query against a corpus of documents.\n    \n    :param corpus: List of documents, where each document is a list of words.\n    :param query: A list of words for which to compute the TF-IDF scores.\n    :return: A list of lists containing the TF-IDF scores for the query words in each document.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus is empty. Please provide a non-empty list of documents.')\n    vocabulary = set((word for doc in corpus for word in doc)) | set(query)\n    df = {word: sum((word in doc for doc in corpus)) for word in vocabulary}\n    idf = {word: math.log((len(corpus) + 1) / (df[word] + 1)) + 1 for word in vocabulary}\n    tfidf_scores = []\n    for doc in corpus:\n        tf = {word: doc.count(word) / len(doc) for word in vocabulary}\n        doc_tfidf = [round(tf[word] * idf.get(word, 0), 4) for word in query]\n        tfidf_scores.append(doc_tfidf)\n    return tfidf_scores"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp + 1e-06)\n    recall = tp / (tp + fn + 1e-06)\n    fscore = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall + 1e-06)\n    return round(fscore, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def sigmoid_derivative(self, z):\n        sg = self.sigmoid(z)\n        return sg * (1 - sg)\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n        \"\"\"\n        T = len(input_sequence)\n        h = np.zeros((hidden_size, T + 1))\n        y_tilde = np.zeros((W_hy.shape[0], T))\n        for t in range(T):\n            x_t = input_sequence[t].reshape(-1, 1)\n            h[:, t + 1] = self.sigmoid(W_hh @ h[:, t].reshape(-1, 1) + W_xh @ x_t + b_h)\n            y_tilde[:, t] = W_hy @ h[:, t + 1] + b_y\n        last_inputs = x_t\n        last_hiddens = h[:, T]\n        outputs = y_tilde\n        return (outputs, last_inputs, last_hiddens)\n\n    def compute_loss(self, expected_output, outputs):\n        \"\"\"\n        Computes the loss using 1/2 * Mean Squared Error (MSE).\n        \"\"\"\n        return 0.5 * np.mean(np.sum(np.square(expected_output - outputs), axis=0))\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        dh_next = np.zeros_like(last_hiddens)\n        dy = outputs - expected_output\n        dW_hy += dy @ last_hiddens.T\n        db_y += dy\n        dhraw = W_hy.T @ dy + dh_next\n        dh = dhraw * self.sigmoid_derivative(last_hiddens)\n        dW_xh += dh @ last_inputs.T\n        dW_hh += dh @ h[:, T - 1].T\n        db_h += dh\n        for t in reversed(range(1, T)):\n            dy = np.copy(dhraw)\n            dW_hy += dy @ h[:, t].T\n            db_y += dy\n            dh = W_hy.T @ dy + dh_next\n            dhraw = dh * self.sigmoid_derivative(h[:, t + 1])\n            dW_xh += dhraw @ input_sequence[t - 1].reshape(-1, 1).T\n            dW_hh += dhraw @ h[:, t].reshape(-1, 1).T\n            db_h += dhraw\n        W_xh = W_xh - learning_rate * dW_xh\n        W_hh = W_hh - learning_rate * dW_hh\n        W_hy = W_hy - learning_rate * dW_hy\n        b_h = b_h - learning_rate * db_h\n        b_y = b_y - learning_rate * db_y\n        return (W_xh, W_hh, W_hy, b_h, b_y)\n\n    def train(self, input_sequence, expected_output, learning_rate=0.1, n_iter=10000):\n        \"\"\"\n        Trains the RNN on the provided input sequence and expected output.\n        \"\"\"\n        for iteration in range(n_iter):\n            (outputs, last_input, last_hidden) = self.rnn_forward(self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y, self.hidden_size, input_sequence)\n            loss = self.compute_loss(expected_output, outputs)\n            if iteration % 1000 == 0:\n                print(f'Iteration {iteration}, Loss: {loss}')\n            (self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y) = self.rnn_backward(self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y, self.hidden_size, input_sequence, expected_output, outputs, last_input, last_hidden, learning_rate)\nexpected_output = np.array([[0.3], [0.5], [0.7], [0.9]])"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros(len(b))\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r\n    rsold = np.dot(r.transpose(), r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p.transpose(), Ap)\n        x = x + np.dot(alpha, p)\n        r = r - np.dot(alpha, Ap)\n        rsnew = np.dot(r.transpose(), r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x.tolist(), 8)"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    (classes, frequency) = np.unique(y, return_counts=True)\n    probabilities = frequency / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_ptr = [0]\n    for row in dense_matrix:\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                col_indices.append(col_idx)\n        row_ptr.append(len(values))\n    return (values, col_indices, row_ptr)"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    projection_vector = scalar_projection * L\n    return list(np.round(projection_vector, 3))"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_pointer = [0] * (num_cols + 1)\n    current_col_start = 0\n    for col in range(num_cols):\n        col_pointer[col + 1] = col_pointer[col]\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n                col_pointer[col + 1] += 1\n    return (values, row_indices, col_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Finds a basis for the image (column space) of matrix A.\n    \n    Parameters:\n    A: A 2D numpy array representing the matrix.\n    \n    Returns:\n    A list of lists, where each inner list represents a basis vector for the image of A,\n    rounded to 8 decimal places.\n    \"\"\"\n    if not A.size or np.linalg.matrix_rank(A) == 0:\n        return []\n    A = np.array(A, dtype=np.float64)\n    rank = np.linalg.matrix_rank(A)\n    (_, pivot_cols, _) = np.linalg.svd(A)\n    basis_vectors = A[:, pivot_cols[:rank]]\n    rounded_basis = np.round(basis_vectors, 8).tolist()\n    return rounded_basis"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for regression analysis.\n\n    Parameters:\n    y_true (array-like): The array of true values.\n    y_pred (array-like): The array of predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n    ss_residual = np.sum((y_true - y_pred) ** 2)\n    if ss_total == 0:\n        return 1.0\n    r2 = 1 - ss_residual / ss_total\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image represented as a 2D list.\n    \n    :param img: List[List[int]] representing the image\n    :return: Average brightness rounded to 2 decimal points or -1 for edge cases.\n    \"\"\"\n    if not img or not all((len(row) == len(img[0]) for row in img)):\n        return -1\n    total_brightness = sum((sum((pixel for pixel in row if 0 <= pixel <= 255)) for row in img))\n    num_pixels = sum((len(row) for row in img))\n    if num_pixels != len([pixel for row in img for pixel in row if 0 <= pixel <= 255]):\n        return -1\n    return round(total_brightness / num_pixels, 2) if num_pixels > 0 else -1"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n    \n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n    \n    Returns:\n    float: RMSE value rounded to 3 decimal places.\n    \n    Raises:\n    ValueError: If input arrays have mismatched shapes or invalid contents.\n    TypeError: If inputs are not array-like.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Inputs must be array-like.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    rmse_value = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays.\n\n    Parameters:\n    - y_true: A list or numpy array of true binary labels.\n    - y_pred: A list or numpy array of predicted binary labels.\n\n    Returns:\n    - float: The Jaccard Index between y_true and y_pred, rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard_index_val = intersection / union\n    return round(jaccard_index_val, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score between the true and predicted binary arrays.\n    \n    Parameters:\n    - y_true: A numpy array of binary values representing true labels.\n    - y_pred: A numpy array of binary values representing predicted labels.\n    \n    Returns:\n    - Dice Score as a float rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    if intersection == 0 and np.sum(y_true) == 0 and (np.sum(y_pred) == 0):\n        return round(1.0, 3)\n    score = 2.0 * intersection / (np.sum(y_true) + np.sum(y_pred))\n    return round(score, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Creates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim: The dimensionality of the hypervectors.\n    - random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    - A composite hypervector representing the entire row as a list.\n    \"\"\"\n\n    def initialize_hdvector(dimension, seed):\n        \"\"\"Initialize a hypervector with the given dimension and seed.\"\"\"\n        np.random.seed(seed)\n        return (np.random.rand(dimension) * 2 - 1).astype(np.float32)\n\n    def bind(hv1, hv2):\n        \"\"\"Bind two hypervectors.\"\"\"\n        return np.sign(hv1 * hv2).astype(np.float32)\n    feature_vectors = {}\n    for feature in row.keys():\n        if feature not in feature_vectors:\n            feature_vectors[feature] = initialize_hdvector(dim, random_seeds[feature])\n    composite_hv = None\n    for (feature, value) in row.items():\n        value_seed = hash(str(value) + feature) % 2 ** 32\n        value_vector = initialize_hdvector(dim, value_seed)\n        feature_value_hv = bind(feature_vectors[feature], value_vector)\n        if composite_hv is None:\n            composite_hv = feature_value_hv\n        else:\n            composite_hv = bind(composite_hv, feature_value_hv)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n\n    Parameters:\n    data (list of lists): A list containing pairs of [y_true, y_pred] for each observation.\n                          y_true is the actual label, and y_pred is the predicted label.\n\n    Returns:\n    list of lists: A 2x2 confusion matrix.\n    \"\"\"\n    counts = Counter(((y_true, y_pred) for (y_true, y_pred) in data))\n    matrix = [[0, 0], [0, 0]]\n    for ((y_true, y_pred), count) in counts.items():\n        matrix[y_true][y_pred] = count\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1, v2: Numpy arrays representing the input vectors.\n    \n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors cannot be empty.')\n    mag_v1 = np.linalg.norm(v1)\n    mag_v2 = np.linalg.norm(v2)\n    if mag_v1 == 0 or mag_v2 == 0:\n        raise ValueError('Vectors cannot have zero magnitude.')\n    cos_sim = np.dot(v1, v2) / (mag_v1 * mag_v2)\n    return round(cos_sim, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    for label in set(actual + predicted):\n        if label not in [0, 1]:\n            raise ValueError('All elements in actual and predicted lists must be 0 or 1.')\n    true_positive = sum(((a == 1) & (p == 1) for (a, p) in zip(actual, predicted)))\n    false_negative = sum(((a == 1) & (p == 0) for (a, p) in zip(actual, predicted)))\n    false_positive = sum(((a == 0) & (p == 1) for (a, p) in zip(actual, predicted)))\n    true_negative = sum(((a == 0) & (p == 0) for (a, p) in zip(actual, predicted)))\n    confusion_matrix = [[true_positive, false_negative], [false_positive, true_negative]]\n    total = len(actual)\n    accuracy = round((true_positive + true_negative) / total, 3) if total > 0 else 0.0\n    precision = round(true_positive / (true_positive + false_positive), 3) if true_positive + false_positive > 0 else 0.0\n    recall = round(true_positive / (true_positive + false_negative), 3) if true_positive + false_negative > 0 else 0.0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall > 0 else 0.0\n    specificity = round(true_negative / (true_negative + false_positive), 3) if true_negative + false_positive > 0 else 0.0\n    negative_predictive_value = round(true_negative / (true_negative + false_negative), 3) if true_negative + false_negative > 0 else 0.0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a given dataset.\n    \n    Parameters:\n    - data: List or NumPy array of numerical values\n    \n    Returns:\n    - Dictionary containing mean, median, mode, variance, standard deviation,\n      25th percentile, 50th percentile, 75th percentile, and interquartile range.\n    \"\"\"\n    if not isinstance(data, np.ndarray):\n        data = np.array(data)\n    result = {}\n    result['mean'] = np.mean(data)\n    result['median'] = np.median(data)\n    mode_result = stats.mode(data)\n    result['mode'] = float(mode_result.mode[0]) if mode_result.count[0] > 0 else 'No unique mode'\n    result['variance'] = round(np.var(data), 4)\n    result['standard_deviation'] = round(np.std(data), 4)\n    result['25th_percentile'] = round(np.percentile(data, 25), 4)\n    result['50th_percentile'] = round(np.percentile(data, 50), 4)\n    result['75th_percentile'] = round(np.percentile(data, 75), 4)\n    result['interquartile_range'] = round(result['75th_percentile'] - result['25th_percentile'], 4)\n    return result\ndata = [10, 20, 20, 30, 40, 50, 60, 70, 80, 90]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    exponent = math.exp(-lam)\n    factorial_k = math.factorial(k)\n    power_lam = math.pow(lam, k)\n    probability = power_lam * exponent / factorial_k\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast value, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    contrast = max_val - min_val\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: Dot product of the two vectors.\n        \n    Raises:\n        ValueError: If the vectors have different lengths or if inputs are not 1D arrays.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both vectors must be 1-dimensional arrays.')\n    if len(vec1) != len(vec2):\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    num_features = len(data)\n    result = []\n    for coefs in combinations_with_replacement(range(num_features), degree):\n        term = np.prod([data[i] for i in coefs])\n        result.append(round(term, 8))\n    return result"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int) -> list:\n    \"\"\"\n    Calculates the positional encoding for a given sequence length and model dimensionality.\n    \n    :param position: An integer representing the sequence length.\n    :param d_model: An integer representing the model dimensionality.\n    :return: A list of positional encodings or -1 for invalid input.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    position = np.arange(position)[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    lr_corr = learning_rate * (np.sqrt(1.0 - beta2 ** t) / (1.0 - beta1 ** t))\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    parameter = parameter - lr_corr * m / (np.sqrt(v) + epsilon)\n    return (parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\nclass MultiHeadAttention:\n\n    def __init__(self, n_head, n_embd):\n        self.n_head = n_head\n        self.head_size = n_embd // n_head\n        self.wq = np.random.randn(n_embd, n_embd)\n        self.wk = np.random.randn(n_embd, n_embd)\n        self.wv = np.random.randn(n_embd, n_embd)\n        self.wo = np.random.randn(n_embd, n_embd)\n\n    def forward(self, x):\n        (B, T, C) = x.shape\n        k = x.dot(self.wk)\n        q = x.dot(self.wq)\n        v = x.dot(self.wv)\n        k = k.reshape(B, T, self.n_head, self.head_size).transpose(0, 2, 1, 3)\n        q = q.reshape(B, T, self.n_head, self.head_size).transpose(0, 2, 1, 3)\n        v = v.reshape(B, T, self.n_head, self.head_size).transpose(0, 2, 1, 3)\n        wei = q @ k.transpose(-2, -1) * self.head_size ** (-0.5)\n        wei = np_softmax(wei, axis=-1)\n        out = wei @ v\n        out = out.transpose(0, 2, 1, 3).reshape(B, T, C)\n        return out.dot(self.wo)\ndef np_softmax(x, axis=None):\n    return np.exp(x - np.max(x, axis=axis, keepdims=True)) / np.sum(np.exp(x - np.max(x, axis=axis, keepdims=True)), axis=axis, keepdims=True)\nclass MLP:\n\n    def __init__(self, n_embd, n_hidden):\n        self.c_fc = np.random.randn(n_embd, n_hidden)\n        self.c_proj = np.random.randn(n_hidden, n_embd)\n\n    def forward(self, x):\n        return x.dot(self.c_fc).dot(self.c_proj)\nclass LayerNorm:\n\n    def __init__(self, features_shape, eps=1e-06):\n        self.eps = eps\n        self.g = np.ones(features_shape)\n        self.b = np.zeros(features_shape)\n\n    def forward(self, x):\n        mean = np.mean(x, axis=-1, keepdims=True)\n        var = np.var(x, axis=-1, keepdims=True)\n        return self.g * (x - mean) / np.sqrt(var + self.eps) + self.b\nclass Block:\n\n    def __init__(self, n_embd, n_head):\n        self.ln_1 = LayerNorm(features_shape=n_embd)\n        self.attn = MultiHeadAttention(n_head=n_head, n_embd=n_embd)\n        self.ln_2 = LayerNorm(features_shape=n_embd)\n        self.mlp = MLP(n_embd=n_embd, n_hidden=4 * n_embd)\n\n    def forward(self, x):\n        x = x + self.attn.forward(self.ln_1.forward(x))\n        x = x + self.mlp.forward(self.ln_2.forward(x))\n        return x\nclass GPTModel:\n\n    def __init__(self, params):\n        self.wte = params['wte']\n        self.wpe = params['wpe']\n        self.blocks = [Block(n_embd=self.wte.shape[1], n_head=params['hparams']['n_head']) for _ in params['blocks']]\n        self.ln_f = LayerNorm(features_shape=self.wte.shape[1])\n        self.ln_f.g = params['ln_f']['g']\n        self.ln_f.b = params['ln_f']['b']\n\n    def forward(self, idx, targets=None):\n        (B, T) = idx.shape\n        tok_emb = self.wte[idx]\n        pos_emb = self.wpe[:T]\n        x = tok_emb + pos_emb\n        for block in self.blocks:\n            x = block.forward(x)\n        x = self.ln_f.forward(x)\n        logits = x @ self.wte.T\n        return logits\ndef load_encoder_hparams_and_params(model_size='124M', models_dir='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{} for _ in range(12)], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    model = GPTModel(params)\n    tokenized_prompt = encoder.encode(prompt)\n    idx = np.array([tokenized_prompt])\n    for _ in range(n_tokens_to_generate):\n        logits = model.forward(idx[:, -min(hparams['n_ctx'], len(idx[0])):])\n        last_token_logits = logits[0, -1, :]\n        probabilities = np_softmax(last_token_logits, axis=-1)\n        next_token = np.random.choice(len(probabilities), p=probabilities)\n        idx = np.append(idx, [[next_token]], axis=1)\n    generated_text = encoder.decode(idx[0].tolist())\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    Q = np.random.randn(dimension, n)\n    K = np.random.randn(dimension, n)\n    V = np.random.randn(dimension, n)\n    attention_scores = np.dot(Q, K.T) / np.sqrt(dimension)\n    attention_weights = softmax(attention_scores)\n    weighted_pattern = np.dot(V, attention_weights.T)\n    return [round(value, 4) for value in weighted_pattern.flatten()]\nn = 5\ndimension = 3"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef average_document_length(corpus):\n    return sum((len(document.split()) for document in corpus)) / len(corpus)\ndef bm25_weights(corpus, query, avgdl, k1=1.5, b=0.75):\n    N = len(corpus)\n    avdl = avgdl\n    query_terms = query.split()\n    unique_query_terms = set(query_terms)\n    document_term_frequencies = [Counter(document.split()) for document in corpus]\n    idf = {}\n    for term in unique_query_terms:\n        doc_with_term_count = sum((1 for document in corpus if term in document))\n        idf[term] = np.log((N - doc_with_term_count + 0.5) / (doc_with_term_count + 0.5)) + 1\n    weights = []\n    for (i, document) in enumerate(corpus):\n        document_tokens = document.split()\n        document_length = len(document_tokens)\n        score = 0\n        for term in query_terms:\n            f_t_d = document_term_frequencies[i].get(term, 0)\n            if term in idf:\n                numerator = idf[term] * f_t_d * (k1 + 1)\n                denominator = f_t_d + k1 * (1 - b + b * document_length / avdl)\n                score += numerator / denominator\n        weights.append(round(score, 3))\n    return weights\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    avgdl = average_document_length(corpus)\n    return bm25_weights(corpus, query, avgdl, k1, b)\ncorpus = ['the cat sat on the mat', 'the dog sat on the log', 'cats and dogs are friends', 'mattocks are tools used for digging']\nquery = 'cat dog'"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    from sklearn.metrics import f1_score\n    if not isinstance(y_true, (list, tuple, np.ndarray)):\n        y_true = list(y_true)\n    if not isinstance(y_pred, (list, tuple, np.ndarray)):\n        y_pred = list(y_pred)\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nfrom scipy import stats\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n\n    def fluctuation(day):\n        return 10 * math.sin(2 * PI * day / 10)\n    detrended_data = [consumption - fluctuation(day) for (day, consumption) in enumerate(consumption_data, 1)]\n    day_numbers = list(range(1, len(consumption_data) + 1))\n    (slope, intercept, _, _, _) = stats.linregress(day_numbers, detrended_data)\n    day_15_base_consumption = slope * 15 + intercept\n    day_15_fluctuation = fluctuation(15)\n    day_15_total = day_15_base_consumption + day_15_fluctuation\n    safety_margin = 1.05\n    result = math.ceil(day_15_total * safety_margin)\n    return int(result)\nconsumption_data = [200, 210, 220, 230, 240, 250, 260, 270, 280, 290]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    abs_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(abs_errors)\n    return round(mae_value, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the query (Q), key (K), and value (V) matrices using input matrix X and weight matrices W_q, W_k, W_v.\n    \n    :param X: Input matrix of shape (batch_size, seq_len, d_model)\n    :param W_q: Weights for computing queries, shape (d_model, d_k)\n    :param W_k: Weights for computing keys, shape (d_model, d_k)\n    :param W_v: Weights for computing values, shape (d_model, d_v)\n    :return: Tuple of Q, K, V matrices each of shape (batch_size, seq_len, n_heads, depth)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask=None) -> np.ndarray:\n    \"\"\"\n    Applies scaled dot-product attention on Q, K, V matrices.\n    \n    :param Q: Queries, shape (batch_size, seq_len, n_heads, d_k)\n    :param K: Keys, shape (batch_size, seq_len, n_heads, d_k)\n    :param V: Values, shape (batch_size, seq_len, n_heads, d_v)\n    :param mask: Masking tensor to be applied during softmax, optional\n    :return: Output tensor of shape (batch_size, seq_len, n_heads, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, np.transpose(K, [0, 1, 3, 2])) / np.sqrt(d_k)\n    if mask is not None:\n        scores += mask\n    weights = np.softmax(scores, axis=-1)\n    output = np.matmul(weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements multi-head attention with multiple attention heads.\n    \n    :param X: Input tensor of shape (batch_size, seq_len, d_model)\n    :param W_q: Query weights of shape (d_model, d_k * n_heads)\n    :param W_k: Key weights of shape (d_model, d_k * n_heads)\n    :param W_v: Value weights of shape (d_model, d_v * n_heads)\n    :param n_heads: Number of attention heads\n    :return: List representation of the final output tensor of shape (batch_size, seq_len, d_model)\n    \"\"\"\n    d_k = W_q.shape[1] // n_heads\n    d_v = W_v.shape[1] // n_heads\n    W_q = W_q.reshape((W_q.shape[0], n_heads, d_k))\n    W_k = W_k.reshape((W_k.shape[0], n_heads, d_k))\n    W_v = W_v.reshape((W_v.shape[0], n_heads, d_v))\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q = np.transpose(Q, [0, 2, 1, 3])\n    K = np.transpose(K, [0, 2, 1, 3])\n    V = np.transpose(V, [0, 2, 1, 3])\n    attention_output = self_attention(Q, K, V)\n    attention_output = np.transpose(attention_output, [0, 2, 1, 3])\n    concat = np.reshape(attention_output, (-1, attention_output.shape[1], attention_output.shape[2] * attention_output.shape[3]))\n    rounded_output = np.round(concat, 4).tolist()\n    return rounded_output\nX = np.random.rand(2, 4, 8)\nW_q = np.random.rand(8, 6)\nW_k = np.random.rand(8, 6)\nW_v = np.random.rand(8, 6)\nn_heads = 2\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Lists must be of equal length.')\n    n = len(x)\n    n_00 = n_01 = n_10 = n_11 = 0\n    for i in range(n):\n        if x[i] == 0 and y[i] == 0:\n            n_00 += 1\n        elif x[i] == 0 and y[i] == 1:\n            n_01 += 1\n        elif x[i] == 1 and y[i] == 0:\n            n_10 += 1\n        else:\n            n_11 += 1\n    n_0 = n_00 + n_01\n    n_1 = n_10 + n_11\n    n_x0 = n_00 + n_10\n    n_x1 = n_01 + n_11\n    e_00 = n_0 * n_x0 / n\n    e_01 = n_0 * n_x1 / n\n    e_10 = n_1 * n_x0 / n\n    e_11 = n_1 * n_x1 / n\n    numerator = (n_00 - e_00) ** 2 + (n_01 - e_01) ** 2 + (n_10 - e_10) ** 2 + (n_11 - e_11) ** 2\n    denominator = n_0 * n_1 * n_x0 * n_x1 / n ** 2\n    phi = (numerator / denominator) ** 0.5\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1]\ny = [1, 1, 0, 0, 1, 1]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    The Hard Sigmoid function is defined as:\n    - 0 if x < -2.5\n    - 1 if x > 2.5\n    - 0.2 * x + 0.5 otherwise\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU (Exponential Linear Unit) activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value, rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0.0, x) + min(0.0, alpha * x)"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef clip_likelihood_ratios(rhos, epsilon):\n    \"\"\"\n    Clip likelihood ratios using the provided epsilon.\n    \n    Args:\n        rhos: Array of likelihood ratios.\n        epsilon: Clipping parameter.\n    \n    Returns:\n        Array of clipped likelihood ratios.\n    \"\"\"\n    return np.clip(rhos, 1 - epsilon, 1 + epsilon)\ndef compute_kl_divergence(pi_theta_old, pi_theta_new):\n    \"\"\"\n    Compute the KL divergence between the old and new policies.\n    \n    Args:\n        pi_theta_old: Probabilities from the old policy.\n        pi_theta_new: Probabilities from the new policy.\n    \n    Returns:\n        The KL divergence value.\n    \"\"\"\n    return np.sum(np.array(pi_theta_old) * (np.log(pi_theta_old) - np.log(pi_theta_new)))\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = clip_likelihood_ratios(rhos, epsilon)\n    surrogate_obj = np.mean(clipped_rhos * np.array(A))\n    ref_ratio = np.mean(np.array(pi_theta_old) / np.array(pi_theta_ref))\n    kl_divergence = compute_kl_divergence(pi_theta_old, pi_theta_ref)\n    grpo_obj = surrogate_obj - beta * kl_divergence * ref_ratio\n    return round(grpo_obj, 6)\nrhos = [1.5, 0.8, 1.2]\nA = [0.1, -0.2, 0.3]\npi_theta_old = [0.2, 0.5, 0.3]\npi_theta_ref = [0.3, 0.4, 0.3]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + np.exp(-x))), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * (alpha * (math.exp(x) - 1)), 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid function.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef softmax(z):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    exp_z = np.exp(z)\n    return exp_z / exp_z.sum(axis=1, keepdims=True)\ndef cross_entropy_loss(y_hat, y_true):\n    m = y_true.shape[0]\n    ce_loss = -np.sum(y_true * np.log(y_hat + 1e-09)) / m\n    return ce_loss\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    X = np.insert(X, 0, 1, axis=1)\n    classes = len(np.unique(y))\n    params = np.zeros((X.shape[1], classes))\n    y_one_hot = np.eye(classes)[y]\n    losses = []\n    for _ in range(iterations):\n        z = X @ params\n        y_hat = softmax(z)\n        loss = cross_entropy_loss(y_hat, y_one_hot)\n        losses.append(round(loss, 4))\n        gradient = X.T @ (y_hat - y_one_hot) / X.shape[0]\n        params -= learning_rate * gradient\n    params = params[1:]\n    params_list = params.tolist()\n    return ([param.round(4) for param in params_list], losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.c_[np.ones(X.shape[0]), X]\n    theta = np.zeros(X.shape[1])\n    loss_history = []\n    theta_history = []\n    for _ in range(iterations):\n        z = np.dot(X, theta)\n        h = 1 / (1 + np.exp(-z))\n        error = h - y\n        gradient = np.dot(X.T, error) / y.size\n        theta -= learning_rate * gradient\n        eps = 1e-15\n        loss = -np.mean(y * np.log(h + eps) + (1 - y) * np.log(1 - h + eps))\n        loss_history.append(np.round(loss, 4))\n        theta_history.append(theta.tolist())\n    return (theta_history, loss_history)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = scores + mask\n    attention_weights = np_softmax(scores)\n    output = np.dot(attention_weights, V)\n    return output.tolist()\ndef np_softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / np.sum(e_x, axis=-1, keepdims=True)\nseq_length = X.shape[0]\nmask = (1 - np.tril(np.ones((seq_length, seq_length)))) * -1000000000.0"}
{"task_id": 108, "completion_id": 0, "solution": "from typing import List\nfrom math import log\nfrom collections import Counter\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is calculated using Shannon entropy, which satisfies the following:\n    - A basket with all apples of the same color has a disorder of 0.\n    - The disorder increases as the diversity of apple colors increases.\n    \"\"\"\n    if not apples:\n        return 0.0\n    total_apples = len(apples)\n    color_counts = Counter(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * log(probability, len(color_counts))\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Applies Layer Normalization to the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, seq_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,)\n    - beta: np.ndarray of shape (feature_dim,)\n    - epsilon: Float to prevent division by zero\n    \n    Returns:\n    - normalized_X: A list representation of the normalized tensor, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized_scaled = gamma * X_normalized + beta\n    return np.round(X_normalized_scaled.tolist(), 5)"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score between a reference translation and a candidate translation.\n    \n    Parameters:\n    - reference (str): The reference translation.\n    - candidate (str): The candidate translation generated by the machine.\n    - alpha (float): Weight for fragmented matches.\n    - beta (int): Exponent for F-mean.\n    - gamma (float): Weight for unigram precision.\n    \n    Returns:\n    - float: METEOR score rounded to 3 decimal places.\n    \"\"\"\n\n    def tokenize(sentence):\n        return sentence.lower().split()\n    ref_tokens = tokenize(reference)\n    cand_tokens = tokenize(candidate)\n    matched_ref = []\n    matched_cand = []\n    for (i, cand_w) in enumerate(cand_tokens):\n        for (j, ref_w) in enumerate(ref_tokens):\n            if cand_w == ref_w:\n                matched_ref.append(ref_w)\n                matched_cand.append(cand_w)\n                ref_tokens.pop(j)\n                break\n    precision = len(matched_cand) / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = len(matched_ref) / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    if precision + recall == 0:\n        fmean = 0\n    else:\n        fmean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n\n    def calc_fragmentation(tokens):\n        gaps = [i for (i, (x, y)) in enumerate(zip(tokens, tokens[1:])) if x != y]\n        num_gaps = len(gaps)\n        num_words = len(tokens)\n        return np.exp(-gamma * num_gaps / (num_words + 1))\n    ref_penalty = calc_fragmentation(matched_ref)\n    cand_penalty = calc_fragmentation(matched_cand)\n    meteor = fmean * ref_penalty * cand_penalty * alpha + (1 - alpha)\n    return round(meteor, 3)\nreference = 'the cat is on the table'\ncandidate = 'there is a cat on the table'"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Calculate the Pointwise Mutual Information (PMI).\n\n    Args:\n    - joint_counts: The observed counts of the co-occurrence of events X and Y.\n    - total_counts_x: The total observed counts of event X.\n    - total_counts_y: The total observed counts of event Y.\n    - total_samples: The total number of samples.\n\n    Returns:\n    A float representing the PMI rounded to 3 decimal places.\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x == 0 or p_y == 0:\n        return 0.0\n    pmi_value = np.log2(p_xy / (p_x * p_y))\n    return round(pmi_value, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers.\n    \n    Args:\n    x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [float(0.5)] * len(x)\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Implements a simple residual block with a shortcut connection.\n    \n    :param x: Input data as a 1D numpy array.\n    :param w1: Weights for the first layer of the block as a 2D numpy array.\n    :param w2: Weights for the second layer of the block as a 2D numpy array.\n    :return: The output of the residual block after applying the operations and rounding to 4 decimal places.\n    \"\"\"\n    layer1 = np.maximum(0, x @ w1)\n    output = np.maximum(0, layer1 @ w2 + x)\n    return np.round(output, decimals=4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D numpy array.\n\n    :param x: A numpy array of shape (height, width, channels)\n    :return: A 1D numpy array of shape (channels,) containing the average of each channel\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Applies batch normalization to the input feature maps X with given gamma and beta,\n    and returns the normalized feature maps rounded to 4 decimal places as a list.\n    \n    Parameters:\n    - X: Input 4D numpy array of shape (B, C, H, W) representing a batch of feature maps.\n    - gamma: Numpy array of shape (C,) representing the scale parameter for each channel.\n    - beta: Numpy array of shape (C,) representing the shift parameter for each channel.\n    - epsilon: A small float number added to the variance to avoid division by zero.\n    \n    Returns:\n    - A list representation of the normalized feature maps, rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_output = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_normalized + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    return np.round(X_output, decimals=4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = n * c * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the list of 2D vectors provided using the Gram-Schmidt process.\n    \n    :param vectors: List of 2D vectors represented as lists of two floats.\n    :param tol: Tolerance to determine linear independence.\n    :return: A list of orthonormal vectors (lists of floats) that span the same subspace as input vectors.\n    \"\"\"\n    vec_array = np.array(vectors, dtype=float)\n    ortho_basis = []\n    for i in range(len(vectors)):\n        u = vec_array[i].copy()\n        for v in ortho_basis:\n            proj = np.dot(u, v) / np.dot(v, v) * v\n            u = u - proj\n        if np.linalg.norm(u) > tol:\n            ortho_basis.append(u / np.linalg.norm(u))\n    return [np.round(vec.tolist(), 4) for vec in ortho_basis]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors `a` and `b`.\n    Rounds the result to 4 decimal places and returns it as a list.\n    \n    Parameters:\n    a (list): A list of three numbers representing the first vector.\n    b (list): A list of three numbers representing the second vector.\n\n    Returns:\n    list: The cross product of `a` and `b`, rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    cp = np.cross(a_np, b_np)\n    rounded_cp = np.round(cp, 4)\n    return rounded_cp.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix, must be a square matrix.\n    b (numpy.ndarray): Constant vector.\n    \n    Returns:\n    list: Solution vector x, rounded to the nearest 4th decimal. Returns -1 if no unique solution exists.\n    \"\"\"\n    try:\n        det_A = np.linalg.det(A)\n        if abs(det_A) < 1e-12:\n            return -1\n        B = [np.column_stack((A[:, :i], b, A[:, i + 1:])) for i in range(len(A))]\n        det_B = []\n        for mat in B:\n            det_B.append(np.linalg.det(mat))\n        x = [d / det_A for d in det_B]\n        return np.round(np.array(x), 4).tolist()\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two discrete probability distributions.\n    \n    Parameters:\n    - p: A list of floats representing the first distribution.\n    - q: A list of floats representing the second distribution.\n    \n    Returns:\n    - A float number representing the Bhattacharyya distance, rounded to 4 decimal places.\n      Returns 0.0 if inputs have different lengths or are empty.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    bc = sum((np.sqrt(pi * qi) for (pi, qi) in zip(p, q)))\n    bd = -np.log2(bc)\n    return round(bd, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    a (list[int|float]): First vector.\n    b (list[int|float]): Second vector.\n    \n    Returns:\n    list[int|float]: A new vector representing the sum of a and b if they are of the same length,\n                     otherwise returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [i + j for (i, j) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]) -> list[float]:\n    \"\"\"\n    Compute the policy gradient using the REINFORCE algorithm.\n    \n    :param theta: NumPy array of shape (num_states, num_actions) representing the policy parameters.\n    :param episodes: List of episodes, where each episode is a list of (state, action, reward) tuples.\n    :return: A list of gradients for each element in theta, rounded to the nearest 4th decimal.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    grad_sum = np.zeros_like(theta)\n    for episode in episodes:\n        returns = 0\n        for (state, action, reward) in reversed(episode):\n            returns = reward + 0.99 * returns\n            log_prob = np.log(softmax(theta)[state, :])\n            log_prob_action = log_prob[action]\n            advantage = returns\n            grad_sum[state] += advantage * (np.eye(num_actions)[action] - softmax(theta)[state, :])\n    avg_grad = grad_sum / len(episodes)\n    return np.round(avg_grad.tolist(), 4)"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Compute the computational efficiency of a MoE layer compared to a dense layer.\n    \n    Parameters:\n    - n_experts: Total number of experts in the MoE layer.\n    - k_active: Number of active experts (sparsity).\n    - d_in: Input dimension.\n    - d_out: Output dimension.\n    \n    Returns:\n    A tuple containing the FLOPs for the dense layer, FLOPs for the MoE layer, and the savings percentage, each rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    return (round(flops_dense, 1), round(flops_moe, 1), round(savings_percentage, 1))\nk_active = 2\nd_in = 32\nd_out = 64"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K Gating function for MoE models.\n    \n    :param X: Input data matrix of shape (batch_size, d)\n    :param W_g: Weight matrix for gating of shape (d, num_experts)\n    :param W_noise: Weight matrix for noise of shape (d, num_experts)\n    :param N: Pre-sampled noise matrix of shape (batch_size, num_experts)\n    :param k: Sparsity constraint, number of experts to select\n    :return: List of top-k indices per batch item, shape (batch_size, k)\n    \"\"\"\n    logits = X @ W_g + N @ W_noise\n    maxes = np.max(logits, axis=1, keepdims=True)\n    exps = np.exp(logits - maxes)\n    partition = np.sum(exps, axis=1, keepdims=True)\n    gating_probs = exps / partition\n    topk_indices = []\n    for probs in gating_probs:\n        ordered_indices = np.argsort(probs)[::-1]\n        topk_indices.append(ordered_indices[:k].tolist())\n    return topk_indices"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.special import softmax\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer with softmax gating and top-k routing.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim).\n    - We: Expert weight matrices of shape (input_dim, output_dim, n_experts).\n    - Wg: Gating weight matrix of shape (input_dim, n_experts).\n    - n_experts: Number of experts.\n    - top_k: Number of top experts to select.\n    \n    Returns:\n    - A rounded list of the final MoE output.\n    \"\"\"\n    (batch_size, _) = x.shape\n    g = x @ Wg\n    p = softmax(g, axis=1)\n    top_k_indices = np.argpartition(p, -top_k, axis=1)[:, -top_k:]\n    y = np.zeros((batch_size, We.shape[1]))\n    for i in range(batch_size):\n        selected_experts = top_k_indices[i]\n        for e in selected_experts:\n            contribution = x[i] @ We[:, :, e]\n            y[i] += contribution * p[i, e]\n    return np.round(y, 4).tolist()\nx = np.array([[0.1, 0.2], [0.3, 0.4]])\nWe = np.random.rand(2, 3, 5)\nWg = np.random.rand(2, 5)\ntop_k = 2"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Applies group normalization to a 4D input tensor X with given gamma and beta parameters,\n    dividing the channels into num_groups for normalization.\n    \n    Parameters:\n    - X: Input 4D array of shape (B, C, H, W)\n    - gamma: Array of multiplicative parameters to scale the normalized values.\n    - beta: Array of additive parameters to shift the normalized values.\n    - num_groups: Number of groups for normalizing channels.\n    - epsilon: Small value to avoid division by zero.\n    \n    Returns:\n    - A list representation of the normalized 4D tensor, rounded to 4 decimals.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels needs to be evenly divisible by num_groups.'\n    X_split = np.array_split(X, num_groups, axis=1)\n    X_normalized = []\n    for X_group in X_split:\n        mean = np.mean(X_group, keepdims=True, axis=(1, 2, 3), dtype=np.float64)\n        var = np.var(X_group, keepdims=True, axis=(1, 2, 3), dtype=np.float64)\n        X_group = (X_group - mean) / np.sqrt(var + epsilon)\n        X_normalized.append(X_group)\n    X_normalized = np.concatenate(X_normalized, axis=1)\n    X_normalized = X_normalized * gamma.reshape(1, C, 1, 1) + beta.reshape(1, C, 1, 1)\n    return np.round(X_normalized, decimals=4).tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Implements gradient descent to find the minimum of the function f(x) = x^4 - 3x^3 + 2.\n    \n    Args:\n    start_x: The initial guess for the location of the treasure.\n    learning_rate: The step size for each iteration of gradient descent.\n    tolerance: The threshold for stopping the algorithm when the change is smaller than this.\n    max_iters: Maximum number of iterations to perform before giving up.\n    \n    Returns:\n    The x-coordinate of the minimum point, rounded to 4 decimal places.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        grad_f = 4 * x ** 3 - 9 * x ** 2\n        new_x = x - learning_rate * grad_f\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) activation function, a normalization-free\n    transformation inspired by the Tanh function. It replaces layer normalization\n    in Transformer architectures while preserving squashing behavior and enabling\n    stable training.\n    \n    Parameters:\n        x (np.ndarray): Input array of any shape.\n        alpha (float): Scale factor for the input.\n        gamma (float): Shift factor applied after scaling.\n        beta (float): Final scaling factor applied to the result.\n    \n    Returns:\n        list[float]: List of floats representing the DyT activated output, rounded to 4 decimal places.\n    \"\"\"\n    transformed = np.tanh(alpha * x + gamma) * beta\n    return np.round(transformed, 4).tolist()"}
